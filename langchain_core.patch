diff -ruN .venv/lib/python3.12/site-packages/langchain_core/__init__.py ./custom_langchain_core/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/__init__.py	1970-01-01 09:00:00
@@ -1,25 +0,0 @@
-"""``langchain-core`` defines the base abstractions for the LangChain ecosystem.
-
-The interfaces for core components like chat models, LLMs, vector stores, retrievers,
-and more are defined here. The universal invocation protocol (Runnables) along with
-a syntax for combining components (LangChain Expression Language) are also defined here.
-
-No third-party integrations are defined here. The dependencies are kept purposefully
-very lightweight.
-"""
-
-from importlib import metadata
-
-from langchain_core._api import (
-    surface_langchain_beta_warnings,
-    surface_langchain_deprecation_warnings,
-)
-
-try:
-    __version__ = metadata.version(__package__)
-except metadata.PackageNotFoundError:
-    # Case where package metadata is not available.
-    __version__ = ""
-
-surface_langchain_deprecation_warnings()
-surface_langchain_beta_warnings()
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/agents.cpython-312.pyc and ./custom_langchain_core/__pycache__/agents.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/caches.cpython-312.pyc and ./custom_langchain_core/__pycache__/caches.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/chat_history.cpython-312.pyc and ./custom_langchain_core/__pycache__/chat_history.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/chat_loaders.cpython-312.pyc and ./custom_langchain_core/__pycache__/chat_loaders.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/chat_sessions.cpython-312.pyc and ./custom_langchain_core/__pycache__/chat_sessions.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/env.cpython-312.pyc and ./custom_langchain_core/__pycache__/env.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/exceptions.cpython-312.pyc and ./custom_langchain_core/__pycache__/exceptions.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/globals.cpython-312.pyc and ./custom_langchain_core/__pycache__/globals.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/memory.cpython-312.pyc and ./custom_langchain_core/__pycache__/memory.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/prompt_values.cpython-312.pyc and ./custom_langchain_core/__pycache__/prompt_values.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/rate_limiters.cpython-312.pyc and ./custom_langchain_core/__pycache__/rate_limiters.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/retrievers.cpython-312.pyc and ./custom_langchain_core/__pycache__/retrievers.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/stores.cpython-312.pyc and ./custom_langchain_core/__pycache__/stores.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/structured_query.cpython-312.pyc and ./custom_langchain_core/__pycache__/structured_query.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/__pycache__/sys_info.cpython-312.pyc and ./custom_langchain_core/__pycache__/sys_info.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/_api/__init__.py ./custom_langchain_core/_api/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/_api/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/_api/__init__.py	1970-01-01 09:00:00
@@ -1,39 +0,0 @@
-"""Helper functions for managing the LangChain API.
-
-This module is only relevant for LangChain developers, not for users.
-
-.. warning::
-
-    This module and its submodules are for internal use only.  Do not use them
-    in your own code.  We may change the API at any time with no warning.
-
-"""
-
-from .beta_decorator import (
-    LangChainBetaWarning,
-    beta,
-    suppress_langchain_beta_warning,
-    surface_langchain_beta_warnings,
-)
-from .deprecation import (
-    LangChainDeprecationWarning,
-    deprecated,
-    suppress_langchain_deprecation_warning,
-    surface_langchain_deprecation_warnings,
-    warn_deprecated,
-)
-from .path import as_import_path, get_relative_path
-
-__all__ = [
-    "as_import_path",
-    "beta",
-    "deprecated",
-    "get_relative_path",
-    "LangChainBetaWarning",
-    "LangChainDeprecationWarning",
-    "suppress_langchain_beta_warning",
-    "surface_langchain_beta_warnings",
-    "suppress_langchain_deprecation_warning",
-    "surface_langchain_deprecation_warnings",
-    "warn_deprecated",
-]
Binary files .venv/lib/python3.12/site-packages/langchain_core/_api/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/_api/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/_api/__pycache__/beta_decorator.cpython-312.pyc and ./custom_langchain_core/_api/__pycache__/beta_decorator.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/_api/__pycache__/deprecation.cpython-312.pyc and ./custom_langchain_core/_api/__pycache__/deprecation.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/_api/__pycache__/internal.cpython-312.pyc and ./custom_langchain_core/_api/__pycache__/internal.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/_api/__pycache__/path.cpython-312.pyc and ./custom_langchain_core/_api/__pycache__/path.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/_api/beta_decorator.py ./custom_langchain_core/_api/beta_decorator.py
--- .venv/lib/python3.12/site-packages/langchain_core/_api/beta_decorator.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/_api/beta_decorator.py	1970-01-01 09:00:00
@@ -1,280 +0,0 @@
-"""Helper functions for marking parts of the LangChain API as beta.
-
-This module was loosely adapted from matplotlibs _api/deprecation.py module:
-
-https://github.com/matplotlib/matplotlib/blob/main/lib/matplotlib/_api/deprecation.py
-
-.. warning::
-
-    This module is for internal use only.  Do not use it in your own code.
-    We may change the API at any time with no warning.
-"""
-
-import contextlib
-import functools
-import inspect
-import warnings
-from collections.abc import Generator
-from typing import Any, Callable, TypeVar, Union, cast
-
-from langchain_core._api.internal import is_caller_internal
-
-
-class LangChainBetaWarning(DeprecationWarning):
-    """A class for issuing beta warnings for LangChain users."""
-
-
-# PUBLIC API
-
-
-T = TypeVar("T", bound=Union[Callable[..., Any], type])
-
-
-def beta(
-    *,
-    message: str = "",
-    name: str = "",
-    obj_type: str = "",
-    addendum: str = "",
-) -> Callable[[T], T]:
-    """Decorator to mark a function, a class, or a property as beta.
-
-    When marking a classmethod, a staticmethod, or a property, the
-    ``@beta`` decorator should go *under* ``@classmethod`` and
-    ``@staticmethod`` (i.e., `beta` should directly decorate the
-    underlying callable), but *over* ``@property``.
-
-    When marking a class ``C`` intended to be used as a base class in a
-    multiple inheritance hierarchy, ``C`` *must* define an ``__init__`` method
-    (if ``C`` instead inherited its ``__init__`` from its own base class, then
-    ``@beta`` would mess up ``__init__`` inheritance when installing its
-    own (annotation-emitting) ``C.__init__``).
-
-    Args:
-        message : str, optional
-            Override the default beta message. The %(since)s,
-            %(name)s, %(alternative)s, %(obj_type)s, %(addendum)s,
-            and %(removal)s format specifiers will be replaced by the
-            values of the respective arguments passed to this function.
-        name : str, optional
-            The name of the beta object.
-        obj_type : str, optional
-            The object type being beta.
-        addendum : str, optional
-            Additional text appended directly to the final message.
-
-    Examples:
-
-        .. code-block:: python
-
-            @beta
-            def the_function_to_annotate():
-                pass
-    """
-
-    def beta(
-        obj: T,
-        *,
-        _obj_type: str = obj_type,
-        _name: str = name,
-        _message: str = message,
-        _addendum: str = addendum,
-    ) -> T:
-        """Implementation of the decorator returned by `beta`."""
-
-        def emit_warning() -> None:
-            """Emit the warning."""
-            warn_beta(
-                message=_message,
-                name=_name,
-                obj_type=_obj_type,
-                addendum=_addendum,
-            )
-
-        warned = False
-
-        def warning_emitting_wrapper(*args: Any, **kwargs: Any) -> Any:
-            """Wrapper for the original wrapped callable that emits a warning.
-
-            Args:
-                *args: The positional arguments to the function.
-                **kwargs: The keyword arguments to the function.
-
-            Returns:
-                The return value of the function being wrapped.
-            """
-            nonlocal warned
-            if not warned and not is_caller_internal():
-                warned = True
-                emit_warning()
-            return wrapped(*args, **kwargs)
-
-        async def awarning_emitting_wrapper(*args: Any, **kwargs: Any) -> Any:
-            """Same as warning_emitting_wrapper, but for async functions."""
-            nonlocal warned
-            if not warned and not is_caller_internal():
-                warned = True
-                emit_warning()
-            return await wrapped(*args, **kwargs)
-
-        if isinstance(obj, type):
-            if not _obj_type:
-                _obj_type = "class"
-            wrapped = obj.__init__  # type: ignore
-            _name = _name or obj.__qualname__
-            old_doc = obj.__doc__
-
-            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:
-                """Finalize the annotation of a class."""
-                # Can't set new_doc on some extension objects.
-                with contextlib.suppress(AttributeError):
-                    obj.__doc__ = new_doc
-
-                def warn_if_direct_instance(
-                    self: Any, *args: Any, **kwargs: Any
-                ) -> Any:
-                    """Warn that the class is in beta."""
-                    nonlocal warned
-                    if not warned and type(self) is obj and not is_caller_internal():
-                        warned = True
-                        emit_warning()
-                    return wrapped(self, *args, **kwargs)
-
-                obj.__init__ = functools.wraps(obj.__init__)(  # type: ignore[misc]
-                    warn_if_direct_instance
-                )
-                return cast(T, obj)
-
-        elif isinstance(obj, property):
-            # note(erick): this block doesn't seem to be used?
-            if not _obj_type:
-                _obj_type = "attribute"
-            wrapped = None
-            _name = _name or obj.fget.__qualname__
-            old_doc = obj.__doc__
-
-            class _BetaProperty(property):
-                """A beta property."""
-
-                def __init__(self, fget=None, fset=None, fdel=None, doc=None):
-                    super().__init__(fget, fset, fdel, doc)
-                    self.__orig_fget = fget
-                    self.__orig_fset = fset
-                    self.__orig_fdel = fdel
-
-                def __get__(self, instance, owner=None):
-                    if instance is not None or owner is not None:
-                        emit_warning()
-                    return self.fget(instance)
-
-                def __set__(self, instance, value):
-                    if instance is not None:
-                        emit_warning()
-                    return self.fset(instance, value)
-
-                def __delete__(self, instance):
-                    if instance is not None:
-                        emit_warning()
-                    return self.fdel(instance)
-
-                def __set_name__(self, owner, set_name):
-                    nonlocal _name
-                    if _name == "<lambda>":
-                        _name = set_name
-
-            def finalize(wrapper: Callable[..., Any], new_doc: str) -> Any:
-                """Finalize the property."""
-                return _BetaProperty(
-                    fget=obj.fget, fset=obj.fset, fdel=obj.fdel, doc=new_doc
-                )
-
-        else:
-            _name = _name or obj.__qualname__
-            if not _obj_type:
-                # edge case: when a function is within another function
-                # within a test, this will call it a "method" not a "function"
-                _obj_type = "function" if "." not in _name else "method"
-            wrapped = obj
-            old_doc = wrapped.__doc__
-
-            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:
-                """Wrap the wrapped function using the wrapper and update the docstring.
-
-                Args:
-                    wrapper: The wrapper function.
-                    new_doc: The new docstring.
-
-                Returns:
-                    The wrapped function.
-                """
-                wrapper = functools.wraps(wrapped)(wrapper)
-                wrapper.__doc__ = new_doc
-                return cast(T, wrapper)
-
-        old_doc = inspect.cleandoc(old_doc or "").strip("\n") or ""
-        components = [message, addendum]
-        details = " ".join([component.strip() for component in components if component])
-        new_doc = f".. beta::\n   {details}\n\n{old_doc}\n"
-
-        if inspect.iscoroutinefunction(obj):
-            finalized = finalize(awarning_emitting_wrapper, new_doc)
-        else:
-            finalized = finalize(warning_emitting_wrapper, new_doc)
-        return cast(T, finalized)
-
-    return beta
-
-
-@contextlib.contextmanager
-def suppress_langchain_beta_warning() -> Generator[None, None, None]:
-    """Context manager to suppress LangChainDeprecationWarning."""
-    with warnings.catch_warnings():
-        warnings.simplefilter("ignore", LangChainBetaWarning)
-        yield
-
-
-def warn_beta(
-    *,
-    message: str = "",
-    name: str = "",
-    obj_type: str = "",
-    addendum: str = "",
-) -> None:
-    """Display a standardized beta annotation.
-
-    Arguments:
-        message : str, optional
-            Override the default beta message. The
-            %(name)s, %(obj_type)s, %(addendum)s
-            format specifiers will be replaced by the
-            values of the respective arguments passed to this function.
-        name : str, optional
-            The name of the annotated object.
-        obj_type : str, optional
-            The object type being annotated.
-        addendum : str, optional
-            Additional text appended directly to the final message.
-    """
-    if not message:
-        message = ""
-
-        if obj_type:
-            message += f"The {obj_type} `{name}`"
-        else:
-            message += f"`{name}`"
-
-        message += " is in beta. It is actively being worked on, so the API may change."
-
-        if addendum:
-            message += f" {addendum}"
-
-    warning = LangChainBetaWarning(message)
-    warnings.warn(warning, category=LangChainBetaWarning, stacklevel=4)
-
-
-def surface_langchain_beta_warnings() -> None:
-    """Unmute LangChain beta warnings."""
-    warnings.filterwarnings(
-        "default",
-        category=LangChainBetaWarning,
-    )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py ./custom_langchain_core/_api/deprecation.py
--- .venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/_api/deprecation.py	1970-01-01 09:00:00
@@ -1,552 +0,0 @@
-"""Helper functions for deprecating parts of the LangChain API.
-
-This module was adapted from matplotlibs _api/deprecation.py module:
-
-https://github.com/matplotlib/matplotlib/blob/main/lib/matplotlib/_api/deprecation.py
-
-.. warning::
-
-    This module is for internal use only.  Do not use it in your own code.
-    We may change the API at any time with no warning.
-"""
-
-import contextlib
-import functools
-import inspect
-import warnings
-from collections.abc import Generator
-from typing import (
-    Any,
-    Callable,
-    TypeVar,
-    Union,
-    cast,
-)
-
-from typing_extensions import ParamSpec
-
-from langchain_core._api.internal import is_caller_internal
-
-
-class LangChainDeprecationWarning(DeprecationWarning):
-    """A class for issuing deprecation warnings for LangChain users."""
-
-
-class LangChainPendingDeprecationWarning(PendingDeprecationWarning):
-    """A class for issuing deprecation warnings for LangChain users."""
-
-
-# PUBLIC API
-
-
-# Last Any should be FieldInfoV1 but this leads to circular imports
-T = TypeVar("T", bound=Union[type, Callable[..., Any], Any])
-
-
-def _validate_deprecation_params(
-    pending: bool,
-    removal: str,
-    alternative: str,
-    alternative_import: str,
-) -> None:
-    """Validate the deprecation parameters."""
-    if pending and removal:
-        msg = "A pending deprecation cannot have a scheduled removal"
-        raise ValueError(msg)
-    if alternative and alternative_import:
-        msg = "Cannot specify both alternative and alternative_import"
-        raise ValueError(msg)
-
-    if alternative_import and "." not in alternative_import:
-        msg = (
-            "alternative_import must be a fully qualified module path. Got "
-            f" {alternative_import}"
-        )
-        raise ValueError(msg)
-
-
-def deprecated(
-    since: str,
-    *,
-    message: str = "",
-    name: str = "",
-    alternative: str = "",
-    alternative_import: str = "",
-    pending: bool = False,
-    obj_type: str = "",
-    addendum: str = "",
-    removal: str = "",
-    package: str = "",
-) -> Callable[[T], T]:
-    """Decorator to mark a function, a class, or a property as deprecated.
-
-    When deprecating a classmethod, a staticmethod, or a property, the
-    ``@deprecated`` decorator should go *under* ``@classmethod`` and
-    ``@staticmethod`` (i.e., `deprecated` should directly decorate the
-    underlying callable), but *over* ``@property``.
-
-    When deprecating a class ``C`` intended to be used as a base class in a
-    multiple inheritance hierarchy, ``C`` *must* define an ``__init__`` method
-    (if ``C`` instead inherited its ``__init__`` from its own base class, then
-    ``@deprecated`` would mess up ``__init__`` inheritance when installing its
-    own (deprecation-emitting) ``C.__init__``).
-
-    Parameters are the same as for `warn_deprecated`, except that *obj_type*
-    defaults to 'class' if decorating a class, 'attribute' if decorating a
-    property, and 'function' otherwise.
-
-    Args:
-        since : str
-            The release at which this API became deprecated.
-        message : str, optional
-            Override the default deprecation message. The %(since)s,
-            %(name)s, %(alternative)s, %(obj_type)s, %(addendum)s,
-            and %(removal)s format specifiers will be replaced by the
-            values of the respective arguments passed to this function.
-        name : str, optional
-            The name of the deprecated object.
-        alternative : str, optional
-            An alternative API that the user may use in place of the
-            deprecated API. The deprecation warning will tell the user
-            about this alternative if provided.
-        pending : bool, optional
-            If True, uses a PendingDeprecationWarning instead of a
-            DeprecationWarning. Cannot be used together with removal.
-        obj_type : str, optional
-            The object type being deprecated.
-        addendum : str, optional
-            Additional text appended directly to the final message.
-        removal : str, optional
-            The expected removal version. With the default (an empty
-            string), a removal version is automatically computed from
-            since. Set to other Falsy values to not schedule a removal
-            date. Cannot be used together with pending.
-
-    Examples:
-
-        .. code-block:: python
-
-            @deprecated('1.4.0')
-            def the_function_to_deprecate():
-                pass
-    """
-    _validate_deprecation_params(pending, removal, alternative, alternative_import)
-
-    def deprecate(
-        obj: T,
-        *,
-        _obj_type: str = obj_type,
-        _name: str = name,
-        _message: str = message,
-        _alternative: str = alternative,
-        _alternative_import: str = alternative_import,
-        _pending: bool = pending,
-        _addendum: str = addendum,
-        _package: str = package,
-    ) -> T:
-        """Implementation of the decorator returned by `deprecated`."""
-        from langchain_core.utils.pydantic import FieldInfoV1, FieldInfoV2
-
-        def emit_warning() -> None:
-            """Emit the warning."""
-            warn_deprecated(
-                since,
-                message=_message,
-                name=_name,
-                alternative=_alternative,
-                alternative_import=_alternative_import,
-                pending=_pending,
-                obj_type=_obj_type,
-                addendum=_addendum,
-                removal=removal,
-                package=_package,
-            )
-
-        warned = False
-
-        def warning_emitting_wrapper(*args: Any, **kwargs: Any) -> Any:
-            """Wrapper for the original wrapped callable that emits a warning.
-
-            Args:
-                *args: The positional arguments to the function.
-                **kwargs: The keyword arguments to the function.
-
-            Returns:
-                The return value of the function being wrapped.
-            """
-            nonlocal warned
-            if not warned and not is_caller_internal():
-                warned = True
-                emit_warning()
-            return wrapped(*args, **kwargs)
-
-        async def awarning_emitting_wrapper(*args: Any, **kwargs: Any) -> Any:
-            """Same as warning_emitting_wrapper, but for async functions."""
-            nonlocal warned
-            if not warned and not is_caller_internal():
-                warned = True
-                emit_warning()
-            return await wrapped(*args, **kwargs)
-
-        _package = _package or obj.__module__.split(".")[0].replace("_", "-")
-
-        if isinstance(obj, type):
-            if not _obj_type:
-                _obj_type = "class"
-            wrapped = obj.__init__  # type: ignore
-            _name = _name or obj.__qualname__
-            old_doc = obj.__doc__
-
-            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:
-                """Finalize the deprecation of a class."""
-                # Can't set new_doc on some extension objects.
-                with contextlib.suppress(AttributeError):
-                    obj.__doc__ = new_doc
-
-                def warn_if_direct_instance(
-                    self: Any, *args: Any, **kwargs: Any
-                ) -> Any:
-                    """Warn that the class is in beta."""
-                    nonlocal warned
-                    if not warned and type(self) is obj and not is_caller_internal():
-                        warned = True
-                        emit_warning()
-                    return wrapped(self, *args, **kwargs)
-
-                obj.__init__ = functools.wraps(obj.__init__)(  # type: ignore[misc]
-                    warn_if_direct_instance
-                )
-                return cast(T, obj)
-
-        elif isinstance(obj, FieldInfoV1):
-            wrapped = None
-            if not _obj_type:
-                _obj_type = "attribute"
-            if not _name:
-                msg = f"Field {obj} must have a name to be deprecated."
-                raise ValueError(msg)
-            old_doc = obj.description
-
-            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:
-                return cast(
-                    T,
-                    FieldInfoV1(
-                        default=obj.default,
-                        default_factory=obj.default_factory,
-                        description=new_doc,
-                        alias=obj.alias,
-                        exclude=obj.exclude,
-                    ),
-                )
-
-        elif isinstance(obj, FieldInfoV2):
-            wrapped = None
-            if not _obj_type:
-                _obj_type = "attribute"
-            if not _name:
-                msg = f"Field {obj} must have a name to be deprecated."
-                raise ValueError(msg)
-            old_doc = obj.description
-
-            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:
-                return cast(
-                    T,
-                    FieldInfoV2(
-                        default=obj.default,
-                        default_factory=obj.default_factory,
-                        description=new_doc,
-                        alias=obj.alias,
-                        exclude=obj.exclude,
-                    ),
-                )
-
-        elif isinstance(obj, property):
-            if not _obj_type:
-                _obj_type = "attribute"
-            wrapped = None
-            _name = _name or cast(Union[type, Callable], obj.fget).__qualname__
-            old_doc = obj.__doc__
-
-            class _DeprecatedProperty(property):
-                """A deprecated property."""
-
-                def __init__(self, fget=None, fset=None, fdel=None, doc=None):  # type: ignore[no-untyped-def]
-                    super().__init__(fget, fset, fdel, doc)
-                    self.__orig_fget = fget
-                    self.__orig_fset = fset
-                    self.__orig_fdel = fdel
-
-                def __get__(self, instance, owner=None):  # type: ignore[no-untyped-def]
-                    if instance is not None or owner is not None:
-                        emit_warning()
-                    return self.fget(instance)
-
-                def __set__(self, instance, value):  # type: ignore[no-untyped-def]
-                    if instance is not None:
-                        emit_warning()
-                    return self.fset(instance, value)
-
-                def __delete__(self, instance):  # type: ignore[no-untyped-def]
-                    if instance is not None:
-                        emit_warning()
-                    return self.fdel(instance)
-
-                def __set_name__(self, owner, set_name):  # type: ignore[no-untyped-def]
-                    nonlocal _name
-                    if _name == "<lambda>":
-                        _name = set_name
-
-            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:
-                """Finalize the property."""
-                return cast(
-                    T,
-                    _DeprecatedProperty(
-                        fget=obj.fget, fset=obj.fset, fdel=obj.fdel, doc=new_doc
-                    ),
-                )
-
-        else:
-            _name = _name or cast(Union[type, Callable], obj).__qualname__
-            if not _obj_type:
-                # edge case: when a function is within another function
-                # within a test, this will call it a "method" not a "function"
-                _obj_type = "function" if "." not in _name else "method"
-            wrapped = obj
-            old_doc = wrapped.__doc__
-
-            def finalize(wrapper: Callable[..., Any], new_doc: str) -> T:
-                """Wrap the wrapped function using the wrapper and update the docstring.
-
-                Args:
-                    wrapper: The wrapper function.
-                    new_doc: The new docstring.
-
-                Returns:
-                    The wrapped function.
-                """
-                wrapper = functools.wraps(wrapped)(wrapper)
-                wrapper.__doc__ = new_doc
-                return cast(T, wrapper)
-
-        old_doc = inspect.cleandoc(old_doc or "").strip("\n")
-
-        # old_doc can be None
-        if not old_doc:
-            old_doc = ""
-
-        # Modify the docstring to include a deprecation notice.
-        if (
-            _alternative
-            and _alternative.split(".")[-1].lower() == _alternative.split(".")[-1]
-        ):
-            _alternative = f":meth:`~{_alternative}`"
-        elif _alternative:
-            _alternative = f":class:`~{_alternative}`"
-
-        if (
-            _alternative_import
-            and _alternative_import.split(".")[-1].lower()
-            == _alternative_import.split(".")[-1]
-        ):
-            _alternative_import = f":meth:`~{_alternative_import}`"
-        elif _alternative_import:
-            _alternative_import = f":class:`~{_alternative_import}`"
-
-        components = [
-            _message,
-            f"Use {_alternative} instead." if _alternative else "",
-            f"Use ``{_alternative_import}`` instead." if _alternative_import else "",
-            _addendum,
-        ]
-        details = " ".join([component.strip() for component in components if component])
-        package = _package or (
-            _name.split(".")[0].replace("_", "-") if "." in _name else None
-        )
-        if removal:
-            if removal.startswith("1.") and package and package.startswith("langchain"):
-                removal_str = f"It will not be removed until {package}=={removal}."
-            else:
-                removal_str = f"It will be removed in {package}=={removal}."
-        else:
-            removal_str = ""
-        new_doc = f"""\
-.. deprecated:: {since} {details} {removal_str}
-
-{old_doc}\
-"""
-
-        if inspect.iscoroutinefunction(obj):
-            finalized = finalize(awarning_emitting_wrapper, new_doc)
-        else:
-            finalized = finalize(warning_emitting_wrapper, new_doc)
-        return cast(T, finalized)
-
-    return deprecate
-
-
-@contextlib.contextmanager
-def suppress_langchain_deprecation_warning() -> Generator[None, None, None]:
-    """Context manager to suppress LangChainDeprecationWarning."""
-    with warnings.catch_warnings():
-        warnings.simplefilter("ignore", LangChainDeprecationWarning)
-        warnings.simplefilter("ignore", LangChainPendingDeprecationWarning)
-        yield
-
-
-def warn_deprecated(
-    since: str,
-    *,
-    message: str = "",
-    name: str = "",
-    alternative: str = "",
-    alternative_import: str = "",
-    pending: bool = False,
-    obj_type: str = "",
-    addendum: str = "",
-    removal: str = "",
-    package: str = "",
-) -> None:
-    """Display a standardized deprecation.
-
-    Arguments:
-        since : str
-            The release at which this API became deprecated.
-        message : str, optional
-            Override the default deprecation message. The %(since)s,
-            %(name)s, %(alternative)s, %(obj_type)s, %(addendum)s,
-            and %(removal)s format specifiers will be replaced by the
-            values of the respective arguments passed to this function.
-        name : str, optional
-            The name of the deprecated object.
-        alternative : str, optional
-            An alternative API that the user may use in place of the
-            deprecated API. The deprecation warning will tell the user
-            about this alternative if provided.
-        pending : bool, optional
-            If True, uses a PendingDeprecationWarning instead of a
-            DeprecationWarning. Cannot be used together with removal.
-        obj_type : str, optional
-            The object type being deprecated.
-        addendum : str, optional
-            Additional text appended directly to the final message.
-        removal : str, optional
-            The expected removal version. With the default (an empty
-            string), a removal version is automatically computed from
-            since. Set to other Falsy values to not schedule a removal
-            date. Cannot be used together with pending.
-    """
-    if not pending:
-        if not removal:
-            removal = f"in {removal}" if removal else "within ?? minor releases"
-            msg = (
-                f"Need to determine which default deprecation schedule to use. "
-                f"{removal}"
-            )
-            raise NotImplementedError(msg)
-        else:
-            removal = f"in {removal}"
-
-    if not message:
-        message = ""
-        _package = (
-            package or name.split(".")[0].replace("_", "-")
-            if "." in name
-            else "LangChain"
-        )
-
-        if obj_type:
-            message += f"The {obj_type} `{name}`"
-        else:
-            message += f"`{name}`"
-
-        if pending:
-            message += " will be deprecated in a future version"
-        else:
-            message += f" was deprecated in {_package} {since}"
-
-            if removal:
-                message += f" and will be removed {removal}"
-
-        if alternative_import:
-            alt_package = alternative_import.split(".")[0].replace("_", "-")
-            if alt_package == _package:
-                message += f". Use {alternative_import} instead."
-            else:
-                alt_module, alt_name = alternative_import.rsplit(".", 1)
-                message += (
-                    f". An updated version of the {obj_type} exists in the "
-                    f"{alt_package} package and should be used instead. To use it run "
-                    f"`pip install -U {alt_package}` and import as "
-                    f"`from {alt_module} import {alt_name}`."
-                )
-        elif alternative:
-            message += f". Use {alternative} instead."
-
-        if addendum:
-            message += f" {addendum}"
-
-    warning_cls = (
-        LangChainPendingDeprecationWarning if pending else LangChainDeprecationWarning
-    )
-    warning = warning_cls(message)
-    warnings.warn(warning, category=LangChainDeprecationWarning, stacklevel=4)
-
-
-def surface_langchain_deprecation_warnings() -> None:
-    """Unmute LangChain deprecation warnings."""
-    warnings.filterwarnings(
-        "default",
-        category=LangChainPendingDeprecationWarning,
-    )
-
-    warnings.filterwarnings(
-        "default",
-        category=LangChainDeprecationWarning,
-    )
-
-
-_P = ParamSpec("_P")
-_R = TypeVar("_R")
-
-
-def rename_parameter(
-    *,
-    since: str,
-    removal: str,
-    old: str,
-    new: str,
-) -> Callable[[Callable[_P, _R]], Callable[_P, _R]]:
-    """Decorator indicating that parameter *old* of *func* is renamed to *new*.
-
-    The actual implementation of *func* should use *new*, not *old*.  If *old*
-    is passed to *func*, a DeprecationWarning is emitted, and its value is
-    used, even if *new* is also passed by keyword.
-
-    Example:
-
-        .. code-block:: python
-
-            @_api.rename_parameter("3.1", "bad_name", "good_name")
-            def func(good_name): ...
-    """
-
-    def decorator(f: Callable[_P, _R]) -> Callable[_P, _R]:
-        @functools.wraps(f)
-        def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _R:
-            if new in kwargs and old in kwargs:
-                msg = f"{f.__name__}() got multiple values for argument {new!r}"
-                raise TypeError(msg)
-            if old in kwargs:
-                warn_deprecated(
-                    since,
-                    removal=removal,
-                    message=f"The parameter `{old}` of `{f.__name__}` was "
-                    f"deprecated in {since} and will be removed "
-                    f"in {removal} Use `{new}` instead.",
-                )
-                kwargs[new] = kwargs.pop(old)
-            return f(*args, **kwargs)
-
-        return wrapper
-
-    return decorator
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/_api/internal.py ./custom_langchain_core/_api/internal.py
--- .venv/lib/python3.12/site-packages/langchain_core/_api/internal.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/_api/internal.py	1970-01-01 09:00:00
@@ -1,22 +0,0 @@
-import inspect
-
-
-def is_caller_internal(depth: int = 2) -> bool:
-    """Return whether the caller at `depth` of this function is internal."""
-    try:
-        frame = inspect.currentframe()
-    except AttributeError:
-        return False
-    if frame is None:
-        return False
-    try:
-        for _ in range(depth):
-            frame = frame.f_back
-            if frame is None:
-                return False
-        # Directly access the module name from the frame's global variables
-        module_globals = frame.f_globals
-        caller_module_name = module_globals.get("__name__", "")
-        return caller_module_name.startswith("langchain")
-    finally:
-        del frame
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/_api/path.py ./custom_langchain_core/_api/path.py
--- .venv/lib/python3.12/site-packages/langchain_core/_api/path.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/_api/path.py	1970-01-01 09:00:00
@@ -1,36 +0,0 @@
-import os
-from pathlib import Path
-from typing import Optional, Union
-
-HERE = Path(__file__).parent
-
-# Get directory of langchain package
-PACKAGE_DIR = HERE.parent
-SEPARATOR = os.sep
-
-
-def get_relative_path(
-    file: Union[Path, str], *, relative_to: Path = PACKAGE_DIR
-) -> str:
-    """Get the path of the file as a relative path to the package directory."""
-    if isinstance(file, str):
-        file = Path(file)
-    return str(file.relative_to(relative_to))
-
-
-def as_import_path(
-    file: Union[Path, str],
-    *,
-    suffix: Optional[str] = None,
-    relative_to: Path = PACKAGE_DIR,
-) -> str:
-    """Path of the file as a LangChain import exclude langchain top namespace."""
-    if isinstance(file, str):
-        file = Path(file)
-    path = get_relative_path(file, relative_to=relative_to)
-    if file.is_file():
-        path = path[: -len(file.suffix)]
-    import_path = path.replace(SEPARATOR, ".")
-    if suffix:
-        import_path += "." + suffix
-    return import_path
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/agents.py ./custom_langchain_core/agents.py
--- .venv/lib/python3.12/site-packages/langchain_core/agents.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/agents.py	1970-01-01 09:00:00
@@ -1,227 +0,0 @@
-"""Schema definitions for representing agent actions, observations, and return values.
-
-**ATTENTION** The schema definitions are provided for backwards compatibility.
-
-    New agents should be built using the langgraph library
-    (https://github.com/langchain-ai/langgraph)), which provides a simpler
-    and more flexible way to define agents.
-
-    Please see the migration guide for information on how to migrate existing
-    agents to modern langgraph agents:
-    https://python.langchain.com/docs/how_to/migrate_agent/
-
-Agents use language models to choose a sequence of actions to take.
-
-A basic agent works in the following manner:
-
-1. Given a prompt an agent uses an LLM to request an action to take (e.g., a tool to run).
-2. The agent executes the action (e.g., runs the tool), and receives an observation.
-3. The agent returns the observation to the LLM, which can then be used to generate the next action.
-4. When the agent reaches a stopping condition, it returns a final return value.
-
-The schemas for the agents themselves are defined in langchain.agents.agent.
-"""  # noqa: E501
-
-from __future__ import annotations
-
-import json
-from collections.abc import Sequence
-from typing import Any, Literal, Union
-
-from langchain_core.load.serializable import Serializable
-from langchain_core.messages import (
-    AIMessage,
-    BaseMessage,
-    FunctionMessage,
-    HumanMessage,
-)
-
-
-class AgentAction(Serializable):
-    """Represents a request to execute an action by an agent.
-
-    The action consists of the name of the tool to execute and the input to pass
-    to the tool. The log is used to pass along extra information about the action.
-    """
-
-    tool: str
-    """The name of the Tool to execute."""
-    tool_input: Union[str, dict]
-    """The input to pass in to the Tool."""
-    log: str
-    """Additional information to log about the action.
-    This log can be used in a few ways. First, it can be used to audit
-    what exactly the LLM predicted to lead to this (tool, tool_input).
-    Second, it can be used in future iterations to show the LLMs prior
-    thoughts. This is useful when (tool, tool_input) does not contain
-    full information about the LLM prediction (for example, any `thought`
-    before the tool/tool_input)."""
-    type: Literal["AgentAction"] = "AgentAction"
-
-    # Override init to support instantiation by position for backward compat.
-    def __init__(
-        self, tool: str, tool_input: Union[str, dict], log: str, **kwargs: Any
-    ):
-        super().__init__(tool=tool, tool_input=tool_input, log=log, **kwargs)
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Return whether or not the class is serializable.
-        Default is True.
-        """
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Default is ["langchain", "schema", "agent"].
-        """
-        return ["langchain", "schema", "agent"]
-
-    @property
-    def messages(self) -> Sequence[BaseMessage]:
-        """Return the messages that correspond to this action."""
-        return _convert_agent_action_to_messages(self)
-
-
-class AgentActionMessageLog(AgentAction):
-    """Representation of an action to be executed by an agent.
-
-    This is similar to AgentAction, but includes a message log consisting of
-    chat messages. This is useful when working with ChatModels, and is used
-    to reconstruct conversation history from the agent's perspective.
-    """
-
-    message_log: Sequence[BaseMessage]
-    """Similar to log, this can be used to pass along extra
-    information about what exact messages were predicted by the LLM
-    before parsing out the (tool, tool_input). This is again useful
-    if (tool, tool_input) cannot be used to fully recreate the LLM
-    prediction, and you need that LLM prediction (for future agent iteration).
-    Compared to `log`, this is useful when the underlying LLM is a
-    ChatModel (and therefore returns messages rather than a string)."""
-    # Ignoring type because we're overriding the type from AgentAction.
-    # And this is the correct thing to do in this case.
-    # The type literal is used for serialization purposes.
-    type: Literal["AgentActionMessageLog"] = "AgentActionMessageLog"  # type: ignore
-
-
-class AgentStep(Serializable):
-    """Result of running an AgentAction."""
-
-    action: AgentAction
-    """The AgentAction that was executed."""
-    observation: Any
-    """The result of the AgentAction."""
-
-    @property
-    def messages(self) -> Sequence[BaseMessage]:
-        """Messages that correspond to this observation."""
-        return _convert_agent_observation_to_messages(self.action, self.observation)
-
-
-class AgentFinish(Serializable):
-    """Final return value of an ActionAgent.
-
-    Agents return an AgentFinish when they have reached a stopping condition.
-    """
-
-    return_values: dict
-    """Dictionary of return values."""
-    log: str
-    """Additional information to log about the return value.
-    This is used to pass along the full LLM prediction, not just the parsed out
-    return value. For example, if the full LLM prediction was
-    `Final Answer: 2` you may want to just return `2` as a return value, but pass
-    along the full string as a `log` (for debugging or observability purposes).
-    """
-    type: Literal["AgentFinish"] = "AgentFinish"
-
-    def __init__(self, return_values: dict, log: str, **kwargs: Any):
-        """Override init to support instantiation by position for backward compat."""
-        super().__init__(return_values=return_values, log=log, **kwargs)
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Return whether or not the class is serializable."""
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "agent"]
-
-    @property
-    def messages(self) -> Sequence[BaseMessage]:
-        """Messages that correspond to this observation."""
-        return [AIMessage(content=self.log)]
-
-
-def _convert_agent_action_to_messages(
-    agent_action: AgentAction,
-) -> Sequence[BaseMessage]:
-    """Convert an agent action to a message.
-
-    This code is used to reconstruct the original AI message from the agent action.
-
-    Args:
-        agent_action: Agent action to convert.
-
-    Returns:
-        AIMessage that corresponds to the original tool invocation.
-    """
-    if isinstance(agent_action, AgentActionMessageLog):
-        return agent_action.message_log
-    else:
-        return [AIMessage(content=agent_action.log)]
-
-
-def _convert_agent_observation_to_messages(
-    agent_action: AgentAction, observation: Any
-) -> Sequence[BaseMessage]:
-    """Convert an agent action to a message.
-
-    This code is used to reconstruct the original AI message from the agent action.
-
-    Args:
-        agent_action: Agent action to convert.
-        observation: Observation to convert to a message.
-
-    Returns:
-        AIMessage that corresponds to the original tool invocation.
-    """
-    if isinstance(agent_action, AgentActionMessageLog):
-        return [_create_function_message(agent_action, observation)]
-    else:
-        content = observation
-        if not isinstance(observation, str):
-            try:
-                content = json.dumps(observation, ensure_ascii=False)
-            except Exception:
-                content = str(observation)
-        return [HumanMessage(content=content)]
-
-
-def _create_function_message(
-    agent_action: AgentAction, observation: Any
-) -> FunctionMessage:
-    """Convert agent action and observation into a function message.
-
-    Args:
-        agent_action: the tool invocation request from the agent.
-        observation: the result of the tool invocation.
-
-    Returns:
-        FunctionMessage that corresponds to the original tool invocation.
-    """
-    if not isinstance(observation, str):
-        try:
-            content = json.dumps(observation, ensure_ascii=False)
-        except Exception:
-            content = str(observation)
-    else:
-        content = observation
-    return FunctionMessage(
-        name=agent_action.tool,
-        content=content,
-    )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/beta/__init__.py ./custom_langchain_core/beta/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/beta/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/beta/__init__.py	1970-01-01 09:00:00
@@ -1 +0,0 @@
-"""Some **beta** features that are not yet ready for production."""
Binary files .venv/lib/python3.12/site-packages/langchain_core/beta/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/beta/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/beta/runnables/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/beta/runnables/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/beta/runnables/__pycache__/context.cpython-312.pyc and ./custom_langchain_core/beta/runnables/__pycache__/context.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/beta/runnables/context.py ./custom_langchain_core/beta/runnables/context.py
--- .venv/lib/python3.12/site-packages/langchain_core/beta/runnables/context.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/beta/runnables/context.py	1970-01-01 09:00:00
@@ -1,401 +0,0 @@
-import asyncio
-import threading
-from collections import defaultdict
-from collections.abc import Awaitable, Mapping, Sequence
-from functools import partial
-from itertools import groupby
-from typing import (
-    Any,
-    Callable,
-    Optional,
-    TypeVar,
-    Union,
-)
-
-from pydantic import ConfigDict
-
-from langchain_core._api.beta_decorator import beta
-from langchain_core.runnables.base import (
-    Runnable,
-    RunnableSerializable,
-    coerce_to_runnable,
-)
-from langchain_core.runnables.config import RunnableConfig, ensure_config, patch_config
-from langchain_core.runnables.utils import ConfigurableFieldSpec, Input, Output
-
-T = TypeVar("T")
-Values = dict[Union[asyncio.Event, threading.Event], Any]
-CONTEXT_CONFIG_PREFIX = "__context__/"
-CONTEXT_CONFIG_SUFFIX_GET = "/get"
-CONTEXT_CONFIG_SUFFIX_SET = "/set"
-
-
-async def _asetter(done: asyncio.Event, values: Values, value: T) -> T:
-    values[done] = value
-    done.set()
-    return value
-
-
-async def _agetter(done: asyncio.Event, values: Values) -> Any:
-    await done.wait()
-    return values[done]
-
-
-def _setter(done: threading.Event, values: Values, value: T) -> T:
-    values[done] = value
-    done.set()
-    return value
-
-
-def _getter(done: threading.Event, values: Values) -> Any:
-    done.wait()
-    return values[done]
-
-
-def _key_from_id(id_: str) -> str:
-    wout_prefix = id_.split(CONTEXT_CONFIG_PREFIX, maxsplit=1)[1]
-    if wout_prefix.endswith(CONTEXT_CONFIG_SUFFIX_GET):
-        return wout_prefix[: -len(CONTEXT_CONFIG_SUFFIX_GET)]
-    elif wout_prefix.endswith(CONTEXT_CONFIG_SUFFIX_SET):
-        return wout_prefix[: -len(CONTEXT_CONFIG_SUFFIX_SET)]
-    else:
-        msg = f"Invalid context config id {id_}"
-        raise ValueError(msg)
-
-
-def _config_with_context(
-    config: RunnableConfig,
-    steps: list[Runnable],
-    setter: Callable,
-    getter: Callable,
-    event_cls: Union[type[threading.Event], type[asyncio.Event]],
-) -> RunnableConfig:
-    if any(k.startswith(CONTEXT_CONFIG_PREFIX) for k in config.get("configurable", {})):
-        return config
-
-    context_specs = [
-        (spec, i)
-        for i, step in enumerate(steps)
-        for spec in step.config_specs
-        if spec.id.startswith(CONTEXT_CONFIG_PREFIX)
-    ]
-    grouped_by_key = {
-        key: list(group)
-        for key, group in groupby(
-            sorted(context_specs, key=lambda s: s[0].id),
-            key=lambda s: _key_from_id(s[0].id),
-        )
-    }
-    deps_by_key = {
-        key: {
-            _key_from_id(dep) for spec in group for dep in (spec[0].dependencies or [])
-        }
-        for key, group in grouped_by_key.items()
-    }
-
-    values: Values = {}
-    events: defaultdict[str, Union[asyncio.Event, threading.Event]] = defaultdict(
-        event_cls
-    )
-    context_funcs: dict[str, Callable[[], Any]] = {}
-    for key, group in grouped_by_key.items():
-        getters = [s for s in group if s[0].id.endswith(CONTEXT_CONFIG_SUFFIX_GET)]
-        setters = [s for s in group if s[0].id.endswith(CONTEXT_CONFIG_SUFFIX_SET)]
-
-        for dep in deps_by_key[key]:
-            if key in deps_by_key[dep]:
-                msg = f"Deadlock detected between context keys {key} and {dep}"
-                raise ValueError(msg)
-        if len(setters) != 1:
-            msg = f"Expected exactly one setter for context key {key}"
-            raise ValueError(msg)
-        setter_idx = setters[0][1]
-        if any(getter_idx < setter_idx for _, getter_idx in getters):
-            msg = f"Context setter for key {key} must be defined after all getters."
-            raise ValueError(msg)
-
-        if getters:
-            context_funcs[getters[0][0].id] = partial(getter, events[key], values)
-        context_funcs[setters[0][0].id] = partial(setter, events[key], values)
-
-    return patch_config(config, configurable=context_funcs)
-
-
-def aconfig_with_context(
-    config: RunnableConfig,
-    steps: list[Runnable],
-) -> RunnableConfig:
-    """Asynchronously patch a runnable config with context getters and setters.
-
-    Args:
-        config: The runnable config.
-        steps: The runnable steps.
-
-    Returns:
-        The patched runnable config.
-    """
-    return _config_with_context(config, steps, _asetter, _agetter, asyncio.Event)
-
-
-def config_with_context(
-    config: RunnableConfig,
-    steps: list[Runnable],
-) -> RunnableConfig:
-    """Patch a runnable config with context getters and setters.
-
-    Args:
-        config: The runnable config.
-        steps: The runnable steps.
-
-    Returns:
-        The patched runnable config.
-    """
-    return _config_with_context(config, steps, _setter, _getter, threading.Event)
-
-
-@beta()
-class ContextGet(RunnableSerializable):
-    """Get a context value."""
-
-    prefix: str = ""
-
-    key: Union[str, list[str]]
-
-    def __str__(self) -> str:
-        return f"ContextGet({_print_keys(self.key)})"
-
-    @property
-    def ids(self) -> list[str]:
-        prefix = self.prefix + "/" if self.prefix else ""
-        keys = self.key if isinstance(self.key, list) else [self.key]
-        return [
-            f"{CONTEXT_CONFIG_PREFIX}{prefix}{k}{CONTEXT_CONFIG_SUFFIX_GET}"
-            for k in keys
-        ]
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        return super().config_specs + [
-            ConfigurableFieldSpec(
-                id=id_,
-                annotation=Callable[[], Any],
-            )
-            for id_ in self.ids
-        ]
-
-    def invoke(
-        self, input: Any, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Any:
-        config = ensure_config(config)
-        configurable = config.get("configurable", {})
-        if isinstance(self.key, list):
-            return {key: configurable[id_]() for key, id_ in zip(self.key, self.ids)}
-        else:
-            return configurable[self.ids[0]]()
-
-    async def ainvoke(
-        self, input: Any, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Any:
-        config = ensure_config(config)
-        configurable = config.get("configurable", {})
-        if isinstance(self.key, list):
-            values = await asyncio.gather(*(configurable[id_]() for id_ in self.ids))
-            return dict(zip(self.key, values))
-        else:
-            return await configurable[self.ids[0]]()
-
-
-SetValue = Union[
-    Runnable[Input, Output],
-    Callable[[Input], Output],
-    Callable[[Input], Awaitable[Output]],
-    Any,
-]
-
-
-def _coerce_set_value(value: SetValue) -> Runnable[Input, Output]:
-    if not isinstance(value, Runnable) and not callable(value):
-        return coerce_to_runnable(lambda _: value)
-    return coerce_to_runnable(value)
-
-
-@beta()
-class ContextSet(RunnableSerializable):
-    """Set a context value."""
-
-    prefix: str = ""
-
-    keys: Mapping[str, Optional[Runnable]]
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    def __init__(
-        self,
-        key: Optional[str] = None,
-        value: Optional[SetValue] = None,
-        prefix: str = "",
-        **kwargs: SetValue,
-    ):
-        if key is not None:
-            kwargs[key] = value
-        super().__init__(  # type: ignore[call-arg]
-            keys={
-                k: _coerce_set_value(v) if v is not None else None
-                for k, v in kwargs.items()
-            },
-            prefix=prefix,
-        )
-
-    def __str__(self) -> str:
-        return f"ContextSet({_print_keys(list(self.keys.keys()))})"
-
-    @property
-    def ids(self) -> list[str]:
-        prefix = self.prefix + "/" if self.prefix else ""
-        return [
-            f"{CONTEXT_CONFIG_PREFIX}{prefix}{key}{CONTEXT_CONFIG_SUFFIX_SET}"
-            for key in self.keys
-        ]
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        mapper_config_specs = [
-            s
-            for mapper in self.keys.values()
-            if mapper is not None
-            for s in mapper.config_specs
-        ]
-        for spec in mapper_config_specs:
-            if spec.id.endswith(CONTEXT_CONFIG_SUFFIX_GET):
-                getter_key = spec.id.split("/")[1]
-                if getter_key in self.keys:
-                    msg = f"Circular reference in context setter for key {getter_key}"
-                    raise ValueError(msg)
-        return super().config_specs + [
-            ConfigurableFieldSpec(
-                id=id_,
-                annotation=Callable[[], Any],
-            )
-            for id_ in self.ids
-        ]
-
-    def invoke(
-        self, input: Any, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Any:
-        config = ensure_config(config)
-        configurable = config.get("configurable", {})
-        for id_, mapper in zip(self.ids, self.keys.values()):
-            if mapper is not None:
-                configurable[id_](mapper.invoke(input, config))
-            else:
-                configurable[id_](input)
-        return input
-
-    async def ainvoke(
-        self, input: Any, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Any:
-        config = ensure_config(config)
-        configurable = config.get("configurable", {})
-        for id_, mapper in zip(self.ids, self.keys.values()):
-            if mapper is not None:
-                await configurable[id_](await mapper.ainvoke(input, config))
-            else:
-                await configurable[id_](input)
-        return input
-
-
-class Context:
-    """Context for a runnable.
-
-    The `Context` class provides methods for creating context scopes,
-    getters, and setters within a runnable. It allows for managing
-    and accessing contextual information throughout the execution
-    of a program.
-
-    Example:
-        .. code-block:: python
-
-            from langchain_core.beta.runnables.context import Context
-            from langchain_core.runnables.passthrough import RunnablePassthrough
-            from langchain_core.prompts.prompt import PromptTemplate
-            from langchain_core.output_parsers.string import StrOutputParser
-            from tests.unit_tests.fake.llm import FakeListLLM
-
-            chain = (
-                Context.setter("input")
-                | {
-                    "context": RunnablePassthrough()
-                            | Context.setter("context"),
-                    "question": RunnablePassthrough(),
-                }
-                | PromptTemplate.from_template("{context} {question}")
-                | FakeListLLM(responses=["hello"])
-                | StrOutputParser()
-                | {
-                    "result": RunnablePassthrough(),
-                    "context": Context.getter("context"),
-                    "input": Context.getter("input"),
-                }
-            )
-
-            # Use the chain
-            output = chain.invoke("What's your name?")
-            print(output["result"])  # Output: "hello"
-            print(output["context"])  # Output: "What's your name?"
-            print(output["input"])  # Output: "What's your name?
-    """
-
-    @staticmethod
-    def create_scope(scope: str, /) -> "PrefixContext":
-        """Create a context scope.
-
-        Args:
-            scope: The scope.
-
-        Returns:
-            The context scope.
-        """
-        return PrefixContext(prefix=scope)
-
-    @staticmethod
-    def getter(key: Union[str, list[str]], /) -> ContextGet:
-        return ContextGet(key=key)
-
-    @staticmethod
-    def setter(
-        _key: Optional[str] = None,
-        _value: Optional[SetValue] = None,
-        /,
-        **kwargs: SetValue,
-    ) -> ContextSet:
-        return ContextSet(_key, _value, prefix="", **kwargs)
-
-
-class PrefixContext:
-    """Context for a runnable with a prefix."""
-
-    prefix: str = ""
-
-    def __init__(self, prefix: str = ""):
-        self.prefix = prefix
-
-    def getter(self, key: Union[str, list[str]], /) -> ContextGet:
-        return ContextGet(key=key, prefix=self.prefix)
-
-    def setter(
-        self,
-        _key: Optional[str] = None,
-        _value: Optional[SetValue] = None,
-        /,
-        **kwargs: SetValue,
-    ) -> ContextSet:
-        return ContextSet(_key, _value, prefix=self.prefix, **kwargs)
-
-
-def _print_keys(keys: Union[str, Sequence[str]]) -> str:
-    if isinstance(keys, str):
-        return f"'{keys}'"
-    else:
-        return ", ".join(f"'{k}'" for k in keys)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/caches.py ./custom_langchain_core/caches.py
--- .venv/lib/python3.12/site-packages/langchain_core/caches.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/caches.py	1970-01-01 09:00:00
@@ -1,231 +0,0 @@
-"""
-.. warning::
-  Beta Feature!
-
-**Cache** provides an optional caching layer for LLMs.
-
-Cache is useful for two reasons:
-
-- It can save you money by reducing the number of API calls you make to the LLM
-  provider if you're often requesting the same completion multiple times.
-- It can speed up your application by reducing the number of API calls you make
-  to the LLM provider.
-
-Cache directly competes with Memory. See documentation for Pros and Cons.
-
-**Class hierarchy:**
-
-.. code-block::
-
-    BaseCache --> <name>Cache  # Examples: InMemoryCache, RedisCache, GPTCache
-"""
-
-from __future__ import annotations
-
-from abc import ABC, abstractmethod
-from collections.abc import Sequence
-from typing import Any, Optional
-
-from langchain_core.outputs import Generation
-from langchain_core.runnables import run_in_executor
-
-RETURN_VAL_TYPE = Sequence[Generation]
-
-
-class BaseCache(ABC):
-    """Interface for a caching layer for LLMs and Chat models.
-
-    The cache interface consists of the following methods:
-
-    - lookup: Look up a value based on a prompt and llm_string.
-    - update: Update the cache based on a prompt and llm_string.
-    - clear: Clear the cache.
-
-    In addition, the cache interface provides an async version of each method.
-
-    The default implementation of the async methods is to run the synchronous
-    method in an executor. It's recommended to override the async methods
-    and provide async implementations to avoid unnecessary overhead.
-    """
-
-    @abstractmethod
-    def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
-        """Look up based on prompt and llm_string.
-
-        A cache implementation is expected to generate a key from the 2-tuple
-        of prompt and llm_string (e.g., by concatenating them with a delimiter).
-
-        Args:
-            prompt: a string representation of the prompt.
-                In the case of a Chat model, the prompt is a non-trivial
-                serialization of the prompt into the language model.
-            llm_string: A string representation of the LLM configuration.
-                This is used to capture the invocation parameters of the LLM
-                (e.g., model name, temperature, stop tokens, max tokens, etc.).
-                These invocation parameters are serialized into a string
-                representation.
-
-        Returns:
-            On a cache miss, return None. On a cache hit, return the cached value.
-            The cached value is a list of Generations (or subclasses).
-        """
-
-    @abstractmethod
-    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:
-        """Update cache based on prompt and llm_string.
-
-        The prompt and llm_string are used to generate a key for the cache.
-        The key should match that of the lookup method.
-
-        Args:
-            prompt: a string representation of the prompt.
-                In the case of a Chat model, the prompt is a non-trivial
-                serialization of the prompt into the language model.
-            llm_string: A string representation of the LLM configuration.
-                This is used to capture the invocation parameters of the LLM
-                (e.g., model name, temperature, stop tokens, max tokens, etc.).
-                These invocation parameters are serialized into a string
-                representation.
-            return_val: The value to be cached. The value is a list of Generations
-                (or subclasses).
-        """
-
-    @abstractmethod
-    def clear(self, **kwargs: Any) -> None:
-        """Clear cache that can take additional keyword arguments."""
-
-    async def alookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
-        """Async look up based on prompt and llm_string.
-
-        A cache implementation is expected to generate a key from the 2-tuple
-        of prompt and llm_string (e.g., by concatenating them with a delimiter).
-
-        Args:
-            prompt: a string representation of the prompt.
-                In the case of a Chat model, the prompt is a non-trivial
-                serialization of the prompt into the language model.
-            llm_string: A string representation of the LLM configuration.
-                This is used to capture the invocation parameters of the LLM
-                (e.g., model name, temperature, stop tokens, max tokens, etc.).
-                These invocation parameters are serialized into a string
-                representation.
-
-        Returns:
-            On a cache miss, return None. On a cache hit, return the cached value.
-            The cached value is a list of Generations (or subclasses).
-        """
-        return await run_in_executor(None, self.lookup, prompt, llm_string)
-
-    async def aupdate(
-        self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE
-    ) -> None:
-        """Async update cache based on prompt and llm_string.
-
-        The prompt and llm_string are used to generate a key for the cache.
-        The key should match that of the look up method.
-
-        Args:
-            prompt: a string representation of the prompt.
-                In the case of a Chat model, the prompt is a non-trivial
-                serialization of the prompt into the language model.
-            llm_string: A string representation of the LLM configuration.
-                This is used to capture the invocation parameters of the LLM
-                (e.g., model name, temperature, stop tokens, max tokens, etc.).
-                These invocation parameters are serialized into a string
-                representation.
-            return_val: The value to be cached. The value is a list of Generations
-                (or subclasses).
-        """
-        return await run_in_executor(None, self.update, prompt, llm_string, return_val)
-
-    async def aclear(self, **kwargs: Any) -> None:
-        """Async clear cache that can take additional keyword arguments."""
-        return await run_in_executor(None, self.clear, **kwargs)
-
-
-class InMemoryCache(BaseCache):
-    """Cache that stores things in memory."""
-
-    def __init__(self, *, maxsize: Optional[int] = None) -> None:
-        """Initialize with empty cache.
-
-        Args:
-            maxsize: The maximum number of items to store in the cache.
-                If None, the cache has no maximum size.
-                If the cache exceeds the maximum size, the oldest items are removed.
-                Default is None.
-
-        Raises:
-            ValueError: If maxsize is less than or equal to 0.
-        """
-        self._cache: dict[tuple[str, str], RETURN_VAL_TYPE] = {}
-        if maxsize is not None and maxsize <= 0:
-            msg = "maxsize must be greater than 0"
-            raise ValueError(msg)
-        self._maxsize = maxsize
-
-    def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
-        """Look up based on prompt and llm_string.
-
-        Args:
-            prompt: a string representation of the prompt.
-                In the case of a Chat model, the prompt is a non-trivial
-                serialization of the prompt into the language model.
-            llm_string: A string representation of the LLM configuration.
-
-        Returns:
-            On a cache miss, return None. On a cache hit, return the cached value.
-        """
-        return self._cache.get((prompt, llm_string), None)
-
-    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:
-        """Update cache based on prompt and llm_string.
-
-        Args:
-            prompt: a string representation of the prompt.
-                In the case of a Chat model, the prompt is a non-trivial
-                serialization of the prompt into the language model.
-            llm_string: A string representation of the LLM configuration.
-            return_val: The value to be cached. The value is a list of Generations
-                (or subclasses).
-        """
-        if self._maxsize is not None and len(self._cache) == self._maxsize:
-            del self._cache[next(iter(self._cache))]
-        self._cache[(prompt, llm_string)] = return_val
-
-    def clear(self, **kwargs: Any) -> None:
-        """Clear cache."""
-        self._cache = {}
-
-    async def alookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
-        """Async look up based on prompt and llm_string.
-
-        Args:
-            prompt: a string representation of the prompt.
-                In the case of a Chat model, the prompt is a non-trivial
-                serialization of the prompt into the language model.
-            llm_string: A string representation of the LLM configuration.
-
-        Returns:
-            On a cache miss, return None. On a cache hit, return the cached value.
-        """
-        return self.lookup(prompt, llm_string)
-
-    async def aupdate(
-        self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE
-    ) -> None:
-        """Async update cache based on prompt and llm_string.
-
-        Args:
-            prompt: a string representation of the prompt.
-                In the case of a Chat model, the prompt is a non-trivial
-                serialization of the prompt into the language model.
-            llm_string: A string representation of the LLM configuration.
-            return_val: The value to be cached. The value is a list of Generations
-                (or subclasses).
-        """
-        self.update(prompt, llm_string, return_val)
-
-    async def aclear(self, **kwargs: Any) -> None:
-        """Async clear cache."""
-        self.clear()
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/callbacks/__init__.py ./custom_langchain_core/callbacks/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/callbacks/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/callbacks/__init__.py	1970-01-01 09:00:00
@@ -1,80 +0,0 @@
-"""**Callback handlers** allow listening to events in LangChain.
-
-**Class hierarchy:**
-
-.. code-block::
-
-    BaseCallbackHandler --> <name>CallbackHandler  # Example: AimCallbackHandler
-"""
-
-from langchain_core.callbacks.base import (
-    AsyncCallbackHandler,
-    BaseCallbackHandler,
-    BaseCallbackManager,
-    CallbackManagerMixin,
-    Callbacks,
-    ChainManagerMixin,
-    LLMManagerMixin,
-    RetrieverManagerMixin,
-    RunManagerMixin,
-    ToolManagerMixin,
-)
-from langchain_core.callbacks.file import FileCallbackHandler
-from langchain_core.callbacks.manager import (
-    AsyncCallbackManager,
-    AsyncCallbackManagerForChainGroup,
-    AsyncCallbackManagerForChainRun,
-    AsyncCallbackManagerForLLMRun,
-    AsyncCallbackManagerForRetrieverRun,
-    AsyncCallbackManagerForToolRun,
-    AsyncParentRunManager,
-    AsyncRunManager,
-    BaseRunManager,
-    CallbackManager,
-    CallbackManagerForChainGroup,
-    CallbackManagerForChainRun,
-    CallbackManagerForLLMRun,
-    CallbackManagerForRetrieverRun,
-    CallbackManagerForToolRun,
-    ParentRunManager,
-    RunManager,
-    adispatch_custom_event,
-    dispatch_custom_event,
-)
-from langchain_core.callbacks.stdout import StdOutCallbackHandler
-from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
-
-__all__ = [
-    "dispatch_custom_event",
-    "adispatch_custom_event",
-    "RetrieverManagerMixin",
-    "LLMManagerMixin",
-    "ChainManagerMixin",
-    "ToolManagerMixin",
-    "Callbacks",
-    "CallbackManagerMixin",
-    "RunManagerMixin",
-    "BaseCallbackHandler",
-    "AsyncCallbackHandler",
-    "BaseCallbackManager",
-    "BaseRunManager",
-    "RunManager",
-    "ParentRunManager",
-    "AsyncRunManager",
-    "AsyncParentRunManager",
-    "CallbackManagerForLLMRun",
-    "AsyncCallbackManagerForLLMRun",
-    "CallbackManagerForChainRun",
-    "AsyncCallbackManagerForChainRun",
-    "CallbackManagerForToolRun",
-    "AsyncCallbackManagerForToolRun",
-    "CallbackManagerForRetrieverRun",
-    "AsyncCallbackManagerForRetrieverRun",
-    "CallbackManager",
-    "CallbackManagerForChainGroup",
-    "AsyncCallbackManager",
-    "AsyncCallbackManagerForChainGroup",
-    "StdOutCallbackHandler",
-    "StreamingStdOutCallbackHandler",
-    "FileCallbackHandler",
-]
Binary files .venv/lib/python3.12/site-packages/langchain_core/callbacks/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/callbacks/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/callbacks/__pycache__/base.cpython-312.pyc and ./custom_langchain_core/callbacks/__pycache__/base.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/callbacks/__pycache__/file.cpython-312.pyc and ./custom_langchain_core/callbacks/__pycache__/file.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/callbacks/__pycache__/manager.cpython-312.pyc and ./custom_langchain_core/callbacks/__pycache__/manager.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/callbacks/__pycache__/stdout.cpython-312.pyc and ./custom_langchain_core/callbacks/__pycache__/stdout.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/callbacks/__pycache__/streaming_stdout.cpython-312.pyc and ./custom_langchain_core/callbacks/__pycache__/streaming_stdout.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/callbacks/base.py ./custom_langchain_core/callbacks/base.py
--- .venv/lib/python3.12/site-packages/langchain_core/callbacks/base.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/callbacks/base.py	1970-01-01 09:00:00
@@ -1,1079 +0,0 @@
-"""Base callback handler for LangChain."""
-
-from __future__ import annotations
-
-import logging
-from collections.abc import Sequence
-from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union
-from uuid import UUID
-
-from tenacity import RetryCallState
-
-if TYPE_CHECKING:
-    from langchain_core.agents import AgentAction, AgentFinish
-    from langchain_core.documents import Document
-    from langchain_core.messages import BaseMessage
-    from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult
-
-_LOGGER = logging.getLogger(__name__)
-
-
-class RetrieverManagerMixin:
-    """Mixin for Retriever callbacks."""
-
-    def on_retriever_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when Retriever errors.
-
-        Args:
-            error (BaseException): The error that occurred.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_retriever_end(
-        self,
-        documents: Sequence[Document],
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when Retriever ends running.
-
-        Args:
-            documents (Sequence[Document]): The documents retrieved.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-
-class LLMManagerMixin:
-    """Mixin for LLM callbacks."""
-
-    def on_llm_new_token(
-        self,
-        token: str,
-        *,
-        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run on new LLM token. Only available when streaming is enabled.
-
-        Args:
-            token (str): The new token.
-            chunk (GenerationChunk | ChatGenerationChunk): The new generated chunk,
-              containing content and other information.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_llm_end(
-        self,
-        response: LLMResult,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when LLM ends running.
-
-        Args:
-            response (LLMResult): The response which was generated.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_llm_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when LLM errors.
-
-        Args:
-            error (BaseException): The error that occurred.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-
-class ChainManagerMixin:
-    """Mixin for chain callbacks."""
-
-    def on_chain_end(
-        self,
-        outputs: dict[str, Any],
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when chain ends running.
-
-        Args:
-            outputs (Dict[str, Any]): The outputs of the chain.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_chain_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when chain errors.
-
-        Args:
-            error (BaseException): The error that occurred.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_agent_action(
-        self,
-        action: AgentAction,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run on agent action.
-
-        Args:
-            action (AgentAction): The agent action.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_agent_finish(
-        self,
-        finish: AgentFinish,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run on the agent end.
-
-        Args:
-            finish (AgentFinish): The agent finish.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-
-class ToolManagerMixin:
-    """Mixin for tool callbacks."""
-
-    def on_tool_end(
-        self,
-        output: Any,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when the tool ends running.
-
-        Args:
-            output (Any): The output of the tool.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_tool_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when tool errors.
-
-        Args:
-            error (BaseException): The error that occurred.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-
-class CallbackManagerMixin:
-    """Mixin for callback manager."""
-
-    def on_llm_start(
-        self,
-        serialized: dict[str, Any],
-        prompts: list[str],
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when LLM starts running.
-
-        **ATTENTION**: This method is called for non-chat models (regular LLMs). If
-            you're implementing a handler for a chat model,
-            you should use on_chat_model_start instead.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized LLM.
-            prompts (List[str]): The prompts.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            metadata (Optional[Dict[str, Any]]): The metadata.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_chat_model_start(
-        self,
-        serialized: dict[str, Any],
-        messages: list[list[BaseMessage]],
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when a chat model starts running.
-
-        **ATTENTION**: This method is called for chat models. If you're implementing
-            a handler for a non-chat model, you should use on_llm_start instead.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized chat model.
-            messages (List[List[BaseMessage]]): The messages.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            metadata (Optional[Dict[str, Any]]): The metadata.
-            kwargs (Any): Additional keyword arguments.
-        """
-        # NotImplementedError is thrown intentionally
-        # Callback handler will fall back to on_llm_start if this is exception is thrown
-        msg = f"{self.__class__.__name__} does not implement `on_chat_model_start`"
-        raise NotImplementedError(msg)
-
-    def on_retriever_start(
-        self,
-        serialized: dict[str, Any],
-        query: str,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when the Retriever starts running.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized Retriever.
-            query (str): The query.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            metadata (Optional[Dict[str, Any]]): The metadata.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_chain_start(
-        self,
-        serialized: dict[str, Any],
-        inputs: dict[str, Any],
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when a chain starts running.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized chain.
-            inputs (Dict[str, Any]): The inputs.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            metadata (Optional[Dict[str, Any]]): The metadata.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_tool_start(
-        self,
-        serialized: dict[str, Any],
-        input_str: str,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        inputs: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when the tool starts running.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized tool.
-            input_str (str): The input string.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            metadata (Optional[Dict[str, Any]]): The metadata.
-            inputs (Optional[Dict[str, Any]]): The inputs.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-
-class RunManagerMixin:
-    """Mixin for run manager."""
-
-    def on_text(
-        self,
-        text: str,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run on an arbitrary text.
-
-        Args:
-            text (str): The text.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_retry(
-        self,
-        retry_state: RetryCallState,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run on a retry event.
-
-        Args:
-            retry_state (RetryCallState): The retry state.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_custom_event(
-        self,
-        name: str,
-        data: Any,
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Override to define a handler for a custom event.
-
-        Args:
-            name: The name of the custom event.
-            data: The data for the custom event. Format will match
-                  the format specified by the user.
-            run_id: The ID of the run.
-            tags: The tags associated with the custom event
-                (includes inherited tags).
-            metadata: The metadata associated with the custom event
-                (includes inherited metadata).
-
-        .. versionadded:: 0.2.15
-        """
-
-
-class BaseCallbackHandler(
-    LLMManagerMixin,
-    ChainManagerMixin,
-    ToolManagerMixin,
-    RetrieverManagerMixin,
-    CallbackManagerMixin,
-    RunManagerMixin,
-):
-    """Base callback handler for LangChain."""
-
-    raise_error: bool = False
-    """Whether to raise an error if an exception occurs."""
-
-    run_inline: bool = False
-    """Whether to run the callback inline."""
-
-    @property
-    def ignore_llm(self) -> bool:
-        """Whether to ignore LLM callbacks."""
-        return False
-
-    @property
-    def ignore_retry(self) -> bool:
-        """Whether to ignore retry callbacks."""
-        return False
-
-    @property
-    def ignore_chain(self) -> bool:
-        """Whether to ignore chain callbacks."""
-        return False
-
-    @property
-    def ignore_agent(self) -> bool:
-        """Whether to ignore agent callbacks."""
-        return False
-
-    @property
-    def ignore_retriever(self) -> bool:
-        """Whether to ignore retriever callbacks."""
-        return False
-
-    @property
-    def ignore_chat_model(self) -> bool:
-        """Whether to ignore chat model callbacks."""
-        return False
-
-    @property
-    def ignore_custom_event(self) -> bool:
-        """Ignore custom event."""
-        return False
-
-
-class AsyncCallbackHandler(BaseCallbackHandler):
-    """Async callback handler for LangChain."""
-
-    async def on_llm_start(
-        self,
-        serialized: dict[str, Any],
-        prompts: list[str],
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run when LLM starts running.
-
-        **ATTENTION**: This method is called for non-chat models (regular LLMs). If
-            you're implementing a handler for a chat model,
-            you should use on_chat_model_start instead.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized LLM.
-            prompts (List[str]): The prompts.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            metadata (Optional[Dict[str, Any]]): The metadata.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_chat_model_start(
-        self,
-        serialized: dict[str, Any],
-        messages: list[list[BaseMessage]],
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when a chat model starts running.
-
-        **ATTENTION**: This method is called for chat models. If you're implementing
-            a handler for a non-chat model, you should use on_llm_start instead.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized chat model.
-            messages (List[List[BaseMessage]]): The messages.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            metadata (Optional[Dict[str, Any]]): The metadata.
-            kwargs (Any): Additional keyword arguments.
-        """
-        # NotImplementedError is thrown intentionally
-        # Callback handler will fall back to on_llm_start if this is exception is thrown
-        msg = f"{self.__class__.__name__} does not implement `on_chat_model_start`"
-        raise NotImplementedError(msg)
-
-    async def on_llm_new_token(
-        self,
-        token: str,
-        *,
-        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run on new LLM token. Only available when streaming is enabled.
-
-        Args:
-            token (str): The new token.
-            chunk (GenerationChunk | ChatGenerationChunk): The new generated chunk,
-              containing content and other information.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_llm_end(
-        self,
-        response: LLMResult,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run when LLM ends running.
-
-        Args:
-            response (LLMResult): The response which was generated.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_llm_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run when LLM errors.
-
-        Args:
-            error: The error that occurred.
-            run_id: The run ID. This is the ID of the current run.
-            parent_run_id: The parent run ID. This is the ID of the parent run.
-            tags: The tags.
-            kwargs (Any): Additional keyword arguments.
-                - response (LLMResult): The response which was generated before
-                    the error occurred.
-        """
-
-    async def on_chain_start(
-        self,
-        serialized: dict[str, Any],
-        inputs: dict[str, Any],
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run when a chain starts running.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized chain.
-            inputs (Dict[str, Any]): The inputs.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            metadata (Optional[Dict[str, Any]]): The metadata.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_chain_end(
-        self,
-        outputs: dict[str, Any],
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run when a chain ends running.
-
-        Args:
-            outputs (Dict[str, Any]): The outputs of the chain.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_chain_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run when chain errors.
-
-        Args:
-            error (BaseException): The error that occurred.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_tool_start(
-        self,
-        serialized: dict[str, Any],
-        input_str: str,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        inputs: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run when the tool starts running.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized tool.
-            input_str (str): The input string.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            metadata (Optional[Dict[str, Any]]): The metadata.
-            inputs (Optional[Dict[str, Any]]): The inputs.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_tool_end(
-        self,
-        output: Any,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run when the tool ends running.
-
-        Args:
-            output (Any): The output of the tool.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_tool_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run when tool errors.
-
-        Args:
-            error (BaseException): The error that occurred.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_text(
-        self,
-        text: str,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run on an arbitrary text.
-
-        Args:
-            text (str): The text.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_retry(
-        self,
-        retry_state: RetryCallState,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run on a retry event.
-
-        Args:
-            retry_state (RetryCallState): The retry state.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_agent_action(
-        self,
-        action: AgentAction,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run on agent action.
-
-        Args:
-            action (AgentAction): The agent action.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_agent_finish(
-        self,
-        finish: AgentFinish,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run on the agent end.
-
-        Args:
-            finish (AgentFinish): The agent finish.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_retriever_start(
-        self,
-        serialized: dict[str, Any],
-        query: str,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run on the retriever start.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized retriever.
-            query (str): The query.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            metadata (Optional[Dict[str, Any]]): The metadata.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_retriever_end(
-        self,
-        documents: Sequence[Document],
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run on the retriever end.
-
-        Args:
-            documents (Sequence[Document]): The documents retrieved.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_retriever_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run on retriever error.
-
-        Args:
-            error (BaseException): The error that occurred.
-            run_id (UUID): The run ID. This is the ID of the current run.
-            parent_run_id (UUID): The parent run ID. This is the ID of the parent run.
-            tags (Optional[List[str]]): The tags.
-            kwargs (Any): Additional keyword arguments.
-        """
-
-    async def on_custom_event(
-        self,
-        name: str,
-        data: Any,
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Override to define a handler for a custom event.
-
-        Args:
-            name: The name of the custom event.
-            data: The data for the custom event. Format will match
-                  the format specified by the user.
-            run_id: The ID of the run.
-            tags: The tags associated with the custom event
-                (includes inherited tags).
-            metadata: The metadata associated with the custom event
-                (includes inherited metadata).
-
-        .. versionadded:: 0.2.15
-        """
-
-
-T = TypeVar("T", bound="BaseCallbackManager")
-
-
-class BaseCallbackManager(CallbackManagerMixin):
-    """Base callback manager for LangChain."""
-
-    def __init__(
-        self,
-        handlers: list[BaseCallbackHandler],
-        inheritable_handlers: Optional[list[BaseCallbackHandler]] = None,
-        parent_run_id: Optional[UUID] = None,
-        *,
-        tags: Optional[list[str]] = None,
-        inheritable_tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        inheritable_metadata: Optional[dict[str, Any]] = None,
-    ) -> None:
-        """Initialize callback manager.
-
-        Args:
-            handlers (List[BaseCallbackHandler]): The handlers.
-            inheritable_handlers (Optional[List[BaseCallbackHandler]]):
-              The inheritable handlers. Default is None.
-            parent_run_id (Optional[UUID]): The parent run ID. Default is None.
-            tags (Optional[List[str]]): The tags. Default is None.
-            inheritable_tags (Optional[List[str]]): The inheritable tags.
-                Default is None.
-            metadata (Optional[Dict[str, Any]]): The metadata. Default is None.
-        """
-        self.handlers: list[BaseCallbackHandler] = handlers
-        self.inheritable_handlers: list[BaseCallbackHandler] = (
-            inheritable_handlers or []
-        )
-        self.parent_run_id: Optional[UUID] = parent_run_id
-        self.tags = tags or []
-        self.inheritable_tags = inheritable_tags or []
-        self.metadata = metadata or {}
-        self.inheritable_metadata = inheritable_metadata or {}
-
-    def copy(self: T) -> T:
-        """Copy the callback manager."""
-        return self.__class__(
-            handlers=self.handlers.copy(),
-            inheritable_handlers=self.inheritable_handlers.copy(),
-            parent_run_id=self.parent_run_id,
-            tags=self.tags.copy(),
-            inheritable_tags=self.inheritable_tags.copy(),
-            metadata=self.metadata.copy(),
-            inheritable_metadata=self.inheritable_metadata.copy(),
-        )
-
-    def merge(self: T, other: BaseCallbackManager) -> T:
-        """Merge the callback manager with another callback manager.
-
-        May be overwritten in subclasses. Primarily used internally
-        within merge_configs.
-
-        Returns:
-            BaseCallbackManager: The merged callback manager of the same type
-                as the current object.
-
-        Example: Merging two callback managers.
-
-            .. code-block:: python
-
-                from langchain_core.callbacks.manager import CallbackManager, trace_as_chain_group
-                from langchain_core.callbacks.stdout import StdOutCallbackHandler
-
-                manager = CallbackManager(handlers=[StdOutCallbackHandler()], tags=["tag2"])
-                with trace_as_chain_group("My Group Name", tags=["tag1"]) as group_manager:
-                    merged_manager = group_manager.merge(manager)
-                    print(merged_manager.handlers)
-                    # [
-                    #    <langchain_core.callbacks.stdout.StdOutCallbackHandler object at ...>,
-                    #    <langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler object at ...>,
-                    # ]
-
-                    print(merged_manager.tags)
-                    #    ['tag2', 'tag1']
-
-        """  # noqa: E501
-        manager = self.__class__(
-            parent_run_id=self.parent_run_id or other.parent_run_id,
-            handlers=[],
-            inheritable_handlers=[],
-            tags=list(set(self.tags + other.tags)),
-            inheritable_tags=list(set(self.inheritable_tags + other.inheritable_tags)),
-            metadata={
-                **self.metadata,
-                **other.metadata,
-            },
-        )
-
-        handlers = self.handlers + other.handlers
-        inheritable_handlers = self.inheritable_handlers + other.inheritable_handlers
-
-        for handler in handlers:
-            manager.add_handler(handler)
-
-        for handler in inheritable_handlers:
-            manager.add_handler(handler, inherit=True)
-        return manager
-
-    @property
-    def is_async(self) -> bool:
-        """Whether the callback manager is async."""
-        return False
-
-    def add_handler(self, handler: BaseCallbackHandler, inherit: bool = True) -> None:
-        """Add a handler to the callback manager.
-
-        Args:
-            handler (BaseCallbackHandler): The handler to add.
-            inherit (bool): Whether to inherit the handler. Default is True.
-        """
-        if handler not in self.handlers:
-            self.handlers.append(handler)
-        if inherit and handler not in self.inheritable_handlers:
-            self.inheritable_handlers.append(handler)
-
-    def remove_handler(self, handler: BaseCallbackHandler) -> None:
-        """Remove a handler from the callback manager.
-
-        Args:
-            handler (BaseCallbackHandler): The handler to remove.
-        """
-        self.handlers.remove(handler)
-        self.inheritable_handlers.remove(handler)
-
-    def set_handlers(
-        self, handlers: list[BaseCallbackHandler], inherit: bool = True
-    ) -> None:
-        """Set handlers as the only handlers on the callback manager.
-
-        Args:
-            handlers (List[BaseCallbackHandler]): The handlers to set.
-            inherit (bool): Whether to inherit the handlers. Default is True.
-        """
-        self.handlers = []
-        self.inheritable_handlers = []
-        for handler in handlers:
-            self.add_handler(handler, inherit=inherit)
-
-    def set_handler(self, handler: BaseCallbackHandler, inherit: bool = True) -> None:
-        """Set handler as the only handler on the callback manager.
-
-        Args:
-            handler (BaseCallbackHandler): The handler to set.
-            inherit (bool): Whether to inherit the handler. Default is True.
-        """
-        self.set_handlers([handler], inherit=inherit)
-
-    def add_tags(self, tags: list[str], inherit: bool = True) -> None:
-        """Add tags to the callback manager.
-
-        Args:
-            tags (List[str]): The tags to add.
-            inherit (bool): Whether to inherit the tags. Default is True.
-        """
-        for tag in tags:
-            if tag in self.tags:
-                self.remove_tags([tag])
-        self.tags.extend(tags)
-        if inherit:
-            self.inheritable_tags.extend(tags)
-
-    def remove_tags(self, tags: list[str]) -> None:
-        """Remove tags from the callback manager.
-
-        Args:
-            tags (List[str]): The tags to remove.
-        """
-        for tag in tags:
-            self.tags.remove(tag)
-            self.inheritable_tags.remove(tag)
-
-    def add_metadata(self, metadata: dict[str, Any], inherit: bool = True) -> None:
-        """Add metadata to the callback manager.
-
-        Args:
-            metadata (Dict[str, Any]): The metadata to add.
-            inherit (bool): Whether to inherit the metadata. Default is True.
-        """
-        self.metadata.update(metadata)
-        if inherit:
-            self.inheritable_metadata.update(metadata)
-
-    def remove_metadata(self, keys: list[str]) -> None:
-        """Remove metadata from the callback manager.
-
-        Args:
-            keys (List[str]): The keys to remove.
-        """
-        for key in keys:
-            self.metadata.pop(key)
-            self.inheritable_metadata.pop(key)
-
-
-Callbacks = Optional[Union[list[BaseCallbackHandler], BaseCallbackManager]]
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/callbacks/file.py ./custom_langchain_core/callbacks/file.py
--- .venv/lib/python3.12/site-packages/langchain_core/callbacks/file.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/callbacks/file.py	1970-01-01 09:00:00
@@ -1,128 +0,0 @@
-"""Callback Handler that writes to a file."""
-
-from __future__ import annotations
-
-from typing import Any, Optional, TextIO, cast
-
-from langchain_core.agents import AgentAction, AgentFinish
-from langchain_core.callbacks import BaseCallbackHandler
-from langchain_core.utils.input import print_text
-
-
-class FileCallbackHandler(BaseCallbackHandler):
-    """Callback Handler that writes to a file.
-
-    Parameters:
-        filename: The file to write to.
-        mode: The mode to open the file in. Defaults to "a".
-        color: The color to use for the text.
-    """
-
-    def __init__(
-        self, filename: str, mode: str = "a", color: Optional[str] = None
-    ) -> None:
-        """Initialize callback handler.
-
-        Args:
-            filename: The filename to write to.
-            mode: The mode to open the file in. Defaults to "a".
-            color: The color to use for the text. Defaults to None.
-        """
-        self.file = cast(TextIO, open(filename, mode, encoding="utf-8"))  # noqa: SIM115
-        self.color = color
-
-    def __del__(self) -> None:
-        """Destructor to cleanup when done."""
-        self.file.close()
-
-    def on_chain_start(
-        self, serialized: dict[str, Any], inputs: dict[str, Any], **kwargs: Any
-    ) -> None:
-        """Print out that we are entering a chain.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized chain.
-            inputs (Dict[str, Any]): The inputs to the chain.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        class_name = serialized.get("name", serialized.get("id", ["<unknown>"])[-1])
-        print_text(
-            f"\n\n\033[1m> Entering new {class_name} chain...\033[0m",
-            end="\n",
-            file=self.file,
-        )
-
-    def on_chain_end(self, outputs: dict[str, Any], **kwargs: Any) -> None:
-        """Print out that we finished a chain.
-
-        Args:
-            outputs (Dict[str, Any]): The outputs of the chain.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        print_text("\n\033[1m> Finished chain.\033[0m", end="\n", file=self.file)
-
-    def on_agent_action(
-        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any
-    ) -> Any:
-        """Run on agent action.
-
-        Args:
-            action (AgentAction): The agent action.
-            color (Optional[str], optional): The color to use for the text.
-                Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        print_text(action.log, color=color or self.color, file=self.file)
-
-    def on_tool_end(
-        self,
-        output: str,
-        color: Optional[str] = None,
-        observation_prefix: Optional[str] = None,
-        llm_prefix: Optional[str] = None,
-        **kwargs: Any,
-    ) -> None:
-        """If not the final action, print out observation.
-
-        Args:
-           output (str): The output to print.
-           color (Optional[str], optional): The color to use for the text.
-                Defaults to None.
-           observation_prefix (Optional[str], optional): The observation prefix.
-            Defaults to None.
-           llm_prefix (Optional[str], optional): The LLM prefix.
-                Defaults to None.
-           **kwargs (Any): Additional keyword arguments.
-        """
-        if observation_prefix is not None:
-            print_text(f"\n{observation_prefix}", file=self.file)
-        print_text(output, color=color or self.color, file=self.file)
-        if llm_prefix is not None:
-            print_text(f"\n{llm_prefix}", file=self.file)
-
-    def on_text(
-        self, text: str, color: Optional[str] = None, end: str = "", **kwargs: Any
-    ) -> None:
-        """Run when the agent ends.
-
-        Args:
-           text (str): The text to print.
-           color (Optional[str], optional): The color to use for the text.
-            Defaults to None.
-           end (str, optional): The end character. Defaults to "".
-           **kwargs (Any): Additional keyword arguments.
-        """
-        print_text(text, color=color or self.color, end=end, file=self.file)
-
-    def on_agent_finish(
-        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any
-    ) -> None:
-        """Run on the agent end.
-
-        Args:
-            finish (AgentFinish): The agent finish.
-            color (Optional[str], optional): The color to use for the text.
-                Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        print_text(finish.log, color=color or self.color, end="\n", file=self.file)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/callbacks/manager.py ./custom_langchain_core/callbacks/manager.py
--- .venv/lib/python3.12/site-packages/langchain_core/callbacks/manager.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/callbacks/manager.py	1970-01-01 09:00:00
@@ -1,2606 +0,0 @@
-from __future__ import annotations
-
-import asyncio
-import functools
-import logging
-import uuid
-from abc import ABC, abstractmethod
-from collections.abc import AsyncGenerator, Coroutine, Generator, Sequence
-from concurrent.futures import ThreadPoolExecutor
-from contextlib import asynccontextmanager, contextmanager
-from contextvars import copy_context
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    Optional,
-    TypeVar,
-    Union,
-    cast,
-)
-from uuid import UUID
-
-from langsmith.run_helpers import get_tracing_context
-from tenacity import RetryCallState
-
-from langchain_core.callbacks.base import (
-    BaseCallbackHandler,
-    BaseCallbackManager,
-    Callbacks,
-    ChainManagerMixin,
-    LLMManagerMixin,
-    RetrieverManagerMixin,
-    RunManagerMixin,
-    ToolManagerMixin,
-)
-from langchain_core.callbacks.stdout import StdOutCallbackHandler
-from langchain_core.messages import BaseMessage, get_buffer_string
-from langchain_core.tracers.schemas import Run
-from langchain_core.utils.env import env_var_is_set
-
-if TYPE_CHECKING:
-    from langchain_core.agents import AgentAction, AgentFinish
-    from langchain_core.documents import Document
-    from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult
-    from langchain_core.runnables.config import RunnableConfig
-
-logger = logging.getLogger(__name__)
-
-
-def _get_debug() -> bool:
-    from langchain_core.globals import get_debug
-
-    return get_debug()
-
-
-@contextmanager
-def trace_as_chain_group(
-    group_name: str,
-    callback_manager: Optional[CallbackManager] = None,
-    *,
-    inputs: Optional[dict[str, Any]] = None,
-    project_name: Optional[str] = None,
-    example_id: Optional[Union[str, UUID]] = None,
-    run_id: Optional[UUID] = None,
-    tags: Optional[list[str]] = None,
-    metadata: Optional[dict[str, Any]] = None,
-) -> Generator[CallbackManagerForChainGroup, None, None]:
-    """Get a callback manager for a chain group in a context manager.
-    Useful for grouping different calls together as a single run even if
-    they aren't composed in a single chain.
-
-    Args:
-        group_name (str): The name of the chain group.
-        callback_manager (CallbackManager, optional): The callback manager to use.
-            Defaults to None.
-        inputs (Dict[str, Any], optional): The inputs to the chain group.
-            Defaults to None.
-        project_name (str, optional): The name of the project.
-            Defaults to None.
-        example_id (str or UUID, optional): The ID of the example.
-            Defaults to None.
-        run_id (UUID, optional): The ID of the run.
-        tags (List[str], optional): The inheritable tags to apply to all runs.
-            Defaults to None.
-        metadata (Dict[str, Any], optional): The metadata to apply to all runs.
-            Defaults to None.
-
-    Note: must have LANGCHAIN_TRACING_V2 env var set to true to see the trace in LangSmith.
-
-    Returns:
-        CallbackManagerForChainGroup: The callback manager for the chain group.
-
-    Example:
-        .. code-block:: python
-
-            llm_input = "Foo"
-            with trace_as_chain_group("group_name", inputs={"input": llm_input}) as manager:
-                # Use the callback manager for the chain group
-                res = llm.invoke(llm_input, {"callbacks": manager})
-                manager.on_chain_end({"output": res})
-    """  # noqa: E501
-    from langchain_core.tracers.context import _get_trace_callbacks
-
-    cb = _get_trace_callbacks(
-        project_name, example_id, callback_manager=callback_manager
-    )
-    cm = CallbackManager.configure(
-        inheritable_callbacks=cb,
-        inheritable_tags=tags,
-        inheritable_metadata=metadata,
-    )
-
-    run_manager = cm.on_chain_start({"name": group_name}, inputs or {}, run_id=run_id)
-    child_cm = run_manager.get_child()
-    group_cm = CallbackManagerForChainGroup(
-        child_cm.handlers,
-        child_cm.inheritable_handlers,
-        child_cm.parent_run_id,
-        parent_run_manager=run_manager,
-        tags=child_cm.tags,
-        inheritable_tags=child_cm.inheritable_tags,
-        metadata=child_cm.metadata,
-        inheritable_metadata=child_cm.inheritable_metadata,
-    )
-    try:
-        yield group_cm
-    except Exception as e:
-        if not group_cm.ended:
-            run_manager.on_chain_error(e)
-        raise
-    else:
-        if not group_cm.ended:
-            run_manager.on_chain_end({})
-
-
-@asynccontextmanager
-async def atrace_as_chain_group(
-    group_name: str,
-    callback_manager: Optional[AsyncCallbackManager] = None,
-    *,
-    inputs: Optional[dict[str, Any]] = None,
-    project_name: Optional[str] = None,
-    example_id: Optional[Union[str, UUID]] = None,
-    run_id: Optional[UUID] = None,
-    tags: Optional[list[str]] = None,
-    metadata: Optional[dict[str, Any]] = None,
-) -> AsyncGenerator[AsyncCallbackManagerForChainGroup, None]:
-    """Get an async callback manager for a chain group in a context manager.
-    Useful for grouping different async calls together as a single run even if
-    they aren't composed in a single chain.
-
-    Args:
-        group_name (str): The name of the chain group.
-        callback_manager (AsyncCallbackManager, optional): The async callback manager to use,
-            which manages tracing and other callback behavior. Defaults to None.
-        inputs (Dict[str, Any], optional): The inputs to the chain group.
-            Defaults to None.
-        project_name (str, optional): The name of the project.
-            Defaults to None.
-        example_id (str or UUID, optional): The ID of the example.
-            Defaults to None.
-        run_id (UUID, optional): The ID of the run.
-        tags (List[str], optional): The inheritable tags to apply to all runs.
-            Defaults to None.
-        metadata (Dict[str, Any], optional): The metadata to apply to all runs.
-            Defaults to None.
-
-    Returns:
-        AsyncCallbackManager: The async callback manager for the chain group.
-
-    Note: must have LANGCHAIN_TRACING_V2 env var set to true to see the trace in LangSmith.
-
-    Example:
-        .. code-block:: python
-
-            llm_input = "Foo"
-            async with atrace_as_chain_group("group_name", inputs={"input": llm_input}) as manager:
-                # Use the async callback manager for the chain group
-                res = await llm.ainvoke(llm_input, {"callbacks": manager})
-                await manager.on_chain_end({"output": res})
-    """  # noqa: E501
-    from langchain_core.tracers.context import _get_trace_callbacks
-
-    cb = _get_trace_callbacks(
-        project_name, example_id, callback_manager=callback_manager
-    )
-    cm = AsyncCallbackManager.configure(
-        inheritable_callbacks=cb, inheritable_tags=tags, inheritable_metadata=metadata
-    )
-
-    run_manager = await cm.on_chain_start(
-        {"name": group_name}, inputs or {}, run_id=run_id
-    )
-    child_cm = run_manager.get_child()
-    group_cm = AsyncCallbackManagerForChainGroup(
-        child_cm.handlers,
-        child_cm.inheritable_handlers,
-        child_cm.parent_run_id,
-        parent_run_manager=run_manager,
-        tags=child_cm.tags,
-        inheritable_tags=child_cm.inheritable_tags,
-        metadata=child_cm.metadata,
-        inheritable_metadata=child_cm.inheritable_metadata,
-    )
-    try:
-        yield group_cm
-    except Exception as e:
-        if not group_cm.ended:
-            await run_manager.on_chain_error(e)
-        raise
-    else:
-        if not group_cm.ended:
-            await run_manager.on_chain_end({})
-
-
-Func = TypeVar("Func", bound=Callable)
-
-
-def shielded(func: Func) -> Func:
-    """Makes so an awaitable method is always shielded from cancellation.
-
-    Args:
-        func (Callable): The function to shield.
-
-    Returns:
-        Callable: The shielded function
-    """
-
-    @functools.wraps(func)
-    async def wrapped(*args: Any, **kwargs: Any) -> Any:
-        return await asyncio.shield(func(*args, **kwargs))
-
-    return cast(Func, wrapped)
-
-
-def handle_event(
-    handlers: list[BaseCallbackHandler],
-    event_name: str,
-    ignore_condition_name: Optional[str],
-    *args: Any,
-    **kwargs: Any,
-) -> None:
-    """Generic event handler for CallbackManager.
-
-    Note: This function is used by LangServe to handle events.
-
-    Args:
-        handlers: The list of handlers that will handle the event.
-        event_name: The name of the event (e.g., "on_llm_start").
-        ignore_condition_name: Name of the attribute defined on handler
-            that if True will cause the handler to be skipped for the given event.
-        *args: The arguments to pass to the event handler.
-        **kwargs: The keyword arguments to pass to the event handler
-    """
-    coros: list[Coroutine[Any, Any, Any]] = []
-
-    try:
-        message_strings: Optional[list[str]] = None
-        for handler in handlers:
-            try:
-                if ignore_condition_name is None or not getattr(
-                    handler, ignore_condition_name
-                ):
-                    event = getattr(handler, event_name)(*args, **kwargs)
-                    if asyncio.iscoroutine(event):
-                        coros.append(event)
-            except NotImplementedError as e:
-                if event_name == "on_chat_model_start":
-                    if message_strings is None:
-                        message_strings = [get_buffer_string(m) for m in args[1]]
-                    handle_event(
-                        [handler],
-                        "on_llm_start",
-                        "ignore_llm",
-                        args[0],
-                        message_strings,
-                        *args[2:],
-                        **kwargs,
-                    )
-                else:
-                    handler_name = handler.__class__.__name__
-                    logger.warning(
-                        f"NotImplementedError in {handler_name}.{event_name}"
-                        f" callback: {repr(e)}"
-                    )
-            except Exception as e:
-                logger.warning(
-                    f"Error in {handler.__class__.__name__}.{event_name} callback:"
-                    f" {repr(e)}"
-                )
-                if handler.raise_error:
-                    raise
-    finally:
-        if coros:
-            try:
-                # Raises RuntimeError if there is no current event loop.
-                asyncio.get_running_loop()
-                loop_running = True
-            except RuntimeError:
-                loop_running = False
-
-            if loop_running:
-                # If we try to submit this coroutine to the running loop
-                # we end up in a deadlock, as we'd have gotten here from a
-                # running coroutine, which we cannot interrupt to run this one.
-                # The solution is to create a new loop in a new thread.
-                with ThreadPoolExecutor(1) as executor:
-                    executor.submit(
-                        cast(Callable, copy_context().run), _run_coros, coros
-                    ).result()
-            else:
-                _run_coros(coros)
-
-
-def _run_coros(coros: list[Coroutine[Any, Any, Any]]) -> None:
-    if hasattr(asyncio, "Runner"):
-        # Python 3.11+
-        # Run the coroutines in a new event loop, taking care to
-        # - install signal handlers
-        # - run pending tasks scheduled by `coros`
-        # - close asyncgens and executors
-        # - close the loop
-        with asyncio.Runner() as runner:
-            # Run the coroutine, get the result
-            for coro in coros:
-                try:
-                    runner.run(coro)
-                except Exception as e:
-                    logger.warning(f"Error in callback coroutine: {repr(e)}")
-
-            # Run pending tasks scheduled by coros until they are all done
-            while pending := asyncio.all_tasks(runner.get_loop()):
-                runner.run(asyncio.wait(pending))
-    else:
-        # Before Python 3.11 we need to run each coroutine in a new event loop
-        # as the Runner api is not available.
-        for coro in coros:
-            try:
-                asyncio.run(coro)
-            except Exception as e:
-                logger.warning(f"Error in callback coroutine: {repr(e)}")
-
-
-async def _ahandle_event_for_handler(
-    handler: BaseCallbackHandler,
-    event_name: str,
-    ignore_condition_name: Optional[str],
-    *args: Any,
-    **kwargs: Any,
-) -> None:
-    try:
-        if ignore_condition_name is None or not getattr(handler, ignore_condition_name):
-            event = getattr(handler, event_name)
-            if asyncio.iscoroutinefunction(event):
-                await event(*args, **kwargs)
-            else:
-                if handler.run_inline:
-                    event(*args, **kwargs)
-                else:
-                    await asyncio.get_event_loop().run_in_executor(
-                        None,
-                        cast(
-                            Callable,
-                            functools.partial(
-                                copy_context().run, event, *args, **kwargs
-                            ),
-                        ),
-                    )
-    except NotImplementedError as e:
-        if event_name == "on_chat_model_start":
-            message_strings = [get_buffer_string(m) for m in args[1]]
-            await _ahandle_event_for_handler(
-                handler,
-                "on_llm_start",
-                "ignore_llm",
-                args[0],
-                message_strings,
-                *args[2:],
-                **kwargs,
-            )
-        else:
-            logger.warning(
-                f"NotImplementedError in {handler.__class__.__name__}.{event_name}"
-                f" callback: {repr(e)}"
-            )
-    except Exception as e:
-        logger.warning(
-            f"Error in {handler.__class__.__name__}.{event_name} callback: {repr(e)}"
-        )
-        if handler.raise_error:
-            raise
-
-
-async def ahandle_event(
-    handlers: list[BaseCallbackHandler],
-    event_name: str,
-    ignore_condition_name: Optional[str],
-    *args: Any,
-    **kwargs: Any,
-) -> None:
-    """Async generic event handler for AsyncCallbackManager.
-
-    Note: This function is used by LangServe to handle events.
-
-    Args:
-        handlers: The list of handlers that will handle the event.
-        event_name: The name of the event (e.g., "on_llm_start").
-        ignore_condition_name: Name of the attribute defined on handler
-            that if True will cause the handler to be skipped for the given event.
-        *args: The arguments to pass to the event handler.
-        **kwargs: The keyword arguments to pass to the event handler.
-    """
-    for handler in [h for h in handlers if h.run_inline]:
-        await _ahandle_event_for_handler(
-            handler, event_name, ignore_condition_name, *args, **kwargs
-        )
-    await asyncio.gather(
-        *(
-            _ahandle_event_for_handler(
-                handler,
-                event_name,
-                ignore_condition_name,
-                *args,
-                **kwargs,
-            )
-            for handler in handlers
-            if not handler.run_inline
-        )
-    )
-
-
-BRM = TypeVar("BRM", bound="BaseRunManager")
-
-
-class BaseRunManager(RunManagerMixin):
-    """Base class for run manager (a bound callback manager)."""
-
-    def __init__(
-        self,
-        *,
-        run_id: UUID,
-        handlers: list[BaseCallbackHandler],
-        inheritable_handlers: list[BaseCallbackHandler],
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        inheritable_tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        inheritable_metadata: Optional[dict[str, Any]] = None,
-    ) -> None:
-        """Initialize the run manager.
-
-        Args:
-            run_id (UUID): The ID of the run.
-            handlers (List[BaseCallbackHandler]): The list of handlers.
-            inheritable_handlers (List[BaseCallbackHandler]):
-                The list of inheritable handlers.
-            parent_run_id (UUID, optional): The ID of the parent run.
-                Defaults to None.
-            tags (Optional[List[str]]): The list of tags. Defaults to None.
-            inheritable_tags (Optional[List[str]]): The list of inheritable tags.
-                Defaults to None.
-            metadata (Optional[Dict[str, Any]]): The metadata.
-                Defaults to None.
-            inheritable_metadata (Optional[Dict[str, Any]]): The inheritable metadata.
-                Defaults to None.
-        """
-        self.run_id = run_id
-        self.handlers = handlers
-        self.inheritable_handlers = inheritable_handlers
-        self.parent_run_id = parent_run_id
-        self.tags = tags or []
-        self.inheritable_tags = inheritable_tags or []
-        self.metadata = metadata or {}
-        self.inheritable_metadata = inheritable_metadata or {}
-
-    @classmethod
-    def get_noop_manager(cls: type[BRM]) -> BRM:
-        """Return a manager that doesn't perform any operations.
-
-        Returns:
-            BaseRunManager: The noop manager.
-        """
-        return cls(
-            run_id=uuid.uuid4(),
-            handlers=[],
-            inheritable_handlers=[],
-            tags=[],
-            inheritable_tags=[],
-            metadata={},
-            inheritable_metadata={},
-        )
-
-
-class RunManager(BaseRunManager):
-    """Sync Run Manager."""
-
-    def on_text(
-        self,
-        text: str,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when a text is received.
-
-        Args:
-            text (str): The received text.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            Any: The result of the callback.
-        """
-        handle_event(
-            self.handlers,
-            "on_text",
-            None,
-            text,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    def on_retry(
-        self,
-        retry_state: RetryCallState,
-        **kwargs: Any,
-    ) -> None:
-        """Run when a retry is received.
-
-        Args:
-            retry_state (RetryCallState): The retry state.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        handle_event(
-            self.handlers,
-            "on_retry",
-            "ignore_retry",
-            retry_state,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-
-class ParentRunManager(RunManager):
-    """Sync Parent Run Manager."""
-
-    def get_child(self, tag: Optional[str] = None) -> CallbackManager:
-        """Get a child callback manager.
-
-        Args:
-            tag (str, optional): The tag for the child callback manager.
-                Defaults to None.
-
-        Returns:
-            CallbackManager: The child callback manager.
-        """
-        manager = CallbackManager(handlers=[], parent_run_id=self.run_id)
-        manager.set_handlers(self.inheritable_handlers)
-        manager.add_tags(self.inheritable_tags)
-        manager.add_metadata(self.inheritable_metadata)
-        if tag is not None:
-            manager.add_tags([tag], False)
-        return manager
-
-
-class AsyncRunManager(BaseRunManager, ABC):
-    """Async Run Manager."""
-
-    @abstractmethod
-    def get_sync(self) -> RunManager:
-        """Get the equivalent sync RunManager.
-
-        Returns:
-            RunManager: The sync RunManager.
-        """
-
-    async def on_text(
-        self,
-        text: str,
-        **kwargs: Any,
-    ) -> Any:
-        """Run when a text is received.
-
-        Args:
-            text (str): The received text.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            Any: The result of the callback.
-        """
-        await ahandle_event(
-            self.handlers,
-            "on_text",
-            None,
-            text,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    async def on_retry(
-        self,
-        retry_state: RetryCallState,
-        **kwargs: Any,
-    ) -> None:
-        """Async run when a retry is received.
-
-        Args:
-            retry_state (RetryCallState): The retry state.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        await ahandle_event(
-            self.handlers,
-            "on_retry",
-            "ignore_retry",
-            retry_state,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-
-class AsyncParentRunManager(AsyncRunManager):
-    """Async Parent Run Manager."""
-
-    def get_child(self, tag: Optional[str] = None) -> AsyncCallbackManager:
-        """Get a child callback manager.
-
-        Args:
-            tag (str, optional): The tag for the child callback manager.
-                Defaults to None.
-
-        Returns:
-            AsyncCallbackManager: The child callback manager.
-        """
-        manager = AsyncCallbackManager(handlers=[], parent_run_id=self.run_id)
-        manager.set_handlers(self.inheritable_handlers)
-        manager.add_tags(self.inheritable_tags)
-        manager.add_metadata(self.inheritable_metadata)
-        if tag is not None:
-            manager.add_tags([tag], False)
-        return manager
-
-
-class CallbackManagerForLLMRun(RunManager, LLMManagerMixin):
-    """Callback manager for LLM run."""
-
-    def on_llm_new_token(
-        self,
-        token: str,
-        *,
-        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run when LLM generates a new token.
-
-        Args:
-            token (str): The new token.
-            chunk (Optional[Union[GenerationChunk, ChatGenerationChunk]], optional):
-                The chunk. Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        handle_event(
-            self.handlers,
-            "on_llm_new_token",
-            "ignore_llm",
-            token=token,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            chunk=chunk,
-            **kwargs,
-        )
-
-    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
-        """Run when LLM ends running.
-
-        Args:
-            response (LLMResult): The LLM result.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        handle_event(
-            self.handlers,
-            "on_llm_end",
-            "ignore_llm",
-            response,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    def on_llm_error(
-        self,
-        error: BaseException,
-        **kwargs: Any,
-    ) -> None:
-        """Run when LLM errors.
-
-        Args:
-            error (Exception or KeyboardInterrupt): The error.
-            kwargs (Any): Additional keyword arguments.
-                - response (LLMResult): The response which was generated before
-                    the error occurred.
-        """
-        handle_event(
-            self.handlers,
-            "on_llm_error",
-            "ignore_llm",
-            error,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-
-class AsyncCallbackManagerForLLMRun(AsyncRunManager, LLMManagerMixin):
-    """Async callback manager for LLM run."""
-
-    def get_sync(self) -> CallbackManagerForLLMRun:
-        """Get the equivalent sync RunManager.
-
-        Returns:
-            CallbackManagerForLLMRun: The sync RunManager.
-        """
-        return CallbackManagerForLLMRun(
-            run_id=self.run_id,
-            handlers=self.handlers,
-            inheritable_handlers=self.inheritable_handlers,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            inheritable_tags=self.inheritable_tags,
-            metadata=self.metadata,
-            inheritable_metadata=self.inheritable_metadata,
-        )
-
-    @shielded
-    async def on_llm_new_token(
-        self,
-        token: str,
-        *,
-        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run when LLM generates a new token.
-
-        Args:
-            token (str): The new token.
-            chunk (Optional[Union[GenerationChunk, ChatGenerationChunk]], optional):
-                The chunk. Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        await ahandle_event(
-            self.handlers,
-            "on_llm_new_token",
-            "ignore_llm",
-            token,
-            chunk=chunk,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    @shielded
-    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
-        """Run when LLM ends running.
-
-        Args:
-            response (LLMResult): The LLM result.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        await ahandle_event(
-            self.handlers,
-            "on_llm_end",
-            "ignore_llm",
-            response,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    @shielded
-    async def on_llm_error(
-        self,
-        error: BaseException,
-        **kwargs: Any,
-    ) -> None:
-        """Run when LLM errors.
-
-        Args:
-            error (Exception or KeyboardInterrupt): The error.
-            kwargs (Any): Additional keyword arguments.
-                - response (LLMResult): The response which was generated before
-                    the error occurred.
-
-
-
-        """
-        await ahandle_event(
-            self.handlers,
-            "on_llm_error",
-            "ignore_llm",
-            error,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-
-class CallbackManagerForChainRun(ParentRunManager, ChainManagerMixin):
-    """Callback manager for chain run."""
-
-    def on_chain_end(self, outputs: Union[dict[str, Any], Any], **kwargs: Any) -> None:
-        """Run when chain ends running.
-
-        Args:
-            outputs (Union[Dict[str, Any], Any]): The outputs of the chain.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        handle_event(
-            self.handlers,
-            "on_chain_end",
-            "ignore_chain",
-            outputs,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    def on_chain_error(
-        self,
-        error: BaseException,
-        **kwargs: Any,
-    ) -> None:
-        """Run when chain errors.
-
-        Args:
-            error (Exception or KeyboardInterrupt): The error.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        handle_event(
-            self.handlers,
-            "on_chain_error",
-            "ignore_chain",
-            error,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
-        """Run when agent action is received.
-
-        Args:
-            action (AgentAction): The agent action.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            Any: The result of the callback.
-        """
-        handle_event(
-            self.handlers,
-            "on_agent_action",
-            "ignore_agent",
-            action,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:
-        """Run when agent finish is received.
-
-        Args:
-            finish (AgentFinish): The agent finish.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            Any: The result of the callback.
-        """
-        handle_event(
-            self.handlers,
-            "on_agent_finish",
-            "ignore_agent",
-            finish,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-
-class AsyncCallbackManagerForChainRun(AsyncParentRunManager, ChainManagerMixin):
-    """Async callback manager for chain run."""
-
-    def get_sync(self) -> CallbackManagerForChainRun:
-        """Get the equivalent sync RunManager.
-
-        Returns:
-            CallbackManagerForChainRun: The sync RunManager.
-        """
-        return CallbackManagerForChainRun(
-            run_id=self.run_id,
-            handlers=self.handlers,
-            inheritable_handlers=self.inheritable_handlers,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            inheritable_tags=self.inheritable_tags,
-            metadata=self.metadata,
-            inheritable_metadata=self.inheritable_metadata,
-        )
-
-    @shielded
-    async def on_chain_end(
-        self, outputs: Union[dict[str, Any], Any], **kwargs: Any
-    ) -> None:
-        """Run when a chain ends running.
-
-        Args:
-            outputs (Union[Dict[str, Any], Any]): The outputs of the chain.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        await ahandle_event(
-            self.handlers,
-            "on_chain_end",
-            "ignore_chain",
-            outputs,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    @shielded
-    async def on_chain_error(
-        self,
-        error: BaseException,
-        **kwargs: Any,
-    ) -> None:
-        """Run when chain errors.
-
-        Args:
-            error (Exception or KeyboardInterrupt): The error.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        await ahandle_event(
-            self.handlers,
-            "on_chain_error",
-            "ignore_chain",
-            error,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    @shielded
-    async def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
-        """Run when agent action is received.
-
-        Args:
-            action (AgentAction): The agent action.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            Any: The result of the callback.
-        """
-        await ahandle_event(
-            self.handlers,
-            "on_agent_action",
-            "ignore_agent",
-            action,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    @shielded
-    async def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:
-        """Run when agent finish is received.
-
-        Args:
-            finish (AgentFinish): The agent finish.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            Any: The result of the callback.
-        """
-        await ahandle_event(
-            self.handlers,
-            "on_agent_finish",
-            "ignore_agent",
-            finish,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-
-class CallbackManagerForToolRun(ParentRunManager, ToolManagerMixin):
-    """Callback manager for tool run."""
-
-    def on_tool_end(
-        self,
-        output: Any,
-        **kwargs: Any,
-    ) -> None:
-        """Run when the tool ends running.
-
-        Args:
-            output (Any): The output of the tool.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        handle_event(
-            self.handlers,
-            "on_tool_end",
-            "ignore_agent",
-            output,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    def on_tool_error(
-        self,
-        error: BaseException,
-        **kwargs: Any,
-    ) -> None:
-        """Run when tool errors.
-
-        Args:
-            error (Exception or KeyboardInterrupt): The error.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        handle_event(
-            self.handlers,
-            "on_tool_error",
-            "ignore_agent",
-            error,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-
-class AsyncCallbackManagerForToolRun(AsyncParentRunManager, ToolManagerMixin):
-    """Async callback manager for tool run."""
-
-    def get_sync(self) -> CallbackManagerForToolRun:
-        """Get the equivalent sync RunManager.
-
-        Returns:
-            CallbackManagerForToolRun: The sync RunManager.
-        """
-        return CallbackManagerForToolRun(
-            run_id=self.run_id,
-            handlers=self.handlers,
-            inheritable_handlers=self.inheritable_handlers,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            inheritable_tags=self.inheritable_tags,
-            metadata=self.metadata,
-            inheritable_metadata=self.inheritable_metadata,
-        )
-
-    @shielded
-    async def on_tool_end(self, output: Any, **kwargs: Any) -> None:
-        """Async run when the tool ends running.
-
-        Args:
-            output (Any): The output of the tool.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        await ahandle_event(
-            self.handlers,
-            "on_tool_end",
-            "ignore_agent",
-            output,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    @shielded
-    async def on_tool_error(
-        self,
-        error: BaseException,
-        **kwargs: Any,
-    ) -> None:
-        """Run when tool errors.
-
-        Args:
-            error (Exception or KeyboardInterrupt): The error.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        await ahandle_event(
-            self.handlers,
-            "on_tool_error",
-            "ignore_agent",
-            error,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-
-class CallbackManagerForRetrieverRun(ParentRunManager, RetrieverManagerMixin):
-    """Callback manager for retriever run."""
-
-    def on_retriever_end(
-        self,
-        documents: Sequence[Document],
-        **kwargs: Any,
-    ) -> None:
-        """Run when retriever ends running.
-
-        Args:
-            documents (Sequence[Document]): The retrieved documents.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        handle_event(
-            self.handlers,
-            "on_retriever_end",
-            "ignore_retriever",
-            documents,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    def on_retriever_error(
-        self,
-        error: BaseException,
-        **kwargs: Any,
-    ) -> None:
-        """Run when retriever errors.
-
-        Args:
-            error (BaseException): The error.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        handle_event(
-            self.handlers,
-            "on_retriever_error",
-            "ignore_retriever",
-            error,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-
-class AsyncCallbackManagerForRetrieverRun(
-    AsyncParentRunManager,
-    RetrieverManagerMixin,
-):
-    """Async callback manager for retriever run."""
-
-    def get_sync(self) -> CallbackManagerForRetrieverRun:
-        """Get the equivalent sync RunManager.
-
-        Returns:
-            CallbackManagerForRetrieverRun: The sync RunManager.
-        """
-        return CallbackManagerForRetrieverRun(
-            run_id=self.run_id,
-            handlers=self.handlers,
-            inheritable_handlers=self.inheritable_handlers,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            inheritable_tags=self.inheritable_tags,
-            metadata=self.metadata,
-            inheritable_metadata=self.inheritable_metadata,
-        )
-
-    @shielded
-    async def on_retriever_end(
-        self, documents: Sequence[Document], **kwargs: Any
-    ) -> None:
-        """Run when the retriever ends running.
-
-        Args:
-            documents (Sequence[Document]): The retrieved documents.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        await ahandle_event(
-            self.handlers,
-            "on_retriever_end",
-            "ignore_retriever",
-            documents,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-    @shielded
-    async def on_retriever_error(
-        self,
-        error: BaseException,
-        **kwargs: Any,
-    ) -> None:
-        """Run when retriever errors.
-
-        Args:
-            error (BaseException): The error.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        await ahandle_event(
-            self.handlers,
-            "on_retriever_error",
-            "ignore_retriever",
-            error,
-            run_id=self.run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            **kwargs,
-        )
-
-
-class CallbackManager(BaseCallbackManager):
-    """Callback manager for LangChain."""
-
-    def on_llm_start(
-        self,
-        serialized: dict[str, Any],
-        prompts: list[str],
-        run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> list[CallbackManagerForLLMRun]:
-        """Run when LLM starts running.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized LLM.
-            prompts (List[str]): The list of prompts.
-            run_id (UUID, optional): The ID of the run. Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            List[CallbackManagerForLLMRun]: A callback manager for each
-                prompt as an LLM run.
-        """
-        managers = []
-        for i, prompt in enumerate(prompts):
-            # Can't have duplicate runs with the same run ID (if provided)
-            run_id_ = run_id if i == 0 and run_id is not None else uuid.uuid4()
-            handle_event(
-                self.handlers,
-                "on_llm_start",
-                "ignore_llm",
-                serialized,
-                [prompt],
-                run_id=run_id_,
-                parent_run_id=self.parent_run_id,
-                tags=self.tags,
-                metadata=self.metadata,
-                **kwargs,
-            )
-
-            managers.append(
-                CallbackManagerForLLMRun(
-                    run_id=run_id_,
-                    handlers=self.handlers,
-                    inheritable_handlers=self.inheritable_handlers,
-                    parent_run_id=self.parent_run_id,
-                    tags=self.tags,
-                    inheritable_tags=self.inheritable_tags,
-                    metadata=self.metadata,
-                    inheritable_metadata=self.inheritable_metadata,
-                )
-            )
-
-        return managers
-
-    def on_chat_model_start(
-        self,
-        serialized: dict[str, Any],
-        messages: list[list[BaseMessage]],
-        run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> list[CallbackManagerForLLMRun]:
-        """Run when chat model starts running.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized LLM.
-            messages (List[List[BaseMessage]]): The list of messages.
-            run_id (UUID, optional): The ID of the run. Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            List[CallbackManagerForLLMRun]: A callback manager for each
-                list of messages as an LLM run.
-        """
-        managers = []
-        for message_list in messages:
-            if run_id is not None:
-                run_id_ = run_id
-                run_id = None
-            else:
-                run_id_ = uuid.uuid4()
-            handle_event(
-                self.handlers,
-                "on_chat_model_start",
-                "ignore_chat_model",
-                serialized,
-                [message_list],
-                run_id=run_id_,
-                parent_run_id=self.parent_run_id,
-                tags=self.tags,
-                metadata=self.metadata,
-                **kwargs,
-            )
-
-            managers.append(
-                CallbackManagerForLLMRun(
-                    run_id=run_id_,
-                    handlers=self.handlers,
-                    inheritable_handlers=self.inheritable_handlers,
-                    parent_run_id=self.parent_run_id,
-                    tags=self.tags,
-                    inheritable_tags=self.inheritable_tags,
-                    metadata=self.metadata,
-                    inheritable_metadata=self.inheritable_metadata,
-                )
-            )
-
-        return managers
-
-    def on_chain_start(
-        self,
-        serialized: Optional[dict[str, Any]],
-        inputs: Union[dict[str, Any], Any],
-        run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> CallbackManagerForChainRun:
-        """Run when chain starts running.
-
-        Args:
-            serialized (Optional[Dict[str, Any]]): The serialized chain.
-            inputs (Union[Dict[str, Any], Any]): The inputs to the chain.
-            run_id (UUID, optional): The ID of the run. Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            CallbackManagerForChainRun: The callback manager for the chain run.
-        """
-        if run_id is None:
-            run_id = uuid.uuid4()
-        handle_event(
-            self.handlers,
-            "on_chain_start",
-            "ignore_chain",
-            serialized,
-            inputs,
-            run_id=run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            metadata=self.metadata,
-            **kwargs,
-        )
-
-        return CallbackManagerForChainRun(
-            run_id=run_id,
-            handlers=self.handlers,
-            inheritable_handlers=self.inheritable_handlers,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            inheritable_tags=self.inheritable_tags,
-            metadata=self.metadata,
-            inheritable_metadata=self.inheritable_metadata,
-        )
-
-    def on_tool_start(
-        self,
-        serialized: Optional[dict[str, Any]],
-        input_str: str,
-        run_id: Optional[UUID] = None,
-        parent_run_id: Optional[UUID] = None,
-        inputs: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> CallbackManagerForToolRun:
-        """Run when tool starts running.
-
-        Args:
-            serialized: Serialized representation of the tool.
-            input_str: The  input to the tool as a string.
-                Non-string inputs are cast to strings.
-            run_id: ID for the run. Defaults to None.
-            parent_run_id: The ID of the parent run. Defaults to None.
-            inputs: The original input to the tool if provided.
-                Recommended for usage instead of input_str when the original
-                input is needed.
-                If provided, the inputs are expected to be formatted as a dict.
-                The keys will correspond to the named-arguments in the tool.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            CallbackManagerForToolRun: The callback manager for the tool run.
-        """
-        if run_id is None:
-            run_id = uuid.uuid4()
-
-        handle_event(
-            self.handlers,
-            "on_tool_start",
-            "ignore_agent",
-            serialized,
-            input_str,
-            run_id=run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            metadata=self.metadata,
-            inputs=inputs,
-            **kwargs,
-        )
-
-        return CallbackManagerForToolRun(
-            run_id=run_id,
-            handlers=self.handlers,
-            inheritable_handlers=self.inheritable_handlers,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            inheritable_tags=self.inheritable_tags,
-            metadata=self.metadata,
-            inheritable_metadata=self.inheritable_metadata,
-        )
-
-    def on_retriever_start(
-        self,
-        serialized: Optional[dict[str, Any]],
-        query: str,
-        run_id: Optional[UUID] = None,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> CallbackManagerForRetrieverRun:
-        """Run when the retriever starts running.
-
-        Args:
-            serialized (Optional[Dict[str, Any]]): The serialized retriever.
-            query (str): The query.
-            run_id (UUID, optional): The ID of the run. Defaults to None.
-            parent_run_id (UUID, optional): The ID of the parent run. Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        if run_id is None:
-            run_id = uuid.uuid4()
-
-        handle_event(
-            self.handlers,
-            "on_retriever_start",
-            "ignore_retriever",
-            serialized,
-            query,
-            run_id=run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            metadata=self.metadata,
-            **kwargs,
-        )
-
-        return CallbackManagerForRetrieverRun(
-            run_id=run_id,
-            handlers=self.handlers,
-            inheritable_handlers=self.inheritable_handlers,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            inheritable_tags=self.inheritable_tags,
-            metadata=self.metadata,
-            inheritable_metadata=self.inheritable_metadata,
-        )
-
-    def on_custom_event(
-        self,
-        name: str,
-        data: Any,
-        run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Dispatch an adhoc event to the handlers (async version).
-
-        This event should NOT be used in any internal LangChain code. The event
-        is meant specifically for users of the library to dispatch custom
-        events that are tailored to their application.
-
-        Args:
-            name: The name of the adhoc event.
-            data: The data for the adhoc event.
-            run_id: The ID of the run. Defaults to None.
-
-        .. versionadded:: 0.2.14
-        """
-        if kwargs:
-            msg = (
-                "The dispatcher API does not accept additional keyword arguments."
-                "Please do not pass any additional keyword arguments, instead "
-                "include them in the data field."
-            )
-            raise ValueError(msg)
-        if run_id is None:
-            run_id = uuid.uuid4()
-
-        handle_event(
-            self.handlers,
-            "on_custom_event",
-            "ignore_custom_event",
-            name,
-            data,
-            run_id=run_id,
-            tags=self.tags,
-            metadata=self.metadata,
-        )
-
-    @classmethod
-    def configure(
-        cls,
-        inheritable_callbacks: Callbacks = None,
-        local_callbacks: Callbacks = None,
-        verbose: bool = False,
-        inheritable_tags: Optional[list[str]] = None,
-        local_tags: Optional[list[str]] = None,
-        inheritable_metadata: Optional[dict[str, Any]] = None,
-        local_metadata: Optional[dict[str, Any]] = None,
-    ) -> CallbackManager:
-        """Configure the callback manager.
-
-        Args:
-            inheritable_callbacks (Optional[Callbacks], optional): The inheritable
-                callbacks. Defaults to None.
-            local_callbacks (Optional[Callbacks], optional): The local callbacks.
-                Defaults to None.
-            verbose (bool, optional): Whether to enable verbose mode. Defaults to False.
-            inheritable_tags (Optional[List[str]], optional): The inheritable tags.
-                Defaults to None.
-            local_tags (Optional[List[str]], optional): The local tags.
-                Defaults to None.
-            inheritable_metadata (Optional[Dict[str, Any]], optional): The inheritable
-                metadata. Defaults to None.
-            local_metadata (Optional[Dict[str, Any]], optional): The local metadata.
-                Defaults to None.
-
-        Returns:
-            CallbackManager: The configured callback manager.
-        """
-        return _configure(
-            cls,
-            inheritable_callbacks,
-            local_callbacks,
-            verbose,
-            inheritable_tags,
-            local_tags,
-            inheritable_metadata,
-            local_metadata,
-        )
-
-
-class CallbackManagerForChainGroup(CallbackManager):
-    """Callback manager for the chain group."""
-
-    def __init__(
-        self,
-        handlers: list[BaseCallbackHandler],
-        inheritable_handlers: Optional[list[BaseCallbackHandler]] = None,
-        parent_run_id: Optional[UUID] = None,
-        *,
-        parent_run_manager: CallbackManagerForChainRun,
-        **kwargs: Any,
-    ) -> None:
-        """Initialize the callback manager.
-
-        Args:
-            handlers (List[BaseCallbackHandler]): The list of handlers.
-            inheritable_handlers (Optional[List[BaseCallbackHandler]]): The list of
-                inheritable handlers. Defaults to None.
-            parent_run_id (Optional[UUID]): The ID of the parent run. Defaults to None.
-            parent_run_manager (CallbackManagerForChainRun): The parent run manager.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        super().__init__(
-            handlers,
-            inheritable_handlers,
-            parent_run_id,
-            **kwargs,
-        )
-        self.parent_run_manager = parent_run_manager
-        self.ended = False
-
-    def copy(self) -> CallbackManagerForChainGroup:
-        """Copy the callback manager."""
-        return self.__class__(
-            handlers=self.handlers.copy(),
-            inheritable_handlers=self.inheritable_handlers.copy(),
-            parent_run_id=self.parent_run_id,
-            tags=self.tags.copy(),
-            inheritable_tags=self.inheritable_tags.copy(),
-            metadata=self.metadata.copy(),
-            inheritable_metadata=self.inheritable_metadata.copy(),
-            parent_run_manager=self.parent_run_manager,
-        )
-
-    def merge(
-        self: CallbackManagerForChainGroup, other: BaseCallbackManager
-    ) -> CallbackManagerForChainGroup:
-        """Merge the group callback manager with another callback manager.
-
-        Overwrites the merge method in the base class to ensure that the
-        parent run manager is preserved. Keeps the parent_run_manager
-        from the current object.
-
-        Returns:
-            CallbackManagerForChainGroup: A copy of the current object with the
-                handlers, tags, and other attributes merged from the other object.
-
-        Example: Merging two callback managers.
-
-            .. code-block:: python
-
-                from langchain_core.callbacks.manager import CallbackManager, trace_as_chain_group
-                from langchain_core.callbacks.stdout import StdOutCallbackHandler
-
-                manager = CallbackManager(handlers=[StdOutCallbackHandler()], tags=["tag2"])
-                with trace_as_chain_group("My Group Name", tags=["tag1"]) as group_manager:
-                    merged_manager = group_manager.merge(manager)
-                    print(type(merged_manager))
-                    # <class 'langchain_core.callbacks.manager.CallbackManagerForChainGroup'>
-
-                    print(merged_manager.handlers)
-                    # [
-                    #    <langchain_core.callbacks.stdout.LangChainTracer object at ...>,
-                    #    <langchain_core.callbacks.streaming_stdout.StdOutCallbackHandler object at ...>,
-                    # ]
-
-                    print(merged_manager.tags)
-                    #    ['tag2', 'tag1']
-
-        """  # noqa: E501
-        manager = self.__class__(
-            parent_run_id=self.parent_run_id or other.parent_run_id,
-            handlers=[],
-            inheritable_handlers=[],
-            tags=list(set(self.tags + other.tags)),
-            inheritable_tags=list(set(self.inheritable_tags + other.inheritable_tags)),
-            metadata={
-                **self.metadata,
-                **other.metadata,
-            },
-            parent_run_manager=self.parent_run_manager,
-        )
-
-        handlers = self.handlers + other.handlers
-        inheritable_handlers = self.inheritable_handlers + other.inheritable_handlers
-
-        for handler in handlers:
-            manager.add_handler(handler)
-
-        for handler in inheritable_handlers:
-            manager.add_handler(handler, inherit=True)
-        return manager
-
-    def on_chain_end(self, outputs: Union[dict[str, Any], Any], **kwargs: Any) -> None:
-        """Run when traced chain group ends.
-
-        Args:
-            outputs (Union[Dict[str, Any], Any]): The outputs of the chain.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        self.ended = True
-        return self.parent_run_manager.on_chain_end(outputs, **kwargs)
-
-    def on_chain_error(
-        self,
-        error: BaseException,
-        **kwargs: Any,
-    ) -> None:
-        """Run when chain errors.
-
-        Args:
-            error (Exception or KeyboardInterrupt): The error.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        self.ended = True
-        return self.parent_run_manager.on_chain_error(error, **kwargs)
-
-
-class AsyncCallbackManager(BaseCallbackManager):
-    """Async callback manager that handles callbacks from LangChain."""
-
-    @property
-    def is_async(self) -> bool:
-        """Return whether the handler is async."""
-        return True
-
-    async def on_llm_start(
-        self,
-        serialized: dict[str, Any],
-        prompts: list[str],
-        run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> list[AsyncCallbackManagerForLLMRun]:
-        """Run when LLM starts running.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized LLM.
-            prompts (List[str]): The list of prompts.
-            run_id (UUID, optional): The ID of the run. Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            List[AsyncCallbackManagerForLLMRun]: The list of async
-                callback managers, one for each LLM Run corresponding
-                to each prompt.
-        """
-        inline_tasks = []
-        non_inline_tasks = []
-        inline_handlers = [handler for handler in self.handlers if handler.run_inline]
-        non_inline_handlers = [
-            handler for handler in self.handlers if not handler.run_inline
-        ]
-        managers = []
-
-        for prompt in prompts:
-            if run_id is not None:
-                run_id_ = run_id
-                run_id = None
-            else:
-                run_id_ = uuid.uuid4()
-
-            if inline_handlers:
-                inline_tasks.append(
-                    ahandle_event(
-                        inline_handlers,
-                        "on_llm_start",
-                        "ignore_llm",
-                        serialized,
-                        [prompt],
-                        run_id=run_id_,
-                        parent_run_id=self.parent_run_id,
-                        tags=self.tags,
-                        metadata=self.metadata,
-                        **kwargs,
-                    )
-                )
-            else:
-                non_inline_tasks.append(
-                    ahandle_event(
-                        non_inline_handlers,
-                        "on_llm_start",
-                        "ignore_llm",
-                        serialized,
-                        [prompt],
-                        run_id=run_id_,
-                        parent_run_id=self.parent_run_id,
-                        tags=self.tags,
-                        metadata=self.metadata,
-                        **kwargs,
-                    )
-                )
-
-            managers.append(
-                AsyncCallbackManagerForLLMRun(
-                    run_id=run_id_,
-                    handlers=self.handlers,
-                    inheritable_handlers=self.inheritable_handlers,
-                    parent_run_id=self.parent_run_id,
-                    tags=self.tags,
-                    inheritable_tags=self.inheritable_tags,
-                    metadata=self.metadata,
-                    inheritable_metadata=self.inheritable_metadata,
-                )
-            )
-
-        # Run inline tasks sequentially
-        for inline_task in inline_tasks:
-            await inline_task
-
-        # Run non-inline tasks concurrently
-        if non_inline_tasks:
-            await asyncio.gather(*non_inline_tasks)
-
-        return managers
-
-    async def on_chat_model_start(
-        self,
-        serialized: dict[str, Any],
-        messages: list[list[BaseMessage]],
-        run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> list[AsyncCallbackManagerForLLMRun]:
-        """Async run when LLM starts running.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized LLM.
-            messages (List[List[BaseMessage]]): The list of messages.
-            run_id (UUID, optional): The ID of the run. Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            List[AsyncCallbackManagerForLLMRun]: The list of
-                async callback managers, one for each LLM Run
-                corresponding to each inner  message list.
-        """
-        inline_tasks = []
-        non_inline_tasks = []
-        managers = []
-
-        for message_list in messages:
-            if run_id is not None:
-                run_id_ = run_id
-                run_id = None
-            else:
-                run_id_ = uuid.uuid4()
-
-            for handler in self.handlers:
-                task = ahandle_event(
-                    [handler],
-                    "on_chat_model_start",
-                    "ignore_chat_model",
-                    serialized,
-                    [message_list],
-                    run_id=run_id_,
-                    parent_run_id=self.parent_run_id,
-                    tags=self.tags,
-                    metadata=self.metadata,
-                    **kwargs,
-                )
-                if handler.run_inline:
-                    inline_tasks.append(task)
-                else:
-                    non_inline_tasks.append(task)
-
-            managers.append(
-                AsyncCallbackManagerForLLMRun(
-                    run_id=run_id_,
-                    handlers=self.handlers,
-                    inheritable_handlers=self.inheritable_handlers,
-                    parent_run_id=self.parent_run_id,
-                    tags=self.tags,
-                    inheritable_tags=self.inheritable_tags,
-                    metadata=self.metadata,
-                    inheritable_metadata=self.inheritable_metadata,
-                )
-            )
-
-        # Run inline tasks sequentially
-        for task in inline_tasks:
-            await task
-
-        # Run non-inline tasks concurrently
-        if non_inline_tasks:
-            await asyncio.gather(*non_inline_tasks)
-
-        return managers
-
-    async def on_chain_start(
-        self,
-        serialized: Optional[dict[str, Any]],
-        inputs: Union[dict[str, Any], Any],
-        run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> AsyncCallbackManagerForChainRun:
-        """Async run when chain starts running.
-
-        Args:
-            serialized (Optional[Dict[str, Any]]): The serialized chain.
-            inputs (Union[Dict[str, Any], Any]): The inputs to the chain.
-            run_id (UUID, optional): The ID of the run. Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            AsyncCallbackManagerForChainRun: The async callback manager
-                for the chain run.
-        """
-        if run_id is None:
-            run_id = uuid.uuid4()
-
-        await ahandle_event(
-            self.handlers,
-            "on_chain_start",
-            "ignore_chain",
-            serialized,
-            inputs,
-            run_id=run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            metadata=self.metadata,
-            **kwargs,
-        )
-
-        return AsyncCallbackManagerForChainRun(
-            run_id=run_id,
-            handlers=self.handlers,
-            inheritable_handlers=self.inheritable_handlers,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            inheritable_tags=self.inheritable_tags,
-            metadata=self.metadata,
-            inheritable_metadata=self.inheritable_metadata,
-        )
-
-    async def on_tool_start(
-        self,
-        serialized: Optional[dict[str, Any]],
-        input_str: str,
-        run_id: Optional[UUID] = None,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> AsyncCallbackManagerForToolRun:
-        """Run when the tool starts running.
-
-        Args:
-            serialized (Optional[Dict[str, Any]]): The serialized tool.
-            input_str (str): The input to the tool.
-            run_id (UUID, optional): The ID of the run. Defaults to None.
-            parent_run_id (UUID, optional): The ID of the parent run.
-                Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            AsyncCallbackManagerForToolRun: The async callback manager
-                for the tool run.
-        """
-        if run_id is None:
-            run_id = uuid.uuid4()
-
-        await ahandle_event(
-            self.handlers,
-            "on_tool_start",
-            "ignore_agent",
-            serialized,
-            input_str,
-            run_id=run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            metadata=self.metadata,
-            **kwargs,
-        )
-
-        return AsyncCallbackManagerForToolRun(
-            run_id=run_id,
-            handlers=self.handlers,
-            inheritable_handlers=self.inheritable_handlers,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            inheritable_tags=self.inheritable_tags,
-            metadata=self.metadata,
-            inheritable_metadata=self.inheritable_metadata,
-        )
-
-    async def on_custom_event(
-        self,
-        name: str,
-        data: Any,
-        run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Dispatch an adhoc event to the handlers (async version).
-
-        This event should NOT be used in any internal LangChain code. The event
-        is meant specifically for users of the library to dispatch custom
-        events that are tailored to their application.
-
-        Args:
-            name: The name of the adhoc event.
-            data: The data for the adhoc event.
-            run_id: The ID of the run. Defaults to None.
-
-        .. versionadded:: 0.2.14
-        """
-        if run_id is None:
-            run_id = uuid.uuid4()
-
-        if kwargs:
-            msg = (
-                "The dispatcher API does not accept additional keyword arguments."
-                "Please do not pass any additional keyword arguments, instead "
-                "include them in the data field."
-            )
-            raise ValueError(msg)
-        await ahandle_event(
-            self.handlers,
-            "on_custom_event",
-            "ignore_custom_event",
-            name,
-            data,
-            run_id=run_id,
-            tags=self.tags,
-            metadata=self.metadata,
-        )
-
-    async def on_retriever_start(
-        self,
-        serialized: Optional[dict[str, Any]],
-        query: str,
-        run_id: Optional[UUID] = None,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> AsyncCallbackManagerForRetrieverRun:
-        """Run when the retriever starts running.
-
-        Args:
-            serialized (Optional[Dict[str, Any]]): The serialized retriever.
-            query (str): The query.
-            run_id (UUID, optional): The ID of the run. Defaults to None.
-            parent_run_id (UUID, optional): The ID of the parent run. Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-
-        Returns:
-            AsyncCallbackManagerForRetrieverRun: The async callback manager
-                for the retriever run.
-        """
-        if run_id is None:
-            run_id = uuid.uuid4()
-
-        await ahandle_event(
-            self.handlers,
-            "on_retriever_start",
-            "ignore_retriever",
-            serialized,
-            query,
-            run_id=run_id,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            metadata=self.metadata,
-            **kwargs,
-        )
-
-        return AsyncCallbackManagerForRetrieverRun(
-            run_id=run_id,
-            handlers=self.handlers,
-            inheritable_handlers=self.inheritable_handlers,
-            parent_run_id=self.parent_run_id,
-            tags=self.tags,
-            inheritable_tags=self.inheritable_tags,
-            metadata=self.metadata,
-            inheritable_metadata=self.inheritable_metadata,
-        )
-
-    @classmethod
-    def configure(
-        cls,
-        inheritable_callbacks: Callbacks = None,
-        local_callbacks: Callbacks = None,
-        verbose: bool = False,
-        inheritable_tags: Optional[list[str]] = None,
-        local_tags: Optional[list[str]] = None,
-        inheritable_metadata: Optional[dict[str, Any]] = None,
-        local_metadata: Optional[dict[str, Any]] = None,
-    ) -> AsyncCallbackManager:
-        """Configure the async callback manager.
-
-        Args:
-            inheritable_callbacks (Optional[Callbacks], optional): The inheritable
-                callbacks. Defaults to None.
-            local_callbacks (Optional[Callbacks], optional): The local callbacks.
-                Defaults to None.
-            verbose (bool, optional): Whether to enable verbose mode. Defaults to False.
-            inheritable_tags (Optional[List[str]], optional): The inheritable tags.
-                Defaults to None.
-            local_tags (Optional[List[str]], optional): The local tags.
-                Defaults to None.
-            inheritable_metadata (Optional[Dict[str, Any]], optional): The inheritable
-                metadata. Defaults to None.
-            local_metadata (Optional[Dict[str, Any]], optional): The local metadata.
-                Defaults to None.
-
-        Returns:
-            AsyncCallbackManager: The configured async callback manager.
-        """
-        return _configure(
-            cls,
-            inheritable_callbacks,
-            local_callbacks,
-            verbose,
-            inheritable_tags,
-            local_tags,
-            inheritable_metadata,
-            local_metadata,
-        )
-
-
-class AsyncCallbackManagerForChainGroup(AsyncCallbackManager):
-    """Async callback manager for the chain group."""
-
-    def __init__(
-        self,
-        handlers: list[BaseCallbackHandler],
-        inheritable_handlers: Optional[list[BaseCallbackHandler]] = None,
-        parent_run_id: Optional[UUID] = None,
-        *,
-        parent_run_manager: AsyncCallbackManagerForChainRun,
-        **kwargs: Any,
-    ) -> None:
-        """Initialize the async callback manager.
-
-        Args:
-            handlers (List[BaseCallbackHandler]): The list of handlers.
-            inheritable_handlers (Optional[List[BaseCallbackHandler]]): The list of
-                inheritable handlers. Defaults to None.
-            parent_run_id (Optional[UUID]): The ID of the parent run. Defaults to None.
-            parent_run_manager (AsyncCallbackManagerForChainRun):
-                The parent run manager.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        super().__init__(
-            handlers,
-            inheritable_handlers,
-            parent_run_id,
-            **kwargs,
-        )
-        self.parent_run_manager = parent_run_manager
-        self.ended = False
-
-    def copy(self) -> AsyncCallbackManagerForChainGroup:
-        """Copy the async callback manager."""
-        return self.__class__(
-            handlers=self.handlers.copy(),
-            inheritable_handlers=self.inheritable_handlers.copy(),
-            parent_run_id=self.parent_run_id,
-            tags=self.tags.copy(),
-            inheritable_tags=self.inheritable_tags.copy(),
-            metadata=self.metadata.copy(),
-            inheritable_metadata=self.inheritable_metadata.copy(),
-            parent_run_manager=self.parent_run_manager,
-        )
-
-    def merge(
-        self: AsyncCallbackManagerForChainGroup, other: BaseCallbackManager
-    ) -> AsyncCallbackManagerForChainGroup:
-        """Merge the group callback manager with another callback manager.
-
-        Overwrites the merge method in the base class to ensure that the
-        parent run manager is preserved. Keeps the parent_run_manager
-        from the current object.
-
-        Returns:
-            AsyncCallbackManagerForChainGroup: A copy of the current AsyncCallbackManagerForChainGroup
-                with the handlers, tags, etc. of the other callback manager merged in.
-
-        Example: Merging two callback managers.
-
-            .. code-block:: python
-
-                from langchain_core.callbacks.manager import CallbackManager, atrace_as_chain_group
-                from langchain_core.callbacks.stdout import StdOutCallbackHandler
-
-                manager = CallbackManager(handlers=[StdOutCallbackHandler()], tags=["tag2"])
-                async with atrace_as_chain_group("My Group Name", tags=["tag1"]) as group_manager:
-                    merged_manager = group_manager.merge(manager)
-                    print(type(merged_manager))
-                    # <class 'langchain_core.callbacks.manager.AsyncCallbackManagerForChainGroup'>
-
-                    print(merged_manager.handlers)
-                    # [
-                    #    <langchain_core.callbacks.stdout.LangChainTracer object at ...>,
-                    #    <langchain_core.callbacks.streaming_stdout.StdOutCallbackHandler object at ...>,
-                    # ]
-
-                    print(merged_manager.tags)
-                    #    ['tag2', 'tag1']
-
-        """  # noqa: E501
-        manager = self.__class__(
-            parent_run_id=self.parent_run_id or other.parent_run_id,
-            handlers=[],
-            inheritable_handlers=[],
-            tags=list(set(self.tags + other.tags)),
-            inheritable_tags=list(set(self.inheritable_tags + other.inheritable_tags)),
-            metadata={
-                **self.metadata,
-                **other.metadata,
-            },
-            parent_run_manager=self.parent_run_manager,
-        )
-
-        handlers = self.handlers + other.handlers
-        inheritable_handlers = self.inheritable_handlers + other.inheritable_handlers
-
-        for handler in handlers:
-            manager.add_handler(handler)
-
-        for handler in inheritable_handlers:
-            manager.add_handler(handler, inherit=True)
-        return manager
-
-    async def on_chain_end(
-        self, outputs: Union[dict[str, Any], Any], **kwargs: Any
-    ) -> None:
-        """Run when traced chain group ends.
-
-        Args:
-            outputs (Union[Dict[str, Any], Any]): The outputs of the chain.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        self.ended = True
-        await self.parent_run_manager.on_chain_end(outputs, **kwargs)
-
-    async def on_chain_error(
-        self,
-        error: BaseException,
-        **kwargs: Any,
-    ) -> None:
-        """Run when chain errors.
-
-        Args:
-            error (Exception or KeyboardInterrupt): The error.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        self.ended = True
-        await self.parent_run_manager.on_chain_error(error, **kwargs)
-
-
-T = TypeVar("T", CallbackManager, AsyncCallbackManager)
-
-
-H = TypeVar("H", bound=BaseCallbackHandler, covariant=True)
-
-
-def _configure(
-    callback_manager_cls: type[T],
-    inheritable_callbacks: Callbacks = None,
-    local_callbacks: Callbacks = None,
-    verbose: bool = False,
-    inheritable_tags: Optional[list[str]] = None,
-    local_tags: Optional[list[str]] = None,
-    inheritable_metadata: Optional[dict[str, Any]] = None,
-    local_metadata: Optional[dict[str, Any]] = None,
-) -> T:
-    """Configure the callback manager.
-
-    Args:
-        callback_manager_cls (Type[T]): The callback manager class.
-        inheritable_callbacks (Optional[Callbacks], optional): The inheritable
-            callbacks. Defaults to None.
-        local_callbacks (Optional[Callbacks], optional): The local callbacks.
-            Defaults to None.
-        verbose (bool, optional): Whether to enable verbose mode. Defaults to False.
-        inheritable_tags (Optional[List[str]], optional): The inheritable tags.
-            Defaults to None.
-        local_tags (Optional[List[str]], optional): The local tags. Defaults to None.
-        inheritable_metadata (Optional[Dict[str, Any]], optional): The inheritable
-            metadata. Defaults to None.
-        local_metadata (Optional[Dict[str, Any]], optional): The local metadata.
-            Defaults to None.
-
-    Returns:
-        T: The configured callback manager.
-    """
-    from langchain_core.tracers.context import (
-        _configure_hooks,
-        _get_tracer_project,
-        _tracing_v2_is_enabled,
-        tracing_v2_callback_var,
-    )
-
-    tracing_context = get_tracing_context()
-    tracing_metadata = tracing_context["metadata"]
-    tracing_tags = tracing_context["tags"]
-    run_tree: Optional[Run] = tracing_context["parent"]
-    parent_run_id = None if run_tree is None else run_tree.id
-    callback_manager = callback_manager_cls(
-        handlers=[],
-        parent_run_id=parent_run_id,
-    )
-    if inheritable_callbacks or local_callbacks:
-        if isinstance(inheritable_callbacks, list) or inheritable_callbacks is None:
-            inheritable_callbacks_ = inheritable_callbacks or []
-            callback_manager = callback_manager_cls(
-                handlers=inheritable_callbacks_.copy(),
-                inheritable_handlers=inheritable_callbacks_.copy(),
-                parent_run_id=parent_run_id,
-            )
-        else:
-            parent_run_id_ = inheritable_callbacks.parent_run_id
-            # Break ties between the external tracing context and inherited context
-            if parent_run_id is not None and (
-                parent_run_id_ is None
-                # If the LC parent has already been reflected
-                # in the run tree, we know the run_tree is either the
-                # same parent or a child of the parent.
-                or (run_tree and str(parent_run_id_) in run_tree.dotted_order)
-            ):
-                parent_run_id_ = parent_run_id
-                # Otherwise, we assume the LC context has progressed
-                # beyond the run tree and we should not inherit the parent.
-            callback_manager = callback_manager_cls(
-                handlers=inheritable_callbacks.handlers.copy(),
-                inheritable_handlers=inheritable_callbacks.inheritable_handlers.copy(),
-                parent_run_id=parent_run_id_,
-                tags=inheritable_callbacks.tags.copy(),
-                inheritable_tags=inheritable_callbacks.inheritable_tags.copy(),
-                metadata=inheritable_callbacks.metadata.copy(),
-                inheritable_metadata=inheritable_callbacks.inheritable_metadata.copy(),
-            )
-        local_handlers_ = (
-            local_callbacks
-            if isinstance(local_callbacks, list)
-            else (local_callbacks.handlers if local_callbacks else [])
-        )
-        for handler in local_handlers_:
-            callback_manager.add_handler(handler, False)
-    if inheritable_tags or local_tags:
-        callback_manager.add_tags(inheritable_tags or [])
-        callback_manager.add_tags(local_tags or [], False)
-    if inheritable_metadata or local_metadata:
-        callback_manager.add_metadata(inheritable_metadata or {})
-        callback_manager.add_metadata(local_metadata or {}, False)
-    if tracing_metadata:
-        callback_manager.add_metadata(tracing_metadata.copy())
-    if tracing_tags:
-        callback_manager.add_tags(tracing_tags.copy())
-
-    v1_tracing_enabled_ = env_var_is_set("LANGCHAIN_TRACING") or env_var_is_set(
-        "LANGCHAIN_HANDLER"
-    )
-
-    tracer_v2 = tracing_v2_callback_var.get()
-    tracing_v2_enabled_ = _tracing_v2_is_enabled()
-
-    if v1_tracing_enabled_ and not tracing_v2_enabled_:
-        # if both are enabled, can silently ignore the v1 tracer
-        msg = (
-            "Tracing using LangChainTracerV1 is no longer supported. "
-            "Please set the LANGCHAIN_TRACING_V2 environment variable to enable "
-            "tracing instead."
-        )
-        raise RuntimeError(msg)
-
-    tracer_project = _get_tracer_project()
-    debug = _get_debug()
-    if verbose or debug or tracing_v2_enabled_:
-        from langchain_core.tracers.langchain import LangChainTracer
-        from langchain_core.tracers.stdout import ConsoleCallbackHandler
-
-        if verbose and not any(
-            isinstance(handler, StdOutCallbackHandler)
-            for handler in callback_manager.handlers
-        ):
-            if debug:
-                pass
-            else:
-                callback_manager.add_handler(StdOutCallbackHandler(), False)
-        if debug and not any(
-            isinstance(handler, ConsoleCallbackHandler)
-            for handler in callback_manager.handlers
-        ):
-            callback_manager.add_handler(ConsoleCallbackHandler(), True)
-        if tracing_v2_enabled_ and not any(
-            isinstance(handler, LangChainTracer)
-            for handler in callback_manager.handlers
-        ):
-            if tracer_v2:
-                callback_manager.add_handler(tracer_v2, True)
-            else:
-                try:
-                    handler = LangChainTracer(
-                        project_name=tracer_project,
-                        client=(
-                            run_tree.client
-                            if run_tree is not None
-                            else tracing_context["client"]
-                        ),
-                        tags=tracing_tags,
-                    )
-                    callback_manager.add_handler(handler, True)
-                except Exception as e:
-                    logger.warning(
-                        "Unable to load requested LangChainTracer."
-                        " To disable this warning,"
-                        " unset the LANGCHAIN_TRACING_V2 environment variables.\n"
-                        f"{repr(e)}",
-                    )
-        if run_tree is not None:
-            for handler in callback_manager.handlers:
-                if isinstance(handler, LangChainTracer):
-                    handler.order_map[run_tree.id] = (
-                        run_tree.trace_id,
-                        run_tree.dotted_order,
-                    )
-                    handler.run_map[str(run_tree.id)] = cast(Run, run_tree)
-    for var, inheritable, handler_class, env_var in _configure_hooks:
-        create_one = (
-            env_var is not None
-            and env_var_is_set(env_var)
-            and handler_class is not None
-        )
-        if var.get() is not None or create_one:
-            var_handler = var.get() or cast(type[BaseCallbackHandler], handler_class)()
-            if handler_class is None:
-                if not any(
-                    handler is var_handler  # direct pointer comparison
-                    for handler in callback_manager.handlers
-                ):
-                    callback_manager.add_handler(var_handler, inheritable)
-            else:
-                if not any(
-                    isinstance(handler, handler_class)
-                    for handler in callback_manager.handlers
-                ):
-                    callback_manager.add_handler(var_handler, inheritable)
-    return callback_manager
-
-
-async def adispatch_custom_event(
-    name: str, data: Any, *, config: Optional[RunnableConfig] = None
-) -> None:
-    """Dispatch an adhoc event to the handlers.
-
-    Args:
-        name: The name of the adhoc event.
-        data: The data for the adhoc event. Free form data. Ideally should be
-              JSON serializable to avoid serialization issues downstream, but
-              this is not enforced.
-        config: Optional config object. Mirrors the async API but not strictly needed.
-
-    Example:
-
-        .. code-block:: python
-
-            from langchain_core.callbacks import (
-                AsyncCallbackHandler,
-                adispatch_custom_event
-            )
-            from langchain_core.runnable import RunnableLambda
-
-            class CustomCallbackManager(AsyncCallbackHandler):
-                async def on_custom_event(
-                    self,
-                    name: str,
-                    data: Any,
-                    *,
-                    run_id: UUID,
-                    tags: Optional[List[str]] = None,
-                    metadata: Optional[Dict[str, Any]] = None,
-                    **kwargs: Any,
-                ) -> None:
-                    print(f"Received custom event: {name} with data: {data}")
-
-            callback = CustomCallbackManager()
-
-            async def foo(inputs):
-                await adispatch_custom_event("my_event", {"bar": "buzz})
-                return inputs
-
-            foo_ = RunnableLambda(foo)
-            await foo_.ainvoke({"a": "1"}, {"callbacks": [CustomCallbackManager()]})
-
-    Example: Use with astream events
-
-        .. code-block:: python
-
-            from langchain_core.callbacks import (
-                AsyncCallbackHandler,
-                adispatch_custom_event
-            )
-            from langchain_core.runnable import RunnableLambda
-
-            class CustomCallbackManager(AsyncCallbackHandler):
-                async def on_custom_event(
-                    self,
-                    name: str,
-                    data: Any,
-                    *,
-                    run_id: UUID,
-                    tags: Optional[List[str]] = None,
-                    metadata: Optional[Dict[str, Any]] = None,
-                    **kwargs: Any,
-                ) -> None:
-                    print(f"Received custom event: {name} with data: {data}")
-
-            callback = CustomCallbackManager()
-
-            async def foo(inputs):
-                await adispatch_custom_event("event_type_1", {"bar": "buzz})
-                await adispatch_custom_event("event_type_2", 5)
-                return inputs
-
-            foo_ = RunnableLambda(foo)
-
-            async for event in foo_.ainvoke_stream(
-                {"a": "1"},
-                version="v2",
-                config={"callbacks": [CustomCallbackManager()]}
-            ):
-                print(event)
-
-    .. warning::
-        If using python <= 3.10 and async, you MUST
-        specify the `config` parameter or the function will raise an error.
-        This is due to a limitation in asyncio for python <= 3.10 that prevents
-        LangChain from automatically propagating the config object on the user's
-        behalf.
-
-    .. versionadded:: 0.2.15
-    """
-    from langchain_core.runnables.config import (
-        ensure_config,
-        get_async_callback_manager_for_config,
-    )
-
-    config = ensure_config(config)
-    callback_manager = get_async_callback_manager_for_config(config)
-    # We want to get the callback manager for the parent run.
-    # This is a work-around for now to be able to dispatch adhoc events from
-    # within a tool or a lambda and have the metadata events associated
-    # with the parent run rather than have a new run id generated for each.
-    if callback_manager.parent_run_id is None:
-        msg = (
-            "Unable to dispatch an adhoc event without a parent run id."
-            "This function can only be called from within an existing run (e.g.,"
-            "inside a tool or a RunnableLambda or a RunnableGenerator.)"
-            "If you are doing that and still seeing this error, try explicitly"
-            "passing the config parameter to this function."
-        )
-        raise RuntimeError(msg)
-
-    await callback_manager.on_custom_event(
-        name,
-        data,
-        run_id=callback_manager.parent_run_id,
-    )
-
-
-def dispatch_custom_event(
-    name: str, data: Any, *, config: Optional[RunnableConfig] = None
-) -> None:
-    """Dispatch an adhoc event.
-
-    Args:
-        name: The name of the adhoc event.
-        data: The data for the adhoc event. Free form data. Ideally should be
-              JSON serializable to avoid serialization issues downstream, but
-              this is not enforced.
-        config: Optional config object. Mirrors the async API but not strictly needed.
-
-    Example:
-
-        .. code-block:: python
-
-            from langchain_core.callbacks import BaseCallbackHandler
-            from langchain_core.callbacks import dispatch_custom_event
-            from langchain_core.runnable import RunnableLambda
-
-            class CustomCallbackManager(BaseCallbackHandler):
-                def on_custom_event(
-                    self,
-                    name: str,
-                    data: Any,
-                    *,
-                    run_id: UUID,
-                    tags: Optional[List[str]] = None,
-                    metadata: Optional[Dict[str, Any]] = None,
-                    **kwargs: Any,
-                ) -> None:
-                    print(f"Received custom event: {name} with data: {data}")
-
-            def foo(inputs):
-                dispatch_custom_event("my_event", {"bar": "buzz})
-                return inputs
-
-            foo_ = RunnableLambda(foo)
-            foo_.invoke({"a": "1"}, {"callbacks": [CustomCallbackManager()]})
-
-    .. versionadded:: 0.2.15
-    """
-    from langchain_core.runnables.config import (
-        ensure_config,
-        get_callback_manager_for_config,
-    )
-
-    config = ensure_config(config)
-    callback_manager = get_callback_manager_for_config(config)
-    # We want to get the callback manager for the parent run.
-    # This is a work-around for now to be able to dispatch adhoc events from
-    # within a tool or a lambda and have the metadata events associated
-    # with the parent run rather than have a new run id generated for each.
-    if callback_manager.parent_run_id is None:
-        msg = (
-            "Unable to dispatch an adhoc event without a parent run id."
-            "This function can only be called from within an existing run (e.g.,"
-            "inside a tool or a RunnableLambda or a RunnableGenerator.)"
-            "If you are doing that and still seeing this error, try explicitly"
-            "passing the config parameter to this function."
-        )
-        raise RuntimeError(msg)
-    callback_manager.on_custom_event(
-        name,
-        data,
-        run_id=callback_manager.parent_run_id,
-    )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/callbacks/stdout.py ./custom_langchain_core/callbacks/stdout.py
--- .venv/lib/python3.12/site-packages/langchain_core/callbacks/stdout.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/callbacks/stdout.py	1970-01-01 09:00:00
@@ -1,117 +0,0 @@
-"""Callback Handler that prints to std out."""
-
-from __future__ import annotations
-
-from typing import TYPE_CHECKING, Any, Optional
-
-from langchain_core.callbacks.base import BaseCallbackHandler
-from langchain_core.utils import print_text
-
-if TYPE_CHECKING:
-    from langchain_core.agents import AgentAction, AgentFinish
-
-
-class StdOutCallbackHandler(BaseCallbackHandler):
-    """Callback Handler that prints to std out."""
-
-    def __init__(self, color: Optional[str] = None) -> None:
-        """Initialize callback handler.
-
-        Args:
-            color: The color to use for the text. Defaults to None.
-        """
-        self.color = color
-
-    def on_chain_start(
-        self, serialized: dict[str, Any], inputs: dict[str, Any], **kwargs: Any
-    ) -> None:
-        """Print out that we are entering a chain.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized chain.
-            inputs (Dict[str, Any]): The inputs to the chain.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        if "name" in kwargs:
-            name = kwargs["name"]
-        else:
-            if serialized:
-                name = serialized.get("name", serialized.get("id", ["<unknown>"])[-1])
-            else:
-                name = "<unknown>"
-        print(f"\n\n\033[1m> Entering new {name} chain...\033[0m")  # noqa: T201
-
-    def on_chain_end(self, outputs: dict[str, Any], **kwargs: Any) -> None:
-        """Print out that we finished a chain.
-
-        Args:
-            outputs (Dict[str, Any]): The outputs of the chain.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        print("\n\033[1m> Finished chain.\033[0m")  # noqa: T201
-
-    def on_agent_action(
-        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any
-    ) -> Any:
-        """Run on agent action.
-
-        Args:
-            action (AgentAction): The agent action.
-            color (Optional[str]): The color to use for the text. Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        print_text(action.log, color=color or self.color)
-
-    def on_tool_end(
-        self,
-        output: Any,
-        color: Optional[str] = None,
-        observation_prefix: Optional[str] = None,
-        llm_prefix: Optional[str] = None,
-        **kwargs: Any,
-    ) -> None:
-        """If not the final action, print out observation.
-
-        Args:
-            output (Any): The output to print.
-            color (Optional[str]): The color to use for the text. Defaults to None.
-            observation_prefix (Optional[str]): The observation prefix.
-                Defaults to None.
-            llm_prefix (Optional[str]): The LLM prefix. Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        output = str(output)
-        if observation_prefix is not None:
-            print_text(f"\n{observation_prefix}")
-        print_text(output, color=color or self.color)
-        if llm_prefix is not None:
-            print_text(f"\n{llm_prefix}")
-
-    def on_text(
-        self,
-        text: str,
-        color: Optional[str] = None,
-        end: str = "",
-        **kwargs: Any,
-    ) -> None:
-        """Run when the agent ends.
-
-        Args:
-            text (str): The text to print.
-            color (Optional[str]): The color to use for the text. Defaults to None.
-            end (str): The end character to use. Defaults to "".
-            **kwargs (Any): Additional keyword arguments.
-        """
-        print_text(text, color=color or self.color, end=end)
-
-    def on_agent_finish(
-        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any
-    ) -> None:
-        """Run on the agent end.
-
-        Args:
-            finish (AgentFinish): The agent finish.
-            color (Optional[str]): The color to use for the text. Defaults to None.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        print_text(finish.log, color=color or self.color, end="\n")
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/callbacks/streaming_stdout.py ./custom_langchain_core/callbacks/streaming_stdout.py
--- .venv/lib/python3.12/site-packages/langchain_core/callbacks/streaming_stdout.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/callbacks/streaming_stdout.py	1970-01-01 09:00:00
@@ -1,146 +0,0 @@
-"""Callback Handler streams to stdout on new llm token."""
-
-from __future__ import annotations
-
-import sys
-from typing import TYPE_CHECKING, Any
-
-from langchain_core.callbacks.base import BaseCallbackHandler
-
-if TYPE_CHECKING:
-    from langchain_core.agents import AgentAction, AgentFinish
-    from langchain_core.messages import BaseMessage
-    from langchain_core.outputs import LLMResult
-
-
-class StreamingStdOutCallbackHandler(BaseCallbackHandler):
-    """Callback handler for streaming. Only works with LLMs that support streaming."""
-
-    def on_llm_start(
-        self, serialized: dict[str, Any], prompts: list[str], **kwargs: Any
-    ) -> None:
-        """Run when LLM starts running.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized LLM.
-            prompts (List[str]): The prompts to run.
-            **kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_chat_model_start(
-        self,
-        serialized: dict[str, Any],
-        messages: list[list[BaseMessage]],
-        **kwargs: Any,
-    ) -> None:
-        """Run when LLM starts running.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized LLM.
-            messages (List[List[BaseMessage]]): The messages to run.
-            **kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
-        """Run on new LLM token. Only available when streaming is enabled.
-
-        Args:
-            token (str): The new token.
-            **kwargs (Any): Additional keyword arguments.
-        """
-        sys.stdout.write(token)
-        sys.stdout.flush()
-
-    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
-        """Run when LLM ends running.
-
-        Args:
-            response (LLMResult): The response from the LLM.
-            **kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_llm_error(self, error: BaseException, **kwargs: Any) -> None:
-        """Run when LLM errors.
-
-        Args:
-            error (BaseException): The error that occurred.
-            **kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_chain_start(
-        self, serialized: dict[str, Any], inputs: dict[str, Any], **kwargs: Any
-    ) -> None:
-        """Run when a chain starts running.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized chain.
-            inputs (Dict[str, Any]): The inputs to the chain.
-            **kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_chain_end(self, outputs: dict[str, Any], **kwargs: Any) -> None:
-        """Run when a chain ends running.
-
-        Args:
-            outputs (Dict[str, Any]): The outputs of the chain.
-            **kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_chain_error(self, error: BaseException, **kwargs: Any) -> None:
-        """Run when chain errors.
-
-        Args:
-            error (BaseException): The error that occurred.
-            **kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_tool_start(
-        self, serialized: dict[str, Any], input_str: str, **kwargs: Any
-    ) -> None:
-        """Run when the tool starts running.
-
-        Args:
-            serialized (Dict[str, Any]): The serialized tool.
-            input_str (str): The input string.
-            **kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
-        """Run on agent action.
-
-        Args:
-            action (AgentAction): The agent action.
-            **kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_tool_end(self, output: Any, **kwargs: Any) -> None:
-        """Run when tool ends running.
-
-        Args:
-            output (Any): The output of the tool.
-            **kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:
-        """Run when tool errors.
-
-        Args:
-            error (BaseException): The error that occurred.
-            **kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_text(self, text: str, **kwargs: Any) -> None:
-        """Run on an arbitrary text.
-
-        Args:
-            text (str): The text to print.
-            **kwargs (Any): Additional keyword arguments.
-        """
-
-    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> None:
-        """Run on the agent end.
-
-        Args:
-            finish (AgentFinish): The agent finish.
-            **kwargs (Any): Additional keyword arguments.
-        """
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/chat_history.py ./custom_langchain_core/chat_history.py
--- .venv/lib/python3.12/site-packages/langchain_core/chat_history.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/chat_history.py	1970-01-01 09:00:00
@@ -1,245 +0,0 @@
-"""**Chat message history** stores a history of the message interactions in a chat.
-
-**Class hierarchy:**
-
-.. code-block::
-
-    BaseChatMessageHistory --> <name>ChatMessageHistory  # Examples: FileChatMessageHistory, PostgresChatMessageHistory
-
-**Main helpers:**
-
-.. code-block::
-
-    AIMessage, HumanMessage, BaseMessage
-
-"""  # noqa: E501
-
-from __future__ import annotations
-
-from abc import ABC, abstractmethod
-from collections.abc import Sequence
-from typing import Union
-
-from pydantic import BaseModel, Field
-
-from langchain_core.messages import (
-    AIMessage,
-    BaseMessage,
-    HumanMessage,
-    get_buffer_string,
-)
-
-
-class BaseChatMessageHistory(ABC):
-    """Abstract base class for storing chat message history.
-
-    Implementations guidelines:
-
-    Implementations are expected to over-ride all or some of the following methods:
-
-    * add_messages: sync variant for bulk addition of messages
-    * aadd_messages: async variant for bulk addition of messages
-    * messages: sync variant for getting messages
-    * aget_messages: async variant for getting messages
-    * clear: sync variant for clearing messages
-    * aclear: async variant for clearing messages
-
-    add_messages contains a default implementation that calls add_message
-    for each message in the sequence. This is provided for backwards compatibility
-    with existing implementations which only had add_message.
-
-    Async variants all have default implementations that call the sync variants.
-    Implementers can choose to over-ride the async implementations to provide
-    truly async implementations.
-
-    Usage guidelines:
-
-    When used for updating history, users should favor usage of `add_messages`
-    over `add_message` or other variants like `add_user_message` and `add_ai_message`
-    to avoid unnecessary round-trips to the underlying persistence layer.
-
-    Example: Shows a default implementation.
-
-        .. code-block:: python
-
-            class FileChatMessageHistory(BaseChatMessageHistory):
-                storage_path:  str
-                session_id: str
-
-               @property
-               def messages(self):
-                   with open(os.path.join(storage_path, session_id), 'r:utf-8') as f:
-                       messages = json.loads(f.read())
-                    return messages_from_dict(messages)
-
-               def add_messages(self, messages: Sequence[BaseMessage]) -> None:
-                   all_messages = list(self.messages) # Existing messages
-                   all_messages.extend(messages) # Add new messages
-
-                   serialized = [message_to_dict(message) for message in all_messages]
-                   # Can be further optimized by only writing new messages
-                   # using append mode.
-                   with open(os.path.join(storage_path, session_id), 'w') as f:
-                       json.dump(f, messages)
-
-               def clear(self):
-                   with open(os.path.join(storage_path, session_id), 'w') as f:
-                       f.write("[]")
-    """
-
-    messages: list[BaseMessage]
-    """A property or attribute that returns a list of messages.
-
-    In general, getting the messages may involve IO to the underlying
-    persistence layer, so this operation is expected to incur some
-    latency.
-    """
-
-    async def aget_messages(self) -> list[BaseMessage]:
-        """Async version of getting messages.
-
-        Can over-ride this method to provide an efficient async implementation.
-
-        In general, fetching messages may involve IO to the underlying
-        persistence layer.
-        """
-        from langchain_core.runnables.config import run_in_executor
-
-        return await run_in_executor(None, lambda: self.messages)
-
-    def add_user_message(self, message: Union[HumanMessage, str]) -> None:
-        """Convenience method for adding a human message string to the store.
-
-        Please note that this is a convenience method. Code should favor the
-        bulk add_messages interface instead to save on round-trips to the underlying
-        persistence layer.
-
-        This method may be deprecated in a future release.
-
-        Args:
-            message: The human message to add to the store.
-        """
-        if isinstance(message, HumanMessage):
-            self.add_message(message)
-        else:
-            self.add_message(HumanMessage(content=message))
-
-    def add_ai_message(self, message: Union[AIMessage, str]) -> None:
-        """Convenience method for adding an AI message string to the store.
-
-        Please note that this is a convenience method. Code should favor the bulk
-        add_messages interface instead to save on round-trips to the underlying
-        persistence layer.
-
-        This method may be deprecated in a future release.
-
-        Args:
-            message: The AI message to add.
-        """
-        if isinstance(message, AIMessage):
-            self.add_message(message)
-        else:
-            self.add_message(AIMessage(content=message))
-
-    def add_message(self, message: BaseMessage) -> None:
-        """Add a Message object to the store.
-
-        Args:
-            message: A BaseMessage object to store.
-
-        Raises:
-            NotImplementedError: If the sub-class has not implemented an efficient
-                add_messages method.
-        """
-        if type(self).add_messages != BaseChatMessageHistory.add_messages:
-            # This means that the sub-class has implemented an efficient add_messages
-            # method, so we should use it.
-            self.add_messages([message])
-        else:
-            msg = (
-                "add_message is not implemented for this class. "
-                "Please implement add_message or add_messages."
-            )
-            raise NotImplementedError(msg)
-
-    def add_messages(self, messages: Sequence[BaseMessage]) -> None:
-        """Add a list of messages.
-
-        Implementations should over-ride this method to handle bulk addition of messages
-        in an efficient manner to avoid unnecessary round-trips to the underlying store.
-
-        Args:
-            messages: A sequence of BaseMessage objects to store.
-        """
-        for message in messages:
-            self.add_message(message)
-
-    async def aadd_messages(self, messages: Sequence[BaseMessage]) -> None:
-        """Async add a list of messages.
-
-        Args:
-            messages: A sequence of BaseMessage objects to store.
-        """
-        from langchain_core.runnables.config import run_in_executor
-
-        await run_in_executor(None, self.add_messages, messages)
-
-    @abstractmethod
-    def clear(self) -> None:
-        """Remove all messages from the store."""
-
-    async def aclear(self) -> None:
-        """Async remove all messages from the store."""
-        from langchain_core.runnables.config import run_in_executor
-
-        await run_in_executor(None, self.clear)
-
-    def __str__(self) -> str:
-        """Return a string representation of the chat history."""
-        return get_buffer_string(self.messages)
-
-
-class InMemoryChatMessageHistory(BaseChatMessageHistory, BaseModel):
-    """In memory implementation of chat message history.
-
-    Stores messages in a memory list.
-    """
-
-    messages: list[BaseMessage] = Field(default_factory=list)
-    """A list of messages stored in memory."""
-
-    async def aget_messages(self) -> list[BaseMessage]:
-        """Async version of getting messages.
-
-        Can over-ride this method to provide an efficient async implementation.
-        In general, fetching messages may involve IO to the underlying
-        persistence layer.
-
-        Returns:
-            List of messages.
-        """
-        return self.messages
-
-    def add_message(self, message: BaseMessage) -> None:
-        """Add a self-created message to the store.
-
-        Args:
-            message: The message to add.
-        """
-        self.messages.append(message)
-
-    async def aadd_messages(self, messages: Sequence[BaseMessage]) -> None:
-        """Async add messages to the store.
-
-        Args:
-            messages: The messages to add.
-        """
-        self.add_messages(messages)
-
-    def clear(self) -> None:
-        """Clear all messages from the store."""
-        self.messages = []
-
-    async def aclear(self) -> None:
-        """Async clear all messages from the store."""
-        self.clear()
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/chat_loaders.py ./custom_langchain_core/chat_loaders.py
--- .venv/lib/python3.12/site-packages/langchain_core/chat_loaders.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/chat_loaders.py	1970-01-01 09:00:00
@@ -1,24 +0,0 @@
-from abc import ABC, abstractmethod
-from collections.abc import Iterator
-
-from langchain_core.chat_sessions import ChatSession
-
-
-class BaseChatLoader(ABC):
-    """Base class for chat loaders."""
-
-    @abstractmethod
-    def lazy_load(self) -> Iterator[ChatSession]:
-        """Lazy load the chat sessions.
-
-        Returns:
-            An iterator of chat sessions.
-        """
-
-    def load(self) -> list[ChatSession]:
-        """Eagerly load the chat sessions into memory.
-
-        Returns:
-            A list of chat sessions.
-        """
-        return list(self.lazy_load())
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/chat_sessions.py ./custom_langchain_core/chat_sessions.py
--- .venv/lib/python3.12/site-packages/langchain_core/chat_sessions.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/chat_sessions.py	1970-01-01 09:00:00
@@ -1,17 +0,0 @@
-"""**Chat Sessions** are a collection of messages and function calls."""
-
-from collections.abc import Sequence
-from typing import TypedDict
-
-from langchain_core.messages import BaseMessage
-
-
-class ChatSession(TypedDict, total=False):
-    """Chat Session represents a single
-    conversation, channel, or other group of messages.
-    """
-
-    messages: Sequence[BaseMessage]
-    """A sequence of the LangChain chat messages loaded from the source."""
-    functions: Sequence[dict]
-    """A sequence of the function calling specs for the messages."""
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/document_loaders/__init__.py ./custom_langchain_core/document_loaders/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/document_loaders/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/document_loaders/__init__.py	1970-01-01 09:00:00
@@ -1,12 +0,0 @@
-from langchain_core.document_loaders.base import BaseBlobParser, BaseLoader
-from langchain_core.document_loaders.blob_loaders import Blob, BlobLoader, PathLike
-from langchain_core.document_loaders.langsmith import LangSmithLoader
-
-__all__ = [
-    "BaseBlobParser",
-    "BaseLoader",
-    "Blob",
-    "BlobLoader",
-    "PathLike",
-    "LangSmithLoader",
-]
Binary files .venv/lib/python3.12/site-packages/langchain_core/document_loaders/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/document_loaders/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/document_loaders/__pycache__/base.cpython-312.pyc and ./custom_langchain_core/document_loaders/__pycache__/base.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/document_loaders/__pycache__/blob_loaders.cpython-312.pyc and ./custom_langchain_core/document_loaders/__pycache__/blob_loaders.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/document_loaders/__pycache__/langsmith.cpython-312.pyc and ./custom_langchain_core/document_loaders/__pycache__/langsmith.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/document_loaders/base.py ./custom_langchain_core/document_loaders/base.py
--- .venv/lib/python3.12/site-packages/langchain_core/document_loaders/base.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/document_loaders/base.py	1970-01-01 09:00:00
@@ -1,126 +0,0 @@
-"""Abstract interface for document loader implementations."""
-
-from __future__ import annotations
-
-from abc import ABC, abstractmethod
-from collections.abc import AsyncIterator, Iterator
-from typing import TYPE_CHECKING, Optional
-
-from langchain_core.documents import Document
-from langchain_core.runnables import run_in_executor
-
-if TYPE_CHECKING:
-    from langchain_text_splitters import TextSplitter
-
-from langchain_core.documents.base import Blob
-
-
-class BaseLoader(ABC):  # noqa: B024
-    """Interface for Document Loader.
-
-    Implementations should implement the lazy-loading method using generators
-    to avoid loading all Documents into memory at once.
-
-    `load` is provided just for user convenience and should not be overridden.
-    """
-
-    # Sub-classes should not implement this method directly. Instead, they
-    # should implement the lazy load method.
-    def load(self) -> list[Document]:
-        """Load data into Document objects."""
-        return list(self.lazy_load())
-
-    async def aload(self) -> list[Document]:
-        """Load data into Document objects."""
-        return [document async for document in self.alazy_load()]
-
-    def load_and_split(
-        self, text_splitter: Optional[TextSplitter] = None
-    ) -> list[Document]:
-        """Load Documents and split into chunks. Chunks are returned as Documents.
-
-        Do not override this method. It should be considered to be deprecated!
-
-        Args:
-            text_splitter: TextSplitter instance to use for splitting documents.
-              Defaults to RecursiveCharacterTextSplitter.
-
-        Returns:
-            List of Documents.
-        """
-        if text_splitter is None:
-            try:
-                from langchain_text_splitters import RecursiveCharacterTextSplitter
-            except ImportError as e:
-                msg = (
-                    "Unable to import from langchain_text_splitters. Please specify "
-                    "text_splitter or install langchain_text_splitters with "
-                    "`pip install -U langchain-text-splitters`."
-                )
-                raise ImportError(msg) from e
-
-            _text_splitter: TextSplitter = RecursiveCharacterTextSplitter()
-        else:
-            _text_splitter = text_splitter
-        docs = self.load()
-        return _text_splitter.split_documents(docs)
-
-    # Attention: This method will be upgraded into an abstractmethod once it's
-    #            implemented in all the existing subclasses.
-    def lazy_load(self) -> Iterator[Document]:
-        """A lazy loader for Documents."""
-        if type(self).load != BaseLoader.load:
-            return iter(self.load())
-        msg = f"{self.__class__.__name__} does not implement lazy_load()"
-        raise NotImplementedError(msg)
-
-    async def alazy_load(self) -> AsyncIterator[Document]:
-        """A lazy loader for Documents."""
-        iterator = await run_in_executor(None, self.lazy_load)
-        done = object()
-        while True:
-            doc = await run_in_executor(None, next, iterator, done)  # type: ignore[call-arg, arg-type]
-            if doc is done:
-                break
-            yield doc  # type: ignore[misc]
-
-
-class BaseBlobParser(ABC):
-    """Abstract interface for blob parsers.
-
-    A blob parser provides a way to parse raw data stored in a blob into one
-    or more documents.
-
-    The parser can be composed with blob loaders, making it easy to reuse
-    a parser independent of how the blob was originally loaded.
-    """
-
-    @abstractmethod
-    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
-        """Lazy parsing interface.
-
-        Subclasses are required to implement this method.
-
-        Args:
-            blob: Blob instance
-
-        Returns:
-            Generator of documents
-        """
-
-    def parse(self, blob: Blob) -> list[Document]:
-        """Eagerly parse the blob into a document or documents.
-
-        This is a convenience method for interactive development environment.
-
-        Production applications should favor the lazy_parse method instead.
-
-        Subclasses should generally not over-ride this parse method.
-
-        Args:
-            blob: Blob instance
-
-        Returns:
-            List of documents
-        """
-        return list(self.lazy_parse(blob))
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/document_loaders/blob_loaders.py ./custom_langchain_core/document_loaders/blob_loaders.py
--- .venv/lib/python3.12/site-packages/langchain_core/document_loaders/blob_loaders.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/document_loaders/blob_loaders.py	1970-01-01 09:00:00
@@ -1,37 +0,0 @@
-"""Schema for Blobs and Blob Loaders.
-
-The goal is to facilitate decoupling of content loading from content parsing code.
-
-In addition, content loading code should provide a lazy loading interface by default.
-"""
-
-from __future__ import annotations
-
-from abc import ABC, abstractmethod
-from collections.abc import Iterable
-
-# Re-export Blob and PathLike for backwards compatibility
-from langchain_core.documents.base import Blob as Blob
-from langchain_core.documents.base import PathLike as PathLike
-
-
-class BlobLoader(ABC):
-    """Abstract interface for blob loaders implementation.
-
-    Implementer should be able to load raw content from a storage system according
-    to some criteria and return the raw content lazily as a stream of blobs.
-    """
-
-    @abstractmethod
-    def yield_blobs(
-        self,
-    ) -> Iterable[Blob]:
-        """A lazy loader for raw data represented by LangChain's Blob object.
-
-        Returns:
-            A generator over blobs
-        """
-
-
-# Re-export Blob and Pathlike for backwards compatibility
-__all__ = ["Blob", "BlobLoader", "PathLike"]
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/document_loaders/langsmith.py ./custom_langchain_core/document_loaders/langsmith.py
--- .venv/lib/python3.12/site-packages/langchain_core/document_loaders/langsmith.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/document_loaders/langsmith.py	1970-01-01 09:00:00
@@ -1,129 +0,0 @@
-import datetime
-import json
-import uuid
-from collections.abc import Iterator, Sequence
-from typing import Any, Callable, Optional, Union
-
-from langsmith import Client as LangSmithClient
-
-from langchain_core.document_loaders.base import BaseLoader
-from langchain_core.documents import Document
-
-
-class LangSmithLoader(BaseLoader):
-    """Load LangSmith Dataset examples as Documents.
-
-    Loads the example inputs as the Document page content and places the entire example
-    into the Document metadata. This allows you to easily create few-shot example
-    retrievers from the loaded documents.
-
-    .. dropdown:: Lazy load
-
-        .. code-block:: python
-
-            from langchain_core.document_loaders import LangSmithLoader
-
-            loader = LangSmithLoader(dataset_id="...", limit=100)
-            docs = []
-            for doc in loader.lazy_load():
-                docs.append(doc)
-
-        .. code-block:: pycon
-
-            # -> [Document("...", metadata={"inputs": {...}, "outputs": {...}, ...}), ...]
-
-    .. versionadded:: 0.2.34
-    """  # noqa: E501
-
-    def __init__(
-        self,
-        *,
-        dataset_id: Optional[Union[uuid.UUID, str]] = None,
-        dataset_name: Optional[str] = None,
-        example_ids: Optional[Sequence[Union[uuid.UUID, str]]] = None,
-        as_of: Optional[Union[datetime.datetime, str]] = None,
-        splits: Optional[Sequence[str]] = None,
-        inline_s3_urls: bool = True,
-        offset: int = 0,
-        limit: Optional[int] = None,
-        metadata: Optional[dict] = None,
-        filter: Optional[str] = None,
-        content_key: str = "",
-        format_content: Optional[Callable[..., str]] = None,
-        client: Optional[LangSmithClient] = None,
-        **client_kwargs: Any,
-    ) -> None:
-        """
-        Args:
-            dataset_id: The ID of the dataset to filter by. Defaults to None.
-            dataset_name: The name of the dataset to filter by. Defaults to None.
-            content_key: The inputs key to set as Document page content. ``"."`` characters
-                are interpreted as nested keys. E.g. ``content_key="first.second"`` will
-                result in
-                ``Document(page_content=format_content(example.inputs["first"]["second"]))``
-            format_content: Function for converting the content extracted from the example
-                inputs into a string. Defaults to JSON-encoding the contents.
-            example_ids: The IDs of the examples to filter by. Defaults to None.
-            as_of: The dataset version tag OR
-                timestamp to retrieve the examples as of.
-                Response examples will only be those that were present at the time
-                of the tagged (or timestamped) version.
-            splits: A list of dataset splits, which are
-                divisions of your dataset such as 'train', 'test', or 'validation'.
-                Returns examples only from the specified splits.
-            inline_s3_urls: Whether to inline S3 URLs. Defaults to True.
-            offset: The offset to start from. Defaults to 0.
-            limit: The maximum number of examples to return.
-            filter: A structured filter string to apply to the examples.
-            client: LangSmith Client. If not provided will be initialized from below args.
-            client_kwargs: Keyword args to pass to LangSmith client init. Should only be
-                specified if ``client`` isn't.
-        """  # noqa: E501
-        if client and client_kwargs:
-            raise ValueError
-        self._client = client or LangSmithClient(**client_kwargs)
-        self.content_key = list(content_key.split(".")) if content_key else []
-        self.format_content = format_content or _stringify
-        self.dataset_id = dataset_id
-        self.dataset_name = dataset_name
-        self.example_ids = example_ids
-        self.as_of = as_of
-        self.splits = splits
-        self.inline_s3_urls = inline_s3_urls
-        self.offset = offset
-        self.limit = limit
-        self.metadata = metadata
-        self.filter = filter
-
-    def lazy_load(self) -> Iterator[Document]:
-        for example in self._client.list_examples(
-            dataset_id=self.dataset_id,
-            dataset_name=self.dataset_name,
-            example_ids=self.example_ids,
-            as_of=self.as_of,
-            splits=self.splits,
-            inline_s3_urls=self.inline_s3_urls,
-            offset=self.offset,
-            limit=self.limit,
-            metadata=self.metadata,
-            filter=self.filter,
-        ):
-            content: Any = example.inputs
-            for key in self.content_key:
-                content = content[key]
-            content_str = self.format_content(content)
-            metadata = example.dict()
-            # Stringify datetime and UUID types.
-            for k in ("dataset_id", "created_at", "modified_at", "source_run_id", "id"):
-                metadata[k] = str(metadata[k]) if metadata[k] else metadata[k]
-            yield Document(content_str, metadata=metadata)
-
-
-def _stringify(x: Union[str, dict]) -> str:
-    if isinstance(x, str):
-        return x
-    else:
-        try:
-            return json.dumps(x, indent=2)
-        except Exception:
-            return str(x)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/documents/__init__.py ./custom_langchain_core/documents/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/documents/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/documents/__init__.py	1970-01-01 09:00:00
@@ -1,10 +0,0 @@
-"""**Document** module is a collection of classes that handle documents
-and their transformations.
-
-"""
-
-from langchain_core.documents.base import Document
-from langchain_core.documents.compressor import BaseDocumentCompressor
-from langchain_core.documents.transformers import BaseDocumentTransformer
-
-__all__ = ["Document", "BaseDocumentTransformer", "BaseDocumentCompressor"]
Binary files .venv/lib/python3.12/site-packages/langchain_core/documents/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/documents/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/documents/__pycache__/base.cpython-312.pyc and ./custom_langchain_core/documents/__pycache__/base.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/documents/__pycache__/compressor.cpython-312.pyc and ./custom_langchain_core/documents/__pycache__/compressor.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/documents/__pycache__/transformers.cpython-312.pyc and ./custom_langchain_core/documents/__pycache__/transformers.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/documents/base.py ./custom_langchain_core/documents/base.py
--- .venv/lib/python3.12/site-packages/langchain_core/documents/base.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/documents/base.py	1970-01-01 09:00:00
@@ -1,310 +0,0 @@
-from __future__ import annotations
-
-import contextlib
-import mimetypes
-from collections.abc import Generator
-from io import BufferedReader, BytesIO
-from pathlib import PurePath
-from typing import Any, Literal, Optional, Union, cast
-
-from pydantic import ConfigDict, Field, field_validator, model_validator
-
-from langchain_core.load.serializable import Serializable
-
-PathLike = Union[str, PurePath]
-
-
-class BaseMedia(Serializable):
-    """Use to represent media content.
-
-    Media objects can be used to represent raw data, such as text or binary data.
-
-    LangChain Media objects allow associating metadata and an optional identifier
-    with the content.
-
-    The presence of an ID and metadata make it easier to store, index, and search
-    over the content in a structured way.
-    """
-
-    # The ID field is optional at the moment.
-    # It will likely become required in a future major release after
-    # it has been adopted by enough vectorstore implementations.
-    id: Optional[str] = None
-    """An optional identifier for the document.
-
-    Ideally this should be unique across the document collection and formatted
-    as a UUID, but this will not be enforced.
-
-    .. versionadded:: 0.2.11
-    """
-
-    metadata: dict = Field(default_factory=dict)
-    """Arbitrary metadata associated with the content."""
-
-    @field_validator("id", mode="before")
-    def cast_id_to_str(cls, id_value: Any) -> Optional[str]:
-        if id_value is not None:
-            return str(id_value)
-        else:
-            return id_value
-
-
-class Blob(BaseMedia):
-    """Blob represents raw data by either reference or value.
-
-    Provides an interface to materialize the blob in different representations, and
-    help to decouple the development of data loaders from the downstream parsing of
-    the raw data.
-
-    Inspired by: https://developer.mozilla.org/en-US/docs/Web/API/Blob
-
-    Example: Initialize a blob from in-memory data
-
-        .. code-block:: python
-
-            from langchain_core.documents import Blob
-
-            blob = Blob.from_data("Hello, world!")
-
-            # Read the blob as a string
-            print(blob.as_string())
-
-            # Read the blob as bytes
-            print(blob.as_bytes())
-
-            # Read the blob as a byte stream
-            with blob.as_bytes_io() as f:
-                print(f.read())
-
-    Example: Load from memory and specify mime-type and metadata
-
-        .. code-block:: python
-
-            from langchain_core.documents import Blob
-
-            blob = Blob.from_data(
-                data="Hello, world!",
-                mime_type="text/plain",
-                metadata={"source": "https://example.com"}
-            )
-
-    Example: Load the blob from a file
-
-        .. code-block:: python
-
-            from langchain_core.documents import Blob
-
-            blob = Blob.from_path("path/to/file.txt")
-
-            # Read the blob as a string
-            print(blob.as_string())
-
-            # Read the blob as bytes
-            print(blob.as_bytes())
-
-            # Read the blob as a byte stream
-            with blob.as_bytes_io() as f:
-                print(f.read())
-    """
-
-    data: Union[bytes, str, None] = None
-    """Raw data associated with the blob."""
-    mimetype: Optional[str] = None
-    """MimeType not to be confused with a file extension."""
-    encoding: str = "utf-8"
-    """Encoding to use if decoding the bytes into a string.
-
-    Use utf-8 as default encoding, if decoding to string.
-    """
-    path: Optional[PathLike] = None
-    """Location where the original content was found."""
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-        frozen=True,
-    )
-
-    @property
-    def source(self) -> Optional[str]:
-        """The source location of the blob as string if known otherwise none.
-
-        If a path is associated with the blob, it will default to the path location.
-
-        Unless explicitly set via a metadata field called "source", in which
-        case that value will be used instead.
-        """
-        if self.metadata and "source" in self.metadata:
-            return cast(Optional[str], self.metadata["source"])
-        return str(self.path) if self.path else None
-
-    @model_validator(mode="before")
-    @classmethod
-    def check_blob_is_valid(cls, values: dict[str, Any]) -> Any:
-        """Verify that either data or path is provided."""
-        if "data" not in values and "path" not in values:
-            msg = "Either data or path must be provided"
-            raise ValueError(msg)
-        return values
-
-    def as_string(self) -> str:
-        """Read data as a string."""
-        if self.data is None and self.path:
-            with open(str(self.path), encoding=self.encoding) as f:
-                return f.read()
-        elif isinstance(self.data, bytes):
-            return self.data.decode(self.encoding)
-        elif isinstance(self.data, str):
-            return self.data
-        else:
-            msg = f"Unable to get string for blob {self}"
-            raise ValueError(msg)
-
-    def as_bytes(self) -> bytes:
-        """Read data as bytes."""
-        if isinstance(self.data, bytes):
-            return self.data
-        elif isinstance(self.data, str):
-            return self.data.encode(self.encoding)
-        elif self.data is None and self.path:
-            with open(str(self.path), "rb") as f:
-                return f.read()
-        else:
-            msg = f"Unable to get bytes for blob {self}"
-            raise ValueError(msg)
-
-    @contextlib.contextmanager
-    def as_bytes_io(self) -> Generator[Union[BytesIO, BufferedReader], None, None]:
-        """Read data as a byte stream."""
-        if isinstance(self.data, bytes):
-            yield BytesIO(self.data)
-        elif self.data is None and self.path:
-            with open(str(self.path), "rb") as f:
-                yield f
-        else:
-            msg = f"Unable to convert blob {self}"
-            raise NotImplementedError(msg)
-
-    @classmethod
-    def from_path(
-        cls,
-        path: PathLike,
-        *,
-        encoding: str = "utf-8",
-        mime_type: Optional[str] = None,
-        guess_type: bool = True,
-        metadata: Optional[dict] = None,
-    ) -> Blob:
-        """Load the blob from a path like object.
-
-        Args:
-            path: path like object to file to be read
-            encoding: Encoding to use if decoding the bytes into a string
-            mime_type: if provided, will be set as the mime-type of the data
-            guess_type: If True, the mimetype will be guessed from the file extension,
-                        if a mime-type was not provided
-            metadata: Metadata to associate with the blob
-
-        Returns:
-            Blob instance
-        """
-        if mime_type is None and guess_type:
-            _mimetype = mimetypes.guess_type(path)[0] if guess_type else None
-        else:
-            _mimetype = mime_type
-        # We do not load the data immediately, instead we treat the blob as a
-        # reference to the underlying data.
-        return cls(
-            data=None,
-            mimetype=_mimetype,
-            encoding=encoding,
-            path=path,
-            metadata=metadata if metadata is not None else {},
-        )
-
-    @classmethod
-    def from_data(
-        cls,
-        data: Union[str, bytes],
-        *,
-        encoding: str = "utf-8",
-        mime_type: Optional[str] = None,
-        path: Optional[str] = None,
-        metadata: Optional[dict] = None,
-    ) -> Blob:
-        """Initialize the blob from in-memory data.
-
-        Args:
-            data: the in-memory data associated with the blob
-            encoding: Encoding to use if decoding the bytes into a string
-            mime_type: if provided, will be set as the mime-type of the data
-            path: if provided, will be set as the source from which the data came
-            metadata: Metadata to associate with the blob
-
-        Returns:
-            Blob instance
-        """
-        return cls(
-            data=data,
-            mimetype=mime_type,
-            encoding=encoding,
-            path=path,
-            metadata=metadata if metadata is not None else {},
-        )
-
-    def __repr__(self) -> str:
-        """Define the blob representation."""
-        str_repr = f"Blob {id(self)}"
-        if self.source:
-            str_repr += f" {self.source}"
-        return str_repr
-
-
-class Document(BaseMedia):
-    """Class for storing a piece of text and associated metadata.
-
-    Example:
-
-        .. code-block:: python
-
-            from langchain_core.documents import Document
-
-            document = Document(
-                page_content="Hello, world!",
-                metadata={"source": "https://example.com"}
-            )
-    """
-
-    page_content: str
-    """String text."""
-    type: Literal["Document"] = "Document"
-
-    def __init__(self, page_content: str, **kwargs: Any) -> None:
-        """Pass page_content in as positional or named arg."""
-        # my-py is complaining that page_content is not defined on the base class.
-        # Here, we're relying on pydantic base class to handle the validation.
-        super().__init__(page_content=page_content, **kwargs)  # type: ignore[call-arg]
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Return whether this class is serializable."""
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "document"]
-
-    def __str__(self) -> str:
-        """Override __str__ to restrict it to page_content and metadata."""
-        # The format matches pydantic format for __str__.
-        #
-        # The purpose of this change is to make sure that user code that
-        # feeds Document objects directly into prompts remains unchanged
-        # due to the addition of the id field (or any other fields in the future).
-        #
-        # This override will likely be removed in the future in favor of
-        # a more general solution of formatting content directly inside the prompts.
-        if self.metadata:
-            return f"page_content='{self.page_content}' metadata={self.metadata}"
-        else:
-            return f"page_content='{self.page_content}'"
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/documents/compressor.py ./custom_langchain_core/documents/compressor.py
--- .venv/lib/python3.12/site-packages/langchain_core/documents/compressor.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/documents/compressor.py	1970-01-01 09:00:00
@@ -1,66 +0,0 @@
-from __future__ import annotations
-
-from abc import ABC, abstractmethod
-from collections.abc import Sequence
-from typing import Optional
-
-from pydantic import BaseModel
-
-from langchain_core.callbacks import Callbacks
-from langchain_core.documents import Document
-from langchain_core.runnables import run_in_executor
-
-
-class BaseDocumentCompressor(BaseModel, ABC):
-    """Base class for document compressors.
-
-    This abstraction is primarily used for
-    post-processing of retrieved documents.
-
-    Documents matching a given query are first retrieved.
-    Then the list of documents can be further processed.
-
-    For example, one could re-rank the retrieved documents
-    using an LLM.
-
-    **Note** users should favor using a RunnableLambda
-    instead of sub-classing from this interface.
-    """
-
-    @abstractmethod
-    def compress_documents(
-        self,
-        documents: Sequence[Document],
-        query: str,
-        callbacks: Optional[Callbacks] = None,
-    ) -> Sequence[Document]:
-        """Compress retrieved documents given the query context.
-
-        Args:
-            documents: The retrieved documents.
-            query: The query context.
-            callbacks: Optional callbacks to run during compression.
-
-        Returns:
-            The compressed documents.
-        """
-
-    async def acompress_documents(
-        self,
-        documents: Sequence[Document],
-        query: str,
-        callbacks: Optional[Callbacks] = None,
-    ) -> Sequence[Document]:
-        """Async compress retrieved documents given the query context.
-
-        Args:
-            documents: The retrieved documents.
-            query: The query context.
-            callbacks: Optional callbacks to run during compression.
-
-        Returns:
-            The compressed documents.
-        """
-        return await run_in_executor(
-            None, self.compress_documents, documents, query, callbacks
-        )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/documents/transformers.py ./custom_langchain_core/documents/transformers.py
--- .venv/lib/python3.12/site-packages/langchain_core/documents/transformers.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/documents/transformers.py	1970-01-01 09:00:00
@@ -1,75 +0,0 @@
-from __future__ import annotations
-
-from abc import ABC, abstractmethod
-from collections.abc import Sequence
-from typing import TYPE_CHECKING, Any
-
-from langchain_core.runnables.config import run_in_executor
-
-if TYPE_CHECKING:
-    from langchain_core.documents import Document
-
-
-class BaseDocumentTransformer(ABC):
-    """Abstract base class for document transformation.
-
-    A document transformation takes a sequence of Documents and returns a
-    sequence of transformed Documents.
-
-    Example:
-        .. code-block:: python
-
-            class EmbeddingsRedundantFilter(BaseDocumentTransformer, BaseModel):
-                embeddings: Embeddings
-                similarity_fn: Callable = cosine_similarity
-                similarity_threshold: float = 0.95
-
-                class Config:
-                    arbitrary_types_allowed = True
-
-                def transform_documents(
-                    self, documents: Sequence[Document], **kwargs: Any
-                ) -> Sequence[Document]:
-                    stateful_documents = get_stateful_documents(documents)
-                    embedded_documents = _get_embeddings_from_stateful_docs(
-                        self.embeddings, stateful_documents
-                    )
-                    included_idxs = _filter_similar_embeddings(
-                        embedded_documents, self.similarity_fn, self.similarity_threshold
-                    )
-                    return [stateful_documents[i] for i in sorted(included_idxs)]
-
-                async def atransform_documents(
-                    self, documents: Sequence[Document], **kwargs: Any
-                ) -> Sequence[Document]:
-                    raise NotImplementedError
-
-    """  # noqa: E501
-
-    @abstractmethod
-    def transform_documents(
-        self, documents: Sequence[Document], **kwargs: Any
-    ) -> Sequence[Document]:
-        """Transform a list of documents.
-
-        Args:
-            documents: A sequence of Documents to be transformed.
-
-        Returns:
-            A sequence of transformed Documents.
-        """
-
-    async def atransform_documents(
-        self, documents: Sequence[Document], **kwargs: Any
-    ) -> Sequence[Document]:
-        """Asynchronously transform a list of documents.
-
-        Args:
-            documents: A sequence of Documents to be transformed.
-
-        Returns:
-            A sequence of transformed Documents.
-        """
-        return await run_in_executor(
-            None, self.transform_documents, documents, **kwargs
-        )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/embeddings/__init__.py ./custom_langchain_core/embeddings/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/embeddings/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/embeddings/__init__.py	1970-01-01 09:00:00
@@ -1,4 +0,0 @@
-from langchain_core.embeddings.embeddings import Embeddings
-from langchain_core.embeddings.fake import DeterministicFakeEmbedding, FakeEmbeddings
-
-__all__ = ["DeterministicFakeEmbedding", "Embeddings", "FakeEmbeddings"]
Binary files .venv/lib/python3.12/site-packages/langchain_core/embeddings/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/embeddings/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/embeddings/__pycache__/embeddings.cpython-312.pyc and ./custom_langchain_core/embeddings/__pycache__/embeddings.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/embeddings/__pycache__/fake.cpython-312.pyc and ./custom_langchain_core/embeddings/__pycache__/fake.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/embeddings/embeddings.py ./custom_langchain_core/embeddings/embeddings.py
--- .venv/lib/python3.12/site-packages/langchain_core/embeddings/embeddings.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/embeddings/embeddings.py	1970-01-01 09:00:00
@@ -1,78 +0,0 @@
-"""**Embeddings** interface."""
-
-from abc import ABC, abstractmethod
-
-from langchain_core.runnables.config import run_in_executor
-
-
-class Embeddings(ABC):
-    """Interface for embedding models.
-
-    This is an interface meant for implementing text embedding models.
-
-    Text embedding models are used to map text to a vector (a point in n-dimensional
-    space).
-
-    Texts that are similar will usually be mapped to points that are close to each
-    other in this space. The exact details of what's considered "similar" and how
-    "distance" is measured in this space are dependent on the specific embedding model.
-
-    This abstraction contains a method for embedding a list of documents and a method
-    for embedding a query text. The embedding of a query text is expected to be a single
-    vector, while the embedding of a list of documents is expected to be a list of
-    vectors.
-
-    Usually the query embedding is identical to the document embedding, but the
-    abstraction allows treating them independently.
-
-    In addition to the synchronous methods, this interface also provides asynchronous
-    versions of the methods.
-
-    By default, the asynchronous methods are implemented using the synchronous methods;
-    however, implementations may choose to override the asynchronous methods with
-    an async native implementation for performance reasons.
-    """
-
-    @abstractmethod
-    def embed_documents(self, texts: list[str]) -> list[list[float]]:
-        """Embed search docs.
-
-        Args:
-            texts: List of text to embed.
-
-        Returns:
-            List of embeddings.
-        """
-
-    @abstractmethod
-    def embed_query(self, text: str) -> list[float]:
-        """Embed query text.
-
-        Args:
-            text: Text to embed.
-
-        Returns:
-            Embedding.
-        """
-
-    async def aembed_documents(self, texts: list[str]) -> list[list[float]]:
-        """Asynchronous Embed search docs.
-
-        Args:
-            texts: List of text to embed.
-
-        Returns:
-            List of embeddings.
-        """
-        return await run_in_executor(None, self.embed_documents, texts)
-
-    async def aembed_query(self, text: str) -> list[float]:
-        """Asynchronous Embed query text.
-
-        Args:
-            text: Text to embed.
-
-        Returns:
-            Embedding.
-        """
-        return await run_in_executor(None, self.embed_query, text)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/embeddings/fake.py ./custom_langchain_core/embeddings/fake.py
--- .venv/lib/python3.12/site-packages/langchain_core/embeddings/fake.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/embeddings/fake.py	1970-01-01 09:00:00
@@ -1,123 +0,0 @@
-"""Module contains a few fake embedding models for testing purposes."""
-
-# Please do not add additional fake embedding model implementations here.
-import hashlib
-
-from pydantic import BaseModel
-
-from langchain_core.embeddings import Embeddings
-
-
-class FakeEmbeddings(Embeddings, BaseModel):
-    """Fake embedding model for unit testing purposes.
-
-    This embedding model creates embeddings by sampling from a normal distribution.
-
-    Do not use this outside of testing, as it is not a real embedding model.
-
-    Instantiate:
-        .. code-block:: python
-
-            from langchain_core.embeddings import FakeEmbeddings
-            embed = FakeEmbeddings(size=100)
-
-    Embed single text:
-        .. code-block:: python
-
-            input_text = "The meaning of life is 42"
-            vector = embed.embed_query(input_text)
-            print(vector[:3])
-
-        .. code-block:: python
-
-            [-0.700234640213188, -0.581266257710429, -1.1328482266445354]
-
-    Embed multiple texts:
-        .. code-block:: python
-
-            input_texts = ["Document 1...", "Document 2..."]
-            vectors = embed.embed_documents(input_texts)
-            print(len(vectors))
-            # The first 3 coordinates for the first vector
-            print(vectors[0][:3])
-
-        .. code-block:: python
-
-            2
-            [-0.5670477847544458, -0.31403828652395727, -0.5840547508955257]
-    """
-
-    size: int
-    """The size of the embedding vector."""
-
-    def _get_embedding(self) -> list[float]:
-        import numpy as np  # type: ignore[import-not-found, import-untyped]
-
-        return list(np.random.default_rng().normal(size=self.size))
-
-    def embed_documents(self, texts: list[str]) -> list[list[float]]:
-        return [self._get_embedding() for _ in texts]
-
-    def embed_query(self, text: str) -> list[float]:
-        return self._get_embedding()
-
-
-class DeterministicFakeEmbedding(Embeddings, BaseModel):
-    """Deterministic fake embedding model for unit testing purposes.
-
-    This embedding model creates embeddings by sampling from a normal distribution
-    with a seed based on the hash of the text.
-
-    Do not use this outside of testing, as it is not a real embedding model.
-
-    Instantiate:
-        .. code-block:: python
-
-            from langchain_core.embeddings import DeterministicFakeEmbedding
-            embed = DeterministicFakeEmbedding(size=100)
-
-    Embed single text:
-        .. code-block:: python
-
-            input_text = "The meaning of life is 42"
-            vector = embed.embed_query(input_text)
-            print(vector[:3])
-
-        .. code-block:: python
-
-            [-0.700234640213188, -0.581266257710429, -1.1328482266445354]
-
-    Embed multiple texts:
-        .. code-block:: python
-
-            input_texts = ["Document 1...", "Document 2..."]
-            vectors = embed.embed_documents(input_texts)
-            print(len(vectors))
-            # The first 3 coordinates for the first vector
-            print(vectors[0][:3])
-
-        .. code-block:: python
-
-            2
-            [-0.5670477847544458, -0.31403828652395727, -0.5840547508955257]
-    """
-
-    size: int
-    """The size of the embedding vector."""
-
-    def _get_embedding(self, seed: int) -> list[float]:
-        import numpy as np  # type: ignore[import-not-found, import-untyped]
-
-        # set the seed for the random generator
-        rng = np.random.default_rng(seed)
-        return list(rng.normal(size=self.size))
-
-    def _get_seed(self, text: str) -> int:
-        """Get a seed for the random generator, using the hash of the text."""
-        return int(hashlib.sha256(text.encode("utf-8")).hexdigest(), 16) % 10**8
-
-    def embed_documents(self, texts: list[str]) -> list[list[float]]:
-        return [self._get_embedding(seed=self._get_seed(_)) for _ in texts]
-
-    def embed_query(self, text: str) -> list[float]:
-        return self._get_embedding(seed=self._get_seed(text))
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/env.py ./custom_langchain_core/env.py
--- .venv/lib/python3.12/site-packages/langchain_core/env.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/env.py	1970-01-01 09:00:00
@@ -1,21 +0,0 @@
-import platform
-from functools import lru_cache
-
-
-@lru_cache(maxsize=1)
-def get_runtime_environment() -> dict:
-    """Get information about the LangChain runtime environment.
-
-    Returns:
-        A dictionary with information about the runtime environment.
-    """
-    # Lazy import to avoid circular imports
-    from langchain_core import __version__
-
-    return {
-        "library_version": __version__,
-        "library": "langchain-core",
-        "platform": platform.platform(),
-        "runtime": "python",
-        "runtime_version": platform.python_version(),
-    }
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/example_selectors/__init__.py ./custom_langchain_core/example_selectors/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/example_selectors/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/example_selectors/__init__.py	1970-01-01 09:00:00
@@ -1,22 +0,0 @@
-"""**Example selector** implements logic for selecting examples to include them
-in prompts.
-This allows us to select examples that are most relevant to the input.
-"""
-
-from langchain_core.example_selectors.base import BaseExampleSelector
-from langchain_core.example_selectors.length_based import (
-    LengthBasedExampleSelector,
-)
-from langchain_core.example_selectors.semantic_similarity import (
-    MaxMarginalRelevanceExampleSelector,
-    SemanticSimilarityExampleSelector,
-    sorted_values,
-)
-
-__all__ = [
-    "BaseExampleSelector",
-    "LengthBasedExampleSelector",
-    "MaxMarginalRelevanceExampleSelector",
-    "SemanticSimilarityExampleSelector",
-    "sorted_values",
-]
Binary files .venv/lib/python3.12/site-packages/langchain_core/example_selectors/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/example_selectors/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/example_selectors/__pycache__/base.cpython-312.pyc and ./custom_langchain_core/example_selectors/__pycache__/base.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/example_selectors/__pycache__/length_based.cpython-312.pyc and ./custom_langchain_core/example_selectors/__pycache__/length_based.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/example_selectors/__pycache__/semantic_similarity.cpython-312.pyc and ./custom_langchain_core/example_selectors/__pycache__/semantic_similarity.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/example_selectors/base.py ./custom_langchain_core/example_selectors/base.py
--- .venv/lib/python3.12/site-packages/langchain_core/example_selectors/base.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/example_selectors/base.py	1970-01-01 09:00:00
@@ -1,46 +0,0 @@
-"""Interface for selecting examples to include in prompts."""
-
-from abc import ABC, abstractmethod
-from typing import Any
-
-from langchain_core.runnables import run_in_executor
-
-
-class BaseExampleSelector(ABC):
-    """Interface for selecting examples to include in prompts."""
-
-    @abstractmethod
-    def add_example(self, example: dict[str, str]) -> Any:
-        """Add new example to store.
-
-        Args:
-            example: A dictionary with keys as input variables
-                and values as their values.
-        """
-
-    async def aadd_example(self, example: dict[str, str]) -> Any:
-        """Async add new example to store.
-
-        Args:
-            example: A dictionary with keys as input variables
-                and values as their values.
-        """
-        return await run_in_executor(None, self.add_example, example)
-
-    @abstractmethod
-    def select_examples(self, input_variables: dict[str, str]) -> list[dict]:
-        """Select which examples to use based on the inputs.
-
-        Args:
-            input_variables: A dictionary with keys as input variables
-                and values as their values.
-        """
-
-    async def aselect_examples(self, input_variables: dict[str, str]) -> list[dict]:
-        """Async select which examples to use based on the inputs.
-
-        Args:
-            input_variables: A dictionary with keys as input variables
-                and values as their values.
-        """
-        return await run_in_executor(None, self.select_examples, input_variables)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/example_selectors/length_based.py ./custom_langchain_core/example_selectors/length_based.py
--- .venv/lib/python3.12/site-packages/langchain_core/example_selectors/length_based.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/example_selectors/length_based.py	1970-01-01 09:00:00
@@ -1,98 +0,0 @@
-"""Select examples based on length."""
-
-import re
-from typing import Callable
-
-from pydantic import BaseModel, Field, model_validator
-from typing_extensions import Self
-
-from langchain_core.example_selectors.base import BaseExampleSelector
-from langchain_core.prompts.prompt import PromptTemplate
-
-
-def _get_length_based(text: str) -> int:
-    return len(re.split("\n| ", text))
-
-
-class LengthBasedExampleSelector(BaseExampleSelector, BaseModel):
-    """Select examples based on length."""
-
-    examples: list[dict]
-    """A list of the examples that the prompt template expects."""
-
-    example_prompt: PromptTemplate
-    """Prompt template used to format the examples."""
-
-    get_text_length: Callable[[str], int] = _get_length_based
-    """Function to measure prompt length. Defaults to word count."""
-
-    max_length: int = 2048
-    """Max length for the prompt, beyond which examples are cut."""
-
-    example_text_lengths: list[int] = Field(default_factory=list)  # :meta private:
-    """Length of each example."""
-
-    def add_example(self, example: dict[str, str]) -> None:
-        """Add new example to list.
-
-        Args:
-            example: A dictionary with keys as input variables
-                and values as their values.
-        """
-        self.examples.append(example)
-        string_example = self.example_prompt.format(**example)
-        self.example_text_lengths.append(self.get_text_length(string_example))
-
-    async def aadd_example(self, example: dict[str, str]) -> None:
-        """Async add new example to list.
-
-        Args:
-            example: A dictionary with keys as input variables
-                and values as their values.
-        """
-        self.add_example(example)
-
-    @model_validator(mode="after")
-    def post_init(self) -> Self:
-        """Validate that the examples are formatted correctly."""
-        if self.example_text_lengths:
-            return self
-        string_examples = [self.example_prompt.format(**eg) for eg in self.examples]
-        self.example_text_lengths = [self.get_text_length(eg) for eg in string_examples]
-        return self
-
-    def select_examples(self, input_variables: dict[str, str]) -> list[dict]:
-        """Select which examples to use based on the input lengths.
-
-        Args:
-            input_variables: A dictionary with keys as input variables
-               and values as their values.
-
-        Returns:
-            A list of examples to include in the prompt.
-        """
-        inputs = " ".join(input_variables.values())
-        remaining_length = self.max_length - self.get_text_length(inputs)
-        i = 0
-        examples = []
-        while remaining_length > 0 and i < len(self.examples):
-            new_length = remaining_length - self.example_text_lengths[i]
-            if new_length < 0:
-                break
-            else:
-                examples.append(self.examples[i])
-                remaining_length = new_length
-            i += 1
-        return examples
-
-    async def aselect_examples(self, input_variables: dict[str, str]) -> list[dict]:
-        """Async select which examples to use based on the input lengths.
-
-        Args:
-            input_variables: A dictionary with keys as input variables
-               and values as their values.
-
-        Returns:
-            A list of examples to include in the prompt.
-        """
-        return self.select_examples(input_variables)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/example_selectors/semantic_similarity.py ./custom_langchain_core/example_selectors/semantic_similarity.py
--- .venv/lib/python3.12/site-packages/langchain_core/example_selectors/semantic_similarity.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/example_selectors/semantic_similarity.py	1970-01-01 09:00:00
@@ -1,364 +0,0 @@
-"""Example selector that selects examples based on SemanticSimilarity."""
-
-from __future__ import annotations
-
-from abc import ABC
-from typing import TYPE_CHECKING, Any, Optional
-
-from pydantic import BaseModel, ConfigDict
-
-from langchain_core.documents import Document
-from langchain_core.example_selectors.base import BaseExampleSelector
-from langchain_core.vectorstores import VectorStore
-
-if TYPE_CHECKING:
-    from langchain_core.embeddings import Embeddings
-
-
-def sorted_values(values: dict[str, str]) -> list[Any]:
-    """Return a list of values in dict sorted by key.
-
-    Args:
-        values: A dictionary with keys as input variables
-            and values as their values.
-
-    Returns:
-        A list of values in dict sorted by key.
-    """
-    return [values[val] for val in sorted(values)]
-
-
-class _VectorStoreExampleSelector(BaseExampleSelector, BaseModel, ABC):
-    """Example selector that selects examples based on SemanticSimilarity."""
-
-    vectorstore: VectorStore
-    """VectorStore that contains information about examples."""
-    k: int = 4
-    """Number of examples to select."""
-    example_keys: Optional[list[str]] = None
-    """Optional keys to filter examples to."""
-    input_keys: Optional[list[str]] = None
-    """Optional keys to filter input to. If provided, the search is based on
-    the input variables instead of all variables."""
-    vectorstore_kwargs: Optional[dict[str, Any]] = None
-    """Extra arguments passed to similarity_search function of the vectorstore."""
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-        extra="forbid",
-    )
-
-    @staticmethod
-    def _example_to_text(
-        example: dict[str, str], input_keys: Optional[list[str]]
-    ) -> str:
-        if input_keys:
-            return " ".join(sorted_values({key: example[key] for key in input_keys}))
-        else:
-            return " ".join(sorted_values(example))
-
-    def _documents_to_examples(self, documents: list[Document]) -> list[dict]:
-        # Get the examples from the metadata.
-        # This assumes that examples are stored in metadata.
-        examples = [dict(e.metadata) for e in documents]
-        # If example keys are provided, filter examples to those keys.
-        if self.example_keys:
-            examples = [{k: eg[k] for k in self.example_keys} for eg in examples]
-        return examples
-
-    def add_example(self, example: dict[str, str]) -> str:
-        """Add a new example to vectorstore.
-
-        Args:
-            example: A dictionary with keys as input variables
-                and values as their values.
-
-        Returns:
-            The ID of the added example.
-        """
-        ids = self.vectorstore.add_texts(
-            [self._example_to_text(example, self.input_keys)], metadatas=[example]
-        )
-        return ids[0]
-
-    async def aadd_example(self, example: dict[str, str]) -> str:
-        """Async add new example to vectorstore.
-
-        Args:
-            example: A dictionary with keys as input variables
-                and values as their values.
-
-        Returns:
-            The ID of the added example.
-        """
-        ids = await self.vectorstore.aadd_texts(
-            [self._example_to_text(example, self.input_keys)], metadatas=[example]
-        )
-        return ids[0]
-
-
-class SemanticSimilarityExampleSelector(_VectorStoreExampleSelector):
-    """Select examples based on semantic similarity."""
-
-    def select_examples(self, input_variables: dict[str, str]) -> list[dict]:
-        """Select examples based on semantic similarity.
-
-        Args:
-            input_variables: The input variables to use for search.
-
-        Returns:
-            The selected examples.
-        """
-        # Get the docs with the highest similarity.
-        vectorstore_kwargs = self.vectorstore_kwargs or {}
-        example_docs = self.vectorstore.similarity_search(
-            self._example_to_text(input_variables, self.input_keys),
-            k=self.k,
-            **vectorstore_kwargs,
-        )
-        return self._documents_to_examples(example_docs)
-
-    async def aselect_examples(self, input_variables: dict[str, str]) -> list[dict]:
-        """Asynchronously select examples based on semantic similarity.
-
-        Args:
-            input_variables: The input variables to use for search.
-
-        Returns:
-            The selected examples.
-        """
-        # Get the docs with the highest similarity.
-        vectorstore_kwargs = self.vectorstore_kwargs or {}
-        example_docs = await self.vectorstore.asimilarity_search(
-            self._example_to_text(input_variables, self.input_keys),
-            k=self.k,
-            **vectorstore_kwargs,
-        )
-        return self._documents_to_examples(example_docs)
-
-    @classmethod
-    def from_examples(
-        cls,
-        examples: list[dict],
-        embeddings: Embeddings,
-        vectorstore_cls: type[VectorStore],
-        k: int = 4,
-        input_keys: Optional[list[str]] = None,
-        *,
-        example_keys: Optional[list[str]] = None,
-        vectorstore_kwargs: Optional[dict] = None,
-        **vectorstore_cls_kwargs: Any,
-    ) -> SemanticSimilarityExampleSelector:
-        """Create k-shot example selector using example list and embeddings.
-
-        Reshuffles examples dynamically based on query similarity.
-
-        Args:
-            examples: List of examples to use in the prompt.
-            embeddings: An initialized embedding API interface, e.g. OpenAIEmbeddings().
-            vectorstore_cls: A vector store DB interface class, e.g. FAISS.
-            k: Number of examples to select. Default is 4.
-            input_keys: If provided, the search is based on the input variables
-                instead of all variables.
-            example_keys: If provided, keys to filter examples to.
-            vectorstore_kwargs: Extra arguments passed to similarity_search function
-                of the vectorstore.
-            vectorstore_cls_kwargs: optional kwargs containing url for vector store
-
-        Returns:
-            The ExampleSelector instantiated, backed by a vector store.
-        """
-        string_examples = [cls._example_to_text(eg, input_keys) for eg in examples]
-        vectorstore = vectorstore_cls.from_texts(
-            string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs
-        )
-        return cls(
-            vectorstore=vectorstore,
-            k=k,
-            input_keys=input_keys,
-            example_keys=example_keys,
-            vectorstore_kwargs=vectorstore_kwargs,
-        )
-
-    @classmethod
-    async def afrom_examples(
-        cls,
-        examples: list[dict],
-        embeddings: Embeddings,
-        vectorstore_cls: type[VectorStore],
-        k: int = 4,
-        input_keys: Optional[list[str]] = None,
-        *,
-        example_keys: Optional[list[str]] = None,
-        vectorstore_kwargs: Optional[dict] = None,
-        **vectorstore_cls_kwargs: Any,
-    ) -> SemanticSimilarityExampleSelector:
-        """Async create k-shot example selector using example list and embeddings.
-
-        Reshuffles examples dynamically based on query similarity.
-
-        Args:
-            examples: List of examples to use in the prompt.
-            embeddings: An initialized embedding API interface, e.g. OpenAIEmbeddings().
-            vectorstore_cls: A vector store DB interface class, e.g. FAISS.
-            k: Number of examples to select. Default is 4.
-            input_keys: If provided, the search is based on the input variables
-                instead of all variables.
-            example_keys: If provided, keys to filter examples to.
-            vectorstore_kwargs: Extra arguments passed to similarity_search function
-                of the vectorstore.
-            vectorstore_cls_kwargs: optional kwargs containing url for vector store
-
-        Returns:
-            The ExampleSelector instantiated, backed by a vector store.
-        """
-        string_examples = [cls._example_to_text(eg, input_keys) for eg in examples]
-        vectorstore = await vectorstore_cls.afrom_texts(
-            string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs
-        )
-        return cls(
-            vectorstore=vectorstore,
-            k=k,
-            input_keys=input_keys,
-            example_keys=example_keys,
-            vectorstore_kwargs=vectorstore_kwargs,
-        )
-
-
-class MaxMarginalRelevanceExampleSelector(_VectorStoreExampleSelector):
-    """Select examples based on Max Marginal Relevance.
-
-    This was shown to improve performance in this paper:
-    https://arxiv.org/pdf/2211.13892.pdf
-    """
-
-    fetch_k: int = 20
-    """Number of examples to fetch to rerank."""
-
-    def select_examples(self, input_variables: dict[str, str]) -> list[dict]:
-        """Select examples based on Max Marginal Relevance.
-
-        Args:
-            input_variables: The input variables to use for search.
-
-        Returns:
-            The selected examples.
-        """
-        example_docs = self.vectorstore.max_marginal_relevance_search(
-            self._example_to_text(input_variables, self.input_keys),
-            k=self.k,
-            fetch_k=self.fetch_k,
-        )
-        return self._documents_to_examples(example_docs)
-
-    async def aselect_examples(self, input_variables: dict[str, str]) -> list[dict]:
-        """Asynchronously select examples based on Max Marginal Relevance.
-
-        Args:
-            input_variables: The input variables to use for search.
-
-        Returns:
-            The selected examples.
-        """
-        example_docs = await self.vectorstore.amax_marginal_relevance_search(
-            self._example_to_text(input_variables, self.input_keys),
-            k=self.k,
-            fetch_k=self.fetch_k,
-        )
-        return self._documents_to_examples(example_docs)
-
-    @classmethod
-    def from_examples(
-        cls,
-        examples: list[dict],
-        embeddings: Embeddings,
-        vectorstore_cls: type[VectorStore],
-        k: int = 4,
-        input_keys: Optional[list[str]] = None,
-        fetch_k: int = 20,
-        example_keys: Optional[list[str]] = None,
-        vectorstore_kwargs: Optional[dict] = None,
-        **vectorstore_cls_kwargs: Any,
-    ) -> MaxMarginalRelevanceExampleSelector:
-        """Create k-shot example selector using example list and embeddings.
-
-        Reshuffles examples dynamically based on Max Marginal Relevance.
-
-        Args:
-            examples: List of examples to use in the prompt.
-            embeddings: An initialized embedding API interface, e.g. OpenAIEmbeddings().
-            vectorstore_cls: A vector store DB interface class, e.g. FAISS.
-            k: Number of examples to select. Default is 4.
-            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
-                Default is 20.
-            input_keys: If provided, the search is based on the input variables
-                instead of all variables.
-            example_keys: If provided, keys to filter examples to.
-            vectorstore_kwargs: Extra arguments passed to similarity_search function
-                of the vectorstore.
-            vectorstore_cls_kwargs: optional kwargs containing url for vector store
-
-        Returns:
-            The ExampleSelector instantiated, backed by a vector store.
-        """
-        string_examples = [cls._example_to_text(eg, input_keys) for eg in examples]
-        vectorstore = vectorstore_cls.from_texts(
-            string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs
-        )
-        return cls(
-            vectorstore=vectorstore,
-            k=k,
-            fetch_k=fetch_k,
-            input_keys=input_keys,
-            example_keys=example_keys,
-            vectorstore_kwargs=vectorstore_kwargs,
-        )
-
-    @classmethod
-    async def afrom_examples(
-        cls,
-        examples: list[dict],
-        embeddings: Embeddings,
-        vectorstore_cls: type[VectorStore],
-        *,
-        k: int = 4,
-        input_keys: Optional[list[str]] = None,
-        fetch_k: int = 20,
-        example_keys: Optional[list[str]] = None,
-        vectorstore_kwargs: Optional[dict] = None,
-        **vectorstore_cls_kwargs: Any,
-    ) -> MaxMarginalRelevanceExampleSelector:
-        """Asynchronously create k-shot example selector using example list and
-        embeddings.
-
-        Reshuffles examples dynamically based on Max Marginal Relevance.
-
-        Args:
-            examples: List of examples to use in the prompt.
-            embeddings: An initialized embedding API interface, e.g. OpenAIEmbeddings().
-            vectorstore_cls: A vector store DB interface class, e.g. FAISS.
-            k: Number of examples to select. Default is 4.
-            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
-                Default is 20.
-            input_keys: If provided, the search is based on the input variables
-                instead of all variables.
-            example_keys: If provided, keys to filter examples to.
-            vectorstore_kwargs: Extra arguments passed to similarity_search function
-                of the vectorstore.
-            vectorstore_cls_kwargs: optional kwargs containing url for vector store
-
-        Returns:
-            The ExampleSelector instantiated, backed by a vector store.
-        """
-        string_examples = [cls._example_to_text(eg, input_keys) for eg in examples]
-        vectorstore = await vectorstore_cls.afrom_texts(
-            string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs
-        )
-        return cls(
-            vectorstore=vectorstore,
-            k=k,
-            fetch_k=fetch_k,
-            input_keys=input_keys,
-            example_keys=example_keys,
-            vectorstore_kwargs=vectorstore_kwargs,
-        )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/exceptions.py ./custom_langchain_core/exceptions.py
--- .venv/lib/python3.12/site-packages/langchain_core/exceptions.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/exceptions.py	1970-01-01 09:00:00
@@ -1,74 +0,0 @@
-"""Custom **exceptions** for LangChain."""
-
-from enum import Enum
-from typing import Any, Optional
-
-
-class LangChainException(Exception):  # noqa: N818
-    """General LangChain exception."""
-
-
-class TracerException(LangChainException):
-    """Base class for exceptions in tracers module."""
-
-
-class OutputParserException(ValueError, LangChainException):  # noqa: N818
-    """Exception that output parsers should raise to signify a parsing error.
-
-    This exists to differentiate parsing errors from other code or execution errors
-    that also may arise inside the output parser. OutputParserExceptions will be
-    available to catch and handle in ways to fix the parsing error, while other
-    errors will be raised.
-
-    Parameters:
-        error: The error that's being re-raised or an error message.
-        observation: String explanation of error which can be passed to a
-            model to try and remediate the issue. Defaults to None.
-        llm_output: String model output which is error-ing.
-            Defaults to None.
-        send_to_llm: Whether to send the observation and llm_output back to an Agent
-            after an OutputParserException has been raised. This gives the underlying
-            model driving the agent the context that the previous output was improperly
-            structured, in the hopes that it will update the output to the correct
-            format. Defaults to False.
-    """
-
-    def __init__(
-        self,
-        error: Any,
-        observation: Optional[str] = None,
-        llm_output: Optional[str] = None,
-        send_to_llm: bool = False,
-    ):
-        if isinstance(error, str):
-            error = create_message(
-                message=error, error_code=ErrorCode.OUTPUT_PARSING_FAILURE
-            )
-        super().__init__(error)
-        if send_to_llm and (observation is None or llm_output is None):
-            msg = (
-                "Arguments 'observation' & 'llm_output'"
-                " are required if 'send_to_llm' is True"
-            )
-            raise ValueError(msg)
-        self.observation = observation
-        self.llm_output = llm_output
-        self.send_to_llm = send_to_llm
-
-
-class ErrorCode(Enum):
-    INVALID_PROMPT_INPUT = "INVALID_PROMPT_INPUT"
-    INVALID_TOOL_RESULTS = "INVALID_TOOL_RESULTS"
-    MESSAGE_COERCION_FAILURE = "MESSAGE_COERCION_FAILURE"
-    MODEL_AUTHENTICATION = "MODEL_AUTHENTICATION"
-    MODEL_NOT_FOUND = "MODEL_NOT_FOUND"
-    MODEL_RATE_LIMIT = "MODEL_RATE_LIMIT"
-    OUTPUT_PARSING_FAILURE = "OUTPUT_PARSING_FAILURE"
-
-
-def create_message(*, message: str, error_code: ErrorCode) -> str:
-    return (
-        f"{message}\n"
-        "For troubleshooting, visit: https://python.langchain.com/docs/"
-        f"troubleshooting/errors/{error_code.value} "
-    )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/globals.py ./custom_langchain_core/globals.py
--- .venv/lib/python3.12/site-packages/langchain_core/globals.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/globals.py	1970-01-01 09:00:00
@@ -1,222 +0,0 @@
-# flake8: noqa
-"""Global values and configuration that apply to all of LangChain."""
-
-import warnings
-from typing import TYPE_CHECKING, Optional
-
-if TYPE_CHECKING:
-    from langchain_core.caches import BaseCache
-
-
-# DO NOT USE THESE VALUES DIRECTLY!
-# Use them only via `get_<X>()` and `set_<X>()` below,
-# or else your code may behave unexpectedly with other uses of these global settings:
-# https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004
-_verbose: bool = False
-_debug: bool = False
-_llm_cache: Optional["BaseCache"] = None
-
-
-def set_verbose(value: bool) -> None:
-    """Set a new value for the `verbose` global setting.
-
-    Args:
-        value: The new value for the `verbose` global setting.
-    """
-    try:
-        import langchain  # type: ignore[import]
-
-        # We're about to run some deprecated code, don't report warnings from it.
-        # The user called the correct (non-deprecated) code path and shouldn't get warnings.
-        with warnings.catch_warnings():
-            warnings.filterwarnings(
-                "ignore",
-                message=(
-                    "Importing verbose from langchain root module is no longer supported"
-                ),
-            )
-            # N.B.: This is a workaround for an unfortunate quirk of Python's
-            #       module-level `__getattr__()` implementation:
-            # https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004
-            #
-            # Remove it once `langchain.verbose` is no longer supported, and once all users
-            # have migrated to using `set_verbose()` here.
-            langchain.verbose = value
-    except ImportError:
-        pass
-
-    global _verbose
-    _verbose = value
-
-
-def get_verbose() -> bool:
-    """Get the value of the `verbose` global setting.
-
-    Returns:
-        The value of the `verbose` global setting.
-    """
-    try:
-        import langchain  # type: ignore[import]
-
-        # We're about to run some deprecated code, don't report warnings from it.
-        # The user called the correct (non-deprecated) code path and shouldn't get warnings.
-        with warnings.catch_warnings():
-            warnings.filterwarnings(
-                "ignore",
-                message=(
-                    ".*Importing verbose from langchain root module is no longer supported"
-                ),
-            )
-            # N.B.: This is a workaround for an unfortunate quirk of Python's
-            #       module-level `__getattr__()` implementation:
-            # https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004
-            #
-            # Remove it once `langchain.verbose` is no longer supported, and once all users
-            # have migrated to using `set_verbose()` here.
-            #
-            # In the meantime, the `verbose` setting is considered True if either the old
-            # or the new value are True. This accommodates users who haven't migrated
-            # to using `set_verbose()` yet. Those users are getting deprecation warnings
-            # directing them to use `set_verbose()` when they import `langchain.verbose`.
-            old_verbose = langchain.verbose
-    except ImportError:
-        old_verbose = False
-
-    global _verbose
-    return _verbose or old_verbose
-
-
-def set_debug(value: bool) -> None:
-    """Set a new value for the `debug` global setting.
-
-    Args:
-        value: The new value for the `debug` global setting.
-    """
-    try:
-        import langchain  # type: ignore[import]
-
-        # We're about to run some deprecated code, don't report warnings from it.
-        # The user called the correct (non-deprecated) code path and shouldn't get warnings.
-        with warnings.catch_warnings():
-            warnings.filterwarnings(
-                "ignore",
-                message="Importing debug from langchain root module is no longer supported",
-            )
-            # N.B.: This is a workaround for an unfortunate quirk of Python's
-            #       module-level `__getattr__()` implementation:
-            # https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004
-            #
-            # Remove it once `langchain.debug` is no longer supported, and once all users
-            # have migrated to using `set_debug()` here.
-            langchain.debug = value
-    except ImportError:
-        pass
-
-    global _debug
-    _debug = value
-
-
-def get_debug() -> bool:
-    """Get the value of the `debug` global setting.
-
-    Returns:
-        The value of the `debug` global setting.
-    """
-    try:
-        import langchain  # type: ignore[import]
-
-        # We're about to run some deprecated code, don't report warnings from it.
-        # The user called the correct (non-deprecated) code path and shouldn't get warnings.
-        with warnings.catch_warnings():
-            warnings.filterwarnings(
-                "ignore",
-                message="Importing debug from langchain root module is no longer supported",
-            )
-            # N.B.: This is a workaround for an unfortunate quirk of Python's
-            #       module-level `__getattr__()` implementation:
-            # https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004
-            #
-            # Remove it once `langchain.debug` is no longer supported, and once all users
-            # have migrated to using `set_debug()` here.
-            #
-            # In the meantime, the `debug` setting is considered True if either the old
-            # or the new value are True. This accommodates users who haven't migrated
-            # to using `set_debug()` yet. Those users are getting deprecation warnings
-            # directing them to use `set_debug()` when they import `langchain.debug`.
-            old_debug = langchain.debug
-    except ImportError:
-        old_debug = False
-
-    global _debug
-    return _debug or old_debug
-
-
-def set_llm_cache(value: Optional["BaseCache"]) -> None:
-    """Set a new LLM cache, overwriting the previous value, if any.
-
-    Args:
-        value: The new LLM cache to use. If `None`, the LLM cache is disabled.
-    """
-    try:
-        import langchain  # type: ignore[import]
-
-        # We're about to run some deprecated code, don't report warnings from it.
-        # The user called the correct (non-deprecated) code path and shouldn't get warnings.
-        with warnings.catch_warnings():
-            warnings.filterwarnings(
-                "ignore",
-                message=(
-                    "Importing llm_cache from langchain root module is no longer supported"
-                ),
-            )
-            # N.B.: This is a workaround for an unfortunate quirk of Python's
-            #       module-level `__getattr__()` implementation:
-            # https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004
-            #
-            # Remove it once `langchain.llm_cache` is no longer supported, and
-            # once all users have migrated to using `set_llm_cache()` here.
-            langchain.llm_cache = value
-    except ImportError:
-        pass
-
-    global _llm_cache
-    _llm_cache = value
-
-
-def get_llm_cache() -> "BaseCache":
-    """Get the value of the `llm_cache` global setting.
-
-    Returns:
-        The value of the `llm_cache` global setting.
-    """
-    try:
-        import langchain  # type: ignore[import]
-
-        # We're about to run some deprecated code, don't report warnings from it.
-        # The user called the correct (non-deprecated) code path and shouldn't get warnings.
-        with warnings.catch_warnings():
-            warnings.filterwarnings(
-                "ignore",
-                message=(
-                    "Importing llm_cache from langchain root module is no longer supported"
-                ),
-            )
-            # N.B.: This is a workaround for an unfortunate quirk of Python's
-            #       module-level `__getattr__()` implementation:
-            # https://github.com/langchain-ai/langchain/pull/11311#issuecomment-1743780004
-            #
-            # Remove it once `langchain.llm_cache` is no longer supported, and
-            # once all users have migrated to using `set_llm_cache()` here.
-            #
-            # In the meantime, the `llm_cache` setting returns whichever of
-            # its two backing sources is truthy (not `None` and non-empty),
-            # or the old value if both are falsy. This accommodates users
-            # who haven't migrated to using `set_llm_cache()` yet.
-            # Those users are getting deprecation warnings directing them
-            # to use `set_llm_cache()` when they import `langchain.llm_cache`.
-            old_llm_cache = langchain.llm_cache
-    except ImportError:
-        old_llm_cache = None
-
-    global _llm_cache
-    return _llm_cache or old_llm_cache
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/indexing/__init__.py ./custom_langchain_core/indexing/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/indexing/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/indexing/__init__.py	1970-01-01 09:00:00
@@ -1,26 +0,0 @@
-"""Code to help indexing data into a vectorstore.
-
-This package contains helper logic to help deal with indexing data into
-a vectorstore while avoiding duplicated content and over-writing content
-if it's unchanged.
-"""
-
-from langchain_core.indexing.api import IndexingResult, aindex, index
-from langchain_core.indexing.base import (
-    DeleteResponse,
-    DocumentIndex,
-    InMemoryRecordManager,
-    RecordManager,
-    UpsertResponse,
-)
-
-__all__ = [
-    "aindex",
-    "DeleteResponse",
-    "DocumentIndex",
-    "index",
-    "IndexingResult",
-    "InMemoryRecordManager",
-    "RecordManager",
-    "UpsertResponse",
-]
Binary files .venv/lib/python3.12/site-packages/langchain_core/indexing/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/indexing/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/indexing/__pycache__/api.cpython-312.pyc and ./custom_langchain_core/indexing/__pycache__/api.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/indexing/__pycache__/base.cpython-312.pyc and ./custom_langchain_core/indexing/__pycache__/base.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/indexing/__pycache__/in_memory.cpython-312.pyc and ./custom_langchain_core/indexing/__pycache__/in_memory.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/indexing/api.py ./custom_langchain_core/indexing/api.py
--- .venv/lib/python3.12/site-packages/langchain_core/indexing/api.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/indexing/api.py	1970-01-01 09:00:00
@@ -1,807 +0,0 @@
-"""Module contains logic for indexing documents into vector stores."""
-
-from __future__ import annotations
-
-import hashlib
-import json
-import uuid
-from collections.abc import AsyncIterable, AsyncIterator, Iterable, Iterator, Sequence
-from itertools import islice
-from typing import (
-    Any,
-    Callable,
-    Literal,
-    Optional,
-    TypedDict,
-    TypeVar,
-    Union,
-    cast,
-)
-
-from pydantic import model_validator
-
-from langchain_core.document_loaders.base import BaseLoader
-from langchain_core.documents import Document
-from langchain_core.exceptions import LangChainException
-from langchain_core.indexing.base import DocumentIndex, RecordManager
-from langchain_core.vectorstores import VectorStore
-
-# Magic UUID to use as a namespace for hashing.
-# Used to try and generate a unique UUID for each document
-# from hashing the document content and metadata.
-NAMESPACE_UUID = uuid.UUID(int=1984)
-
-
-T = TypeVar("T")
-
-
-def _hash_string_to_uuid(input_string: str) -> uuid.UUID:
-    """Hashes a string and returns the corresponding UUID."""
-    hash_value = hashlib.sha1(input_string.encode("utf-8")).hexdigest()  # noqa: S324
-    return uuid.uuid5(NAMESPACE_UUID, hash_value)
-
-
-def _hash_nested_dict_to_uuid(data: dict[Any, Any]) -> uuid.UUID:
-    """Hashes a nested dictionary and returns the corresponding UUID."""
-    serialized_data = json.dumps(data, sort_keys=True)
-    hash_value = hashlib.sha1(serialized_data.encode("utf-8")).hexdigest()  # noqa: S324
-    return uuid.uuid5(NAMESPACE_UUID, hash_value)
-
-
-class _HashedDocument(Document):
-    """A hashed document with a unique ID."""
-
-    uid: str
-    hash_: str
-    """The hash of the document including content and metadata."""
-    content_hash: str
-    """The hash of the document content."""
-    metadata_hash: str
-    """The hash of the document metadata."""
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        return False
-
-    @model_validator(mode="before")
-    @classmethod
-    def calculate_hashes(cls, values: dict[str, Any]) -> Any:
-        """Root validator to calculate content and metadata hash."""
-        content = values.get("page_content", "")
-        metadata = values.get("metadata", {})
-
-        forbidden_keys = ("hash_", "content_hash", "metadata_hash")
-
-        for key in forbidden_keys:
-            if key in metadata:
-                msg = (
-                    f"Metadata cannot contain key {key} as it "
-                    f"is reserved for internal use."
-                )
-                raise ValueError(msg)
-
-        content_hash = str(_hash_string_to_uuid(content))
-
-        try:
-            metadata_hash = str(_hash_nested_dict_to_uuid(metadata))
-        except Exception as e:
-            msg = (
-                f"Failed to hash metadata: {e}. "
-                f"Please use a dict that can be serialized using json."
-            )
-            raise ValueError(msg) from e
-
-        values["content_hash"] = content_hash
-        values["metadata_hash"] = metadata_hash
-        values["hash_"] = str(_hash_string_to_uuid(content_hash + metadata_hash))
-
-        _uid = values.get("uid")
-
-        if _uid is None:
-            values["uid"] = values["hash_"]
-        return values
-
-    def to_document(self) -> Document:
-        """Return a Document object."""
-        return Document(
-            id=self.uid,
-            page_content=self.page_content,
-            metadata=self.metadata,
-        )
-
-    @classmethod
-    def from_document(
-        cls, document: Document, *, uid: Optional[str] = None
-    ) -> _HashedDocument:
-        """Create a HashedDocument from a Document."""
-        return cls(  # type: ignore[call-arg]
-            uid=uid,  # type: ignore[arg-type]
-            page_content=document.page_content,
-            metadata=document.metadata,
-        )
-
-
-def _batch(size: int, iterable: Iterable[T]) -> Iterator[list[T]]:
-    """Utility batching function."""
-    it = iter(iterable)
-    while True:
-        chunk = list(islice(it, size))
-        if not chunk:
-            return
-        yield chunk
-
-
-async def _abatch(size: int, iterable: AsyncIterable[T]) -> AsyncIterator[list[T]]:
-    """Utility batching function."""
-    batch: list[T] = []
-    async for element in iterable:
-        if len(batch) < size:
-            batch.append(element)
-
-        if len(batch) >= size:
-            yield batch
-            batch = []
-
-    if batch:
-        yield batch
-
-
-def _get_source_id_assigner(
-    source_id_key: Union[str, Callable[[Document], str], None],
-) -> Callable[[Document], Union[str, None]]:
-    """Get the source id from the document."""
-    if source_id_key is None:
-        return lambda doc: None
-    elif isinstance(source_id_key, str):
-        return lambda doc: doc.metadata[source_id_key]
-    elif callable(source_id_key):
-        return source_id_key
-    else:
-        msg = (
-            f"source_id_key should be either None, a string or a callable. "
-            f"Got {source_id_key} of type {type(source_id_key)}."
-        )
-        raise ValueError(msg)
-
-
-def _deduplicate_in_order(
-    hashed_documents: Iterable[_HashedDocument],
-) -> Iterator[_HashedDocument]:
-    """Deduplicate a list of hashed documents while preserving order."""
-    seen: set[str] = set()
-
-    for hashed_doc in hashed_documents:
-        if hashed_doc.hash_ not in seen:
-            seen.add(hashed_doc.hash_)
-            yield hashed_doc
-
-
-class IndexingException(LangChainException):
-    """Raised when an indexing operation fails."""
-
-
-def _delete(
-    vector_store: Union[VectorStore, DocumentIndex],
-    ids: list[str],
-) -> None:
-    if isinstance(vector_store, VectorStore):
-        delete_ok = vector_store.delete(ids)
-        if delete_ok is not None and delete_ok is False:
-            msg = "The delete operation to VectorStore failed."
-            raise IndexingException(msg)
-    elif isinstance(vector_store, DocumentIndex):
-        delete_response = vector_store.delete(ids)
-        if "num_failed" in delete_response and delete_response["num_failed"] > 0:
-            msg = "The delete operation to DocumentIndex failed."
-            raise IndexingException(msg)
-    else:
-        msg = (
-            f"Vectorstore should be either a VectorStore or a DocumentIndex. "
-            f"Got {type(vector_store)}."
-        )
-        raise TypeError(msg)
-
-
-# PUBLIC API
-
-
-class IndexingResult(TypedDict):
-    """Return a detailed a breakdown of the result of the indexing operation."""
-
-    num_added: int
-    """Number of added documents."""
-    num_updated: int
-    """Number of updated documents because they were not up to date."""
-    num_deleted: int
-    """Number of deleted documents."""
-    num_skipped: int
-    """Number of skipped documents because they were already up to date."""
-
-
-def index(
-    docs_source: Union[BaseLoader, Iterable[Document]],
-    record_manager: RecordManager,
-    vector_store: Union[VectorStore, DocumentIndex],
-    *,
-    batch_size: int = 100,
-    cleanup: Literal["incremental", "full", "scoped_full", None] = None,
-    source_id_key: Union[str, Callable[[Document], str], None] = None,
-    cleanup_batch_size: int = 1_000,
-    force_update: bool = False,
-    upsert_kwargs: Optional[dict[str, Any]] = None,
-) -> IndexingResult:
-    """Index data from the loader into the vector store.
-
-    Indexing functionality uses a manager to keep track of which documents
-    are in the vector store.
-
-    This allows us to keep track of which documents were updated, and which
-    documents were deleted, which documents should be skipped.
-
-    For the time being, documents are indexed using their hashes, and users
-     are not able to specify the uid of the document.
-
-    Important:
-       * In full mode, the loader should be returning
-         the entire dataset, and not just a subset of the dataset.
-         Otherwise, the auto_cleanup will remove documents that it is not
-         supposed to.
-       * In incremental mode, if documents associated with a particular
-         source id appear across different batches, the indexing API
-         will do some redundant work. This will still result in the
-         correct end state of the index, but will unfortunately not be
-         100% efficient. For example, if a given document is split into 15
-         chunks, and we index them using a batch size of 5, we'll have 3 batches
-         all with the same source id. In general, to avoid doing too much
-         redundant work select as big a batch size as possible.
-        * The `scoped_full` mode is suitable if determining an appropriate batch size
-          is challenging or if your data loader cannot return the entire dataset at
-          once. This mode keeps track of source IDs in memory, which should be fine
-          for most use cases. If your dataset is large (10M+ docs), you will likely
-          need to parallelize the indexing process regardless.
-
-    Args:
-        docs_source: Data loader or iterable of documents to index.
-        record_manager: Timestamped set to keep track of which documents were
-                         updated.
-        vector_store: VectorStore or DocumentIndex to index the documents into.
-        batch_size: Batch size to use when indexing. Default is 100.
-        cleanup: How to handle clean up of documents. Default is None.
-            - incremental: Cleans up all documents that haven't been updated AND
-                           that are associated with source ids that were seen
-                           during indexing.
-                           Clean up is done continuously during indexing helping
-                           to minimize the probability of users seeing duplicated
-                           content.
-            - full: Delete all documents that have not been returned by the loader
-                    during this run of indexing.
-                    Clean up runs after all documents have been indexed.
-                    This means that users may see duplicated content during indexing.
-            - scoped_full: Similar to Full, but only deletes all documents
-                           that haven't been updated AND that are associated with
-                           source ids that were seen during indexing.
-            - None: Do not delete any documents.
-        source_id_key: Optional key that helps identify the original source
-            of the document. Default is None.
-        cleanup_batch_size: Batch size to use when cleaning up documents.
-            Default is 1_000.
-        force_update: Force update documents even if they are present in the
-            record manager. Useful if you are re-indexing with updated embeddings.
-            Default is False.
-        upsert_kwargs: Additional keyword arguments to pass to the add_documents
-                       method of the VectorStore or the upsert method of the
-                       DocumentIndex. For example, you can use this to
-                       specify a custom vector_field:
-                       upsert_kwargs={"vector_field": "embedding"}
-            .. versionadded:: 0.3.10
-
-    Returns:
-        Indexing result which contains information about how many documents
-        were added, updated, deleted, or skipped.
-
-    Raises:
-        ValueError: If cleanup mode is not one of 'incremental', 'full' or None
-        ValueError: If cleanup mode is incremental and source_id_key is None.
-        ValueError: If vectorstore does not have
-            "delete" and "add_documents" required methods.
-        ValueError: If source_id_key is not None, but is not a string or callable.
-
-    .. version_modified:: 0.3.25
-
-        * Added `scoped_full` cleanup mode.
-    """
-    if cleanup not in {"incremental", "full", "scoped_full", None}:
-        msg = (
-            f"cleanup should be one of 'incremental', 'full', 'scoped_full' or None. "
-            f"Got {cleanup}."
-        )
-        raise ValueError(msg)
-
-    if (cleanup == "incremental" or cleanup == "scoped_full") and source_id_key is None:
-        msg = (
-            "Source id key is required when cleanup mode is incremental or scoped_full."
-        )
-        raise ValueError(msg)
-
-    destination = vector_store  # Renaming internally for clarity
-
-    # If it's a vectorstore, let's check if it has the required methods.
-    if isinstance(destination, VectorStore):
-        # Check that the Vectorstore has required methods implemented
-        methods = ["delete", "add_documents"]
-
-        for method in methods:
-            if not hasattr(destination, method):
-                msg = (
-                    f"Vectorstore {destination} does not have required method {method}"
-                )
-                raise ValueError(msg)
-
-        if type(destination).delete == VectorStore.delete:
-            # Checking if the vectorstore has overridden the default delete method
-            # implementation which just raises a NotImplementedError
-            msg = "Vectorstore has not implemented the delete method"
-            raise ValueError(msg)
-    elif isinstance(destination, DocumentIndex):
-        pass
-    else:
-        msg = (
-            f"Vectorstore should be either a VectorStore or a DocumentIndex. "
-            f"Got {type(destination)}."
-        )
-        raise TypeError(msg)
-
-    if isinstance(docs_source, BaseLoader):
-        try:
-            doc_iterator = docs_source.lazy_load()
-        except NotImplementedError:
-            doc_iterator = iter(docs_source.load())
-    else:
-        doc_iterator = iter(docs_source)
-
-    source_id_assigner = _get_source_id_assigner(source_id_key)
-
-    # Mark when the update started.
-    index_start_dt = record_manager.get_time()
-    num_added = 0
-    num_skipped = 0
-    num_updated = 0
-    num_deleted = 0
-    scoped_full_cleanup_source_ids: set[str] = set()
-
-    for doc_batch in _batch(batch_size, doc_iterator):
-        hashed_docs = list(
-            _deduplicate_in_order(
-                [_HashedDocument.from_document(doc) for doc in doc_batch]
-            )
-        )
-
-        source_ids: Sequence[Optional[str]] = [
-            source_id_assigner(doc) for doc in hashed_docs
-        ]
-
-        if cleanup == "incremental" or cleanup == "scoped_full":
-            # source ids are required.
-            for source_id, hashed_doc in zip(source_ids, hashed_docs):
-                if source_id is None:
-                    msg = (
-                        f"Source ids are required when cleanup mode is "
-                        f"incremental or scoped_full. "
-                        f"Document that starts with "
-                        f"content: {hashed_doc.page_content[:100]} was not assigned "
-                        f"as source id."
-                    )
-                    raise ValueError(msg)
-                if cleanup == "scoped_full":
-                    scoped_full_cleanup_source_ids.add(source_id)
-            # source ids cannot be None after for loop above.
-            source_ids = cast(Sequence[str], source_ids)  # type: ignore[assignment]
-
-        exists_batch = record_manager.exists([doc.uid for doc in hashed_docs])
-
-        # Filter out documents that already exist in the record store.
-        uids = []
-        docs_to_index = []
-        uids_to_refresh = []
-        seen_docs: set[str] = set()
-        for hashed_doc, doc_exists in zip(hashed_docs, exists_batch):
-            if doc_exists:
-                if force_update:
-                    seen_docs.add(hashed_doc.uid)
-                else:
-                    uids_to_refresh.append(hashed_doc.uid)
-                    continue
-            uids.append(hashed_doc.uid)
-            docs_to_index.append(hashed_doc.to_document())
-
-        # Update refresh timestamp
-        if uids_to_refresh:
-            record_manager.update(uids_to_refresh, time_at_least=index_start_dt)
-            num_skipped += len(uids_to_refresh)
-
-        # Be pessimistic and assume that all vector store write will fail.
-        # First write to vector store
-        if docs_to_index:
-            if isinstance(destination, VectorStore):
-                destination.add_documents(
-                    docs_to_index,
-                    ids=uids,
-                    batch_size=batch_size,
-                    **(upsert_kwargs or {}),
-                )
-            elif isinstance(destination, DocumentIndex):
-                destination.upsert(
-                    docs_to_index,
-                    **(upsert_kwargs or {}),
-                )
-
-            num_added += len(docs_to_index) - len(seen_docs)
-            num_updated += len(seen_docs)
-
-        # And only then update the record store.
-        # Update ALL records, even if they already exist since we want to refresh
-        # their timestamp.
-        record_manager.update(
-            [doc.uid for doc in hashed_docs],
-            group_ids=source_ids,
-            time_at_least=index_start_dt,
-        )
-
-        # If source IDs are provided, we can do the deletion incrementally!
-        if cleanup == "incremental":
-            # Get the uids of the documents that were not returned by the loader.
-
-            # mypy isn't good enough to determine that source ids cannot be None
-            # here due to a check that's happening above, so we check again.
-            for source_id in source_ids:
-                if source_id is None:
-                    msg = (
-                        "source_id cannot be None at this point. "
-                        "Reached unreachable code."
-                    )
-                    raise AssertionError(msg)
-
-            _source_ids = cast(Sequence[str], source_ids)
-
-            uids_to_delete = record_manager.list_keys(
-                group_ids=_source_ids, before=index_start_dt
-            )
-            if uids_to_delete:
-                # Then delete from vector store.
-                _delete(destination, uids_to_delete)
-                # First delete from record store.
-                record_manager.delete_keys(uids_to_delete)
-                num_deleted += len(uids_to_delete)
-
-    if cleanup == "full" or cleanup == "scoped_full":
-        delete_group_ids: Optional[Sequence[str]] = None
-        if cleanup == "scoped_full":
-            delete_group_ids = list(scoped_full_cleanup_source_ids)
-        while uids_to_delete := record_manager.list_keys(
-            group_ids=delete_group_ids, before=index_start_dt, limit=cleanup_batch_size
-        ):
-            # First delete from record store.
-            _delete(destination, uids_to_delete)
-            # Then delete from record manager.
-            record_manager.delete_keys(uids_to_delete)
-            num_deleted += len(uids_to_delete)
-
-    return {
-        "num_added": num_added,
-        "num_updated": num_updated,
-        "num_skipped": num_skipped,
-        "num_deleted": num_deleted,
-    }
-
-
-# Define an asynchronous generator function
-async def _to_async_iterator(iterator: Iterable[T]) -> AsyncIterator[T]:
-    """Convert an iterable to an async iterator."""
-    for item in iterator:
-        yield item
-
-
-async def _adelete(
-    vector_store: Union[VectorStore, DocumentIndex],
-    ids: list[str],
-) -> None:
-    if isinstance(vector_store, VectorStore):
-        delete_ok = await vector_store.adelete(ids)
-        if delete_ok is not None and delete_ok is False:
-            msg = "The delete operation to VectorStore failed."
-            raise IndexingException(msg)
-    elif isinstance(vector_store, DocumentIndex):
-        delete_response = await vector_store.adelete(ids)
-        if "num_failed" in delete_response and delete_response["num_failed"] > 0:
-            msg = "The delete operation to DocumentIndex failed."
-            raise IndexingException(msg)
-    else:
-        msg = (
-            f"Vectorstore should be either a VectorStore or a DocumentIndex. "
-            f"Got {type(vector_store)}."
-        )
-        raise TypeError(msg)
-
-
-async def aindex(
-    docs_source: Union[BaseLoader, Iterable[Document], AsyncIterator[Document]],
-    record_manager: RecordManager,
-    vector_store: Union[VectorStore, DocumentIndex],
-    *,
-    batch_size: int = 100,
-    cleanup: Literal["incremental", "full", "scoped_full", None] = None,
-    source_id_key: Union[str, Callable[[Document], str], None] = None,
-    cleanup_batch_size: int = 1_000,
-    force_update: bool = False,
-    upsert_kwargs: Optional[dict[str, Any]] = None,
-) -> IndexingResult:
-    """Async index data from the loader into the vector store.
-
-    Indexing functionality uses a manager to keep track of which documents
-    are in the vector store.
-
-    This allows us to keep track of which documents were updated, and which
-    documents were deleted, which documents should be skipped.
-
-    For the time being, documents are indexed using their hashes, and users
-     are not able to specify the uid of the document.
-
-    Important:
-       * In full mode, the loader should be returning
-         the entire dataset, and not just a subset of the dataset.
-         Otherwise, the auto_cleanup will remove documents that it is not
-         supposed to.
-       * In incremental mode, if documents associated with a particular
-         source id appear across different batches, the indexing API
-         will do some redundant work. This will still result in the
-         correct end state of the index, but will unfortunately not be
-         100% efficient. For example, if a given document is split into 15
-         chunks, and we index them using a batch size of 5, we'll have 3 batches
-         all with the same source id. In general, to avoid doing too much
-         redundant work select as big a batch size as possible.
-       * The `scoped_full` mode is suitable if determining an appropriate batch size
-         is challenging or if your data loader cannot return the entire dataset at
-         once. This mode keeps track of source IDs in memory, which should be fine
-         for most use cases. If your dataset is large (10M+ docs), you will likely
-         need to parallelize the indexing process regardless.
-
-    Args:
-        docs_source: Data loader or iterable of documents to index.
-        record_manager: Timestamped set to keep track of which documents were
-                         updated.
-        vector_store: VectorStore or DocumentIndex to index the documents into.
-        batch_size: Batch size to use when indexing. Default is 100.
-        cleanup: How to handle clean up of documents. Default is None.
-            - incremental: Cleans up all documents that haven't been updated AND
-                           that are associated with source ids that were seen
-                           during indexing.
-                           Clean up is done continuously during indexing helping
-                           to minimize the probability of users seeing duplicated
-                           content.
-            - full: Delete all documents that haven to been returned by the loader.
-                    Clean up runs after all documents have been indexed.
-                    This means that users may see duplicated content during indexing.
-            - scoped_full: Similar to Full, but only deletes all documents
-                           that haven't been updated AND that are associated with
-                           source ids that were seen during indexing.
-            - None: Do not delete any documents.
-        source_id_key: Optional key that helps identify the original source
-            of the document. Default is None.
-        cleanup_batch_size: Batch size to use when cleaning up documents.
-            Default is 1_000.
-        force_update: Force update documents even if they are present in the
-            record manager. Useful if you are re-indexing with updated embeddings.
-            Default is False.
-        upsert_kwargs: Additional keyword arguments to pass to the aadd_documents
-                       method of the VectorStore or the aupsert method of the
-                       DocumentIndex. For example, you can use this to
-                       specify a custom vector_field:
-                       upsert_kwargs={"vector_field": "embedding"}
-            .. versionadded:: 0.3.10
-
-    Returns:
-        Indexing result which contains information about how many documents
-        were added, updated, deleted, or skipped.
-
-    Raises:
-        ValueError: If cleanup mode is not one of 'incremental', 'full' or None
-        ValueError: If cleanup mode is incremental and source_id_key is None.
-        ValueError: If vectorstore does not have
-            "adelete" and "aadd_documents" required methods.
-        ValueError: If source_id_key is not None, but is not a string or callable.
-
-    .. version_modified:: 0.3.25
-
-        * Added `scoped_full` cleanup mode.
-    """
-    if cleanup not in {"incremental", "full", "scoped_full", None}:
-        msg = (
-            f"cleanup should be one of 'incremental', 'full', 'scoped_full' or None. "
-            f"Got {cleanup}."
-        )
-        raise ValueError(msg)
-
-    if (cleanup == "incremental" or cleanup == "scoped_full") and source_id_key is None:
-        msg = (
-            "Source id key is required when cleanup mode is incremental or scoped_full."
-        )
-        raise ValueError(msg)
-
-    destination = vector_store  # Renaming internally for clarity
-
-    # If it's a vectorstore, let's check if it has the required methods.
-    if isinstance(destination, VectorStore):
-        # Check that the Vectorstore has required methods implemented
-        # Check that the Vectorstore has required methods implemented
-        methods = ["adelete", "aadd_documents"]
-
-        for method in methods:
-            if not hasattr(destination, method):
-                msg = (
-                    f"Vectorstore {destination} does not have required method {method}"
-                )
-                raise ValueError(msg)
-
-        if type(destination).adelete == VectorStore.adelete:
-            # Checking if the vectorstore has overridden the default delete method
-            # implementation which just raises a NotImplementedError
-            msg = "Vectorstore has not implemented the delete method"
-            raise ValueError(msg)
-    elif isinstance(destination, DocumentIndex):
-        pass
-    else:
-        msg = (
-            f"Vectorstore should be either a VectorStore or a DocumentIndex. "
-            f"Got {type(destination)}."
-        )
-        raise TypeError(msg)
-    async_doc_iterator: AsyncIterator[Document]
-    if isinstance(docs_source, BaseLoader):
-        try:
-            async_doc_iterator = docs_source.alazy_load()
-        except NotImplementedError:
-            # Exception triggered when neither lazy_load nor alazy_load are implemented.
-            # * The default implementation of alazy_load uses lazy_load.
-            # * The default implementation of lazy_load raises NotImplementedError.
-            # In such a case, we use the load method and convert it to an async
-            # iterator.
-            async_doc_iterator = _to_async_iterator(docs_source.load())
-    else:
-        if hasattr(docs_source, "__aiter__"):
-            async_doc_iterator = docs_source  # type: ignore[assignment]
-        else:
-            async_doc_iterator = _to_async_iterator(docs_source)
-
-    source_id_assigner = _get_source_id_assigner(source_id_key)
-
-    # Mark when the update started.
-    index_start_dt = await record_manager.aget_time()
-    num_added = 0
-    num_skipped = 0
-    num_updated = 0
-    num_deleted = 0
-    scoped_full_cleanup_source_ids: set[str] = set()
-
-    async for doc_batch in _abatch(batch_size, async_doc_iterator):
-        hashed_docs = list(
-            _deduplicate_in_order(
-                [_HashedDocument.from_document(doc) for doc in doc_batch]
-            )
-        )
-
-        source_ids: Sequence[Optional[str]] = [
-            source_id_assigner(doc) for doc in hashed_docs
-        ]
-
-        if cleanup == "incremental" or cleanup == "scoped_full":
-            # If the cleanup mode is incremental, source ids are required.
-            for source_id, hashed_doc in zip(source_ids, hashed_docs):
-                if source_id is None:
-                    msg = (
-                        f"Source ids are required when cleanup mode is "
-                        f"incremental or scoped_full. "
-                        f"Document that starts with "
-                        f"content: {hashed_doc.page_content[:100]} was not assigned "
-                        f"as source id."
-                    )
-                    raise ValueError(msg)
-                if cleanup == "scoped_full":
-                    scoped_full_cleanup_source_ids.add(source_id)
-            # source ids cannot be None after for loop above.
-            source_ids = cast(Sequence[str], source_ids)
-
-        exists_batch = await record_manager.aexists([doc.uid for doc in hashed_docs])
-
-        # Filter out documents that already exist in the record store.
-        uids: list[str] = []
-        docs_to_index: list[Document] = []
-        uids_to_refresh = []
-        seen_docs: set[str] = set()
-        for hashed_doc, doc_exists in zip(hashed_docs, exists_batch):
-            if doc_exists:
-                if force_update:
-                    seen_docs.add(hashed_doc.uid)
-                else:
-                    uids_to_refresh.append(hashed_doc.uid)
-                    continue
-            uids.append(hashed_doc.uid)
-            docs_to_index.append(hashed_doc.to_document())
-
-        if uids_to_refresh:
-            # Must be updated to refresh timestamp.
-            await record_manager.aupdate(uids_to_refresh, time_at_least=index_start_dt)
-            num_skipped += len(uids_to_refresh)
-
-        # Be pessimistic and assume that all vector store write will fail.
-        # First write to vector store
-        if docs_to_index:
-            if isinstance(destination, VectorStore):
-                await destination.aadd_documents(
-                    docs_to_index,
-                    ids=uids,
-                    batch_size=batch_size,
-                    **(upsert_kwargs or {}),
-                )
-            elif isinstance(destination, DocumentIndex):
-                await destination.aupsert(
-                    docs_to_index,
-                    **(upsert_kwargs or {}),
-                )
-            num_added += len(docs_to_index) - len(seen_docs)
-            num_updated += len(seen_docs)
-
-        # And only then update the record store.
-        # Update ALL records, even if they already exist since we want to refresh
-        # their timestamp.
-        await record_manager.aupdate(
-            [doc.uid for doc in hashed_docs],
-            group_ids=source_ids,
-            time_at_least=index_start_dt,
-        )
-
-        # If source IDs are provided, we can do the deletion incrementally!
-
-        if cleanup == "incremental":
-            # Get the uids of the documents that were not returned by the loader.
-
-            # mypy isn't good enough to determine that source ids cannot be None
-            # here due to a check that's happening above, so we check again.
-            for source_id in source_ids:
-                if source_id is None:
-                    msg = (
-                        "source_id cannot be None at this point. "
-                        "Reached unreachable code."
-                    )
-                    raise AssertionError(msg)
-
-            _source_ids = cast(Sequence[str], source_ids)
-
-            uids_to_delete = await record_manager.alist_keys(
-                group_ids=_source_ids, before=index_start_dt
-            )
-            if uids_to_delete:
-                # Then delete from vector store.
-                await _adelete(destination, uids_to_delete)
-                # First delete from record store.
-                await record_manager.adelete_keys(uids_to_delete)
-                num_deleted += len(uids_to_delete)
-
-    if cleanup == "full" or cleanup == "scoped_full":
-        delete_group_ids: Optional[Sequence[str]] = None
-        if cleanup == "scoped_full":
-            delete_group_ids = list(scoped_full_cleanup_source_ids)
-        while uids_to_delete := await record_manager.alist_keys(
-            group_ids=delete_group_ids, before=index_start_dt, limit=cleanup_batch_size
-        ):
-            # First delete from record store.
-            await _adelete(destination, uids_to_delete)
-            # Then delete from record manager.
-            await record_manager.adelete_keys(uids_to_delete)
-            num_deleted += len(uids_to_delete)
-
-    return {
-        "num_added": num_added,
-        "num_updated": num_updated,
-        "num_skipped": num_skipped,
-        "num_deleted": num_deleted,
-    }
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/indexing/base.py ./custom_langchain_core/indexing/base.py
--- .venv/lib/python3.12/site-packages/langchain_core/indexing/base.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/indexing/base.py	1970-01-01 09:00:00
@@ -1,663 +0,0 @@
-from __future__ import annotations
-
-import abc
-import time
-from abc import ABC, abstractmethod
-from collections.abc import Sequence
-from typing import Any, Optional, TypedDict
-
-from langchain_core._api import beta
-from langchain_core.documents import Document
-from langchain_core.retrievers import BaseRetriever
-from langchain_core.runnables import run_in_executor
-
-
-class RecordManager(ABC):
-    """Abstract base class representing the interface for a record manager.
-
-    The record manager abstraction is used by the langchain indexing API.
-
-    The record manager keeps track of which documents have been
-    written into a vectorstore and when they were written.
-
-    The indexing API computes hashes for each document and stores the hash
-    together with the write time and the source id in the record manager.
-
-    On subsequent indexing runs, the indexing API can check the record manager
-    to determine which documents have already been indexed and which have not.
-
-    This allows the indexing API to avoid re-indexing documents that have
-    already been indexed, and to only index new documents.
-
-    The main benefit of this abstraction is that it works across many vectorstores.
-    To be supported, a vectorstore needs to only support the ability to add and
-    delete documents by ID. Using the record manager, the indexing API will
-    be able to delete outdated documents and avoid redundant indexing of documents
-    that have already been indexed.
-
-    The main constraints of this abstraction are:
-
-    1. It relies on the time-stamps to determine which documents have been
-       indexed and which have not. This means that the time-stamps must be
-       monotonically increasing. The timestamp should be the timestamp
-       as measured by the server to minimize issues.
-    2. The record manager is currently implemented separately from the
-       vectorstore, which means that the overall system becomes distributed
-       and may create issues with consistency. For example, writing to
-       record manager succeeds, but corresponding writing to vectorstore fails.
-    """
-
-    def __init__(
-        self,
-        namespace: str,
-    ) -> None:
-        """Initialize the record manager.
-
-        Args:
-            namespace (str): The namespace for the record manager.
-        """
-        self.namespace = namespace
-
-    @abstractmethod
-    def create_schema(self) -> None:
-        """Create the database schema for the record manager."""
-
-    @abstractmethod
-    async def acreate_schema(self) -> None:
-        """Asynchronously create the database schema for the record manager."""
-
-    @abstractmethod
-    def get_time(self) -> float:
-        """Get the current server time as a high resolution timestamp!
-
-        It's important to get this from the server to ensure a monotonic clock,
-        otherwise there may be data loss when cleaning up old documents!
-
-        Returns:
-            The current server time as a float timestamp.
-        """
-
-    @abstractmethod
-    async def aget_time(self) -> float:
-        """Asynchronously get the current server time as a high resolution timestamp.
-
-        It's important to get this from the server to ensure a monotonic clock,
-        otherwise there may be data loss when cleaning up old documents!
-
-        Returns:
-            The current server time as a float timestamp.
-        """
-
-    @abstractmethod
-    def update(
-        self,
-        keys: Sequence[str],
-        *,
-        group_ids: Optional[Sequence[Optional[str]]] = None,
-        time_at_least: Optional[float] = None,
-    ) -> None:
-        """Upsert records into the database.
-
-        Args:
-            keys: A list of record keys to upsert.
-            group_ids: A list of group IDs corresponding to the keys.
-            time_at_least: Optional timestamp. Implementation can use this
-                to optionally verify that the timestamp IS at least this time
-                in the system that stores the data.
-
-                e.g., use to validate that the time in the postgres database
-                is equal to or larger than the given timestamp, if not
-                raise an error.
-
-                This is meant to help prevent time-drift issues since
-                time may not be monotonically increasing!
-
-        Raises:
-            ValueError: If the length of keys doesn't match the length of group_ids.
-        """
-
-    @abstractmethod
-    async def aupdate(
-        self,
-        keys: Sequence[str],
-        *,
-        group_ids: Optional[Sequence[Optional[str]]] = None,
-        time_at_least: Optional[float] = None,
-    ) -> None:
-        """Asynchronously upsert records into the database.
-
-        Args:
-            keys: A list of record keys to upsert.
-            group_ids: A list of group IDs corresponding to the keys.
-            time_at_least: Optional timestamp. Implementation can use this
-                to optionally verify that the timestamp IS at least this time
-                in the system that stores the data.
-
-                e.g., use to validate that the time in the postgres database
-                is equal to or larger than the given timestamp, if not
-                raise an error.
-
-                This is meant to help prevent time-drift issues since
-                time may not be monotonically increasing!
-
-        Raises:
-            ValueError: If the length of keys doesn't match the length of group_ids.
-        """
-
-    @abstractmethod
-    def exists(self, keys: Sequence[str]) -> list[bool]:
-        """Check if the provided keys exist in the database.
-
-        Args:
-            keys: A list of keys to check.
-
-        Returns:
-            A list of boolean values indicating the existence of each key.
-        """
-
-    @abstractmethod
-    async def aexists(self, keys: Sequence[str]) -> list[bool]:
-        """Asynchronously check if the provided keys exist in the database.
-
-        Args:
-            keys: A list of keys to check.
-
-        Returns:
-            A list of boolean values indicating the existence of each key.
-        """
-
-    @abstractmethod
-    def list_keys(
-        self,
-        *,
-        before: Optional[float] = None,
-        after: Optional[float] = None,
-        group_ids: Optional[Sequence[str]] = None,
-        limit: Optional[int] = None,
-    ) -> list[str]:
-        """List records in the database based on the provided filters.
-
-        Args:
-            before: Filter to list records updated before this time.
-            after: Filter to list records updated after this time.
-            group_ids: Filter to list records with specific group IDs.
-            limit: optional limit on the number of records to return.
-
-        Returns:
-            A list of keys for the matching records.
-        """
-
-    @abstractmethod
-    async def alist_keys(
-        self,
-        *,
-        before: Optional[float] = None,
-        after: Optional[float] = None,
-        group_ids: Optional[Sequence[str]] = None,
-        limit: Optional[int] = None,
-    ) -> list[str]:
-        """Asynchronously list records in the database based on the provided filters.
-
-        Args:
-            before: Filter to list records updated before this time.
-            after: Filter to list records updated after this time.
-            group_ids: Filter to list records with specific group IDs.
-            limit: optional limit on the number of records to return.
-
-        Returns:
-            A list of keys for the matching records.
-        """
-
-    @abstractmethod
-    def delete_keys(self, keys: Sequence[str]) -> None:
-        """Delete specified records from the database.
-
-        Args:
-            keys: A list of keys to delete.
-        """
-
-    @abstractmethod
-    async def adelete_keys(self, keys: Sequence[str]) -> None:
-        """Asynchronously delete specified records from the database.
-
-        Args:
-            keys: A list of keys to delete.
-        """
-
-
-class _Record(TypedDict):
-    group_id: Optional[str]
-    updated_at: float
-
-
-class InMemoryRecordManager(RecordManager):
-    """An in-memory record manager for testing purposes."""
-
-    def __init__(self, namespace: str) -> None:
-        """Initialize the in-memory record manager.
-
-        Args:
-            namespace (str): The namespace for the record manager.
-        """
-        super().__init__(namespace)
-        # Each key points to a dictionary
-        # of {'group_id': group_id, 'updated_at': timestamp}
-        self.records: dict[str, _Record] = {}
-        self.namespace = namespace
-
-    def create_schema(self) -> None:
-        """In-memory schema creation is simply ensuring the structure is initialized."""
-
-    async def acreate_schema(self) -> None:
-        """Async in-memory schema creation is simply ensuring
-        the structure is initialized.
-        """
-
-    def get_time(self) -> float:
-        """Get the current server time as a high resolution timestamp!"""
-        return time.time()
-
-    async def aget_time(self) -> float:
-        """Async get the current server time as a high resolution timestamp!"""
-        return self.get_time()
-
-    def update(
-        self,
-        keys: Sequence[str],
-        *,
-        group_ids: Optional[Sequence[Optional[str]]] = None,
-        time_at_least: Optional[float] = None,
-    ) -> None:
-        """Upsert records into the database.
-
-        Args:
-            keys: A list of record keys to upsert.
-            group_ids: A list of group IDs corresponding to the keys.
-                Defaults to None.
-            time_at_least: Optional timestamp. Implementation can use this
-                to optionally verify that the timestamp IS at least this time
-                in the system that stores. Defaults to None.
-                E.g., use to validate that the time in the postgres database
-                is equal to or larger than the given timestamp, if not
-                raise an error.
-                This is meant to help prevent time-drift issues since
-                time may not be monotonically increasing!
-
-        Raises:
-            ValueError: If the length of keys doesn't match the length of group
-                ids.
-            ValueError: If time_at_least is in the future.
-        """
-        if group_ids and len(keys) != len(group_ids):
-            msg = "Length of keys must match length of group_ids"
-            raise ValueError(msg)
-        for index, key in enumerate(keys):
-            group_id = group_ids[index] if group_ids else None
-            if time_at_least and time_at_least > self.get_time():
-                msg = "time_at_least must be in the past"
-                raise ValueError(msg)
-            self.records[key] = {"group_id": group_id, "updated_at": self.get_time()}
-
-    async def aupdate(
-        self,
-        keys: Sequence[str],
-        *,
-        group_ids: Optional[Sequence[Optional[str]]] = None,
-        time_at_least: Optional[float] = None,
-    ) -> None:
-        """Async upsert records into the database.
-
-        Args:
-            keys: A list of record keys to upsert.
-            group_ids: A list of group IDs corresponding to the keys.
-                Defaults to None.
-            time_at_least: Optional timestamp. Implementation can use this
-                to optionally verify that the timestamp IS at least this time
-                in the system that stores. Defaults to None.
-                E.g., use to validate that the time in the postgres database
-                is equal to or larger than the given timestamp, if not
-                raise an error.
-                This is meant to help prevent time-drift issues since
-                time may not be monotonically increasing!
-
-        Raises:
-            ValueError: If the length of keys doesn't match the length of group
-                ids.
-            ValueError: If time_at_least is in the future.
-        """
-        self.update(keys, group_ids=group_ids, time_at_least=time_at_least)
-
-    def exists(self, keys: Sequence[str]) -> list[bool]:
-        """Check if the provided keys exist in the database.
-
-        Args:
-            keys: A list of keys to check.
-
-        Returns:
-            A list of boolean values indicating the existence of each key.
-        """
-        return [key in self.records for key in keys]
-
-    async def aexists(self, keys: Sequence[str]) -> list[bool]:
-        """Async check if the provided keys exist in the database.
-
-        Args:
-            keys: A list of keys to check.
-
-        Returns:
-            A list of boolean values indicating the existence of each key.
-        """
-        return self.exists(keys)
-
-    def list_keys(
-        self,
-        *,
-        before: Optional[float] = None,
-        after: Optional[float] = None,
-        group_ids: Optional[Sequence[str]] = None,
-        limit: Optional[int] = None,
-    ) -> list[str]:
-        """List records in the database based on the provided filters.
-
-        Args:
-            before: Filter to list records updated before this time.
-                Defaults to None.
-            after: Filter to list records updated after this time.
-                Defaults to None.
-            group_ids: Filter to list records with specific group IDs.
-                Defaults to None.
-            limit: optional limit on the number of records to return.
-                Defaults to None.
-
-        Returns:
-            A list of keys for the matching records.
-        """
-        result = []
-        for key, data in self.records.items():
-            if before and data["updated_at"] >= before:
-                continue
-            if after and data["updated_at"] <= after:
-                continue
-            if group_ids and data["group_id"] not in group_ids:
-                continue
-            result.append(key)
-        if limit:
-            return result[:limit]
-        return result
-
-    async def alist_keys(
-        self,
-        *,
-        before: Optional[float] = None,
-        after: Optional[float] = None,
-        group_ids: Optional[Sequence[str]] = None,
-        limit: Optional[int] = None,
-    ) -> list[str]:
-        """Async list records in the database based on the provided filters.
-
-        Args:
-            before: Filter to list records updated before this time.
-                Defaults to None.
-            after: Filter to list records updated after this time.
-                Defaults to None.
-            group_ids: Filter to list records with specific group IDs.
-                Defaults to None.
-            limit: optional limit on the number of records to return.
-                Defaults to None.
-
-        Returns:
-            A list of keys for the matching records.
-        """
-        return self.list_keys(
-            before=before, after=after, group_ids=group_ids, limit=limit
-        )
-
-    def delete_keys(self, keys: Sequence[str]) -> None:
-        """Delete specified records from the database.
-
-        Args:
-            keys: A list of keys to delete.
-        """
-        for key in keys:
-            if key in self.records:
-                del self.records[key]
-
-    async def adelete_keys(self, keys: Sequence[str]) -> None:
-        """Async delete specified records from the database.
-
-        Args:
-            keys: A list of keys to delete.
-        """
-        self.delete_keys(keys)
-
-
-class UpsertResponse(TypedDict):
-    """A generic response for upsert operations.
-
-    The upsert response will be used by abstractions that implement an upsert
-    operation for content that can be upserted by ID.
-
-    Upsert APIs that accept inputs with IDs and generate IDs internally
-    will return a response that includes the IDs that succeeded and the IDs
-    that failed.
-
-    If there are no failures, the failed list will be empty, and the order
-    of the IDs in the succeeded list will match the order of the input documents.
-
-    If there are failures, the response becomes ill defined, and a user of the API
-    cannot determine which generated ID corresponds to which input document.
-
-    It is recommended for users explicitly attach the IDs to the items being
-    indexed to avoid this issue.
-    """
-
-    succeeded: list[str]
-    """The IDs that were successfully indexed."""
-    failed: list[str]
-    """The IDs that failed to index."""
-
-
-class DeleteResponse(TypedDict, total=False):
-    """A generic response for delete operation.
-
-    The fields in this response are optional and whether the vectorstore
-    returns them or not is up to the implementation.
-    """
-
-    num_deleted: int
-    """The number of items that were successfully deleted.
-
-    If returned, this should only include *actual* deletions.
-
-    If the ID did not exist to begin with,
-    it should not be included in this count.
-    """
-
-    succeeded: Sequence[str]
-    """The IDs that were successfully deleted.
-
-    If returned, this should only include *actual* deletions.
-
-    If the ID did not exist to begin with,
-    it should not be included in this list.
-    """
-
-    failed: Sequence[str]
-    """The IDs that failed to be deleted.
-
-    Please note that deleting an ID that
-    does not exist is **NOT** considered a failure.
-    """
-
-    num_failed: int
-    """The number of items that failed to be deleted."""
-
-
-@beta(message="Added in 0.2.29. The abstraction is subject to change.")
-class DocumentIndex(BaseRetriever):
-    """A document retriever that supports indexing operations.
-
-    This indexing interface is designed to be a generic abstraction for storing and
-    querying documents that has an ID and metadata associated with it.
-
-    The interface is designed to be agnostic to the underlying implementation of the
-    indexing system.
-
-    The interface is designed to support the following operations:
-
-    1. Storing document in the index.
-    2. Fetching document by ID.
-    3. Searching for document using a query.
-
-    .. versionadded:: 0.2.29
-    """
-
-    @abc.abstractmethod
-    def upsert(self, items: Sequence[Document], /, **kwargs: Any) -> UpsertResponse:
-        """Upsert documents into the index.
-
-        The upsert functionality should utilize the ID field of the content object
-        if it is provided. If the ID is not provided, the upsert method is free
-        to generate an ID for the content.
-
-        When an ID is specified and the content already exists in the vectorstore,
-        the upsert method should update the content with the new data. If the content
-        does not exist, the upsert method should add the item to the vectorstore.
-
-        Args:
-            items: Sequence of documents to add to the vectorstore.
-            **kwargs: Additional keyword arguments.
-
-        Returns:
-            UpsertResponse: A response object that contains the list of IDs that were
-            successfully added or updated in the vectorstore and the list of IDs that
-            failed to be added or updated.
-        """
-
-    async def aupsert(
-        self, items: Sequence[Document], /, **kwargs: Any
-    ) -> UpsertResponse:
-        """Add or update documents in the vectorstore. Async version of upsert.
-
-        The upsert functionality should utilize the ID field of the item
-        if it is provided. If the ID is not provided, the upsert method is free
-        to generate an ID for the item.
-
-        When an ID is specified and the item already exists in the vectorstore,
-        the upsert method should update the item with the new data. If the item
-        does not exist, the upsert method should add the item to the vectorstore.
-
-        Args:
-            items: Sequence of documents to add to the vectorstore.
-            **kwargs: Additional keyword arguments.
-
-        Returns:
-            UpsertResponse: A response object that contains the list of IDs that were
-            successfully added or updated in the vectorstore and the list of IDs that
-            failed to be added or updated.
-        """
-        return await run_in_executor(
-            None,
-            self.upsert,
-            items,
-            **kwargs,
-        )
-
-    @abc.abstractmethod
-    def delete(self, ids: Optional[list[str]] = None, **kwargs: Any) -> DeleteResponse:
-        """Delete by IDs or other criteria.
-
-        Calling delete without any input parameters should raise a ValueError!
-
-        Args:
-            ids: List of ids to delete.
-            kwargs: Additional keyword arguments. This is up to the implementation.
-                For example, can include an option to delete the entire index,
-                or else issue a non-blocking delete etc.
-
-        Returns:
-            DeleteResponse: A response object that contains the list of IDs that were
-            successfully deleted and the list of IDs that failed to be deleted.
-        """
-
-    async def adelete(
-        self, ids: Optional[list[str]] = None, **kwargs: Any
-    ) -> DeleteResponse:
-        """Delete by IDs or other criteria. Async variant.
-
-        Calling adelete without any input parameters should raise a ValueError!
-
-        Args:
-            ids: List of ids to delete.
-            kwargs: Additional keyword arguments. This is up to the implementation.
-                For example, can include an option to delete the entire index.
-
-        Returns:
-            DeleteResponse: A response object that contains the list of IDs that were
-            successfully deleted and the list of IDs that failed to be deleted.
-        """
-        return await run_in_executor(
-            None,
-            self.delete,
-            ids,
-            **kwargs,
-        )
-
-    @abc.abstractmethod
-    def get(
-        self,
-        ids: Sequence[str],
-        /,
-        **kwargs: Any,
-    ) -> list[Document]:
-        """Get documents by id.
-
-        Fewer documents may be returned than requested if some IDs are not found or
-        if there are duplicated IDs.
-
-        Users should not assume that the order of the returned documents matches
-        the order of the input IDs. Instead, users should rely on the ID field of the
-        returned documents.
-
-        This method should **NOT** raise exceptions if no documents are found for
-        some IDs.
-
-        Args:
-            ids: List of IDs to get.
-            kwargs: Additional keyword arguments. These are up to the implementation.
-
-        Returns:
-            List[Document]: List of documents that were found.
-        """
-
-    async def aget(
-        self,
-        ids: Sequence[str],
-        /,
-        **kwargs: Any,
-    ) -> list[Document]:
-        """Get documents by id.
-
-        Fewer documents may be returned than requested if some IDs are not found or
-        if there are duplicated IDs.
-
-        Users should not assume that the order of the returned documents matches
-        the order of the input IDs. Instead, users should rely on the ID field of the
-        returned documents.
-
-        This method should **NOT** raise exceptions if no documents are found for
-        some IDs.
-
-        Args:
-            ids: List of IDs to get.
-            kwargs: Additional keyword arguments. These are up to the implementation.
-
-        Returns:
-            List[Document]: List of documents that were found.
-        """
-        return await run_in_executor(
-            None,
-            self.get,
-            ids,
-            **kwargs,
-        )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/indexing/in_memory.py ./custom_langchain_core/indexing/in_memory.py
--- .venv/lib/python3.12/site-packages/langchain_core/indexing/in_memory.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/indexing/in_memory.py	1970-01-01 09:00:00
@@ -1,85 +0,0 @@
-import operator
-import uuid
-from collections.abc import Sequence
-from typing import Any, Optional, cast
-
-from pydantic import Field
-
-from langchain_core._api import beta
-from langchain_core.callbacks import CallbackManagerForRetrieverRun
-from langchain_core.documents import Document
-from langchain_core.indexing import UpsertResponse
-from langchain_core.indexing.base import DeleteResponse, DocumentIndex
-
-
-@beta(message="Introduced in version 0.2.29. Underlying abstraction subject to change.")
-class InMemoryDocumentIndex(DocumentIndex):
-    """In memory document index.
-
-    This is an in-memory document index that stores documents in a dictionary.
-
-    It provides a simple search API that returns documents by the number of
-    counts the given query appears in the document.
-
-    .. versionadded:: 0.2.29
-    """
-
-    store: dict[str, Document] = Field(default_factory=dict)
-    top_k: int = 4
-
-    def upsert(self, items: Sequence[Document], /, **kwargs: Any) -> UpsertResponse:
-        """Upsert items into the index."""
-        ok_ids = []
-
-        for item in items:
-            if item.id is None:
-                id_ = str(uuid.uuid4())
-                item_ = item.model_copy()
-                item_.id = id_
-            else:
-                item_ = item
-                id_ = item.id
-
-            self.store[id_] = item_
-            ok_ids.append(cast(str, item_.id))
-
-        return UpsertResponse(succeeded=ok_ids, failed=[])
-
-    def delete(self, ids: Optional[list[str]] = None, **kwargs: Any) -> DeleteResponse:
-        """Delete by ID."""
-        if ids is None:
-            msg = "IDs must be provided for deletion"
-            raise ValueError(msg)
-
-        ok_ids = []
-
-        for id_ in ids:
-            if id_ in self.store:
-                del self.store[id_]
-                ok_ids.append(id_)
-
-        return DeleteResponse(
-            succeeded=ok_ids, num_deleted=len(ok_ids), num_failed=0, failed=[]
-        )
-
-    def get(self, ids: Sequence[str], /, **kwargs: Any) -> list[Document]:
-        """Get by ids."""
-        found_documents = []
-
-        for id_ in ids:
-            if id_ in self.store:
-                found_documents.append(self.store[id_])
-
-        return found_documents
-
-    def _get_relevant_documents(
-        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
-    ) -> list[Document]:
-        counts_by_doc = []
-
-        for document in self.store.values():
-            count = document.page_content.count(query)
-            counts_by_doc.append((document, count))
-
-        counts_by_doc.sort(key=operator.itemgetter(1), reverse=True)
-        return [doc.model_copy() for doc, count in counts_by_doc[: self.top_k]]
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/language_models/__init__.py ./custom_langchain_core/language_models/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/language_models/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/language_models/__init__.py	1970-01-01 09:00:00
@@ -1,77 +0,0 @@
-"""**Language Model** is a type of model that can generate text or complete
-text prompts.
-
-LangChain has two main classes to work with language models: **Chat Models**
-and "old-fashioned" **LLMs**.
-
-**Chat Models**
-
-Language models that use a sequence of messages as inputs and return chat messages
-as outputs (as opposed to using plain text). These are traditionally newer models (
-older models are generally LLMs, see below). Chat models support the assignment of
-distinct roles to conversation messages, helping to distinguish messages from the AI,
-users, and instructions such as system messages.
-
-The key abstraction for chat models is `BaseChatModel`. Implementations
-should inherit from this class. Please see LangChain how-to guides with more
-information on how to implement a custom chat model.
-
-To implement a custom Chat Model, inherit from `BaseChatModel`. See
-the following guide for more information on how to implement a custom Chat Model:
-
-https://python.langchain.com/docs/how_to/custom_chat_model/
-
-**LLMs**
-
-Language models that takes a string as input and returns a string.
-These are traditionally older models (newer models generally are Chat Models, see below).
-
-Although the underlying models are string in, string out, the LangChain wrappers
-also allow these models to take messages as input. This gives them the same interface
-as Chat Models. When messages are passed in as input, they will be formatted into a
-string under the hood before being passed to the underlying model.
-
-To implement a custom LLM, inherit from `BaseLLM` or `LLM`.
-Please see the following guide for more information on how to implement a custom LLM:
-
-https://python.langchain.com/docs/how_to/custom_llm/
-
-
-"""  # noqa: E501
-
-from langchain_core.language_models.base import (
-    BaseLanguageModel,
-    LangSmithParams,
-    LanguageModelInput,
-    LanguageModelLike,
-    LanguageModelOutput,
-    get_tokenizer,
-)
-from langchain_core.language_models.chat_models import BaseChatModel, SimpleChatModel
-from langchain_core.language_models.fake import FakeListLLM, FakeStreamingListLLM
-from langchain_core.language_models.fake_chat_models import (
-    FakeListChatModel,
-    FakeMessagesListChatModel,
-    GenericFakeChatModel,
-    ParrotFakeChatModel,
-)
-from langchain_core.language_models.llms import LLM, BaseLLM
-
-__all__ = [
-    "BaseLanguageModel",
-    "BaseChatModel",
-    "SimpleChatModel",
-    "BaseLLM",
-    "LLM",
-    "LanguageModelInput",
-    "get_tokenizer",
-    "LangSmithParams",
-    "LanguageModelOutput",
-    "LanguageModelLike",
-    "FakeListLLM",
-    "FakeStreamingListLLM",
-    "FakeListChatModel",
-    "FakeMessagesListChatModel",
-    "GenericFakeChatModel",
-    "ParrotFakeChatModel",
-]
Binary files .venv/lib/python3.12/site-packages/langchain_core/language_models/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/language_models/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/language_models/__pycache__/base.cpython-312.pyc and ./custom_langchain_core/language_models/__pycache__/base.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/language_models/__pycache__/chat_models.cpython-312.pyc and ./custom_langchain_core/language_models/__pycache__/chat_models.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/language_models/__pycache__/fake.cpython-312.pyc and ./custom_langchain_core/language_models/__pycache__/fake.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/language_models/__pycache__/fake_chat_models.cpython-312.pyc and ./custom_langchain_core/language_models/__pycache__/fake_chat_models.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/language_models/__pycache__/llms.cpython-312.pyc and ./custom_langchain_core/language_models/__pycache__/llms.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/language_models/base.py ./custom_langchain_core/language_models/base.py
--- .venv/lib/python3.12/site-packages/langchain_core/language_models/base.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/language_models/base.py	1970-01-01 09:00:00
@@ -1,401 +0,0 @@
-from __future__ import annotations
-
-import warnings
-from abc import ABC, abstractmethod
-from collections.abc import Mapping, Sequence
-from functools import cache
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    Literal,
-    Optional,
-    TypeVar,
-    Union,
-)
-
-from pydantic import BaseModel, ConfigDict, Field, field_validator
-from typing_extensions import TypeAlias, TypedDict, override
-
-from langchain_core._api import deprecated
-from langchain_core.messages import (
-    AnyMessage,
-    BaseMessage,
-    MessageLikeRepresentation,
-    get_buffer_string,
-)
-from langchain_core.prompt_values import PromptValue
-from langchain_core.runnables import Runnable, RunnableSerializable
-from langchain_core.utils import get_pydantic_field_names
-
-if TYPE_CHECKING:
-    from langchain_core.caches import BaseCache
-    from langchain_core.callbacks import Callbacks
-    from langchain_core.outputs import LLMResult
-
-
-class LangSmithParams(TypedDict, total=False):
-    """LangSmith parameters for tracing."""
-
-    ls_provider: str
-    """Provider of the model."""
-    ls_model_name: str
-    """Name of the model."""
-    ls_model_type: Literal["chat", "llm"]
-    """Type of the model. Should be 'chat' or 'llm'."""
-    ls_temperature: Optional[float]
-    """Temperature for generation."""
-    ls_max_tokens: Optional[int]
-    """Max tokens for generation."""
-    ls_stop: Optional[list[str]]
-    """Stop words for generation."""
-
-
-@cache  # Cache the tokenizer
-def get_tokenizer() -> Any:
-    """Get a GPT-2 tokenizer instance.
-
-    This function is cached to avoid re-loading the tokenizer
-    every time it is called.
-    """
-    try:
-        from transformers import GPT2TokenizerFast  # type: ignore[import]
-    except ImportError as e:
-        msg = (
-            "Could not import transformers python package. "
-            "This is needed in order to calculate get_token_ids. "
-            "Please install it with `pip install transformers`."
-        )
-        raise ImportError(msg) from e
-    # create a GPT-2 tokenizer instance
-    return GPT2TokenizerFast.from_pretrained("gpt2")
-
-
-def _get_token_ids_default_method(text: str) -> list[int]:
-    """Encode the text into token IDs."""
-    # get the cached tokenizer
-    tokenizer = get_tokenizer()
-
-    # tokenize the text using the GPT-2 tokenizer
-    return tokenizer.encode(text)
-
-
-LanguageModelInput = Union[PromptValue, str, Sequence[MessageLikeRepresentation]]
-LanguageModelOutput = Union[BaseMessage, str]
-LanguageModelLike = Runnable[LanguageModelInput, LanguageModelOutput]
-LanguageModelOutputVar = TypeVar("LanguageModelOutputVar", BaseMessage, str)
-
-
-def _get_verbosity() -> bool:
-    from langchain_core.globals import get_verbose
-
-    return get_verbose()
-
-
-class BaseLanguageModel(
-    RunnableSerializable[LanguageModelInput, LanguageModelOutputVar], ABC
-):
-    """Abstract base class for interfacing with language models.
-
-    All language model wrappers inherited from BaseLanguageModel.
-    """
-
-    cache: Union[BaseCache, bool, None] = Field(default=None, exclude=True)
-    """Whether to cache the response.
-
-    * If true, will use the global cache.
-    * If false, will not use a cache
-    * If None, will use the global cache if it's set, otherwise no cache.
-    * If instance of BaseCache, will use the provided cache.
-
-    Caching is not currently supported for streaming methods of models.
-    """
-    verbose: bool = Field(default_factory=_get_verbosity, exclude=True, repr=False)
-    """Whether to print out response text."""
-    callbacks: Callbacks = Field(default=None, exclude=True)
-    """Callbacks to add to the run trace."""
-    tags: Optional[list[str]] = Field(default=None, exclude=True)
-    """Tags to add to the run trace."""
-    metadata: Optional[dict[str, Any]] = Field(default=None, exclude=True)
-    """Metadata to add to the run trace."""
-    custom_get_token_ids: Optional[Callable[[str], list[int]]] = Field(
-        default=None, exclude=True
-    )
-    """Optional encoder to use for counting tokens."""
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    @field_validator("verbose", mode="before")
-    def set_verbose(cls, verbose: Optional[bool]) -> bool:
-        """If verbose is None, set it.
-
-        This allows users to pass in None as verbose to access the global setting.
-
-        Args:
-            verbose: The verbosity setting to use.
-
-        Returns:
-            The verbosity setting to use.
-        """
-        if verbose is None:
-            return _get_verbosity()
-        else:
-            return verbose
-
-    @property
-    @override
-    def InputType(self) -> TypeAlias:
-        """Get the input type for this runnable."""
-        from langchain_core.prompt_values import (
-            ChatPromptValueConcrete,
-            StringPromptValue,
-        )
-
-        # This is a version of LanguageModelInput which replaces the abstract
-        # base class BaseMessage with a union of its subclasses, which makes
-        # for a much better schema.
-        return Union[
-            str,
-            Union[StringPromptValue, ChatPromptValueConcrete],
-            list[AnyMessage],
-        ]
-
-    @abstractmethod
-    def generate_prompt(
-        self,
-        prompts: list[PromptValue],
-        stop: Optional[list[str]] = None,
-        callbacks: Callbacks = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        """Pass a sequence of prompts to the model and return model generations.
-
-        This method should make use of batched calls for models that expose a batched
-        API.
-
-        Use this method when you want to:
-            1. take advantage of batched calls,
-            2. need more output from the model than just the top generated value,
-            3. are building chains that are agnostic to the underlying language model
-                type (e.g., pure text completion models vs chat models).
-
-        Args:
-            prompts: List of PromptValues. A PromptValue is an object that can be
-                converted to match the format of any language model (string for pure
-                text generation models and BaseMessages for chat models).
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of these substrings.
-            callbacks: Callbacks to pass through. Used for executing additional
-                functionality, such as logging or streaming, throughout generation.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            An LLMResult, which contains a list of candidate Generations for each input
-                prompt and additional model provider-specific output.
-        """
-
-    @abstractmethod
-    async def agenerate_prompt(
-        self,
-        prompts: list[PromptValue],
-        stop: Optional[list[str]] = None,
-        callbacks: Callbacks = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        """Asynchronously pass a sequence of prompts and return model generations.
-
-        This method should make use of batched calls for models that expose a batched
-        API.
-
-        Use this method when you want to:
-            1. take advantage of batched calls,
-            2. need more output from the model than just the top generated value,
-            3. are building chains that are agnostic to the underlying language model
-                type (e.g., pure text completion models vs chat models).
-
-        Args:
-            prompts: List of PromptValues. A PromptValue is an object that can be
-                converted to match the format of any language model (string for pure
-                text generation models and BaseMessages for chat models).
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of these substrings.
-            callbacks: Callbacks to pass through. Used for executing additional
-                functionality, such as logging or streaming, throughout generation.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            An LLMResult, which contains a list of candidate Generations for each input
-                prompt and additional model provider-specific output.
-        """
-
-    def with_structured_output(
-        self, schema: Union[dict, type], **kwargs: Any
-    ) -> Runnable[LanguageModelInput, Union[dict, BaseModel]]:
-        """Not implemented on this class."""
-        # Implement this on child class if there is a way of steering the model to
-        # generate responses that match a given schema.
-        raise NotImplementedError
-
-    @deprecated("0.1.7", alternative="invoke", removal="1.0")
-    @abstractmethod
-    def predict(
-        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any
-    ) -> str:
-        """Pass a single string input to the model and return a string.
-
-         Use this method when passing in raw text. If you want to pass in specific
-            types of chat messages, use predict_messages.
-
-        Args:
-            text: String input to pass to the model.
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of these substrings.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            Top model prediction as a string.
-        """
-
-    @deprecated("0.1.7", alternative="invoke", removal="1.0")
-    @abstractmethod
-    def predict_messages(
-        self,
-        messages: list[BaseMessage],
-        *,
-        stop: Optional[Sequence[str]] = None,
-        **kwargs: Any,
-    ) -> BaseMessage:
-        """Pass a message sequence to the model and return a message.
-
-        Use this method when passing in chat messages. If you want to pass in raw text,
-            use predict.
-
-        Args:
-            messages: A sequence of chat messages corresponding to a single model input.
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of these substrings.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            Top model prediction as a message.
-        """
-
-    @deprecated("0.1.7", alternative="ainvoke", removal="1.0")
-    @abstractmethod
-    async def apredict(
-        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any
-    ) -> str:
-        """Asynchronously pass a string to the model and return a string.
-
-        Use this method when calling pure text generation models and only the top
-            candidate generation is needed.
-
-        Args:
-            text: String input to pass to the model.
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of these substrings.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            Top model prediction as a string.
-        """
-
-    @deprecated("0.1.7", alternative="ainvoke", removal="1.0")
-    @abstractmethod
-    async def apredict_messages(
-        self,
-        messages: list[BaseMessage],
-        *,
-        stop: Optional[Sequence[str]] = None,
-        **kwargs: Any,
-    ) -> BaseMessage:
-        """Asynchronously pass messages to the model and return a message.
-
-        Use this method when calling chat models and only the top
-            candidate generation is needed.
-
-        Args:
-            messages: A sequence of chat messages corresponding to a single model input.
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of these substrings.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            Top model prediction as a message.
-        """
-
-    @property
-    def _identifying_params(self) -> Mapping[str, Any]:
-        """Get the identifying parameters."""
-        return self.lc_attributes
-
-    def get_token_ids(self, text: str) -> list[int]:
-        """Return the ordered ids of the tokens in a text.
-
-        Args:
-            text: The string input to tokenize.
-
-        Returns:
-            A list of ids corresponding to the tokens in the text, in order they occur
-                in the text.
-        """
-        if self.custom_get_token_ids is not None:
-            return self.custom_get_token_ids(text)
-        else:
-            return _get_token_ids_default_method(text)
-
-    def get_num_tokens(self, text: str) -> int:
-        """Get the number of tokens present in the text.
-
-        Useful for checking if an input fits in a model's context window.
-
-        Args:
-            text: The string input to tokenize.
-
-        Returns:
-            The integer number of tokens in the text.
-        """
-        return len(self.get_token_ids(text))
-
-    def get_num_tokens_from_messages(
-        self,
-        messages: list[BaseMessage],
-        tools: Optional[Sequence] = None,
-    ) -> int:
-        """Get the number of tokens in the messages.
-
-        Useful for checking if an input fits in a model's context window.
-
-        **Note**: the base implementation of get_num_tokens_from_messages ignores
-        tool schemas.
-
-        Args:
-            messages: The message inputs to tokenize.
-            tools: If provided, sequence of dict, BaseModel, function, or BaseTools
-                to be converted to tool schemas.
-
-        Returns:
-            The sum of the number of tokens across the messages.
-        """
-        if tools is not None:
-            warnings.warn(
-                "Counting tokens in tool schemas is not yet supported. Ignoring tools.",
-                stacklevel=2,
-            )
-        return sum(self.get_num_tokens(get_buffer_string([m])) for m in messages)
-
-    @classmethod
-    def _all_required_field_names(cls) -> set:
-        """DEPRECATED: Kept for backwards compatibility.
-
-        Use get_pydantic_field_names.
-        """
-        return get_pydantic_field_names(cls)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py ./custom_langchain_core/language_models/chat_models.py
--- .venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/language_models/chat_models.py	1970-01-01 09:00:00
@@ -1,1417 +0,0 @@
-from __future__ import annotations
-
-import asyncio
-import inspect
-import json
-import typing
-import uuid
-import warnings
-from abc import ABC, abstractmethod
-from collections.abc import AsyncIterator, Iterator, Sequence
-from functools import cached_property
-from operator import itemgetter
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    Literal,
-    Optional,
-    Union,
-    cast,
-)
-
-from pydantic import (
-    BaseModel,
-    ConfigDict,
-    Field,
-    model_validator,
-)
-from typing_extensions import override
-
-from langchain_core._api import deprecated
-from langchain_core.caches import BaseCache
-from langchain_core.callbacks import (
-    AsyncCallbackManager,
-    AsyncCallbackManagerForLLMRun,
-    BaseCallbackManager,
-    CallbackManager,
-    CallbackManagerForLLMRun,
-    Callbacks,
-)
-from langchain_core.globals import get_llm_cache
-from langchain_core.language_models.base import (
-    BaseLanguageModel,
-    LangSmithParams,
-    LanguageModelInput,
-)
-from langchain_core.load import dumpd, dumps
-from langchain_core.messages import (
-    AIMessage,
-    AnyMessage,
-    BaseMessage,
-    BaseMessageChunk,
-    HumanMessage,
-    convert_to_messages,
-    message_chunk_to_message,
-)
-from langchain_core.outputs import (
-    ChatGeneration,
-    ChatGenerationChunk,
-    ChatResult,
-    LLMResult,
-    RunInfo,
-)
-from langchain_core.prompt_values import ChatPromptValue, PromptValue, StringPromptValue
-from langchain_core.rate_limiters import BaseRateLimiter
-from langchain_core.runnables import RunnableMap, RunnablePassthrough
-from langchain_core.runnables.config import ensure_config, run_in_executor
-from langchain_core.tracers._streaming import _StreamingCallbackHandler
-from langchain_core.utils.function_calling import convert_to_openai_tool
-from langchain_core.utils.pydantic import TypeBaseModel, is_basemodel_subclass
-
-if TYPE_CHECKING:
-    from langchain_core.output_parsers.base import OutputParserLike
-    from langchain_core.runnables import Runnable, RunnableConfig
-    from langchain_core.tools import BaseTool
-
-
-def generate_from_stream(stream: Iterator[ChatGenerationChunk]) -> ChatResult:
-    """Generate from a stream.
-
-    Args:
-        stream: Iterator of ChatGenerationChunk.
-
-    Returns:
-        ChatResult: Chat result.
-    """
-    generation = next(stream, None)
-    if generation:
-        generation += list(stream)
-    if generation is None:
-        msg = "No generations found in stream."
-        raise ValueError(msg)
-    return ChatResult(
-        generations=[
-            ChatGeneration(
-                message=message_chunk_to_message(generation.message),
-                generation_info=generation.generation_info,
-            )
-        ]
-    )
-
-
-async def agenerate_from_stream(
-    stream: AsyncIterator[ChatGenerationChunk],
-) -> ChatResult:
-    """Async generate from a stream.
-
-    Args:
-        stream: Iterator of ChatGenerationChunk.
-
-    Returns:
-        ChatResult: Chat result.
-    """
-    chunks = [chunk async for chunk in stream]
-    return await run_in_executor(None, generate_from_stream, iter(chunks))
-
-
-class BaseChatModel(BaseLanguageModel[BaseMessage], ABC):
-    """Base class for chat models.
-
-    Key imperative methods:
-        Methods that actually call the underlying model.
-
-        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+
-        | Method                    | Input                                                          | Output                                                              | Description                                                                                      |
-        +===========================+================================================================+=====================================================================+==================================================================================================+
-        | `invoke`                  | str | List[dict | tuple | BaseMessage] | PromptValue           | BaseMessage                                                         | A single chat model call.                                                                        |
-        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+
-        | `ainvoke`                 | '''                                                            | BaseMessage                                                         | Defaults to running invoke in an async executor.                                                 |
-        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+
-        | `stream`                  | '''                                                            | Iterator[BaseMessageChunk]                                          | Defaults to yielding output of invoke.                                                           |
-        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+
-        | `astream`                 | '''                                                            | AsyncIterator[BaseMessageChunk]                                     | Defaults to yielding output of ainvoke.                                                          |
-        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+
-        | `astream_events`          | '''                                                            | AsyncIterator[StreamEvent]                                          | Event types: 'on_chat_model_start', 'on_chat_model_stream', 'on_chat_model_end'.                 |
-        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+
-        | `batch`                   | List[''']                                                      | List[BaseMessage]                                                   | Defaults to running invoke in concurrent threads.                                                |
-        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+
-        | `abatch`                  | List[''']                                                      | List[BaseMessage]                                                   | Defaults to running ainvoke in concurrent threads.                                               |
-        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+
-        | `batch_as_completed`      | List[''']                                                      | Iterator[Tuple[int, Union[BaseMessage, Exception]]]                 | Defaults to running invoke in concurrent threads.                                                |
-        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+
-        | `abatch_as_completed`     | List[''']                                                      | AsyncIterator[Tuple[int, Union[BaseMessage, Exception]]]            | Defaults to running ainvoke in concurrent threads.                                               |
-        +---------------------------+----------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+
-
-        This table provides a brief overview of the main imperative methods. Please see the base Runnable reference for full documentation.
-
-    Key declarative methods:
-        Methods for creating another Runnable using the ChatModel.
-
-        +----------------------------------+-----------------------------------------------------------------------------------------------------------+
-        | Method                           | Description                                                                                               |
-        +==================================+===========================================================================================================+
-        | `bind_tools`                     | Create ChatModel that can call tools.                                                                     |
-        +----------------------------------+-----------------------------------------------------------------------------------------------------------+
-        | `with_structured_output`         | Create wrapper that structures model output using schema.                                                 |
-        +----------------------------------+-----------------------------------------------------------------------------------------------------------+
-        | `with_retry`                     | Create wrapper that retries model calls on failure.                                                       |
-        +----------------------------------+-----------------------------------------------------------------------------------------------------------+
-        | `with_fallbacks`                 | Create wrapper that falls back to other models on failure.                                                |
-        +----------------------------------+-----------------------------------------------------------------------------------------------------------+
-        | `configurable_fields`            | Specify init args of the model that can be configured at runtime via the RunnableConfig.                  |
-        +----------------------------------+-----------------------------------------------------------------------------------------------------------+
-        | `configurable_alternatives`      | Specify alternative models which can be swapped in at runtime via the RunnableConfig.                     |
-        +----------------------------------+-----------------------------------------------------------------------------------------------------------+
-
-        This table provides a brief overview of the main declarative methods. Please see the reference for each method for full documentation.
-
-    Creating custom chat model:
-        Custom chat model implementations should inherit from this class.
-        Please reference the table below for information about which
-        methods and properties are required or optional for implementations.
-
-        +----------------------------------+--------------------------------------------------------------------+-------------------+
-        | Method/Property                  | Description                                                        | Required/Optional |
-        +==================================+====================================================================+===================+
-        | `_generate`                      | Use to generate a chat result from a prompt                        | Required          |
-        +----------------------------------+--------------------------------------------------------------------+-------------------+
-        | `_llm_type` (property)           | Used to uniquely identify the type of the model. Used for logging. | Required          |
-        +----------------------------------+--------------------------------------------------------------------+-------------------+
-        | `_identifying_params` (property) | Represent model parameterization for tracing purposes.             | Optional          |
-        +----------------------------------+--------------------------------------------------------------------+-------------------+
-        | `_stream`                        | Use to implement streaming                                         | Optional          |
-        +----------------------------------+--------------------------------------------------------------------+-------------------+
-        | `_agenerate`                     | Use to implement a native async method                             | Optional          |
-        +----------------------------------+--------------------------------------------------------------------+-------------------+
-        | `_astream`                       | Use to implement async version of `_stream`                        | Optional          |
-        +----------------------------------+--------------------------------------------------------------------+-------------------+
-
-        Follow the guide for more information on how to implement a custom Chat Model:
-        [Guide](https://python.langchain.com/docs/how_to/custom_chat_model/).
-
-    """  # noqa: E501
-
-    callback_manager: Optional[BaseCallbackManager] = deprecated(
-        name="callback_manager", since="0.1.7", removal="1.0", alternative="callbacks"
-    )(
-        Field(
-            default=None,
-            exclude=True,
-            description="Callback manager to add to the run trace.",
-        )
-    )
-
-    rate_limiter: Optional[BaseRateLimiter] = Field(default=None, exclude=True)
-    "An optional rate limiter to use for limiting the number of requests."
-
-    disable_streaming: Union[bool, Literal["tool_calling"]] = False
-    """Whether to disable streaming for this model.
-
-    If streaming is bypassed, then ``stream()/astream()`` will defer to
-    ``invoke()/ainvoke()``.
-
-    - If True, will always bypass streaming case.
-    - If "tool_calling", will bypass streaming case only when the model is called
-      with a ``tools`` keyword argument.
-    - If False (default), will always use streaming case if available.
-    """
-
-    @model_validator(mode="before")
-    @classmethod
-    def raise_deprecation(cls, values: dict) -> Any:
-        """Raise deprecation warning if callback_manager is used.
-
-        Args:
-            values (Dict): Values to validate.
-
-        Returns:
-            Dict: Validated values.
-
-        Raises:
-            DeprecationWarning: If callback_manager is used.
-        """
-        if values.get("callback_manager") is not None:
-            warnings.warn(
-                "callback_manager is deprecated. Please use callbacks instead.",
-                DeprecationWarning,
-                stacklevel=5,
-            )
-            values["callbacks"] = values.pop("callback_manager", None)
-        return values
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    @cached_property
-    def _serialized(self) -> dict[str, Any]:
-        return dumpd(self)
-
-    # --- Runnable methods ---
-
-    @property
-    @override
-    def OutputType(self) -> Any:
-        """Get the output type for this runnable."""
-        return AnyMessage
-
-    def _convert_input(self, input: LanguageModelInput) -> PromptValue:
-        if isinstance(input, PromptValue):
-            return input
-        elif isinstance(input, str):
-            return StringPromptValue(text=input)
-        elif isinstance(input, Sequence):
-            return ChatPromptValue(messages=convert_to_messages(input))
-        else:
-            msg = (
-                f"Invalid input type {type(input)}. "
-                "Must be a PromptValue, str, or list of BaseMessages."
-            )
-            raise ValueError(msg)  # noqa: TRY004
-
-    def invoke(
-        self,
-        input: LanguageModelInput,
-        config: Optional[RunnableConfig] = None,
-        *,
-        stop: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> BaseMessage:
-        config = ensure_config(config)
-        return cast(
-            ChatGeneration,
-            self.generate_prompt(
-                [self._convert_input(input)],
-                stop=stop,
-                callbacks=config.get("callbacks"),
-                tags=config.get("tags"),
-                metadata=config.get("metadata"),
-                run_name=config.get("run_name"),
-                run_id=config.pop("run_id", None),
-                **kwargs,
-            ).generations[0][0],
-        ).message
-
-    async def ainvoke(
-        self,
-        input: LanguageModelInput,
-        config: Optional[RunnableConfig] = None,
-        *,
-        stop: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> BaseMessage:
-        config = ensure_config(config)
-        llm_result = await self.agenerate_prompt(
-            [self._convert_input(input)],
-            stop=stop,
-            callbacks=config.get("callbacks"),
-            tags=config.get("tags"),
-            metadata=config.get("metadata"),
-            run_name=config.get("run_name"),
-            run_id=config.pop("run_id", None),
-            **kwargs,
-        )
-        return cast(ChatGeneration, llm_result.generations[0][0]).message
-
-    def _should_stream(
-        self,
-        *,
-        async_api: bool,
-        run_manager: Optional[
-            Union[CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun]
-        ] = None,
-        **kwargs: Any,
-    ) -> bool:
-        """Determine if a given model call should hit the streaming API."""
-        sync_not_implemented = type(self)._stream == BaseChatModel._stream
-        async_not_implemented = type(self)._astream == BaseChatModel._astream
-
-        # Check if streaming is implemented.
-        if (not async_api) and sync_not_implemented:
-            return False
-        # Note, since async falls back to sync we check both here.
-        if async_api and async_not_implemented and sync_not_implemented:
-            return False
-
-        # Check if streaming has been disabled on this instance.
-        if self.disable_streaming is True:
-            return False
-        # We assume tools are passed in via "tools" kwarg in all models.
-        if self.disable_streaming == "tool_calling" and kwargs.get("tools"):
-            return False
-
-        # Check if a runtime streaming flag has been passed in.
-        if "stream" in kwargs:
-            return kwargs["stream"]
-
-        # Check if any streaming callback handlers have been passed in.
-        handlers = run_manager.handlers if run_manager else []
-        return any(isinstance(h, _StreamingCallbackHandler) for h in handlers)
-
-    def stream(
-        self,
-        input: LanguageModelInput,
-        config: Optional[RunnableConfig] = None,
-        *,
-        stop: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> Iterator[BaseMessageChunk]:
-        if not self._should_stream(async_api=False, **{**kwargs, "stream": True}):
-            # model doesn't implement streaming, so use default implementation
-            yield cast(
-                BaseMessageChunk, self.invoke(input, config=config, stop=stop, **kwargs)
-            )
-        else:
-            config = ensure_config(config)
-            messages = self._convert_input(input).to_messages()
-            structured_output_format = kwargs.pop("structured_output_format", None)
-            if structured_output_format:
-                try:
-                    structured_output_format_dict = {
-                        "structured_output_format": {
-                            "kwargs": structured_output_format.get("kwargs", {}),
-                            "schema": convert_to_openai_tool(
-                                structured_output_format["schema"]
-                            ),
-                        }
-                    }
-                except ValueError:
-                    structured_output_format_dict = {}
-            else:
-                structured_output_format_dict = {}
-
-            params = self._get_invocation_params(stop=stop, **kwargs)
-            options = {"stop": stop, **kwargs}
-            inheritable_metadata = {
-                **(config.get("metadata") or {}),
-                **self._get_ls_params(stop=stop, **kwargs),
-                **structured_output_format_dict,
-            }
-            callback_manager = CallbackManager.configure(
-                config.get("callbacks"),
-                self.callbacks,
-                self.verbose,
-                config.get("tags"),
-                self.tags,
-                inheritable_metadata,
-                self.metadata,
-            )
-            (run_manager,) = callback_manager.on_chat_model_start(
-                self._serialized,
-                [messages],
-                invocation_params=params,
-                options=options,
-                name=config.get("run_name"),
-                run_id=config.pop("run_id", None),
-                batch_size=1,
-            )
-            generation: Optional[ChatGenerationChunk] = None
-
-            if self.rate_limiter:
-                self.rate_limiter.acquire(blocking=True)
-
-            try:
-                for chunk in self._stream(messages, stop=stop, **kwargs):
-                    if chunk.message.id is None:
-                        chunk.message.id = f"run-{run_manager.run_id}"
-                    chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)
-                    run_manager.on_llm_new_token(
-                        cast(str, chunk.message.content), chunk=chunk
-                    )
-                    yield chunk.message
-                    if generation is None:
-                        generation = chunk
-                    else:
-                        generation += chunk
-            except BaseException as e:
-                run_manager.on_llm_error(
-                    e,
-                    response=LLMResult(
-                        generations=[[generation]] if generation else []
-                    ),
-                )
-                raise
-
-            if generation is None:
-                err = ValueError("No generation chunks were returned")
-                run_manager.on_llm_error(err, response=LLMResult(generations=[]))
-                raise err
-
-            run_manager.on_llm_end(LLMResult(generations=[[generation]]))
-
-    async def astream(
-        self,
-        input: LanguageModelInput,
-        config: Optional[RunnableConfig] = None,
-        *,
-        stop: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[BaseMessageChunk]:
-        if not self._should_stream(async_api=True, **{**kwargs, "stream": True}):
-            # No async or sync stream is implemented, so fall back to ainvoke
-            yield cast(
-                BaseMessageChunk,
-                await self.ainvoke(input, config=config, stop=stop, **kwargs),
-            )
-            return
-
-        config = ensure_config(config)
-        messages = self._convert_input(input).to_messages()
-
-        structured_output_format = kwargs.pop("structured_output_format", None)
-        if structured_output_format:
-            try:
-                structured_output_format_dict = {
-                    "structured_output_format": {
-                        "kwargs": structured_output_format.get("kwargs", {}),
-                        "schema": convert_to_openai_tool(
-                            structured_output_format["schema"]
-                        ),
-                    }
-                }
-            except ValueError:
-                structured_output_format_dict = {}
-        else:
-            structured_output_format_dict = {}
-
-        params = self._get_invocation_params(stop=stop, **kwargs)
-        options = {"stop": stop, **kwargs}
-        inheritable_metadata = {
-            **(config.get("metadata") or {}),
-            **self._get_ls_params(stop=stop, **kwargs),
-            **structured_output_format_dict,
-        }
-        callback_manager = AsyncCallbackManager.configure(
-            config.get("callbacks"),
-            self.callbacks,
-            self.verbose,
-            config.get("tags"),
-            self.tags,
-            inheritable_metadata,
-            self.metadata,
-        )
-        (run_manager,) = await callback_manager.on_chat_model_start(
-            self._serialized,
-            [messages],
-            invocation_params=params,
-            options=options,
-            name=config.get("run_name"),
-            run_id=config.pop("run_id", None),
-            batch_size=1,
-        )
-
-        if self.rate_limiter:
-            await self.rate_limiter.aacquire(blocking=True)
-
-        generation: Optional[ChatGenerationChunk] = None
-        try:
-            async for chunk in self._astream(
-                messages,
-                stop=stop,
-                **kwargs,
-            ):
-                if chunk.message.id is None:
-                    chunk.message.id = f"run-{run_manager.run_id}"
-                chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)
-                await run_manager.on_llm_new_token(
-                    cast(str, chunk.message.content), chunk=chunk
-                )
-                yield chunk.message
-                if generation is None:
-                    generation = chunk
-                else:
-                    generation += chunk
-        except BaseException as e:
-            await run_manager.on_llm_error(
-                e,
-                response=LLMResult(generations=[[generation]] if generation else []),
-            )
-            raise
-
-        if generation is None:
-            err = ValueError("No generation chunks were returned")
-            await run_manager.on_llm_error(err, response=LLMResult(generations=[]))
-            raise err
-
-        await run_manager.on_llm_end(
-            LLMResult(generations=[[generation]]),
-        )
-
-    # --- Custom methods ---
-
-    def _combine_llm_outputs(self, llm_outputs: list[Optional[dict]]) -> dict:
-        return {}
-
-    def _get_invocation_params(
-        self,
-        stop: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> dict:
-        params = self.dict()
-        params["stop"] = stop
-        return {**params, **kwargs}
-
-    def _get_ls_params(
-        self,
-        stop: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> LangSmithParams:
-        """Get standard params for tracing."""
-        # get default provider from class name
-        default_provider = self.__class__.__name__
-        if default_provider.startswith("Chat"):
-            default_provider = default_provider[4:].lower()
-        elif default_provider.endswith("Chat"):
-            default_provider = default_provider[:-4]
-        default_provider = default_provider.lower()
-
-        ls_params = LangSmithParams(ls_provider=default_provider, ls_model_type="chat")
-        if stop:
-            ls_params["ls_stop"] = stop
-
-        # model
-        if hasattr(self, "model") and isinstance(self.model, str):
-            ls_params["ls_model_name"] = self.model
-        elif hasattr(self, "model_name") and isinstance(self.model_name, str):
-            ls_params["ls_model_name"] = self.model_name
-
-        # temperature
-        if "temperature" in kwargs and isinstance(kwargs["temperature"], float):
-            ls_params["ls_temperature"] = kwargs["temperature"]
-        elif hasattr(self, "temperature") and isinstance(self.temperature, float):
-            ls_params["ls_temperature"] = self.temperature
-
-        # max_tokens
-        if "max_tokens" in kwargs and isinstance(kwargs["max_tokens"], int):
-            ls_params["ls_max_tokens"] = kwargs["max_tokens"]
-        elif hasattr(self, "max_tokens") and isinstance(self.max_tokens, int):
-            ls_params["ls_max_tokens"] = self.max_tokens
-
-        return ls_params
-
-    def _get_llm_string(self, stop: Optional[list[str]] = None, **kwargs: Any) -> str:
-        if self.is_lc_serializable():
-            params = {**kwargs, "stop": stop}
-            param_string = str(sorted(params.items()))
-            # This code is not super efficient as it goes back and forth between
-            # json and dict.
-            serialized_repr = self._serialized
-            _cleanup_llm_representation(serialized_repr, 1)
-            llm_string = json.dumps(serialized_repr, sort_keys=True)
-            return llm_string + "---" + param_string
-        else:
-            params = self._get_invocation_params(stop=stop, **kwargs)
-            params = {**params, **kwargs}
-            return str(sorted(params.items()))
-
-    def generate(
-        self,
-        messages: list[list[BaseMessage]],
-        stop: Optional[list[str]] = None,
-        callbacks: Callbacks = None,
-        *,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        run_name: Optional[str] = None,
-        run_id: Optional[uuid.UUID] = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        """Pass a sequence of prompts to the model and return model generations.
-
-        This method should make use of batched calls for models that expose a batched
-        API.
-
-        Use this method when you want to:
-            1. take advantage of batched calls,
-            2. need more output from the model than just the top generated value,
-            3. are building chains that are agnostic to the underlying language model
-                type (e.g., pure text completion models vs chat models).
-
-        Args:
-            messages: List of list of messages.
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of these substrings.
-            callbacks: Callbacks to pass through. Used for executing additional
-                functionality, such as logging or streaming, throughout generation.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            An LLMResult, which contains a list of candidate Generations for each input
-                prompt and additional model provider-specific output.
-        """
-        structured_output_format = kwargs.pop("structured_output_format", None)
-        if structured_output_format:
-            try:
-                structured_output_format_dict = {
-                    "structured_output_format": {
-                        "kwargs": structured_output_format.get("kwargs", {}),
-                        "schema": convert_to_openai_tool(
-                            structured_output_format["schema"]
-                        ),
-                    }
-                }
-            except ValueError:
-                structured_output_format_dict = {}
-        else:
-            structured_output_format_dict = {}
-
-        params = self._get_invocation_params(stop=stop, **kwargs)
-        options = {"stop": stop}
-        inheritable_metadata = {
-            **(metadata or {}),
-            **self._get_ls_params(stop=stop, **kwargs),
-            **structured_output_format_dict,
-        }
-
-        callback_manager = CallbackManager.configure(
-            callbacks,
-            self.callbacks,
-            self.verbose,
-            tags,
-            self.tags,
-            inheritable_metadata,
-            self.metadata,
-        )
-        run_managers = callback_manager.on_chat_model_start(
-            self._serialized,
-            messages,
-            invocation_params=params,
-            options=options,
-            name=run_name,
-            run_id=run_id,
-            batch_size=len(messages),
-        )
-        results = []
-        for i, m in enumerate(messages):
-            try:
-                results.append(
-                    self._generate_with_cache(
-                        m,
-                        stop=stop,
-                        run_manager=run_managers[i] if run_managers else None,
-                        **kwargs,
-                    )
-                )
-            except BaseException as e:
-                if run_managers:
-                    run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))
-                raise
-        flattened_outputs = [
-            LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item]
-            for res in results
-        ]
-        llm_output = self._combine_llm_outputs([res.llm_output for res in results])
-        generations = [res.generations for res in results]
-        output = LLMResult(generations=generations, llm_output=llm_output)  # type: ignore[arg-type]
-        if run_managers:
-            run_infos = []
-            for manager, flattened_output in zip(run_managers, flattened_outputs):
-                manager.on_llm_end(flattened_output)
-                run_infos.append(RunInfo(run_id=manager.run_id))
-            output.run = run_infos
-        return output
-
-    async def agenerate(
-        self,
-        messages: list[list[BaseMessage]],
-        stop: Optional[list[str]] = None,
-        callbacks: Callbacks = None,
-        *,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        run_name: Optional[str] = None,
-        run_id: Optional[uuid.UUID] = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        """Asynchronously pass a sequence of prompts to a model and return generations.
-
-        This method should make use of batched calls for models that expose a batched
-        API.
-
-        Use this method when you want to:
-            1. take advantage of batched calls,
-            2. need more output from the model than just the top generated value,
-            3. are building chains that are agnostic to the underlying language model
-                type (e.g., pure text completion models vs chat models).
-
-        Args:
-            messages: List of list of messages.
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of these substrings.
-            callbacks: Callbacks to pass through. Used for executing additional
-                functionality, such as logging or streaming, throughout generation.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            An LLMResult, which contains a list of candidate Generations for each input
-                prompt and additional model provider-specific output.
-        """
-        structured_output_format = kwargs.pop("structured_output_format", None)
-        if structured_output_format:
-            try:
-                structured_output_format_dict = {
-                    "structured_output_format": {
-                        "kwargs": structured_output_format.get("kwargs", {}),
-                        "schema": convert_to_openai_tool(
-                            structured_output_format["schema"]
-                        ),
-                    }
-                }
-            except ValueError:
-                structured_output_format_dict = {}
-        else:
-            structured_output_format_dict = {}
-
-        params = self._get_invocation_params(stop=stop, **kwargs)
-        options = {"stop": stop}
-        inheritable_metadata = {
-            **(metadata or {}),
-            **self._get_ls_params(stop=stop, **kwargs),
-            **structured_output_format_dict,
-        }
-
-        callback_manager = AsyncCallbackManager.configure(
-            callbacks,
-            self.callbacks,
-            self.verbose,
-            tags,
-            self.tags,
-            inheritable_metadata,
-            self.metadata,
-        )
-
-        run_managers = await callback_manager.on_chat_model_start(
-            self._serialized,
-            messages,
-            invocation_params=params,
-            options=options,
-            name=run_name,
-            batch_size=len(messages),
-            run_id=run_id,
-        )
-
-        results = await asyncio.gather(
-            *[
-                self._agenerate_with_cache(
-                    m,
-                    stop=stop,
-                    run_manager=run_managers[i] if run_managers else None,
-                    **kwargs,
-                )
-                for i, m in enumerate(messages)
-            ],
-            return_exceptions=True,
-        )
-        exceptions = []
-        for i, res in enumerate(results):
-            if isinstance(res, BaseException):
-                if run_managers:
-                    await run_managers[i].on_llm_error(
-                        res, response=LLMResult(generations=[])
-                    )
-                exceptions.append(res)
-        if exceptions:
-            if run_managers:
-                await asyncio.gather(
-                    *[
-                        run_manager.on_llm_end(
-                            LLMResult(
-                                generations=[res.generations],  # type: ignore[list-item, union-attr]
-                                llm_output=res.llm_output,  # type: ignore[list-item, union-attr]
-                            )
-                        )
-                        for run_manager, res in zip(run_managers, results)
-                        if not isinstance(res, Exception)
-                    ]
-                )
-            raise exceptions[0]
-        flattened_outputs = [
-            LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item, union-attr]
-            for res in results
-        ]
-        llm_output = self._combine_llm_outputs([res.llm_output for res in results])  # type: ignore[union-attr]
-        generations = [res.generations for res in results]  # type: ignore[union-attr]
-        output = LLMResult(generations=generations, llm_output=llm_output)  # type: ignore[arg-type]
-        await asyncio.gather(
-            *[
-                run_manager.on_llm_end(flattened_output)
-                for run_manager, flattened_output in zip(
-                    run_managers, flattened_outputs
-                )
-            ]
-        )
-        if run_managers:
-            output.run = [
-                RunInfo(run_id=run_manager.run_id) for run_manager in run_managers
-            ]
-        return output
-
-    def generate_prompt(
-        self,
-        prompts: list[PromptValue],
-        stop: Optional[list[str]] = None,
-        callbacks: Callbacks = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        prompt_messages = [p.to_messages() for p in prompts]
-        return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
-
-    async def agenerate_prompt(
-        self,
-        prompts: list[PromptValue],
-        stop: Optional[list[str]] = None,
-        callbacks: Callbacks = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        prompt_messages = [p.to_messages() for p in prompts]
-        return await self.agenerate(
-            prompt_messages, stop=stop, callbacks=callbacks, **kwargs
-        )
-
-    def _generate_with_cache(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        llm_cache = self.cache if isinstance(self.cache, BaseCache) else get_llm_cache()
-        # We should check the cache unless it's explicitly set to False
-        # A None cache means we should use the default global cache
-        # if it's configured.
-        check_cache = self.cache or self.cache is None
-        if check_cache:
-            if llm_cache:
-                llm_string = self._get_llm_string(stop=stop, **kwargs)
-                prompt = dumps(messages)
-                cache_val = llm_cache.lookup(prompt, llm_string)
-                if isinstance(cache_val, list):
-                    return ChatResult(generations=cache_val)
-            elif self.cache is None:
-                pass
-            else:
-                msg = "Asked to cache, but no cache found at `langchain.cache`."
-                raise ValueError(msg)
-
-        # Apply the rate limiter after checking the cache, since
-        # we usually don't want to rate limit cache lookups, but
-        # we do want to rate limit API requests.
-        if self.rate_limiter:
-            self.rate_limiter.acquire(blocking=True)
-
-        # If stream is not explicitly set, check if implicitly requested by
-        # astream_events() or astream_log(). Bail out if _stream not implemented
-        if self._should_stream(
-            async_api=False,
-            run_manager=run_manager,
-            **kwargs,
-        ):
-            chunks: list[ChatGenerationChunk] = []
-            for chunk in self._stream(messages, stop=stop, **kwargs):
-                chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)
-                if run_manager:
-                    if chunk.message.id is None:
-                        chunk.message.id = f"run-{run_manager.run_id}"
-                    run_manager.on_llm_new_token(
-                        cast(str, chunk.message.content), chunk=chunk
-                    )
-                chunks.append(chunk)
-            result = generate_from_stream(iter(chunks))
-        else:
-            if inspect.signature(self._generate).parameters.get("run_manager"):
-                result = self._generate(
-                    messages, stop=stop, run_manager=run_manager, **kwargs
-                )
-            else:
-                result = self._generate(messages, stop=stop, **kwargs)
-
-        # Add response metadata to each generation
-        for idx, generation in enumerate(result.generations):
-            if run_manager and generation.message.id is None:
-                generation.message.id = f"run-{run_manager.run_id}-{idx}"
-            generation.message.response_metadata = _gen_info_and_msg_metadata(
-                generation
-            )
-        if len(result.generations) == 1 and result.llm_output is not None:
-            result.generations[0].message.response_metadata = {
-                **result.llm_output,
-                **result.generations[0].message.response_metadata,
-            }
-        if check_cache and llm_cache:
-            llm_cache.update(prompt, llm_string, result.generations)
-        return result
-
-    async def _agenerate_with_cache(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        llm_cache = self.cache if isinstance(self.cache, BaseCache) else get_llm_cache()
-        # We should check the cache unless it's explicitly set to False
-        # A None cache means we should use the default global cache
-        # if it's configured.
-        check_cache = self.cache or self.cache is None
-        if check_cache:
-            if llm_cache:
-                llm_string = self._get_llm_string(stop=stop, **kwargs)
-                prompt = dumps(messages)
-                cache_val = await llm_cache.alookup(prompt, llm_string)
-                if isinstance(cache_val, list):
-                    return ChatResult(generations=cache_val)
-            elif self.cache is None:
-                pass
-            else:
-                msg = "Asked to cache, but no cache found at `langchain.cache`."
-                raise ValueError(msg)
-
-        # Apply the rate limiter after checking the cache, since
-        # we usually don't want to rate limit cache lookups, but
-        # we do want to rate limit API requests.
-        if self.rate_limiter:
-            await self.rate_limiter.aacquire(blocking=True)
-
-        # If stream is not explicitly set, check if implicitly requested by
-        # astream_events() or astream_log(). Bail out if _astream not implemented
-        if self._should_stream(
-            async_api=True,
-            run_manager=run_manager,
-            **kwargs,
-        ):
-            chunks: list[ChatGenerationChunk] = []
-            async for chunk in self._astream(messages, stop=stop, **kwargs):
-                chunk.message.response_metadata = _gen_info_and_msg_metadata(chunk)
-                if run_manager:
-                    if chunk.message.id is None:
-                        chunk.message.id = f"run-{run_manager.run_id}"
-                    await run_manager.on_llm_new_token(
-                        cast(str, chunk.message.content), chunk=chunk
-                    )
-                chunks.append(chunk)
-            result = generate_from_stream(iter(chunks))
-        else:
-            if inspect.signature(self._agenerate).parameters.get("run_manager"):
-                result = await self._agenerate(
-                    messages, stop=stop, run_manager=run_manager, **kwargs
-                )
-            else:
-                result = await self._agenerate(messages, stop=stop, **kwargs)
-
-        # Add response metadata to each generation
-        for idx, generation in enumerate(result.generations):
-            if run_manager and generation.message.id is None:
-                generation.message.id = f"run-{run_manager.run_id}-{idx}"
-            generation.message.response_metadata = _gen_info_and_msg_metadata(
-                generation
-            )
-        if len(result.generations) == 1 and result.llm_output is not None:
-            result.generations[0].message.response_metadata = {
-                **result.llm_output,
-                **result.generations[0].message.response_metadata,
-            }
-        if check_cache and llm_cache:
-            await llm_cache.aupdate(prompt, llm_string, result.generations)
-        return result
-
-    @abstractmethod
-    def _generate(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        """Top Level call."""
-
-    async def _agenerate(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        """Top Level call."""
-        return await run_in_executor(
-            None,
-            self._generate,
-            messages,
-            stop,
-            run_manager.get_sync() if run_manager else None,
-            **kwargs,
-        )
-
-    def _stream(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> Iterator[ChatGenerationChunk]:
-        raise NotImplementedError
-
-    async def _astream(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[ChatGenerationChunk]:
-        iterator = await run_in_executor(
-            None,
-            self._stream,
-            messages,
-            stop,
-            run_manager.get_sync() if run_manager else None,
-            **kwargs,
-        )
-        done = object()
-        while True:
-            item = await run_in_executor(
-                None,
-                next,
-                iterator,
-                done,  # type: ignore[call-arg, arg-type]
-            )
-            if item is done:
-                break
-            yield item  # type: ignore[misc]
-
-    @deprecated("0.1.7", alternative="invoke", removal="1.0")
-    def __call__(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        callbacks: Callbacks = None,
-        **kwargs: Any,
-    ) -> BaseMessage:
-        generation = self.generate(
-            [messages], stop=stop, callbacks=callbacks, **kwargs
-        ).generations[0][0]
-        if isinstance(generation, ChatGeneration):
-            return generation.message
-        else:
-            msg = "Unexpected generation type"
-            raise ValueError(msg)  # noqa: TRY004
-
-    async def _call_async(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        callbacks: Callbacks = None,
-        **kwargs: Any,
-    ) -> BaseMessage:
-        result = await self.agenerate(
-            [messages], stop=stop, callbacks=callbacks, **kwargs
-        )
-        generation = result.generations[0][0]
-        if isinstance(generation, ChatGeneration):
-            return generation.message
-        else:
-            msg = "Unexpected generation type"
-            raise ValueError(msg)  # noqa: TRY004
-
-    @deprecated("0.1.7", alternative="invoke", removal="1.0")
-    def call_as_llm(
-        self, message: str, stop: Optional[list[str]] = None, **kwargs: Any
-    ) -> str:
-        return self.predict(message, stop=stop, **kwargs)
-
-    @deprecated("0.1.7", alternative="invoke", removal="1.0")
-    def predict(
-        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any
-    ) -> str:
-        _stop = None if stop is None else list(stop)
-        result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
-        if isinstance(result.content, str):
-            return result.content
-        else:
-            msg = "Cannot use predict when output is not a string."
-            raise ValueError(msg)  # noqa: TRY004
-
-    @deprecated("0.1.7", alternative="invoke", removal="1.0")
-    def predict_messages(
-        self,
-        messages: list[BaseMessage],
-        *,
-        stop: Optional[Sequence[str]] = None,
-        **kwargs: Any,
-    ) -> BaseMessage:
-        _stop = None if stop is None else list(stop)
-        return self(messages, stop=_stop, **kwargs)
-
-    @deprecated("0.1.7", alternative="ainvoke", removal="1.0")
-    async def apredict(
-        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any
-    ) -> str:
-        _stop = None if stop is None else list(stop)
-        result = await self._call_async(
-            [HumanMessage(content=text)], stop=_stop, **kwargs
-        )
-        if isinstance(result.content, str):
-            return result.content
-        else:
-            msg = "Cannot use predict when output is not a string."
-            raise ValueError(msg)  # noqa: TRY004
-
-    @deprecated("0.1.7", alternative="ainvoke", removal="1.0")
-    async def apredict_messages(
-        self,
-        messages: list[BaseMessage],
-        *,
-        stop: Optional[Sequence[str]] = None,
-        **kwargs: Any,
-    ) -> BaseMessage:
-        _stop = None if stop is None else list(stop)
-        return await self._call_async(messages, stop=_stop, **kwargs)
-
-    @property
-    @abstractmethod
-    def _llm_type(self) -> str:
-        """Return type of chat model."""
-
-    def dict(self, **kwargs: Any) -> dict:
-        """Return a dictionary of the LLM."""
-        starter_dict = dict(self._identifying_params)
-        starter_dict["_type"] = self._llm_type
-        return starter_dict
-
-    def bind_tools(
-        self,
-        tools: Sequence[
-            Union[typing.Dict[str, Any], type, Callable, BaseTool]  # noqa: UP006
-        ],
-        **kwargs: Any,
-    ) -> Runnable[LanguageModelInput, BaseMessage]:
-        raise NotImplementedError
-
-    def with_structured_output(
-        self,
-        schema: Union[typing.Dict, type],  # noqa: UP006
-        *,
-        include_raw: bool = False,
-        **kwargs: Any,
-    ) -> Runnable[LanguageModelInput, Union[typing.Dict, BaseModel]]:  # noqa: UP006
-        """Model wrapper that returns outputs formatted to match the given schema.
-
-        Args:
-            schema:
-                The output schema. Can be passed in as:
-                    - an OpenAI function/tool schema,
-                    - a JSON Schema,
-                    - a TypedDict class,
-                    - or a Pydantic class.
-                If ``schema`` is a Pydantic class then the model output will be a
-                Pydantic instance of that class, and the model-generated fields will be
-                validated by the Pydantic class. Otherwise the model output will be a
-                dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`
-                for more on how to properly specify types and descriptions of
-                schema fields when specifying a Pydantic or TypedDict class.
-
-            include_raw:
-                If False then only the parsed structured output is returned. If
-                an error occurs during model output parsing it will be raised. If True
-                then both the raw model response (a BaseMessage) and the parsed model
-                response will be returned. If an error occurs during output parsing it
-                will be caught and returned as well. The final output is always a dict
-                with keys "raw", "parsed", and "parsing_error".
-
-        Returns:
-            A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.
-
-            If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs
-            an instance of ``schema`` (i.e., a Pydantic object).
-
-            Otherwise, if ``include_raw`` is False then Runnable outputs a dict.
-
-            If ``include_raw`` is True, then Runnable outputs a dict with keys:
-                - ``"raw"``: BaseMessage
-                - ``"parsed"``: None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.
-                - ``"parsing_error"``: Optional[BaseException]
-
-        Example: Pydantic schema (include_raw=False):
-            .. code-block:: python
-
-                from pydantic import BaseModel
-
-                class AnswerWithJustification(BaseModel):
-                    '''An answer to the user question along with justification for the answer.'''
-                    answer: str
-                    justification: str
-
-                llm = ChatModel(model="model-name", temperature=0)
-                structured_llm = llm.with_structured_output(AnswerWithJustification)
-
-                structured_llm.invoke("What weighs more a pound of bricks or a pound of feathers")
-
-                # -> AnswerWithJustification(
-                #     answer='They weigh the same',
-                #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'
-                # )
-
-        Example: Pydantic schema (include_raw=True):
-            .. code-block:: python
-
-                from pydantic import BaseModel
-
-                class AnswerWithJustification(BaseModel):
-                    '''An answer to the user question along with justification for the answer.'''
-                    answer: str
-                    justification: str
-
-                llm = ChatModel(model="model-name", temperature=0)
-                structured_llm = llm.with_structured_output(AnswerWithJustification, include_raw=True)
-
-                structured_llm.invoke("What weighs more a pound of bricks or a pound of feathers")
-                # -> {
-                #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{"answer":"They weigh the same.","justification":"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ."}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),
-                #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),
-                #     'parsing_error': None
-                # }
-
-        Example: Dict schema (include_raw=False):
-            .. code-block:: python
-
-                from pydantic import BaseModel
-                from langchain_core.utils.function_calling import convert_to_openai_tool
-
-                class AnswerWithJustification(BaseModel):
-                    '''An answer to the user question along with justification for the answer.'''
-                    answer: str
-                    justification: str
-
-                dict_schema = convert_to_openai_tool(AnswerWithJustification)
-                llm = ChatModel(model="model-name", temperature=0)
-                structured_llm = llm.with_structured_output(dict_schema)
-
-                structured_llm.invoke("What weighs more a pound of bricks or a pound of feathers")
-                # -> {
-                #     'answer': 'They weigh the same',
-                #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'
-                # }
-
-        .. versionchanged:: 0.2.26
-
-                Added support for TypedDict class.
-        """  # noqa: E501
-        if kwargs:
-            msg = f"Received unsupported arguments {kwargs}"
-            raise ValueError(msg)
-
-        from langchain_core.output_parsers.openai_tools import (
-            JsonOutputKeyToolsParser,
-            PydanticToolsParser,
-        )
-
-        if self.bind_tools is BaseChatModel.bind_tools:
-            msg = "with_structured_output is not implemented for this model."
-            raise NotImplementedError(msg)
-
-        llm = self.bind_tools(
-            [schema],
-            tool_choice="any",
-            structured_output_format={"kwargs": {}, "schema": schema},
-        )
-        if isinstance(schema, type) and is_basemodel_subclass(schema):
-            output_parser: OutputParserLike = PydanticToolsParser(
-                tools=[cast(TypeBaseModel, schema)], first_tool_only=True
-            )
-        else:
-            key_name = convert_to_openai_tool(schema)["function"]["name"]
-            output_parser = JsonOutputKeyToolsParser(
-                key_name=key_name, first_tool_only=True
-            )
-        if include_raw:
-            parser_assign = RunnablePassthrough.assign(
-                parsed=itemgetter("raw") | output_parser, parsing_error=lambda _: None
-            )
-            parser_none = RunnablePassthrough.assign(parsed=lambda _: None)
-            parser_with_fallback = parser_assign.with_fallbacks(
-                [parser_none], exception_key="parsing_error"
-            )
-            return RunnableMap(raw=llm) | parser_with_fallback
-        else:
-            return llm | output_parser
-
-
-class SimpleChatModel(BaseChatModel):
-    """Simplified implementation for a chat model to inherit from.
-
-    **Note** This implementation is primarily here for backwards compatibility.
-        For new implementations, please use `BaseChatModel` directly.
-    """
-
-    def _generate(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        output_str = self._call(messages, stop=stop, run_manager=run_manager, **kwargs)
-        message = AIMessage(content=output_str)
-        generation = ChatGeneration(message=message)
-        return ChatResult(generations=[generation])
-
-    @abstractmethod
-    def _call(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> str:
-        """Simpler interface."""
-
-    async def _agenerate(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        return await run_in_executor(
-            None,
-            self._generate,
-            messages,
-            stop=stop,
-            run_manager=run_manager.get_sync() if run_manager else None,
-            **kwargs,
-        )
-
-
-def _gen_info_and_msg_metadata(
-    generation: Union[ChatGeneration, ChatGenerationChunk],
-) -> dict:
-    return {
-        **(generation.generation_info or {}),
-        **generation.message.response_metadata,
-    }
-
-
-def _cleanup_llm_representation(serialized: Any, depth: int) -> None:
-    """Remove non-serializable objects from a serialized object."""
-    if depth > 100:  # Don't cooperate for pathological cases
-        return
-
-    if not isinstance(serialized, dict):
-        return
-
-    if (
-        "type" in serialized
-        and serialized["type"] == "not_implemented"
-        and "repr" in serialized
-    ):
-        del serialized["repr"]
-
-    if "graph" in serialized:
-        del serialized["graph"]
-
-    if "kwargs" in serialized:
-        kwargs = serialized["kwargs"]
-
-        for value in kwargs.values():
-            _cleanup_llm_representation(value, depth + 1)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/language_models/fake.py ./custom_langchain_core/language_models/fake.py
--- .venv/lib/python3.12/site-packages/langchain_core/language_models/fake.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/language_models/fake.py	1970-01-01 09:00:00
@@ -1,127 +0,0 @@
-import asyncio
-import time
-from collections.abc import AsyncIterator, Iterator, Mapping
-from typing import Any, Optional
-
-from langchain_core.callbacks import (
-    AsyncCallbackManagerForLLMRun,
-    CallbackManagerForLLMRun,
-)
-from langchain_core.language_models import LanguageModelInput
-from langchain_core.language_models.llms import LLM
-from langchain_core.runnables import RunnableConfig
-
-
-class FakeListLLM(LLM):
-    """Fake LLM for testing purposes."""
-
-    responses: list[str]
-    """List of responses to return in order."""
-    # This parameter should be removed from FakeListLLM since
-    # it's only used by sub-classes.
-    sleep: Optional[float] = None
-    """Sleep time in seconds between responses.
-
-    Ignored by FakeListLLM, but used by sub-classes.
-    """
-    i: int = 0
-    """Internally incremented after every model invocation.
-
-    Useful primarily for testing purposes.
-    """
-
-    @property
-    def _llm_type(self) -> str:
-        """Return type of llm."""
-        return "fake-list"
-
-    def _call(
-        self,
-        prompt: str,
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> str:
-        """Return next response."""
-        response = self.responses[self.i]
-        if self.i < len(self.responses) - 1:
-            self.i += 1
-        else:
-            self.i = 0
-        return response
-
-    async def _acall(
-        self,
-        prompt: str,
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> str:
-        """Return next response."""
-        response = self.responses[self.i]
-        if self.i < len(self.responses) - 1:
-            self.i += 1
-        else:
-            self.i = 0
-        return response
-
-    @property
-    def _identifying_params(self) -> Mapping[str, Any]:
-        return {"responses": self.responses}
-
-
-class FakeListLLMError(Exception):
-    """Fake error for testing purposes."""
-
-
-class FakeStreamingListLLM(FakeListLLM):
-    """Fake streaming list LLM for testing purposes.
-
-    An LLM that will return responses from a list in order.
-
-    This model also supports optionally sleeping between successive
-    chunks in a streaming implementation.
-    """
-
-    error_on_chunk_number: Optional[int] = None
-    """If set, will raise an exception on the specified chunk number."""
-
-    def stream(
-        self,
-        input: LanguageModelInput,
-        config: Optional[RunnableConfig] = None,
-        *,
-        stop: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> Iterator[str]:
-        result = self.invoke(input, config)
-        for i_c, c in enumerate(result):
-            if self.sleep is not None:
-                time.sleep(self.sleep)
-
-            if (
-                self.error_on_chunk_number is not None
-                and i_c == self.error_on_chunk_number
-            ):
-                raise FakeListLLMError
-            yield c
-
-    async def astream(
-        self,
-        input: LanguageModelInput,
-        config: Optional[RunnableConfig] = None,
-        *,
-        stop: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[str]:
-        result = await self.ainvoke(input, config)
-        for i_c, c in enumerate(result):
-            if self.sleep is not None:
-                await asyncio.sleep(self.sleep)
-
-            if (
-                self.error_on_chunk_number is not None
-                and i_c == self.error_on_chunk_number
-            ):
-                raise FakeListLLMError
-            yield c
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/language_models/fake_chat_models.py ./custom_langchain_core/language_models/fake_chat_models.py
--- .venv/lib/python3.12/site-packages/langchain_core/language_models/fake_chat_models.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/language_models/fake_chat_models.py	1970-01-01 09:00:00
@@ -1,350 +0,0 @@
-"""Fake ChatModel for testing purposes."""
-
-import asyncio
-import re
-import time
-from collections.abc import AsyncIterator, Iterator
-from typing import Any, Optional, Union, cast
-
-from langchain_core.callbacks import (
-    AsyncCallbackManagerForLLMRun,
-    CallbackManagerForLLMRun,
-)
-from langchain_core.language_models.chat_models import BaseChatModel, SimpleChatModel
-from langchain_core.messages import AIMessage, AIMessageChunk, BaseMessage
-from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
-from langchain_core.runnables import RunnableConfig
-
-
-class FakeMessagesListChatModel(BaseChatModel):
-    """Fake ChatModel for testing purposes."""
-
-    responses: list[BaseMessage]
-    """List of responses to **cycle** through in order."""
-    sleep: Optional[float] = None
-    """Sleep time in seconds between responses."""
-    i: int = 0
-    """Internally incremented after every model invocation."""
-
-    def _generate(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        response = self.responses[self.i]
-        if self.i < len(self.responses) - 1:
-            self.i += 1
-        else:
-            self.i = 0
-        generation = ChatGeneration(message=response)
-        return ChatResult(generations=[generation])
-
-    @property
-    def _llm_type(self) -> str:
-        return "fake-messages-list-chat-model"
-
-
-class FakeListChatModelError(Exception):
-    pass
-
-
-class FakeListChatModel(SimpleChatModel):
-    """Fake ChatModel for testing purposes."""
-
-    responses: list[str]
-    """List of responses to **cycle** through in order."""
-    sleep: Optional[float] = None
-    i: int = 0
-    """List of responses to **cycle** through in order."""
-    error_on_chunk_number: Optional[int] = None
-    """Internally incremented after every model invocation."""
-
-    @property
-    def _llm_type(self) -> str:
-        return "fake-list-chat-model"
-
-    def _call(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> str:
-        """First try to lookup in queries, else return 'foo' or 'bar'."""
-        response = self.responses[self.i]
-        if self.i < len(self.responses) - 1:
-            self.i += 1
-        else:
-            self.i = 0
-        return response
-
-    def _stream(
-        self,
-        messages: list[BaseMessage],
-        stop: Union[list[str], None] = None,
-        run_manager: Union[CallbackManagerForLLMRun, None] = None,
-        **kwargs: Any,
-    ) -> Iterator[ChatGenerationChunk]:
-        response = self.responses[self.i]
-        if self.i < len(self.responses) - 1:
-            self.i += 1
-        else:
-            self.i = 0
-        for i_c, c in enumerate(response):
-            if self.sleep is not None:
-                time.sleep(self.sleep)
-            if (
-                self.error_on_chunk_number is not None
-                and i_c == self.error_on_chunk_number
-            ):
-                raise FakeListChatModelError
-
-            yield ChatGenerationChunk(message=AIMessageChunk(content=c))
-
-    async def _astream(
-        self,
-        messages: list[BaseMessage],
-        stop: Union[list[str], None] = None,
-        run_manager: Union[AsyncCallbackManagerForLLMRun, None] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[ChatGenerationChunk]:
-        response = self.responses[self.i]
-        if self.i < len(self.responses) - 1:
-            self.i += 1
-        else:
-            self.i = 0
-        for i_c, c in enumerate(response):
-            if self.sleep is not None:
-                await asyncio.sleep(self.sleep)
-            if (
-                self.error_on_chunk_number is not None
-                and i_c == self.error_on_chunk_number
-            ):
-                raise FakeListChatModelError
-            yield ChatGenerationChunk(message=AIMessageChunk(content=c))
-
-    @property
-    def _identifying_params(self) -> dict[str, Any]:
-        return {"responses": self.responses}
-
-    # manually override batch to preserve batch ordering with no concurrency
-    def batch(
-        self,
-        inputs: list[Any],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Any,
-    ) -> list[BaseMessage]:
-        if isinstance(config, list):
-            return [self.invoke(m, c, **kwargs) for m, c in zip(inputs, config)]
-        return [self.invoke(m, config, **kwargs) for m in inputs]
-
-    async def abatch(
-        self,
-        inputs: list[Any],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Any,
-    ) -> list[BaseMessage]:
-        if isinstance(config, list):
-            # do Not use an async iterator here because need explicit ordering
-            return [await self.ainvoke(m, c, **kwargs) for m, c in zip(inputs, config)]
-        # do Not use an async iterator here because need explicit ordering
-        return [await self.ainvoke(m, config, **kwargs) for m in inputs]
-
-
-class FakeChatModel(SimpleChatModel):
-    """Fake Chat Model wrapper for testing purposes."""
-
-    def _call(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> str:
-        return "fake response"
-
-    async def _agenerate(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        output_str = "fake response"
-        message = AIMessage(content=output_str)
-        generation = ChatGeneration(message=message)
-        return ChatResult(generations=[generation])
-
-    @property
-    def _llm_type(self) -> str:
-        return "fake-chat-model"
-
-    @property
-    def _identifying_params(self) -> dict[str, Any]:
-        return {"key": "fake"}
-
-
-class GenericFakeChatModel(BaseChatModel):
-    """Generic fake chat model that can be used to test the chat model interface.
-
-    * Chat model should be usable in both sync and async tests
-    * Invokes on_llm_new_token to allow for testing of callback related code for new
-      tokens.
-    * Includes logic to break messages into message chunk to facilitate testing of
-      streaming.
-    """
-
-    messages: Iterator[Union[AIMessage, str]]
-    """Get an iterator over messages.
-
-    This can be expanded to accept other types like Callables / dicts / strings
-    to make the interface more generic if needed.
-
-    Note: if you want to pass a list, you can use `iter` to convert it to an iterator.
-
-    Please note that streaming is not implemented yet. We should try to implement it
-    in the future by delegating to invoke and then breaking the resulting output
-    into message chunks.
-    """
-
-    def _generate(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        """Top Level call."""
-        message = next(self.messages)
-        message_ = AIMessage(content=message) if isinstance(message, str) else message
-        generation = ChatGeneration(message=message_)
-        return ChatResult(generations=[generation])
-
-    def _stream(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> Iterator[ChatGenerationChunk]:
-        """Stream the output of the model."""
-        chat_result = self._generate(
-            messages, stop=stop, run_manager=run_manager, **kwargs
-        )
-        if not isinstance(chat_result, ChatResult):
-            msg = (
-                f"Expected generate to return a ChatResult, "
-                f"but got {type(chat_result)} instead."
-            )
-            raise ValueError(msg)  # noqa: TRY004
-
-        message = chat_result.generations[0].message
-
-        if not isinstance(message, AIMessage):
-            msg = (
-                f"Expected invoke to return an AIMessage, "
-                f"but got {type(message)} instead."
-            )
-            raise ValueError(msg)  # noqa: TRY004
-
-        content = message.content
-
-        if content:
-            # Use a regular expression to split on whitespace with a capture group
-            # so that we can preserve the whitespace in the output.
-            if not isinstance(content, str):
-                msg = "Expected content to be a string."
-                raise ValueError(msg)
-
-            content_chunks = cast(list[str], re.split(r"(\s)", content))
-
-            for token in content_chunks:
-                chunk = ChatGenerationChunk(
-                    message=AIMessageChunk(content=token, id=message.id)
-                )
-                if run_manager:
-                    run_manager.on_llm_new_token(token, chunk=chunk)
-                yield chunk
-
-        if message.additional_kwargs:
-            for key, value in message.additional_kwargs.items():
-                # We should further break down the additional kwargs into chunks
-                # Special case for function call
-                if key == "function_call":
-                    for fkey, fvalue in value.items():
-                        if isinstance(fvalue, str):
-                            # Break function call by `,`
-                            fvalue_chunks = cast(list[str], re.split(r"(,)", fvalue))
-                            for fvalue_chunk in fvalue_chunks:
-                                chunk = ChatGenerationChunk(
-                                    message=AIMessageChunk(
-                                        id=message.id,
-                                        content="",
-                                        additional_kwargs={
-                                            "function_call": {fkey: fvalue_chunk}
-                                        },
-                                    )
-                                )
-                                if run_manager:
-                                    run_manager.on_llm_new_token(
-                                        "",
-                                        chunk=chunk,  # No token for function call
-                                    )
-                                yield chunk
-                        else:
-                            chunk = ChatGenerationChunk(
-                                message=AIMessageChunk(
-                                    id=message.id,
-                                    content="",
-                                    additional_kwargs={"function_call": {fkey: fvalue}},
-                                )
-                            )
-                            if run_manager:
-                                run_manager.on_llm_new_token(
-                                    "",
-                                    chunk=chunk,  # No token for function call
-                                )
-                            yield chunk
-                else:
-                    chunk = ChatGenerationChunk(
-                        message=AIMessageChunk(
-                            id=message.id, content="", additional_kwargs={key: value}
-                        )
-                    )
-                    if run_manager:
-                        run_manager.on_llm_new_token(
-                            "",
-                            chunk=chunk,  # No token for function call
-                        )
-                    yield chunk
-
-    @property
-    def _llm_type(self) -> str:
-        return "generic-fake-chat-model"
-
-
-class ParrotFakeChatModel(BaseChatModel):
-    """Generic fake chat model that can be used to test the chat model interface.
-
-    * Chat model should be usable in both sync and async tests
-    """
-
-    def _generate(
-        self,
-        messages: list[BaseMessage],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        """Top Level call."""
-        return ChatResult(generations=[ChatGeneration(message=messages[-1])])
-
-    @property
-    def _llm_type(self) -> str:
-        return "parrot-fake-chat-model"
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py ./custom_langchain_core/language_models/llms.py
--- .venv/lib/python3.12/site-packages/langchain_core/language_models/llms.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/language_models/llms.py	1970-01-01 09:00:00
@@ -1,1547 +0,0 @@
-"""Base interface for large language models to expose."""
-
-from __future__ import annotations
-
-import asyncio
-import functools
-import inspect
-import json
-import logging
-import uuid
-import warnings
-from abc import ABC, abstractmethod
-from collections.abc import AsyncIterator, Iterator, Sequence
-from pathlib import Path
-from typing import (
-    Any,
-    Callable,
-    Optional,
-    Union,
-    cast,
-)
-
-import yaml
-from pydantic import ConfigDict, Field, model_validator
-from tenacity import (
-    RetryCallState,
-    before_sleep_log,
-    retry,
-    retry_base,
-    retry_if_exception_type,
-    stop_after_attempt,
-    wait_exponential,
-)
-from typing_extensions import override
-
-from langchain_core._api import deprecated
-from langchain_core.caches import BaseCache
-from langchain_core.callbacks import (
-    AsyncCallbackManager,
-    AsyncCallbackManagerForLLMRun,
-    BaseCallbackManager,
-    CallbackManager,
-    CallbackManagerForLLMRun,
-    Callbacks,
-)
-from langchain_core.globals import get_llm_cache
-from langchain_core.language_models.base import (
-    BaseLanguageModel,
-    LangSmithParams,
-    LanguageModelInput,
-)
-from langchain_core.load import dumpd
-from langchain_core.messages import (
-    AIMessage,
-    BaseMessage,
-    convert_to_messages,
-    get_buffer_string,
-)
-from langchain_core.outputs import Generation, GenerationChunk, LLMResult, RunInfo
-from langchain_core.prompt_values import ChatPromptValue, PromptValue, StringPromptValue
-from langchain_core.runnables import RunnableConfig, ensure_config, get_config_list
-from langchain_core.runnables.config import run_in_executor
-
-logger = logging.getLogger(__name__)
-
-
-@functools.lru_cache
-def _log_error_once(msg: str) -> None:
-    """Log an error once."""
-    logger.error(msg)
-
-
-def create_base_retry_decorator(
-    error_types: list[type[BaseException]],
-    max_retries: int = 1,
-    run_manager: Optional[
-        Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]
-    ] = None,
-) -> Callable[[Any], Any]:
-    """Create a retry decorator for a given LLM and provided
-     a list of error types.
-
-    Args:
-        error_types: List of error types to retry on.
-        max_retries: Number of retries. Default is 1.
-        run_manager: Callback manager for the run. Default is None.
-
-    Returns:
-        A retry decorator.
-
-    Raises:
-        ValueError: If the cache is not set and cache is True.
-    """
-    _logging = before_sleep_log(logger, logging.WARNING)
-
-    def _before_sleep(retry_state: RetryCallState) -> None:
-        _logging(retry_state)
-        if run_manager:
-            if isinstance(run_manager, AsyncCallbackManagerForLLMRun):
-                coro = run_manager.on_retry(retry_state)
-                try:
-                    loop = asyncio.get_event_loop()
-                    if loop.is_running():
-                        loop.create_task(coro)
-                    else:
-                        asyncio.run(coro)
-                except Exception as e:
-                    _log_error_once(f"Error in on_retry: {e}")
-            else:
-                run_manager.on_retry(retry_state)
-
-    min_seconds = 4
-    max_seconds = 10
-    # Wait 2^x * 1 second between each retry starting with
-    # 4 seconds, then up to 10 seconds, then 10 seconds afterwards
-    retry_instance: retry_base = retry_if_exception_type(error_types[0])
-    for error in error_types[1:]:
-        retry_instance = retry_instance | retry_if_exception_type(error)
-    return retry(
-        reraise=True,
-        stop=stop_after_attempt(max_retries),
-        wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),
-        retry=retry_instance,
-        before_sleep=_before_sleep,
-    )
-
-
-def _resolve_cache(cache: Union[BaseCache, bool, None]) -> Optional[BaseCache]:
-    """Resolve the cache."""
-    if isinstance(cache, BaseCache):
-        llm_cache = cache
-    elif cache is None:
-        llm_cache = get_llm_cache()
-    elif cache is True:
-        llm_cache = get_llm_cache()
-        if llm_cache is None:
-            msg = (
-                "No global cache was configured. Use `set_llm_cache`."
-                "to set a global cache if you want to use a global cache."
-                "Otherwise either pass a cache object or set cache to False/None"
-            )
-            raise ValueError(msg)
-    elif cache is False:
-        llm_cache = None
-    else:
-        msg = f"Unsupported cache value {cache}"
-        raise ValueError(msg)
-    return llm_cache
-
-
-def get_prompts(
-    params: dict[str, Any],
-    prompts: list[str],
-    cache: Optional[Union[BaseCache, bool, None]] = None,
-) -> tuple[dict[int, list], str, list[int], list[str]]:
-    """Get prompts that are already cached.
-
-    Args:
-        params: Dictionary of parameters.
-        prompts: List of prompts.
-        cache: Cache object. Default is None.
-
-    Returns:
-        A tuple of existing prompts, llm_string, missing prompt indexes,
-            and missing prompts.
-
-    Raises:
-        ValueError: If the cache is not set and cache is True.
-    """
-    llm_string = str(sorted(params.items()))
-    missing_prompts = []
-    missing_prompt_idxs = []
-    existing_prompts = {}
-
-    llm_cache = _resolve_cache(cache)
-    for i, prompt in enumerate(prompts):
-        if llm_cache:
-            cache_val = llm_cache.lookup(prompt, llm_string)
-            if isinstance(cache_val, list):
-                existing_prompts[i] = cache_val
-            else:
-                missing_prompts.append(prompt)
-                missing_prompt_idxs.append(i)
-    return existing_prompts, llm_string, missing_prompt_idxs, missing_prompts
-
-
-async def aget_prompts(
-    params: dict[str, Any],
-    prompts: list[str],
-    cache: Optional[Union[BaseCache, bool, None]] = None,
-) -> tuple[dict[int, list], str, list[int], list[str]]:
-    """Get prompts that are already cached. Async version.
-
-    Args:
-        params: Dictionary of parameters.
-        prompts: List of prompts.
-        cache: Cache object. Default is None.
-
-    Returns:
-        A tuple of existing prompts, llm_string, missing prompt indexes,
-            and missing prompts.
-
-    Raises:
-        ValueError: If the cache is not set and cache is True.
-    """
-    llm_string = str(sorted(params.items()))
-    missing_prompts = []
-    missing_prompt_idxs = []
-    existing_prompts = {}
-    llm_cache = _resolve_cache(cache)
-    for i, prompt in enumerate(prompts):
-        if llm_cache:
-            cache_val = await llm_cache.alookup(prompt, llm_string)
-            if isinstance(cache_val, list):
-                existing_prompts[i] = cache_val
-            else:
-                missing_prompts.append(prompt)
-                missing_prompt_idxs.append(i)
-    return existing_prompts, llm_string, missing_prompt_idxs, missing_prompts
-
-
-def update_cache(
-    cache: Union[BaseCache, bool, None],
-    existing_prompts: dict[int, list],
-    llm_string: str,
-    missing_prompt_idxs: list[int],
-    new_results: LLMResult,
-    prompts: list[str],
-) -> Optional[dict]:
-    """Update the cache and get the LLM output.
-
-    Args:
-        cache: Cache object.
-        existing_prompts: Dictionary of existing prompts.
-        llm_string: LLM string.
-        missing_prompt_idxs: List of missing prompt indexes.
-        new_results: LLMResult object.
-        prompts: List of prompts.
-
-    Returns:
-        LLM output.
-
-    Raises:
-        ValueError: If the cache is not set and cache is True.
-    """
-    llm_cache = _resolve_cache(cache)
-    for i, result in enumerate(new_results.generations):
-        existing_prompts[missing_prompt_idxs[i]] = result
-        prompt = prompts[missing_prompt_idxs[i]]
-        if llm_cache is not None:
-            llm_cache.update(prompt, llm_string, result)
-    llm_output = new_results.llm_output
-    return llm_output
-
-
-async def aupdate_cache(
-    cache: Union[BaseCache, bool, None],
-    existing_prompts: dict[int, list],
-    llm_string: str,
-    missing_prompt_idxs: list[int],
-    new_results: LLMResult,
-    prompts: list[str],
-) -> Optional[dict]:
-    """Update the cache and get the LLM output. Async version.
-
-    Args:
-        cache: Cache object.
-        existing_prompts: Dictionary of existing prompts.
-        llm_string: LLM string.
-        missing_prompt_idxs: List of missing prompt indexes.
-        new_results: LLMResult object.
-        prompts: List of prompts.
-
-    Returns:
-        LLM output.
-
-    Raises:
-        ValueError: If the cache is not set and cache is True.
-    """
-    llm_cache = _resolve_cache(cache)
-    for i, result in enumerate(new_results.generations):
-        existing_prompts[missing_prompt_idxs[i]] = result
-        prompt = prompts[missing_prompt_idxs[i]]
-        if llm_cache:
-            await llm_cache.aupdate(prompt, llm_string, result)
-    llm_output = new_results.llm_output
-    return llm_output
-
-
-class BaseLLM(BaseLanguageModel[str], ABC):
-    """Base LLM abstract interface.
-
-    It should take in a prompt and return a string.
-    """
-
-    callback_manager: Optional[BaseCallbackManager] = Field(default=None, exclude=True)
-    """[DEPRECATED]"""
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    @model_validator(mode="before")
-    @classmethod
-    def raise_deprecation(cls, values: dict) -> Any:
-        """Raise deprecation warning if callback_manager is used."""
-        if values.get("callback_manager") is not None:
-            warnings.warn(
-                "callback_manager is deprecated. Please use callbacks instead.",
-                DeprecationWarning,
-                stacklevel=5,
-            )
-            values["callbacks"] = values.pop("callback_manager", None)
-        return values
-
-    @functools.cached_property
-    def _serialized(self) -> dict[str, Any]:
-        return dumpd(self)
-
-    # --- Runnable methods ---
-
-    @property
-    @override
-    def OutputType(self) -> type[str]:
-        """Get the input type for this runnable."""
-        return str
-
-    def _convert_input(self, input: LanguageModelInput) -> PromptValue:
-        if isinstance(input, PromptValue):
-            return input
-        elif isinstance(input, str):
-            return StringPromptValue(text=input)
-        elif isinstance(input, Sequence):
-            return ChatPromptValue(messages=convert_to_messages(input))
-        else:
-            msg = (
-                f"Invalid input type {type(input)}. "
-                "Must be a PromptValue, str, or list of BaseMessages."
-            )
-            raise ValueError(msg)  # noqa: TRY004
-
-    def _get_ls_params(
-        self,
-        stop: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> LangSmithParams:
-        """Get standard params for tracing."""
-        # get default provider from class name
-        default_provider = self.__class__.__name__
-        default_provider = default_provider.removesuffix("LLM")
-        default_provider = default_provider.lower()
-
-        ls_params = LangSmithParams(ls_provider=default_provider, ls_model_type="llm")
-        if stop:
-            ls_params["ls_stop"] = stop
-
-        # model
-        if hasattr(self, "model") and isinstance(self.model, str):
-            ls_params["ls_model_name"] = self.model
-        elif hasattr(self, "model_name") and isinstance(self.model_name, str):
-            ls_params["ls_model_name"] = self.model_name
-
-        # temperature
-        if "temperature" in kwargs and isinstance(kwargs["temperature"], float):
-            ls_params["ls_temperature"] = kwargs["temperature"]
-        elif hasattr(self, "temperature") and isinstance(self.temperature, float):
-            ls_params["ls_temperature"] = self.temperature
-
-        # max_tokens
-        if "max_tokens" in kwargs and isinstance(kwargs["max_tokens"], int):
-            ls_params["ls_max_tokens"] = kwargs["max_tokens"]
-        elif hasattr(self, "max_tokens") and isinstance(self.max_tokens, int):
-            ls_params["ls_max_tokens"] = self.max_tokens
-
-        return ls_params
-
-    def invoke(
-        self,
-        input: LanguageModelInput,
-        config: Optional[RunnableConfig] = None,
-        *,
-        stop: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> str:
-        config = ensure_config(config)
-        return (
-            self.generate_prompt(
-                [self._convert_input(input)],
-                stop=stop,
-                callbacks=config.get("callbacks"),
-                tags=config.get("tags"),
-                metadata=config.get("metadata"),
-                run_name=config.get("run_name"),
-                run_id=config.pop("run_id", None),
-                **kwargs,
-            )
-            .generations[0][0]
-            .text
-        )
-
-    async def ainvoke(
-        self,
-        input: LanguageModelInput,
-        config: Optional[RunnableConfig] = None,
-        *,
-        stop: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> str:
-        config = ensure_config(config)
-        llm_result = await self.agenerate_prompt(
-            [self._convert_input(input)],
-            stop=stop,
-            callbacks=config.get("callbacks"),
-            tags=config.get("tags"),
-            metadata=config.get("metadata"),
-            run_name=config.get("run_name"),
-            run_id=config.pop("run_id", None),
-            **kwargs,
-        )
-        return llm_result.generations[0][0].text
-
-    def batch(
-        self,
-        inputs: list[LanguageModelInput],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Any,
-    ) -> list[str]:
-        if not inputs:
-            return []
-
-        config = get_config_list(config, len(inputs))
-        max_concurrency = config[0].get("max_concurrency")
-
-        if max_concurrency is None:
-            try:
-                llm_result = self.generate_prompt(
-                    [self._convert_input(input) for input in inputs],
-                    callbacks=[c.get("callbacks") for c in config],
-                    tags=[c.get("tags") for c in config],
-                    metadata=[c.get("metadata") for c in config],
-                    run_name=[c.get("run_name") for c in config],
-                    **kwargs,
-                )
-                return [g[0].text for g in llm_result.generations]
-            except Exception as e:
-                if return_exceptions:
-                    return cast(list[str], [e for _ in inputs])
-                else:
-                    raise
-        else:
-            batches = [
-                inputs[i : i + max_concurrency]
-                for i in range(0, len(inputs), max_concurrency)
-            ]
-            config = [{**c, "max_concurrency": None} for c in config]  # type: ignore[misc]
-            return [
-                output
-                for i, batch in enumerate(batches)
-                for output in self.batch(
-                    batch,
-                    config=config[i * max_concurrency : (i + 1) * max_concurrency],
-                    return_exceptions=return_exceptions,
-                    **kwargs,
-                )
-            ]
-
-    async def abatch(
-        self,
-        inputs: list[LanguageModelInput],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Any,
-    ) -> list[str]:
-        if not inputs:
-            return []
-        config = get_config_list(config, len(inputs))
-        max_concurrency = config[0].get("max_concurrency")
-
-        if max_concurrency is None:
-            try:
-                llm_result = await self.agenerate_prompt(
-                    [self._convert_input(input) for input in inputs],
-                    callbacks=[c.get("callbacks") for c in config],
-                    tags=[c.get("tags") for c in config],
-                    metadata=[c.get("metadata") for c in config],
-                    run_name=[c.get("run_name") for c in config],
-                    **kwargs,
-                )
-                return [g[0].text for g in llm_result.generations]
-            except Exception as e:
-                if return_exceptions:
-                    return cast(list[str], [e for _ in inputs])
-                else:
-                    raise
-        else:
-            batches = [
-                inputs[i : i + max_concurrency]
-                for i in range(0, len(inputs), max_concurrency)
-            ]
-            config = [{**c, "max_concurrency": None} for c in config]  # type: ignore[misc]
-            return [
-                output
-                for i, batch in enumerate(batches)
-                for output in await self.abatch(
-                    batch,
-                    config=config[i * max_concurrency : (i + 1) * max_concurrency],
-                    return_exceptions=return_exceptions,
-                    **kwargs,
-                )
-            ]
-
-    def stream(
-        self,
-        input: LanguageModelInput,
-        config: Optional[RunnableConfig] = None,
-        *,
-        stop: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> Iterator[str]:
-        if type(self)._stream == BaseLLM._stream:
-            # model doesn't implement streaming, so use default implementation
-            yield self.invoke(input, config=config, stop=stop, **kwargs)
-        else:
-            prompt = self._convert_input(input).to_string()
-            config = ensure_config(config)
-            params = self.dict()
-            params["stop"] = stop
-            params = {**params, **kwargs}
-            options = {"stop": stop}
-            inheritable_metadata = {
-                **(config.get("metadata") or {}),
-                **self._get_ls_params(stop=stop, **kwargs),
-            }
-            callback_manager = CallbackManager.configure(
-                config.get("callbacks"),
-                self.callbacks,
-                self.verbose,
-                config.get("tags"),
-                self.tags,
-                inheritable_metadata,
-                self.metadata,
-            )
-            (run_manager,) = callback_manager.on_llm_start(
-                self._serialized,
-                [prompt],
-                invocation_params=params,
-                options=options,
-                name=config.get("run_name"),
-                run_id=config.pop("run_id", None),
-                batch_size=1,
-            )
-            generation: Optional[GenerationChunk] = None
-            try:
-                for chunk in self._stream(
-                    prompt, stop=stop, run_manager=run_manager, **kwargs
-                ):
-                    yield chunk.text
-                    if generation is None:
-                        generation = chunk
-                    else:
-                        generation += chunk
-            except BaseException as e:
-                run_manager.on_llm_error(
-                    e,
-                    response=LLMResult(
-                        generations=[[generation]] if generation else []
-                    ),
-                )
-                raise
-
-            if generation is None:
-                err = ValueError("No generation chunks were returned")
-                run_manager.on_llm_error(err, response=LLMResult(generations=[]))
-                raise err
-
-            run_manager.on_llm_end(LLMResult(generations=[[generation]]))
-
-    async def astream(
-        self,
-        input: LanguageModelInput,
-        config: Optional[RunnableConfig] = None,
-        *,
-        stop: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[str]:
-        if (
-            type(self)._astream is BaseLLM._astream
-            and type(self)._stream is BaseLLM._stream
-        ):
-            yield await self.ainvoke(input, config=config, stop=stop, **kwargs)
-            return
-
-        prompt = self._convert_input(input).to_string()
-        config = ensure_config(config)
-        params = self.dict()
-        params["stop"] = stop
-        params = {**params, **kwargs}
-        options = {"stop": stop}
-        inheritable_metadata = {
-            **(config.get("metadata") or {}),
-            **self._get_ls_params(stop=stop, **kwargs),
-        }
-        callback_manager = AsyncCallbackManager.configure(
-            config.get("callbacks"),
-            self.callbacks,
-            self.verbose,
-            config.get("tags"),
-            self.tags,
-            inheritable_metadata,
-            self.metadata,
-        )
-        (run_manager,) = await callback_manager.on_llm_start(
-            self._serialized,
-            [prompt],
-            invocation_params=params,
-            options=options,
-            name=config.get("run_name"),
-            run_id=config.pop("run_id", None),
-            batch_size=1,
-        )
-        generation: Optional[GenerationChunk] = None
-        try:
-            async for chunk in self._astream(
-                prompt,
-                stop=stop,
-                run_manager=run_manager,
-                **kwargs,
-            ):
-                yield chunk.text
-                if generation is None:
-                    generation = chunk
-                else:
-                    generation += chunk
-        except BaseException as e:
-            await run_manager.on_llm_error(
-                e,
-                response=LLMResult(generations=[[generation]] if generation else []),
-            )
-            raise
-
-        if generation is None:
-            err = ValueError("No generation chunks were returned")
-            await run_manager.on_llm_error(err, response=LLMResult(generations=[]))
-            raise err
-
-        await run_manager.on_llm_end(LLMResult(generations=[[generation]]))
-
-    # --- Custom methods ---
-
-    @abstractmethod
-    def _generate(
-        self,
-        prompts: list[str],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        """Run the LLM on the given prompts."""
-
-    async def _agenerate(
-        self,
-        prompts: list[str],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        """Run the LLM on the given prompts."""
-        return await run_in_executor(
-            None,
-            self._generate,
-            prompts,
-            stop,
-            run_manager.get_sync() if run_manager else None,
-            **kwargs,
-        )
-
-    def _stream(
-        self,
-        prompt: str,
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> Iterator[GenerationChunk]:
-        """Stream the LLM on the given prompt.
-
-        This method should be overridden by subclasses that support streaming.
-
-        If not implemented, the default behavior of calls to stream will be to
-        fallback to the non-streaming version of the model and return
-        the output as a single chunk.
-
-        Args:
-            prompt: The prompt to generate from.
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of these substrings.
-            run_manager: Callback manager for the run.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            An iterator of GenerationChunks.
-        """
-        raise NotImplementedError
-
-    async def _astream(
-        self,
-        prompt: str,
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[GenerationChunk]:
-        """An async version of the _stream method.
-
-        The default implementation uses the synchronous _stream method and wraps it in
-        an async iterator. Subclasses that need to provide a true async implementation
-        should override this method.
-
-        Args:
-            prompt: The prompt to generate from.
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of these substrings.
-            run_manager: Callback manager for the run.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            An async iterator of GenerationChunks.
-        """
-        iterator = await run_in_executor(
-            None,
-            self._stream,
-            prompt,
-            stop,
-            run_manager.get_sync() if run_manager else None,
-            **kwargs,
-        )
-        done = object()
-        while True:
-            item = await run_in_executor(
-                None,
-                next,
-                iterator,
-                done,  # type: ignore[call-arg, arg-type]
-            )
-            if item is done:
-                break
-            yield item  # type: ignore[misc]
-
-    def generate_prompt(
-        self,
-        prompts: list[PromptValue],
-        stop: Optional[list[str]] = None,
-        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        prompt_strings = [p.to_string() for p in prompts]
-        return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
-
-    async def agenerate_prompt(
-        self,
-        prompts: list[PromptValue],
-        stop: Optional[list[str]] = None,
-        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        prompt_strings = [p.to_string() for p in prompts]
-        return await self.agenerate(
-            prompt_strings, stop=stop, callbacks=callbacks, **kwargs
-        )
-
-    def _generate_helper(
-        self,
-        prompts: list[str],
-        stop: Optional[list[str]],
-        run_managers: list[CallbackManagerForLLMRun],
-        new_arg_supported: bool,
-        **kwargs: Any,
-    ) -> LLMResult:
-        try:
-            output = (
-                self._generate(
-                    prompts,
-                    stop=stop,
-                    # TODO: support multiple run managers
-                    run_manager=run_managers[0] if run_managers else None,
-                    **kwargs,
-                )
-                if new_arg_supported
-                else self._generate(prompts, stop=stop)
-            )
-        except BaseException as e:
-            for run_manager in run_managers:
-                run_manager.on_llm_error(e, response=LLMResult(generations=[]))
-            raise
-        flattened_outputs = output.flatten()
-        for manager, flattened_output in zip(run_managers, flattened_outputs):
-            manager.on_llm_end(flattened_output)
-        if run_managers:
-            output.run = [
-                RunInfo(run_id=run_manager.run_id) for run_manager in run_managers
-            ]
-        return output
-
-    def generate(
-        self,
-        prompts: list[str],
-        stop: Optional[list[str]] = None,
-        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,
-        *,
-        tags: Optional[Union[list[str], list[list[str]]]] = None,
-        metadata: Optional[Union[dict[str, Any], list[dict[str, Any]]]] = None,
-        run_name: Optional[Union[str, list[str]]] = None,
-        run_id: Optional[Union[uuid.UUID, list[Optional[uuid.UUID]]]] = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        """Pass a sequence of prompts to a model and return generations.
-
-        This method should make use of batched calls for models that expose a batched
-        API.
-
-        Use this method when you want to:
-            1. take advantage of batched calls,
-            2. need more output from the model than just the top generated value,
-            3. are building chains that are agnostic to the underlying language model
-                type (e.g., pure text completion models vs chat models).
-
-        Args:
-            prompts: List of string prompts.
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of these substrings.
-            callbacks: Callbacks to pass through. Used for executing additional
-                functionality, such as logging or streaming, throughout generation.
-            tags: List of tags to associate with each prompt. If provided, the length
-                of the list must match the length of the prompts list.
-            metadata: List of metadata dictionaries to associate with each prompt. If
-                provided, the length of the list must match the length of the prompts
-                list.
-            run_name: List of run names to associate with each prompt. If provided, the
-                length of the list must match the length of the prompts list.
-            run_id: List of run IDs to associate with each prompt. If provided, the
-                length of the list must match the length of the prompts list.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            An LLMResult, which contains a list of candidate Generations for each input
-                prompt and additional model provider-specific output.
-        """
-        if not isinstance(prompts, list):
-            msg = (
-                "Argument 'prompts' is expected to be of type List[str], received"
-                f" argument of type {type(prompts)}."
-            )
-            raise ValueError(msg)  # noqa: TRY004
-        # Create callback managers
-        if isinstance(metadata, list):
-            metadata = [
-                {
-                    **(meta or {}),
-                    **self._get_ls_params(stop=stop, **kwargs),
-                }
-                for meta in metadata
-            ]
-        elif isinstance(metadata, dict):
-            metadata = {
-                **(metadata or {}),
-                **self._get_ls_params(stop=stop, **kwargs),
-            }
-        else:
-            pass
-        if (
-            isinstance(callbacks, list)
-            and callbacks
-            and (
-                isinstance(callbacks[0], (list, BaseCallbackManager))
-                or callbacks[0] is None
-            )
-        ):
-            # We've received a list of callbacks args to apply to each input
-            if len(callbacks) != len(prompts):
-                msg = "callbacks must be the same length as prompts"
-                raise ValueError(msg)
-            if tags is not None and not (
-                isinstance(tags, list) and len(tags) == len(prompts)
-            ):
-                msg = "tags must be a list of the same length as prompts"
-                raise ValueError(msg)
-            if metadata is not None and not (
-                isinstance(metadata, list) and len(metadata) == len(prompts)
-            ):
-                msg = "metadata must be a list of the same length as prompts"
-                raise ValueError(msg)
-            if run_name is not None and not (
-                isinstance(run_name, list) and len(run_name) == len(prompts)
-            ):
-                msg = "run_name must be a list of the same length as prompts"
-                raise ValueError(msg)
-            callbacks = cast(list[Callbacks], callbacks)
-            tags_list = cast(list[Optional[list[str]]], tags or ([None] * len(prompts)))
-            metadata_list = cast(
-                list[Optional[dict[str, Any]]], metadata or ([{}] * len(prompts))
-            )
-            run_name_list = run_name or cast(
-                list[Optional[str]], ([None] * len(prompts))
-            )
-            callback_managers = [
-                CallbackManager.configure(
-                    callback,
-                    self.callbacks,
-                    self.verbose,
-                    tag,
-                    self.tags,
-                    meta,
-                    self.metadata,
-                )
-                for callback, tag, meta in zip(callbacks, tags_list, metadata_list)
-            ]
-        else:
-            # We've received a single callbacks arg to apply to all inputs
-            callback_managers = [
-                CallbackManager.configure(
-                    cast(Callbacks, callbacks),
-                    self.callbacks,
-                    self.verbose,
-                    cast(list[str], tags),
-                    self.tags,
-                    cast(dict[str, Any], metadata),
-                    self.metadata,
-                )
-            ] * len(prompts)
-            run_name_list = [cast(Optional[str], run_name)] * len(prompts)
-        run_ids_list = self._get_run_ids_list(run_id, prompts)
-        params = self.dict()
-        params["stop"] = stop
-        options = {"stop": stop}
-        (
-            existing_prompts,
-            llm_string,
-            missing_prompt_idxs,
-            missing_prompts,
-        ) = get_prompts(params, prompts, self.cache)
-        new_arg_supported = inspect.signature(self._generate).parameters.get(
-            "run_manager"
-        )
-        if (self.cache is None and get_llm_cache() is None) or self.cache is False:
-            run_managers = [
-                callback_manager.on_llm_start(
-                    self._serialized,
-                    [prompt],
-                    invocation_params=params,
-                    options=options,
-                    name=run_name,
-                    batch_size=len(prompts),
-                    run_id=run_id_,
-                )[0]
-                for callback_manager, prompt, run_name, run_id_ in zip(
-                    callback_managers, prompts, run_name_list, run_ids_list
-                )
-            ]
-            output = self._generate_helper(
-                prompts, stop, run_managers, bool(new_arg_supported), **kwargs
-            )
-            return output
-        if len(missing_prompts) > 0:
-            run_managers = [
-                callback_managers[idx].on_llm_start(
-                    self._serialized,
-                    [prompts[idx]],
-                    invocation_params=params,
-                    options=options,
-                    name=run_name_list[idx],
-                    batch_size=len(missing_prompts),
-                )[0]
-                for idx in missing_prompt_idxs
-            ]
-            new_results = self._generate_helper(
-                missing_prompts, stop, run_managers, bool(new_arg_supported), **kwargs
-            )
-            llm_output = update_cache(
-                self.cache,
-                existing_prompts,
-                llm_string,
-                missing_prompt_idxs,
-                new_results,
-                prompts,
-            )
-            run_info = (
-                [RunInfo(run_id=run_manager.run_id) for run_manager in run_managers]
-                if run_managers
-                else None
-            )
-        else:
-            llm_output = {}
-            run_info = None
-        generations = [existing_prompts[i] for i in range(len(prompts))]
-        return LLMResult(generations=generations, llm_output=llm_output, run=run_info)
-
-    @staticmethod
-    def _get_run_ids_list(
-        run_id: Optional[Union[uuid.UUID, list[Optional[uuid.UUID]]]], prompts: list
-    ) -> list:
-        if run_id is None:
-            return [None] * len(prompts)
-        if isinstance(run_id, list):
-            if len(run_id) != len(prompts):
-                msg = (
-                    "Number of manually provided run_id's does not match batch length."
-                    f" {len(run_id)} != {len(prompts)}"
-                )
-                raise ValueError(msg)
-            return run_id
-        return [run_id] + [None] * (len(prompts) - 1)
-
-    async def _agenerate_helper(
-        self,
-        prompts: list[str],
-        stop: Optional[list[str]],
-        run_managers: list[AsyncCallbackManagerForLLMRun],
-        new_arg_supported: bool,
-        **kwargs: Any,
-    ) -> LLMResult:
-        try:
-            output = (
-                await self._agenerate(
-                    prompts,
-                    stop=stop,
-                    run_manager=run_managers[0] if run_managers else None,
-                    **kwargs,
-                )
-                if new_arg_supported
-                else await self._agenerate(prompts, stop=stop)
-            )
-        except BaseException as e:
-            await asyncio.gather(
-                *[
-                    run_manager.on_llm_error(e, response=LLMResult(generations=[]))
-                    for run_manager in run_managers
-                ]
-            )
-            raise
-        flattened_outputs = output.flatten()
-        await asyncio.gather(
-            *[
-                run_manager.on_llm_end(flattened_output)
-                for run_manager, flattened_output in zip(
-                    run_managers, flattened_outputs
-                )
-            ]
-        )
-        if run_managers:
-            output.run = [
-                RunInfo(run_id=run_manager.run_id) for run_manager in run_managers
-            ]
-        return output
-
-    async def agenerate(
-        self,
-        prompts: list[str],
-        stop: Optional[list[str]] = None,
-        callbacks: Optional[Union[Callbacks, list[Callbacks]]] = None,
-        *,
-        tags: Optional[Union[list[str], list[list[str]]]] = None,
-        metadata: Optional[Union[dict[str, Any], list[dict[str, Any]]]] = None,
-        run_name: Optional[Union[str, list[str]]] = None,
-        run_id: Optional[Union[uuid.UUID, list[Optional[uuid.UUID]]]] = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        """Asynchronously pass a sequence of prompts to a model and return generations.
-
-        This method should make use of batched calls for models that expose a batched
-        API.
-
-        Use this method when you want to:
-            1. take advantage of batched calls,
-            2. need more output from the model than just the top generated value,
-            3. are building chains that are agnostic to the underlying language model
-                type (e.g., pure text completion models vs chat models).
-
-        Args:
-            prompts: List of string prompts.
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of these substrings.
-            callbacks: Callbacks to pass through. Used for executing additional
-                functionality, such as logging or streaming, throughout generation.
-            tags: List of tags to associate with each prompt. If provided, the length
-                of the list must match the length of the prompts list.
-            metadata: List of metadata dictionaries to associate with each prompt. If
-                provided, the length of the list must match the length of the prompts
-                list.
-            run_name: List of run names to associate with each prompt. If provided, the
-                length of the list must match the length of the prompts list.
-            run_id: List of run IDs to associate with each prompt. If provided, the
-                length of the list must match the length of the prompts list.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            An LLMResult, which contains a list of candidate Generations for each input
-                prompt and additional model provider-specific output.
-        """
-        if isinstance(metadata, list):
-            metadata = [
-                {
-                    **(meta or {}),
-                    **self._get_ls_params(stop=stop, **kwargs),
-                }
-                for meta in metadata
-            ]
-        elif isinstance(metadata, dict):
-            metadata = {
-                **(metadata or {}),
-                **self._get_ls_params(stop=stop, **kwargs),
-            }
-        else:
-            pass
-        # Create callback managers
-        if isinstance(callbacks, list) and (
-            isinstance(callbacks[0], (list, BaseCallbackManager))
-            or callbacks[0] is None
-        ):
-            # We've received a list of callbacks args to apply to each input
-            if len(callbacks) != len(prompts):
-                msg = "callbacks must be the same length as prompts"
-                raise ValueError(msg)
-            if tags is not None and not (
-                isinstance(tags, list) and len(tags) == len(prompts)
-            ):
-                msg = "tags must be a list of the same length as prompts"
-                raise ValueError(msg)
-            if metadata is not None and not (
-                isinstance(metadata, list) and len(metadata) == len(prompts)
-            ):
-                msg = "metadata must be a list of the same length as prompts"
-                raise ValueError(msg)
-            if run_name is not None and not (
-                isinstance(run_name, list) and len(run_name) == len(prompts)
-            ):
-                msg = "run_name must be a list of the same length as prompts"
-                raise ValueError(msg)
-            callbacks = cast(list[Callbacks], callbacks)
-            tags_list = cast(list[Optional[list[str]]], tags or ([None] * len(prompts)))
-            metadata_list = cast(
-                list[Optional[dict[str, Any]]], metadata or ([{}] * len(prompts))
-            )
-            run_name_list = run_name or cast(
-                list[Optional[str]], ([None] * len(prompts))
-            )
-            callback_managers = [
-                AsyncCallbackManager.configure(
-                    callback,
-                    self.callbacks,
-                    self.verbose,
-                    tag,
-                    self.tags,
-                    meta,
-                    self.metadata,
-                )
-                for callback, tag, meta in zip(callbacks, tags_list, metadata_list)
-            ]
-        else:
-            # We've received a single callbacks arg to apply to all inputs
-            callback_managers = [
-                AsyncCallbackManager.configure(
-                    cast(Callbacks, callbacks),
-                    self.callbacks,
-                    self.verbose,
-                    cast(list[str], tags),
-                    self.tags,
-                    cast(dict[str, Any], metadata),
-                    self.metadata,
-                )
-            ] * len(prompts)
-            run_name_list = [cast(Optional[str], run_name)] * len(prompts)
-        run_ids_list = self._get_run_ids_list(run_id, prompts)
-        params = self.dict()
-        params["stop"] = stop
-        options = {"stop": stop}
-        (
-            existing_prompts,
-            llm_string,
-            missing_prompt_idxs,
-            missing_prompts,
-        ) = await aget_prompts(params, prompts, self.cache)
-
-        # Verify whether the cache is set, and if the cache is set,
-        # verify whether the cache is available.
-        new_arg_supported = inspect.signature(self._agenerate).parameters.get(
-            "run_manager"
-        )
-        if (self.cache is None and get_llm_cache() is None) or self.cache is False:
-            run_managers = await asyncio.gather(
-                *[
-                    callback_manager.on_llm_start(
-                        self._serialized,
-                        [prompt],
-                        invocation_params=params,
-                        options=options,
-                        name=run_name,
-                        batch_size=len(prompts),
-                        run_id=run_id_,
-                    )
-                    for callback_manager, prompt, run_name, run_id_ in zip(
-                        callback_managers, prompts, run_name_list, run_ids_list
-                    )
-                ]
-            )
-            run_managers = [r[0] for r in run_managers]  # type: ignore[misc]
-            output = await self._agenerate_helper(
-                prompts,
-                stop,
-                run_managers,  # type: ignore[arg-type]
-                bool(new_arg_supported),
-                **kwargs,  # type: ignore[arg-type]
-            )
-            return output
-        if len(missing_prompts) > 0:
-            run_managers = await asyncio.gather(
-                *[
-                    callback_managers[idx].on_llm_start(
-                        self._serialized,
-                        [prompts[idx]],
-                        invocation_params=params,
-                        options=options,
-                        name=run_name_list[idx],
-                        batch_size=len(missing_prompts),
-                    )
-                    for idx in missing_prompt_idxs
-                ]
-            )
-            run_managers = [r[0] for r in run_managers]  # type: ignore[misc]
-            new_results = await self._agenerate_helper(
-                missing_prompts,
-                stop,
-                run_managers,  # type: ignore[arg-type]
-                bool(new_arg_supported),
-                **kwargs,  # type: ignore[arg-type]
-            )
-            llm_output = await aupdate_cache(
-                self.cache,
-                existing_prompts,
-                llm_string,
-                missing_prompt_idxs,
-                new_results,
-                prompts,
-            )
-            run_info = (
-                [RunInfo(run_id=run_manager.run_id) for run_manager in run_managers]  # type: ignore[attr-defined]
-                if run_managers
-                else None
-            )
-        else:
-            llm_output = {}
-            run_info = None
-        generations = [existing_prompts[i] for i in range(len(prompts))]
-        return LLMResult(generations=generations, llm_output=llm_output, run=run_info)
-
-    @deprecated("0.1.7", alternative="invoke", removal="1.0")
-    def __call__(
-        self,
-        prompt: str,
-        stop: Optional[list[str]] = None,
-        callbacks: Callbacks = None,
-        *,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> str:
-        """Check Cache and run the LLM on the given prompt and input.
-
-        Args:
-            prompt: The prompt to generate from.
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of these substrings.
-            callbacks: Callbacks to pass through. Used for executing additional
-                functionality, such as logging or streaming, throughout generation.
-            tags: List of tags to associate with the prompt.
-            metadata: Metadata to associate with the prompt.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            The generated text.
-
-        Raises:
-            ValueError: If the prompt is not a string.
-        """
-        if not isinstance(prompt, str):
-            msg = (
-                "Argument `prompt` is expected to be a string. Instead found "
-                f"{type(prompt)}. If you want to run the LLM on multiple prompts, use "
-                "`generate` instead."
-            )
-            raise ValueError(msg)  # noqa: TRY004
-        return (
-            self.generate(
-                [prompt],
-                stop=stop,
-                callbacks=callbacks,
-                tags=tags,
-                metadata=metadata,
-                **kwargs,
-            )
-            .generations[0][0]
-            .text
-        )
-
-    async def _call_async(
-        self,
-        prompt: str,
-        stop: Optional[list[str]] = None,
-        callbacks: Callbacks = None,
-        *,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> str:
-        """Check Cache and run the LLM on the given prompt and input."""
-        result = await self.agenerate(
-            [prompt],
-            stop=stop,
-            callbacks=callbacks,
-            tags=tags,
-            metadata=metadata,
-            **kwargs,
-        )
-        return result.generations[0][0].text
-
-    @deprecated("0.1.7", alternative="invoke", removal="1.0")
-    def predict(
-        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any
-    ) -> str:
-        _stop = None if stop is None else list(stop)
-        return self(text, stop=_stop, **kwargs)
-
-    @deprecated("0.1.7", alternative="invoke", removal="1.0")
-    def predict_messages(
-        self,
-        messages: list[BaseMessage],
-        *,
-        stop: Optional[Sequence[str]] = None,
-        **kwargs: Any,
-    ) -> BaseMessage:
-        text = get_buffer_string(messages)
-        _stop = None if stop is None else list(stop)
-        content = self(text, stop=_stop, **kwargs)
-        return AIMessage(content=content)
-
-    @deprecated("0.1.7", alternative="ainvoke", removal="1.0")
-    async def apredict(
-        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any
-    ) -> str:
-        _stop = None if stop is None else list(stop)
-        return await self._call_async(text, stop=_stop, **kwargs)
-
-    @deprecated("0.1.7", alternative="ainvoke", removal="1.0")
-    async def apredict_messages(
-        self,
-        messages: list[BaseMessage],
-        *,
-        stop: Optional[Sequence[str]] = None,
-        **kwargs: Any,
-    ) -> BaseMessage:
-        text = get_buffer_string(messages)
-        _stop = None if stop is None else list(stop)
-        content = await self._call_async(text, stop=_stop, **kwargs)
-        return AIMessage(content=content)
-
-    def __str__(self) -> str:
-        """Get a string representation of the object for printing."""
-        cls_name = f"\033[1m{self.__class__.__name__}\033[0m"
-        return f"{cls_name}\nParams: {self._identifying_params}"
-
-    @property
-    @abstractmethod
-    def _llm_type(self) -> str:
-        """Return type of llm."""
-
-    def dict(self, **kwargs: Any) -> dict:
-        """Return a dictionary of the LLM."""
-        starter_dict = dict(self._identifying_params)
-        starter_dict["_type"] = self._llm_type
-        return starter_dict
-
-    def save(self, file_path: Union[Path, str]) -> None:
-        """Save the LLM.
-
-        Args:
-            file_path: Path to file to save the LLM to.
-
-        Raises:
-            ValueError: If the file path is not a string or Path object.
-
-        Example:
-        .. code-block:: python
-
-            llm.save(file_path="path/llm.yaml")
-        """
-        # Convert file to Path object.
-        save_path = Path(file_path) if isinstance(file_path, str) else file_path
-
-        directory_path = save_path.parent
-        directory_path.mkdir(parents=True, exist_ok=True)
-
-        # Fetch dictionary to save
-        prompt_dict = self.dict()
-
-        if save_path.suffix == ".json":
-            with open(file_path, "w") as f:
-                json.dump(prompt_dict, f, indent=4)
-        elif save_path.suffix.endswith((".yaml", ".yml")):
-            with open(file_path, "w") as f:
-                yaml.dump(prompt_dict, f, default_flow_style=False)
-        else:
-            msg = f"{save_path} must be json or yaml"
-            raise ValueError(msg)
-
-
-class LLM(BaseLLM):
-    """Simple interface for implementing a custom LLM.
-
-    You should subclass this class and implement the following:
-
-    - `_call` method: Run the LLM on the given prompt and input (used by `invoke`).
-    - `_identifying_params` property: Return a dictionary of the identifying parameters
-        This is critical for caching and tracing purposes. Identifying parameters
-        is a dict that identifies the LLM.
-        It should mostly include a `model_name`.
-
-    Optional: Override the following methods to provide more optimizations:
-
-    - `_acall`: Provide a native async version of the `_call` method.
-        If not provided, will delegate to the synchronous version using
-        `run_in_executor`. (Used by `ainvoke`).
-    - `_stream`: Stream the LLM on the given prompt and input.
-        `stream` will use `_stream` if provided, otherwise it
-        use `_call` and output will arrive in one chunk.
-    - `_astream`: Override to provide a native async version of the `_stream` method.
-        `astream` will use `_astream` if provided, otherwise it will implement
-        a fallback behavior that will use `_stream` if `_stream` is implemented,
-        and use `_acall` if `_stream` is not implemented.
-
-    Please see the following guide for more information on how to
-    implement a custom LLM:
-
-    https://python.langchain.com/docs/how_to/custom_llm/
-    """
-
-    @abstractmethod
-    def _call(
-        self,
-        prompt: str,
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> str:
-        """Run the LLM on the given input.
-
-        Override this method to implement the LLM logic.
-
-        Args:
-            prompt: The prompt to generate from.
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of the stop substrings.
-                If stop tokens are not supported consider raising NotImplementedError.
-            run_manager: Callback manager for the run.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            The model output as a string. SHOULD NOT include the prompt.
-        """
-
-    async def _acall(
-        self,
-        prompt: str,
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> str:
-        """Async version of the _call method.
-
-        The default implementation delegates to the synchronous _call method using
-        `run_in_executor`. Subclasses that need to provide a true async implementation
-        should override this method to reduce the overhead of using `run_in_executor`.
-
-        Args:
-            prompt: The prompt to generate from.
-            stop: Stop words to use when generating. Model output is cut off at the
-                first occurrence of any of the stop substrings.
-                If stop tokens are not supported consider raising NotImplementedError.
-            run_manager: Callback manager for the run.
-            **kwargs: Arbitrary additional keyword arguments. These are usually passed
-                to the model provider API call.
-
-        Returns:
-            The model output as a string. SHOULD NOT include the prompt.
-        """
-        return await run_in_executor(
-            None,
-            self._call,
-            prompt,
-            stop,
-            run_manager.get_sync() if run_manager else None,
-            **kwargs,
-        )
-
-    def _generate(
-        self,
-        prompts: list[str],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        """Run the LLM on the given prompt and input."""
-        # TODO: add caching here.
-        generations = []
-        new_arg_supported = inspect.signature(self._call).parameters.get("run_manager")
-        for prompt in prompts:
-            text = (
-                self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
-                if new_arg_supported
-                else self._call(prompt, stop=stop, **kwargs)
-            )
-            generations.append([Generation(text=text)])
-        return LLMResult(generations=generations)
-
-    async def _agenerate(
-        self,
-        prompts: list[str],
-        stop: Optional[list[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        """Async run the LLM on the given prompt and input."""
-        generations = []
-        new_arg_supported = inspect.signature(self._acall).parameters.get("run_manager")
-        for prompt in prompts:
-            text = (
-                await self._acall(prompt, stop=stop, run_manager=run_manager, **kwargs)
-                if new_arg_supported
-                else await self._acall(prompt, stop=stop, **kwargs)
-            )
-            generations.append([Generation(text=text)])
-        return LLMResult(generations=generations)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/load/__init__.py ./custom_langchain_core/load/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/load/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/load/__init__.py	1970-01-01 09:00:00
@@ -1,7 +0,0 @@
-"""**Load** module helps with serialization and deserialization."""
-
-from langchain_core.load.dump import dumpd, dumps
-from langchain_core.load.load import load, loads
-from langchain_core.load.serializable import Serializable
-
-__all__ = ["dumpd", "dumps", "load", "loads", "Serializable"]
Binary files .venv/lib/python3.12/site-packages/langchain_core/load/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/load/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/load/__pycache__/dump.cpython-312.pyc and ./custom_langchain_core/load/__pycache__/dump.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/load/__pycache__/load.cpython-312.pyc and ./custom_langchain_core/load/__pycache__/load.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/load/__pycache__/mapping.cpython-312.pyc and ./custom_langchain_core/load/__pycache__/mapping.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/load/__pycache__/serializable.cpython-312.pyc and ./custom_langchain_core/load/__pycache__/serializable.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/load/dump.py ./custom_langchain_core/load/dump.py
--- .venv/lib/python3.12/site-packages/langchain_core/load/dump.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/load/dump.py	1970-01-01 09:00:00
@@ -1,70 +0,0 @@
-import json
-from typing import Any
-
-from langchain_core.load.serializable import Serializable, to_json_not_implemented
-
-
-def default(obj: Any) -> Any:
-    """Return a default value for a Serializable object or
-    a SerializedNotImplemented object.
-
-    Args:
-        obj: The object to serialize to json if it is a Serializable object.
-
-    Returns:
-        A json serializable object or a SerializedNotImplemented object.
-    """
-    if isinstance(obj, Serializable):
-        return obj.to_json()
-    else:
-        return to_json_not_implemented(obj)
-
-
-def dumps(obj: Any, *, pretty: bool = False, **kwargs: Any) -> str:
-    """Return a json string representation of an object.
-
-    Args:
-        obj: The object to dump.
-        pretty: Whether to pretty print the json. If true, the json will be
-            indented with 2 spaces (if no indent is provided as part of kwargs).
-            Default is False.
-        kwargs: Additional arguments to pass to json.dumps
-
-    Returns:
-        A json string representation of the object.
-
-    Raises:
-        ValueError: If `default` is passed as a kwarg.
-    """
-    if "default" in kwargs:
-        msg = "`default` should not be passed to dumps"
-        raise ValueError(msg)
-    try:
-        if pretty:
-            indent = kwargs.pop("indent", 2)
-            return json.dumps(obj, default=default, indent=indent, **kwargs)
-        else:
-            return json.dumps(obj, default=default, **kwargs)
-    except TypeError:
-        if pretty:
-            indent = kwargs.pop("indent", 2)
-            return json.dumps(to_json_not_implemented(obj), indent=indent, **kwargs)
-        else:
-            return json.dumps(to_json_not_implemented(obj), **kwargs)
-
-
-def dumpd(obj: Any) -> Any:
-    """Return a dict representation of an object.
-
-    Note:
-        Unfortunately this function is not as efficient as it could be
-        because it first dumps the object to a json string and then loads it
-        back into a dictionary.
-
-    Args:
-        obj: The object to dump.
-
-    Returns:
-        dictionary that can be serialized to json using json.dumps
-    """
-    return json.loads(dumps(obj))
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/load/load.py ./custom_langchain_core/load/load.py
--- .venv/lib/python3.12/site-packages/langchain_core/load/load.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/load/load.py	1970-01-01 09:00:00
@@ -1,239 +0,0 @@
-import importlib
-import json
-import os
-from typing import Any, Optional
-
-from langchain_core._api import beta
-from langchain_core.load.mapping import (
-    _JS_SERIALIZABLE_MAPPING,
-    _OG_SERIALIZABLE_MAPPING,
-    OLD_CORE_NAMESPACES_MAPPING,
-    SERIALIZABLE_MAPPING,
-)
-from langchain_core.load.serializable import Serializable
-
-DEFAULT_NAMESPACES = [
-    "langchain",
-    "langchain_core",
-    "langchain_community",
-    "langchain_anthropic",
-    "langchain_groq",
-    "langchain_google_genai",
-    "langchain_aws",
-    "langchain_openai",
-    "langchain_google_vertexai",
-    "langchain_mistralai",
-    "langchain_fireworks",
-    "langchain_xai",
-]
-# Namespaces for which only deserializing via the SERIALIZABLE_MAPPING is allowed.
-# Load by path is not allowed.
-DISALLOW_LOAD_FROM_PATH = [
-    "langchain_community",
-    "langchain",
-]
-
-ALL_SERIALIZABLE_MAPPINGS = {
-    **SERIALIZABLE_MAPPING,
-    **OLD_CORE_NAMESPACES_MAPPING,
-    **_OG_SERIALIZABLE_MAPPING,
-    **_JS_SERIALIZABLE_MAPPING,
-}
-
-
-class Reviver:
-    """Reviver for JSON objects."""
-
-    def __init__(
-        self,
-        secrets_map: Optional[dict[str, str]] = None,
-        valid_namespaces: Optional[list[str]] = None,
-        secrets_from_env: bool = True,
-        additional_import_mappings: Optional[
-            dict[tuple[str, ...], tuple[str, ...]]
-        ] = None,
-    ) -> None:
-        """Initialize the reviver.
-
-        Args:
-            secrets_map: A map of secrets to load. If a secret is not found in
-                the map, it will be loaded from the environment if `secrets_from_env`
-                is True. Defaults to None.
-            valid_namespaces: A list of additional namespaces (modules)
-                to allow to be deserialized. Defaults to None.
-            secrets_from_env: Whether to load secrets from the environment.
-                Defaults to True.
-            additional_import_mappings: A dictionary of additional namespace mappings
-                You can use this to override default mappings or add new mappings.
-                Defaults to None.
-        """
-        self.secrets_from_env = secrets_from_env
-        self.secrets_map = secrets_map or {}
-        # By default, only support langchain, but user can pass in additional namespaces
-        self.valid_namespaces = (
-            [*DEFAULT_NAMESPACES, *valid_namespaces]
-            if valid_namespaces
-            else DEFAULT_NAMESPACES
-        )
-        self.additional_import_mappings = additional_import_mappings or {}
-        self.import_mappings = (
-            {
-                **ALL_SERIALIZABLE_MAPPINGS,
-                **self.additional_import_mappings,
-            }
-            if self.additional_import_mappings
-            else ALL_SERIALIZABLE_MAPPINGS
-        )
-
-    def __call__(self, value: dict[str, Any]) -> Any:
-        if (
-            value.get("lc") == 1
-            and value.get("type") == "secret"
-            and value.get("id") is not None
-        ):
-            [key] = value["id"]
-            if key in self.secrets_map:
-                return self.secrets_map[key]
-            else:
-                if self.secrets_from_env and key in os.environ and os.environ[key]:
-                    return os.environ[key]
-                msg = f'Missing key "{key}" in load(secrets_map)'
-                raise KeyError(msg)
-
-        if (
-            value.get("lc") == 1
-            and value.get("type") == "not_implemented"
-            and value.get("id") is not None
-        ):
-            msg = (
-                "Trying to load an object that doesn't implement "
-                f"serialization: {value}"
-            )
-            raise NotImplementedError(msg)
-
-        if (
-            value.get("lc") == 1
-            and value.get("type") == "constructor"
-            and value.get("id") is not None
-        ):
-            [*namespace, name] = value["id"]
-            mapping_key = tuple(value["id"])
-
-            if (
-                namespace[0] not in self.valid_namespaces
-                # The root namespace ["langchain"] is not a valid identifier.
-                or namespace == ["langchain"]
-            ):
-                msg = f"Invalid namespace: {value}"
-                raise ValueError(msg)
-            # Has explicit import path.
-            elif mapping_key in self.import_mappings:
-                import_path = self.import_mappings[mapping_key]
-                # Split into module and name
-                import_dir, name = import_path[:-1], import_path[-1]
-                # Import module
-                mod = importlib.import_module(".".join(import_dir))
-            elif namespace[0] in DISALLOW_LOAD_FROM_PATH:
-                msg = (
-                    "Trying to deserialize something that cannot "
-                    "be deserialized in current version of langchain-core: "
-                    f"{mapping_key}."
-                )
-                raise ValueError(msg)
-            # Otherwise, treat namespace as path.
-            else:
-                mod = importlib.import_module(".".join(namespace))
-
-            cls = getattr(mod, name)
-
-            # The class must be a subclass of Serializable.
-            if not issubclass(cls, Serializable):
-                msg = f"Invalid namespace: {value}"
-                raise ValueError(msg)
-
-            # We don't need to recurse on kwargs
-            # as json.loads will do that for us.
-            kwargs = value.get("kwargs", {})
-            return cls(**kwargs)
-
-        return value
-
-
-@beta()
-def loads(
-    text: str,
-    *,
-    secrets_map: Optional[dict[str, str]] = None,
-    valid_namespaces: Optional[list[str]] = None,
-    secrets_from_env: bool = True,
-    additional_import_mappings: Optional[dict[tuple[str, ...], tuple[str, ...]]] = None,
-) -> Any:
-    """Revive a LangChain class from a JSON string.
-    Equivalent to `load(json.loads(text))`.
-
-    Args:
-        text: The string to load.
-        secrets_map: A map of secrets to load. If a secret is not found in
-            the map, it will be loaded from the environment if `secrets_from_env`
-            is True. Defaults to None.
-        valid_namespaces: A list of additional namespaces (modules)
-            to allow to be deserialized. Defaults to None.
-        secrets_from_env: Whether to load secrets from the environment.
-            Defaults to True.
-        additional_import_mappings: A dictionary of additional namespace mappings
-            You can use this to override default mappings or add new mappings.
-            Defaults to None.
-
-    Returns:
-        Revived LangChain objects.
-    """
-    return json.loads(
-        text,
-        object_hook=Reviver(
-            secrets_map, valid_namespaces, secrets_from_env, additional_import_mappings
-        ),
-    )
-
-
-@beta()
-def load(
-    obj: Any,
-    *,
-    secrets_map: Optional[dict[str, str]] = None,
-    valid_namespaces: Optional[list[str]] = None,
-    secrets_from_env: bool = True,
-    additional_import_mappings: Optional[dict[tuple[str, ...], tuple[str, ...]]] = None,
-) -> Any:
-    """Revive a LangChain class from a JSON object. Use this if you already
-    have a parsed JSON object, eg. from `json.load` or `orjson.loads`.
-
-    Args:
-        obj: The object to load.
-        secrets_map: A map of secrets to load. If a secret is not found in
-            the map, it will be loaded from the environment if `secrets_from_env`
-            is True. Defaults to None.
-        valid_namespaces: A list of additional namespaces (modules)
-            to allow to be deserialized. Defaults to None.
-        secrets_from_env: Whether to load secrets from the environment.
-            Defaults to True.
-        additional_import_mappings: A dictionary of additional namespace mappings
-            You can use this to override default mappings or add new mappings.
-            Defaults to None.
-
-    Returns:
-        Revived LangChain objects.
-    """
-    reviver = Reviver(
-        secrets_map, valid_namespaces, secrets_from_env, additional_import_mappings
-    )
-
-    def _load(obj: Any) -> Any:
-        if isinstance(obj, dict):
-            # Need to revive leaf nodes before reviving this node
-            loaded_obj = {k: _load(v) for k, v in obj.items()}
-            return reviver(loaded_obj)
-        if isinstance(obj, list):
-            return [_load(o) for o in obj]
-        return obj
-
-    return _load(obj)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/load/mapping.py ./custom_langchain_core/load/mapping.py
--- .venv/lib/python3.12/site-packages/langchain_core/load/mapping.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/load/mapping.py	1970-01-01 09:00:00
@@ -1,1048 +0,0 @@
-"""This file contains a mapping between the lc_namespace path for a given
-subclass that implements from Serializable to the namespace
-where that class is actually located.
-
-This mapping helps maintain the ability to serialize and deserialize
-well-known LangChain objects even if they are moved around in the codebase
-across different LangChain versions.
-
-For example,
-
-The code for AIMessage class is located in langchain_core.messages.ai.AIMessage,
-This message is associated with the lc_namespace
-["langchain", "schema", "messages", "AIMessage"],
-because this code was originally in langchain.schema.messages.AIMessage.
-
-The mapping allows us to deserialize an AIMessage created with an older
-version of LangChain where the code was in a different location.
-"""
-
-# First value is the value that it is serialized as
-# Second value is the path to load it from
-SERIALIZABLE_MAPPING: dict[tuple[str, ...], tuple[str, ...]] = {
-    ("langchain", "schema", "messages", "AIMessage"): (
-        "langchain_core",
-        "messages",
-        "ai",
-        "AIMessage",
-    ),
-    ("langchain", "schema", "messages", "AIMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "ai",
-        "AIMessageChunk",
-    ),
-    ("langchain", "schema", "messages", "BaseMessage"): (
-        "langchain_core",
-        "messages",
-        "base",
-        "BaseMessage",
-    ),
-    ("langchain", "schema", "messages", "BaseMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "base",
-        "BaseMessageChunk",
-    ),
-    ("langchain", "schema", "messages", "ChatMessage"): (
-        "langchain_core",
-        "messages",
-        "chat",
-        "ChatMessage",
-    ),
-    ("langchain", "schema", "messages", "FunctionMessage"): (
-        "langchain_core",
-        "messages",
-        "function",
-        "FunctionMessage",
-    ),
-    ("langchain", "schema", "messages", "HumanMessage"): (
-        "langchain_core",
-        "messages",
-        "human",
-        "HumanMessage",
-    ),
-    ("langchain", "schema", "messages", "SystemMessage"): (
-        "langchain_core",
-        "messages",
-        "system",
-        "SystemMessage",
-    ),
-    ("langchain", "schema", "messages", "ToolMessage"): (
-        "langchain_core",
-        "messages",
-        "tool",
-        "ToolMessage",
-    ),
-    ("langchain", "schema", "messages", "RemoveMessage"): (
-        "langchain_core",
-        "messages",
-        "modifier",
-        "RemoveMessage",
-    ),
-    ("langchain", "schema", "agent", "AgentAction"): (
-        "langchain_core",
-        "agents",
-        "AgentAction",
-    ),
-    ("langchain", "schema", "agent", "AgentFinish"): (
-        "langchain_core",
-        "agents",
-        "AgentFinish",
-    ),
-    ("langchain", "schema", "prompt_template", "BasePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "base",
-        "BasePromptTemplate",
-    ),
-    ("langchain", "chains", "llm", "LLMChain"): (
-        "langchain",
-        "chains",
-        "llm",
-        "LLMChain",
-    ),
-    ("langchain", "prompts", "prompt", "PromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "prompt",
-        "PromptTemplate",
-    ),
-    ("langchain", "prompts", "chat", "MessagesPlaceholder"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "MessagesPlaceholder",
-    ),
-    ("langchain", "llms", "openai", "OpenAI"): (
-        "langchain_openai",
-        "llms",
-        "base",
-        "OpenAI",
-    ),
-    ("langchain", "prompts", "chat", "ChatPromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "ChatPromptTemplate",
-    ),
-    ("langchain", "prompts", "chat", "HumanMessagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "HumanMessagePromptTemplate",
-    ),
-    ("langchain", "prompts", "chat", "SystemMessagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "SystemMessagePromptTemplate",
-    ),
-    ("langchain", "prompts", "image", "ImagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "image",
-        "ImagePromptTemplate",
-    ),
-    ("langchain", "schema", "agent", "AgentActionMessageLog"): (
-        "langchain_core",
-        "agents",
-        "AgentActionMessageLog",
-    ),
-    ("langchain", "schema", "agent", "ToolAgentAction"): (
-        "langchain",
-        "agents",
-        "output_parsers",
-        "tools",
-        "ToolAgentAction",
-    ),
-    ("langchain", "prompts", "chat", "BaseMessagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "BaseMessagePromptTemplate",
-    ),
-    ("langchain", "schema", "output", "ChatGeneration"): (
-        "langchain_core",
-        "outputs",
-        "chat_generation",
-        "ChatGeneration",
-    ),
-    ("langchain", "schema", "output", "Generation"): (
-        "langchain_core",
-        "outputs",
-        "generation",
-        "Generation",
-    ),
-    ("langchain", "schema", "document", "Document"): (
-        "langchain_core",
-        "documents",
-        "base",
-        "Document",
-    ),
-    ("langchain", "output_parsers", "fix", "OutputFixingParser"): (
-        "langchain",
-        "output_parsers",
-        "fix",
-        "OutputFixingParser",
-    ),
-    ("langchain", "prompts", "chat", "AIMessagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "AIMessagePromptTemplate",
-    ),
-    ("langchain", "output_parsers", "regex", "RegexParser"): (
-        "langchain",
-        "output_parsers",
-        "regex",
-        "RegexParser",
-    ),
-    ("langchain", "schema", "runnable", "DynamicRunnable"): (
-        "langchain_core",
-        "runnables",
-        "configurable",
-        "DynamicRunnable",
-    ),
-    ("langchain", "schema", "prompt", "PromptValue"): (
-        "langchain_core",
-        "prompt_values",
-        "PromptValue",
-    ),
-    ("langchain", "schema", "runnable", "RunnableBinding"): (
-        "langchain_core",
-        "runnables",
-        "base",
-        "RunnableBinding",
-    ),
-    ("langchain", "schema", "runnable", "RunnableBranch"): (
-        "langchain_core",
-        "runnables",
-        "branch",
-        "RunnableBranch",
-    ),
-    ("langchain", "schema", "runnable", "RunnableWithFallbacks"): (
-        "langchain_core",
-        "runnables",
-        "fallbacks",
-        "RunnableWithFallbacks",
-    ),
-    ("langchain", "schema", "output_parser", "StrOutputParser"): (
-        "langchain_core",
-        "output_parsers",
-        "string",
-        "StrOutputParser",
-    ),
-    ("langchain", "chat_models", "openai", "ChatOpenAI"): (
-        "langchain_openai",
-        "chat_models",
-        "base",
-        "ChatOpenAI",
-    ),
-    ("langchain", "output_parsers", "list", "CommaSeparatedListOutputParser"): (
-        "langchain_core",
-        "output_parsers",
-        "list",
-        "CommaSeparatedListOutputParser",
-    ),
-    ("langchain", "schema", "runnable", "RunnableParallel"): (
-        "langchain_core",
-        "runnables",
-        "base",
-        "RunnableParallel",
-    ),
-    ("langchain", "chat_models", "azure_openai", "AzureChatOpenAI"): (
-        "langchain_openai",
-        "chat_models",
-        "azure",
-        "AzureChatOpenAI",
-    ),
-    ("langchain", "chat_models", "bedrock", "BedrockChat"): (
-        "langchain_aws",
-        "chat_models",
-        "bedrock",
-        "ChatBedrock",
-    ),
-    ("langchain", "chat_models", "anthropic", "ChatAnthropic"): (
-        "langchain_anthropic",
-        "chat_models",
-        "ChatAnthropic",
-    ),
-    ("langchain_groq", "chat_models", "ChatGroq"): (
-        "langchain_groq",
-        "chat_models",
-        "ChatGroq",
-    ),
-    ("langchain", "chat_models", "fireworks", "ChatFireworks"): (
-        "langchain_fireworks",
-        "chat_models",
-        "ChatFireworks",
-    ),
-    ("langchain", "chat_models", "google_palm", "ChatGooglePalm"): (
-        "langchain",
-        "chat_models",
-        "google_palm",
-        "ChatGooglePalm",
-    ),
-    ("langchain", "chat_models", "vertexai", "ChatVertexAI"): (
-        "langchain_google_vertexai",
-        "chat_models",
-        "ChatVertexAI",
-    ),
-    ("langchain", "chat_models", "mistralai", "ChatMistralAI"): (
-        "langchain_mistralai",
-        "chat_models",
-        "ChatMistralAI",
-    ),
-    ("langchain", "chat_models", "bedrock", "ChatBedrock"): (
-        "langchain_aws",
-        "chat_models",
-        "bedrock",
-        "ChatBedrock",
-    ),
-    ("langchain_google_genai", "chat_models", "ChatGoogleGenerativeAI"): (
-        "langchain_google_genai",
-        "chat_models",
-        "ChatGoogleGenerativeAI",
-    ),
-    ("langchain", "schema", "output", "ChatGenerationChunk"): (
-        "langchain_core",
-        "outputs",
-        "chat_generation",
-        "ChatGenerationChunk",
-    ),
-    ("langchain", "schema", "messages", "ChatMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "chat",
-        "ChatMessageChunk",
-    ),
-    ("langchain", "schema", "messages", "HumanMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "human",
-        "HumanMessageChunk",
-    ),
-    ("langchain", "schema", "messages", "FunctionMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "function",
-        "FunctionMessageChunk",
-    ),
-    ("langchain", "schema", "messages", "SystemMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "system",
-        "SystemMessageChunk",
-    ),
-    ("langchain", "schema", "messages", "ToolMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "tool",
-        "ToolMessageChunk",
-    ),
-    ("langchain", "schema", "output", "GenerationChunk"): (
-        "langchain_core",
-        "outputs",
-        "generation",
-        "GenerationChunk",
-    ),
-    ("langchain", "llms", "openai", "BaseOpenAI"): (
-        "langchain",
-        "llms",
-        "openai",
-        "BaseOpenAI",
-    ),
-    ("langchain", "llms", "bedrock", "Bedrock"): (
-        "langchain_aws",
-        "llms",
-        "bedrock",
-        "BedrockLLM",
-    ),
-    ("langchain", "llms", "fireworks", "Fireworks"): (
-        "langchain_fireworks",
-        "llms",
-        "Fireworks",
-    ),
-    ("langchain", "llms", "google_palm", "GooglePalm"): (
-        "langchain",
-        "llms",
-        "google_palm",
-        "GooglePalm",
-    ),
-    ("langchain", "llms", "openai", "AzureOpenAI"): (
-        "langchain_openai",
-        "llms",
-        "azure",
-        "AzureOpenAI",
-    ),
-    ("langchain", "llms", "replicate", "Replicate"): (
-        "langchain",
-        "llms",
-        "replicate",
-        "Replicate",
-    ),
-    ("langchain", "llms", "vertexai", "VertexAI"): (
-        "langchain_vertexai",
-        "llms",
-        "VertexAI",
-    ),
-    ("langchain", "output_parsers", "combining", "CombiningOutputParser"): (
-        "langchain",
-        "output_parsers",
-        "combining",
-        "CombiningOutputParser",
-    ),
-    ("langchain", "schema", "prompt_template", "BaseChatPromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "BaseChatPromptTemplate",
-    ),
-    ("langchain", "prompts", "chat", "ChatMessagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "ChatMessagePromptTemplate",
-    ),
-    ("langchain", "prompts", "few_shot_with_templates", "FewShotPromptWithTemplates"): (
-        "langchain_core",
-        "prompts",
-        "few_shot_with_templates",
-        "FewShotPromptWithTemplates",
-    ),
-    ("langchain", "prompts", "pipeline", "PipelinePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "pipeline",
-        "PipelinePromptTemplate",
-    ),
-    ("langchain", "prompts", "base", "StringPromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "string",
-        "StringPromptTemplate",
-    ),
-    ("langchain", "prompts", "base", "StringPromptValue"): (
-        "langchain_core",
-        "prompt_values",
-        "StringPromptValue",
-    ),
-    ("langchain", "prompts", "chat", "BaseStringMessagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "BaseStringMessagePromptTemplate",
-    ),
-    ("langchain", "prompts", "chat", "ChatPromptValue"): (
-        "langchain_core",
-        "prompt_values",
-        "ChatPromptValue",
-    ),
-    ("langchain", "prompts", "chat", "ChatPromptValueConcrete"): (
-        "langchain_core",
-        "prompt_values",
-        "ChatPromptValueConcrete",
-    ),
-    ("langchain", "schema", "runnable", "HubRunnable"): (
-        "langchain",
-        "runnables",
-        "hub",
-        "HubRunnable",
-    ),
-    ("langchain", "schema", "runnable", "RunnableBindingBase"): (
-        "langchain_core",
-        "runnables",
-        "base",
-        "RunnableBindingBase",
-    ),
-    ("langchain", "schema", "runnable", "OpenAIFunctionsRouter"): (
-        "langchain",
-        "runnables",
-        "openai_functions",
-        "OpenAIFunctionsRouter",
-    ),
-    ("langchain", "schema", "runnable", "RouterRunnable"): (
-        "langchain_core",
-        "runnables",
-        "router",
-        "RouterRunnable",
-    ),
-    ("langchain", "schema", "runnable", "RunnablePassthrough"): (
-        "langchain_core",
-        "runnables",
-        "passthrough",
-        "RunnablePassthrough",
-    ),
-    ("langchain", "schema", "runnable", "RunnableSequence"): (
-        "langchain_core",
-        "runnables",
-        "base",
-        "RunnableSequence",
-    ),
-    ("langchain", "schema", "runnable", "RunnableEach"): (
-        "langchain_core",
-        "runnables",
-        "base",
-        "RunnableEach",
-    ),
-    ("langchain", "schema", "runnable", "RunnableEachBase"): (
-        "langchain_core",
-        "runnables",
-        "base",
-        "RunnableEachBase",
-    ),
-    ("langchain", "schema", "runnable", "RunnableConfigurableAlternatives"): (
-        "langchain_core",
-        "runnables",
-        "configurable",
-        "RunnableConfigurableAlternatives",
-    ),
-    ("langchain", "schema", "runnable", "RunnableConfigurableFields"): (
-        "langchain_core",
-        "runnables",
-        "configurable",
-        "RunnableConfigurableFields",
-    ),
-    ("langchain", "schema", "runnable", "RunnableWithMessageHistory"): (
-        "langchain_core",
-        "runnables",
-        "history",
-        "RunnableWithMessageHistory",
-    ),
-    ("langchain", "schema", "runnable", "RunnableAssign"): (
-        "langchain_core",
-        "runnables",
-        "passthrough",
-        "RunnableAssign",
-    ),
-    ("langchain", "schema", "runnable", "RunnableRetry"): (
-        "langchain_core",
-        "runnables",
-        "retry",
-        "RunnableRetry",
-    ),
-    ("langchain_core", "prompts", "structured", "StructuredPrompt"): (
-        "langchain_core",
-        "prompts",
-        "structured",
-        "StructuredPrompt",
-    ),
-}
-
-# Needed for backwards compatibility for old versions of LangChain where things
-# Were in different place
-_OG_SERIALIZABLE_MAPPING: dict[tuple[str, ...], tuple[str, ...]] = {
-    ("langchain", "schema", "AIMessage"): (
-        "langchain_core",
-        "messages",
-        "ai",
-        "AIMessage",
-    ),
-    ("langchain", "schema", "ChatMessage"): (
-        "langchain_core",
-        "messages",
-        "chat",
-        "ChatMessage",
-    ),
-    ("langchain", "schema", "FunctionMessage"): (
-        "langchain_core",
-        "messages",
-        "function",
-        "FunctionMessage",
-    ),
-    ("langchain", "schema", "HumanMessage"): (
-        "langchain_core",
-        "messages",
-        "human",
-        "HumanMessage",
-    ),
-    ("langchain", "schema", "SystemMessage"): (
-        "langchain_core",
-        "messages",
-        "system",
-        "SystemMessage",
-    ),
-    ("langchain", "schema", "prompt_template", "ImagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "image",
-        "ImagePromptTemplate",
-    ),
-    ("langchain", "schema", "agent", "OpenAIToolAgentAction"): (
-        "langchain",
-        "agents",
-        "output_parsers",
-        "openai_tools",
-        "OpenAIToolAgentAction",
-    ),
-}
-
-# Needed for backwards compatibility for a few versions where we serialized
-# with langchain_core paths.
-OLD_CORE_NAMESPACES_MAPPING: dict[tuple[str, ...], tuple[str, ...]] = {
-    ("langchain_core", "messages", "ai", "AIMessage"): (
-        "langchain_core",
-        "messages",
-        "ai",
-        "AIMessage",
-    ),
-    ("langchain_core", "messages", "ai", "AIMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "ai",
-        "AIMessageChunk",
-    ),
-    ("langchain_core", "messages", "base", "BaseMessage"): (
-        "langchain_core",
-        "messages",
-        "base",
-        "BaseMessage",
-    ),
-    ("langchain_core", "messages", "base", "BaseMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "base",
-        "BaseMessageChunk",
-    ),
-    ("langchain_core", "messages", "chat", "ChatMessage"): (
-        "langchain_core",
-        "messages",
-        "chat",
-        "ChatMessage",
-    ),
-    ("langchain_core", "messages", "function", "FunctionMessage"): (
-        "langchain_core",
-        "messages",
-        "function",
-        "FunctionMessage",
-    ),
-    ("langchain_core", "messages", "human", "HumanMessage"): (
-        "langchain_core",
-        "messages",
-        "human",
-        "HumanMessage",
-    ),
-    ("langchain_core", "messages", "system", "SystemMessage"): (
-        "langchain_core",
-        "messages",
-        "system",
-        "SystemMessage",
-    ),
-    ("langchain_core", "messages", "tool", "ToolMessage"): (
-        "langchain_core",
-        "messages",
-        "tool",
-        "ToolMessage",
-    ),
-    ("langchain_core", "agents", "AgentAction"): (
-        "langchain_core",
-        "agents",
-        "AgentAction",
-    ),
-    ("langchain_core", "agents", "AgentFinish"): (
-        "langchain_core",
-        "agents",
-        "AgentFinish",
-    ),
-    ("langchain_core", "prompts", "base", "BasePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "base",
-        "BasePromptTemplate",
-    ),
-    ("langchain_core", "prompts", "prompt", "PromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "prompt",
-        "PromptTemplate",
-    ),
-    ("langchain_core", "prompts", "chat", "MessagesPlaceholder"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "MessagesPlaceholder",
-    ),
-    ("langchain_core", "prompts", "chat", "ChatPromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "ChatPromptTemplate",
-    ),
-    ("langchain_core", "prompts", "chat", "HumanMessagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "HumanMessagePromptTemplate",
-    ),
-    ("langchain_core", "prompts", "chat", "SystemMessagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "SystemMessagePromptTemplate",
-    ),
-    ("langchain_core", "agents", "AgentActionMessageLog"): (
-        "langchain_core",
-        "agents",
-        "AgentActionMessageLog",
-    ),
-    ("langchain_core", "prompts", "chat", "BaseMessagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "BaseMessagePromptTemplate",
-    ),
-    ("langchain_core", "outputs", "chat_generation", "ChatGeneration"): (
-        "langchain_core",
-        "outputs",
-        "chat_generation",
-        "ChatGeneration",
-    ),
-    ("langchain_core", "outputs", "generation", "Generation"): (
-        "langchain_core",
-        "outputs",
-        "generation",
-        "Generation",
-    ),
-    ("langchain_core", "documents", "base", "Document"): (
-        "langchain_core",
-        "documents",
-        "base",
-        "Document",
-    ),
-    ("langchain_core", "prompts", "chat", "AIMessagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "AIMessagePromptTemplate",
-    ),
-    ("langchain_core", "runnables", "configurable", "DynamicRunnable"): (
-        "langchain_core",
-        "runnables",
-        "configurable",
-        "DynamicRunnable",
-    ),
-    ("langchain_core", "prompt_values", "PromptValue"): (
-        "langchain_core",
-        "prompt_values",
-        "PromptValue",
-    ),
-    ("langchain_core", "runnables", "base", "RunnableBinding"): (
-        "langchain_core",
-        "runnables",
-        "base",
-        "RunnableBinding",
-    ),
-    ("langchain_core", "runnables", "branch", "RunnableBranch"): (
-        "langchain_core",
-        "runnables",
-        "branch",
-        "RunnableBranch",
-    ),
-    ("langchain_core", "runnables", "fallbacks", "RunnableWithFallbacks"): (
-        "langchain_core",
-        "runnables",
-        "fallbacks",
-        "RunnableWithFallbacks",
-    ),
-    ("langchain_core", "output_parsers", "string", "StrOutputParser"): (
-        "langchain_core",
-        "output_parsers",
-        "string",
-        "StrOutputParser",
-    ),
-    ("langchain_core", "output_parsers", "list", "CommaSeparatedListOutputParser"): (
-        "langchain_core",
-        "output_parsers",
-        "list",
-        "CommaSeparatedListOutputParser",
-    ),
-    ("langchain_core", "runnables", "base", "RunnableParallel"): (
-        "langchain_core",
-        "runnables",
-        "base",
-        "RunnableParallel",
-    ),
-    ("langchain_core", "outputs", "chat_generation", "ChatGenerationChunk"): (
-        "langchain_core",
-        "outputs",
-        "chat_generation",
-        "ChatGenerationChunk",
-    ),
-    ("langchain_core", "messages", "chat", "ChatMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "chat",
-        "ChatMessageChunk",
-    ),
-    ("langchain_core", "messages", "human", "HumanMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "human",
-        "HumanMessageChunk",
-    ),
-    ("langchain_core", "messages", "function", "FunctionMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "function",
-        "FunctionMessageChunk",
-    ),
-    ("langchain_core", "messages", "system", "SystemMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "system",
-        "SystemMessageChunk",
-    ),
-    ("langchain_core", "messages", "tool", "ToolMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "tool",
-        "ToolMessageChunk",
-    ),
-    ("langchain_core", "outputs", "generation", "GenerationChunk"): (
-        "langchain_core",
-        "outputs",
-        "generation",
-        "GenerationChunk",
-    ),
-    ("langchain_core", "prompts", "chat", "BaseChatPromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "BaseChatPromptTemplate",
-    ),
-    ("langchain_core", "prompts", "chat", "ChatMessagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "ChatMessagePromptTemplate",
-    ),
-    (
-        "langchain_core",
-        "prompts",
-        "few_shot_with_templates",
-        "FewShotPromptWithTemplates",
-    ): (
-        "langchain_core",
-        "prompts",
-        "few_shot_with_templates",
-        "FewShotPromptWithTemplates",
-    ),
-    ("langchain_core", "prompts", "pipeline", "PipelinePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "pipeline",
-        "PipelinePromptTemplate",
-    ),
-    ("langchain_core", "prompts", "string", "StringPromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "string",
-        "StringPromptTemplate",
-    ),
-    ("langchain_core", "prompt_values", "StringPromptValue"): (
-        "langchain_core",
-        "prompt_values",
-        "StringPromptValue",
-    ),
-    ("langchain_core", "prompts", "chat", "BaseStringMessagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "chat",
-        "BaseStringMessagePromptTemplate",
-    ),
-    ("langchain_core", "prompt_values", "ChatPromptValue"): (
-        "langchain_core",
-        "prompt_values",
-        "ChatPromptValue",
-    ),
-    ("langchain_core", "prompt_values", "ChatPromptValueConcrete"): (
-        "langchain_core",
-        "prompt_values",
-        "ChatPromptValueConcrete",
-    ),
-    ("langchain_core", "runnables", "base", "RunnableBindingBase"): (
-        "langchain_core",
-        "runnables",
-        "base",
-        "RunnableBindingBase",
-    ),
-    ("langchain_core", "runnables", "router", "RouterRunnable"): (
-        "langchain_core",
-        "runnables",
-        "router",
-        "RouterRunnable",
-    ),
-    ("langchain_core", "runnables", "passthrough", "RunnablePassthrough"): (
-        "langchain_core",
-        "runnables",
-        "passthrough",
-        "RunnablePassthrough",
-    ),
-    ("langchain_core", "runnables", "base", "RunnableSequence"): (
-        "langchain_core",
-        "runnables",
-        "base",
-        "RunnableSequence",
-    ),
-    ("langchain_core", "runnables", "base", "RunnableEach"): (
-        "langchain_core",
-        "runnables",
-        "base",
-        "RunnableEach",
-    ),
-    ("langchain_core", "runnables", "base", "RunnableEachBase"): (
-        "langchain_core",
-        "runnables",
-        "base",
-        "RunnableEachBase",
-    ),
-    (
-        "langchain_core",
-        "runnables",
-        "configurable",
-        "RunnableConfigurableAlternatives",
-    ): (
-        "langchain_core",
-        "runnables",
-        "configurable",
-        "RunnableConfigurableAlternatives",
-    ),
-    ("langchain_core", "runnables", "configurable", "RunnableConfigurableFields"): (
-        "langchain_core",
-        "runnables",
-        "configurable",
-        "RunnableConfigurableFields",
-    ),
-    ("langchain_core", "runnables", "history", "RunnableWithMessageHistory"): (
-        "langchain_core",
-        "runnables",
-        "history",
-        "RunnableWithMessageHistory",
-    ),
-    ("langchain_core", "runnables", "passthrough", "RunnableAssign"): (
-        "langchain_core",
-        "runnables",
-        "passthrough",
-        "RunnableAssign",
-    ),
-    ("langchain_core", "runnables", "retry", "RunnableRetry"): (
-        "langchain_core",
-        "runnables",
-        "retry",
-        "RunnableRetry",
-    ),
-}
-
-_JS_SERIALIZABLE_MAPPING: dict[tuple[str, ...], tuple[str, ...]] = {
-    ("langchain_core", "messages", "AIMessage"): (
-        "langchain_core",
-        "messages",
-        "ai",
-        "AIMessage",
-    ),
-    ("langchain_core", "messages", "AIMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "ai",
-        "AIMessageChunk",
-    ),
-    ("langchain_core", "messages", "BaseMessage"): (
-        "langchain_core",
-        "messages",
-        "base",
-        "BaseMessage",
-    ),
-    ("langchain_core", "messages", "BaseMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "base",
-        "BaseMessageChunk",
-    ),
-    ("langchain_core", "messages", "ChatMessage"): (
-        "langchain_core",
-        "messages",
-        "chat",
-        "ChatMessage",
-    ),
-    ("langchain_core", "messages", "ChatMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "chat",
-        "ChatMessageChunk",
-    ),
-    ("langchain_core", "messages", "FunctionMessage"): (
-        "langchain_core",
-        "messages",
-        "function",
-        "FunctionMessage",
-    ),
-    ("langchain_core", "messages", "FunctionMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "function",
-        "FunctionMessageChunk",
-    ),
-    ("langchain_core", "messages", "HumanMessage"): (
-        "langchain_core",
-        "messages",
-        "human",
-        "HumanMessage",
-    ),
-    ("langchain_core", "messages", "HumanMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "human",
-        "HumanMessageChunk",
-    ),
-    ("langchain_core", "messages", "SystemMessage"): (
-        "langchain_core",
-        "messages",
-        "system",
-        "SystemMessage",
-    ),
-    ("langchain_core", "messages", "SystemMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "system",
-        "SystemMessageChunk",
-    ),
-    ("langchain_core", "messages", "ToolMessage"): (
-        "langchain_core",
-        "messages",
-        "tool",
-        "ToolMessage",
-    ),
-    ("langchain_core", "messages", "ToolMessageChunk"): (
-        "langchain_core",
-        "messages",
-        "tool",
-        "ToolMessageChunk",
-    ),
-    ("langchain_core", "prompts", "image", "ImagePromptTemplate"): (
-        "langchain_core",
-        "prompts",
-        "image",
-        "ImagePromptTemplate",
-    ),
-    ("langchain", "chat_models", "bedrock", "ChatBedrock"): (
-        "langchain_aws",
-        "chat_models",
-        "ChatBedrock",
-    ),
-    ("langchain", "chat_models", "google_genai", "ChatGoogleGenerativeAI"): (
-        "langchain_google_genai",
-        "chat_models",
-        "ChatGoogleGenerativeAI",
-    ),
-    ("langchain", "chat_models", "groq", "ChatGroq"): (
-        "langchain_groq",
-        "chat_models",
-        "ChatGroq",
-    ),
-    ("langchain", "chat_models", "bedrock", "BedrockChat"): (
-        "langchain_aws",
-        "chat_models",
-        "ChatBedrock",
-    ),
-}
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/load/serializable.py ./custom_langchain_core/load/serializable.py
--- .venv/lib/python3.12/site-packages/langchain_core/load/serializable.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/load/serializable.py	1970-01-01 09:00:00
@@ -1,367 +0,0 @@
-import contextlib
-from abc import ABC
-from typing import (
-    Any,
-    Literal,
-    Optional,
-    TypedDict,
-    Union,
-    cast,
-)
-
-from pydantic import BaseModel, ConfigDict
-from pydantic.fields import FieldInfo
-from typing_extensions import NotRequired
-
-
-class BaseSerialized(TypedDict):
-    """Base class for serialized objects.
-
-    Parameters:
-        lc: The version of the serialization format.
-        id: The unique identifier of the object.
-        name: The name of the object. Optional.
-        graph: The graph of the object. Optional.
-    """
-
-    lc: int
-    id: list[str]
-    name: NotRequired[str]
-    graph: NotRequired[dict[str, Any]]
-
-
-class SerializedConstructor(BaseSerialized):
-    """Serialized constructor.
-
-    Parameters:
-        type: The type of the object. Must be "constructor".
-        kwargs: The constructor arguments.
-    """
-
-    type: Literal["constructor"]
-    kwargs: dict[str, Any]
-
-
-class SerializedSecret(BaseSerialized):
-    """Serialized secret.
-
-    Parameters:
-        type: The type of the object. Must be "secret".
-    """
-
-    type: Literal["secret"]
-
-
-class SerializedNotImplemented(BaseSerialized):
-    """Serialized not implemented.
-
-    Parameters:
-        type: The type of the object. Must be "not_implemented".
-        repr: The representation of the object. Optional.
-    """
-
-    type: Literal["not_implemented"]
-    repr: Optional[str]
-
-
-def try_neq_default(value: Any, key: str, model: BaseModel) -> bool:
-    """Try to determine if a value is different from the default.
-
-    Args:
-        value: The value.
-        key: The key.
-        model: The pydantic model.
-
-    Returns:
-        Whether the value is different from the default.
-
-    Raises:
-        Exception: If the key is not in the model.
-    """
-    field = model.model_fields[key]
-    return _try_neq_default(value, field)
-
-
-def _try_neq_default(value: Any, field: FieldInfo) -> bool:
-    # Handle edge case: inequality of two objects does not evaluate to a bool (e.g. two
-    # Pandas DataFrames).
-    try:
-        return bool(field.get_default() != value)
-    except Exception as _:
-        try:
-            return all(field.get_default() != value)
-        except Exception as _:
-            try:
-                return value is not field.default
-            except Exception as _:
-                return False
-
-
-class Serializable(BaseModel, ABC):
-    """Serializable base class.
-
-    This class is used to serialize objects to JSON.
-
-    It relies on the following methods and properties:
-
-    - `is_lc_serializable`: Is this class serializable?
-        By design, even if a class inherits from Serializable, it is not serializable by
-        default. This is to prevent accidental serialization of objects that should not
-        be serialized.
-    - `get_lc_namespace`: Get the namespace of the langchain object.
-        During deserialization, this namespace is used to identify
-        the correct class to instantiate.
-        Please see the `Reviver` class in `langchain_core.load.load` for more details.
-        During deserialization an additional mapping is handle
-        classes that have moved or been renamed across package versions.
-    - `lc_secrets`: A map of constructor argument names to secret ids.
-    - `lc_attributes`: List of additional attribute names that should be included
-        as part of the serialized representation.
-    """
-
-    # Remove default BaseModel init docstring.
-    def __init__(self, *args: Any, **kwargs: Any) -> None:
-        """"""
-        super().__init__(*args, **kwargs)
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Is this class serializable?
-
-        By design, even if a class inherits from Serializable, it is not serializable by
-        default. This is to prevent accidental serialization of objects that should not
-        be serialized.
-
-        Returns:
-            Whether the class is serializable. Default is False.
-        """
-        return False
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-
-        For example, if the class is `langchain.llms.openai.OpenAI`, then the
-        namespace is ["langchain", "llms", "openai"]
-        """
-        return cls.__module__.split(".")
-
-    @property
-    def lc_secrets(self) -> dict[str, str]:
-        """A map of constructor argument names to secret ids.
-
-        For example,
-            {"openai_api_key": "OPENAI_API_KEY"}
-        """
-        return {}
-
-    @property
-    def lc_attributes(self) -> dict:
-        """List of attribute names that should be included in the serialized kwargs.
-
-        These attributes must be accepted by the constructor.
-        Default is an empty dictionary.
-        """
-        return {}
-
-    @classmethod
-    def lc_id(cls) -> list[str]:
-        """A unique identifier for this class for serialization purposes.
-
-        The unique identifier is a list of strings that describes the path
-        to the object.
-        For example, for the class `langchain.llms.openai.OpenAI`, the id is
-        ["langchain", "llms", "openai", "OpenAI"].
-        """
-        # Pydantic generics change the class name. So we need to do the following
-        if (
-            "origin" in cls.__pydantic_generic_metadata__
-            and cls.__pydantic_generic_metadata__["origin"] is not None
-        ):
-            original_name = cls.__pydantic_generic_metadata__["origin"].__name__
-        else:
-            original_name = cls.__name__
-        return [*cls.get_lc_namespace(), original_name]
-
-    model_config = ConfigDict(
-        extra="ignore",
-    )
-
-    def __repr_args__(self) -> Any:
-        return [
-            (k, v)
-            for k, v in super().__repr_args__()
-            if (k not in self.model_fields or try_neq_default(v, k, self))
-        ]
-
-    def to_json(self) -> Union[SerializedConstructor, SerializedNotImplemented]:
-        """Serialize the object to JSON.
-
-        Returns:
-            A json serializable object or a SerializedNotImplemented object.
-        """
-        if not self.is_lc_serializable():
-            return self.to_json_not_implemented()
-
-        secrets = {}
-        # Get latest values for kwargs if there is an attribute with same name
-        lc_kwargs = {}
-        for k, v in self:
-            if not _is_field_useful(self, k, v):
-                continue
-            # Do nothing if the field is excluded
-            if k in self.model_fields and self.model_fields[k].exclude:
-                continue
-
-            lc_kwargs[k] = getattr(self, k, v)
-
-        # Merge the lc_secrets and lc_attributes from every class in the MRO
-        for cls in [None, *self.__class__.mro()]:
-            # Once we get to Serializable, we're done
-            if cls is Serializable:
-                break
-
-            if cls:
-                deprecated_attributes = [
-                    "lc_namespace",
-                    "lc_serializable",
-                ]
-
-                for attr in deprecated_attributes:
-                    if hasattr(cls, attr):
-                        msg = (
-                            f"Class {self.__class__} has a deprecated "
-                            f"attribute {attr}. Please use the corresponding "
-                            f"classmethod instead."
-                        )
-                        raise ValueError(msg)
-
-            # Get a reference to self bound to each class in the MRO
-            this = cast(Serializable, self if cls is None else super(cls, self))
-
-            secrets.update(this.lc_secrets)
-            # Now also add the aliases for the secrets
-            # This ensures known secret aliases are hidden.
-            # Note: this does NOT hide any other extra kwargs
-            # that are not present in the fields.
-            for key in list(secrets):
-                value = secrets[key]
-                if key in this.model_fields:
-                    alias = this.model_fields[key].alias
-                    if alias is not None:
-                        secrets[alias] = value
-            lc_kwargs.update(this.lc_attributes)
-
-        # include all secrets, even if not specified in kwargs
-        # as these secrets may be passed as an environment variable instead
-        for key in secrets:
-            secret_value = getattr(self, key, None) or lc_kwargs.get(key)
-            if secret_value is not None:
-                lc_kwargs.update({key: secret_value})
-
-        return {
-            "lc": 1,
-            "type": "constructor",
-            "id": self.lc_id(),
-            "kwargs": lc_kwargs
-            if not secrets
-            else _replace_secrets(lc_kwargs, secrets),
-        }
-
-    def to_json_not_implemented(self) -> SerializedNotImplemented:
-        return to_json_not_implemented(self)
-
-
-def _is_field_useful(inst: Serializable, key: str, value: Any) -> bool:
-    """Check if a field is useful as a constructor argument.
-
-    Args:
-        inst: The instance.
-        key: The key.
-        value: The value.
-
-    Returns:
-        Whether the field is useful. If the field is required, it is useful.
-        If the field is not required, it is useful if the value is not None.
-        If the field is not required and the value is None, it is useful if the
-        default value is different from the value.
-    """
-    field = inst.model_fields.get(key)
-    if not field:
-        return False
-
-    if field.is_required():
-        return True
-
-    # Handle edge case: a value cannot be converted to a boolean (e.g. a
-    # Pandas DataFrame).
-    try:
-        value_is_truthy = bool(value)
-    except Exception as _:
-        value_is_truthy = False
-
-    if value_is_truthy:
-        return True
-
-    # Value is still falsy here!
-    if field.default_factory is dict and isinstance(value, dict):
-        return False
-
-    # Value is still falsy here!
-    if field.default_factory is list and isinstance(value, list):
-        return False
-
-    value_neq_default = _try_neq_default(value, field)
-
-    # If value is falsy and does not match the default
-    return value_is_truthy or value_neq_default
-
-
-def _replace_secrets(
-    root: dict[Any, Any], secrets_map: dict[str, str]
-) -> dict[Any, Any]:
-    result = root.copy()
-    for path, secret_id in secrets_map.items():
-        [*parts, last] = path.split(".")
-        current = result
-        for part in parts:
-            if part not in current:
-                break
-            current[part] = current[part].copy()
-            current = current[part]
-        if last in current:
-            current[last] = {
-                "lc": 1,
-                "type": "secret",
-                "id": [secret_id],
-            }
-    return result
-
-
-def to_json_not_implemented(obj: object) -> SerializedNotImplemented:
-    """Serialize a "not implemented" object.
-
-    Args:
-        obj: object to serialize.
-
-    Returns:
-        SerializedNotImplemented
-    """
-    _id: list[str] = []
-    try:
-        if hasattr(obj, "__name__"):
-            _id = [*obj.__module__.split("."), obj.__name__]
-        elif hasattr(obj, "__class__"):
-            _id = [*obj.__class__.__module__.split("."), obj.__class__.__name__]
-    except Exception:
-        pass
-
-    result: SerializedNotImplemented = {
-        "lc": 1,
-        "type": "not_implemented",
-        "id": _id,
-        "repr": None,
-    }
-    with contextlib.suppress(Exception):
-        result["repr"] = repr(obj)
-    return result
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/memory.py ./custom_langchain_core/memory.py
--- .venv/lib/python3.12/site-packages/langchain_core/memory.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/memory.py	1970-01-01 09:00:00
@@ -1,115 +0,0 @@
-"""**Memory** maintains Chain state, incorporating context from past runs.
-
-This module contains memory abstractions from LangChain v0.0.x.
-
-These abstractions are now deprecated and will be removed in LangChain v1.0.0.
-"""  # noqa: E501
-
-from __future__ import annotations
-
-from abc import ABC, abstractmethod
-from typing import Any
-
-from pydantic import ConfigDict
-
-from langchain_core._api import deprecated
-from langchain_core.load.serializable import Serializable
-from langchain_core.runnables import run_in_executor
-
-
-@deprecated(
-    since="0.3.3",
-    removal="1.0.0",
-    message=(
-        "Please see the migration guide at: "
-        "https://python.langchain.com/docs/versions/migrating_memory/"
-    ),
-)
-class BaseMemory(Serializable, ABC):
-    """Abstract base class for memory in Chains.
-
-    Memory refers to state in Chains. Memory can be used to store information about
-        past executions of a Chain and inject that information into the inputs of
-        future executions of the Chain. For example, for conversational Chains Memory
-        can be used to store conversations and automatically add them to future model
-        prompts so that the model has the necessary context to respond coherently to
-        the latest input.
-
-    Example:
-        .. code-block:: python
-
-            class SimpleMemory(BaseMemory):
-                memories: Dict[str, Any] = dict()
-
-                @property
-                def memory_variables(self) -> List[str]:
-                    return list(self.memories.keys())
-
-                def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:
-                    return self.memories
-
-                def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
-                    pass
-
-                def clear(self) -> None:
-                    pass
-    """  # noqa: E501
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    @property
-    @abstractmethod
-    def memory_variables(self) -> list[str]:
-        """The string keys this memory class will add to chain inputs."""
-
-    @abstractmethod
-    def load_memory_variables(self, inputs: dict[str, Any]) -> dict[str, Any]:
-        """Return key-value pairs given the text input to the chain.
-
-        Args:
-            inputs: The inputs to the chain.
-
-        Returns:
-            A dictionary of key-value pairs.
-        """
-
-    async def aload_memory_variables(self, inputs: dict[str, Any]) -> dict[str, Any]:
-        """Async return key-value pairs given the text input to the chain.
-
-        Args:
-            inputs: The inputs to the chain.
-
-        Returns:
-            A dictionary of key-value pairs.
-        """
-        return await run_in_executor(None, self.load_memory_variables, inputs)
-
-    @abstractmethod
-    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -> None:
-        """Save the context of this chain run to memory.
-
-        Args:
-            inputs: The inputs to the chain.
-            outputs: The outputs of the chain.
-        """
-
-    async def asave_context(
-        self, inputs: dict[str, Any], outputs: dict[str, str]
-    ) -> None:
-        """Async save the context of this chain run to memory.
-
-        Args:
-            inputs: The inputs to the chain.
-            outputs: The outputs of the chain.
-        """
-        await run_in_executor(None, self.save_context, inputs, outputs)
-
-    @abstractmethod
-    def clear(self) -> None:
-        """Clear memory contents."""
-
-    async def aclear(self) -> None:
-        """Async clear memory contents."""
-        await run_in_executor(None, self.clear)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/messages/__init__.py ./custom_langchain_core/messages/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/messages/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/messages/__init__.py	1970-01-01 09:00:00
@@ -1,88 +0,0 @@
-"""**Messages** are objects used in prompts and chat conversations.
-
-**Class hierarchy:**
-
-.. code-block::
-
-    BaseMessage --> SystemMessage, AIMessage, HumanMessage, ChatMessage, FunctionMessage, ToolMessage
-                --> BaseMessageChunk --> SystemMessageChunk, AIMessageChunk, HumanMessageChunk, ChatMessageChunk, FunctionMessageChunk, ToolMessageChunk
-
-**Main helpers:**
-
-.. code-block::
-
-    ChatPromptTemplate
-
-"""  # noqa: E501
-
-from langchain_core.messages.ai import (
-    AIMessage,
-    AIMessageChunk,
-)
-from langchain_core.messages.base import (
-    BaseMessage,
-    BaseMessageChunk,
-    merge_content,
-    message_to_dict,
-    messages_to_dict,
-)
-from langchain_core.messages.chat import ChatMessage, ChatMessageChunk
-from langchain_core.messages.function import FunctionMessage, FunctionMessageChunk
-from langchain_core.messages.human import HumanMessage, HumanMessageChunk
-from langchain_core.messages.modifier import RemoveMessage
-from langchain_core.messages.system import SystemMessage, SystemMessageChunk
-from langchain_core.messages.tool import (
-    InvalidToolCall,
-    ToolCall,
-    ToolCallChunk,
-    ToolMessage,
-    ToolMessageChunk,
-)
-from langchain_core.messages.utils import (
-    AnyMessage,
-    MessageLikeRepresentation,
-    _message_from_dict,
-    convert_to_messages,
-    convert_to_openai_messages,
-    filter_messages,
-    get_buffer_string,
-    merge_message_runs,
-    message_chunk_to_message,
-    messages_from_dict,
-    trim_messages,
-)
-
-__all__ = [
-    "AIMessage",
-    "AIMessageChunk",
-    "AnyMessage",
-    "BaseMessage",
-    "BaseMessageChunk",
-    "ChatMessage",
-    "ChatMessageChunk",
-    "FunctionMessage",
-    "FunctionMessageChunk",
-    "HumanMessage",
-    "HumanMessageChunk",
-    "InvalidToolCall",
-    "MessageLikeRepresentation",
-    "SystemMessage",
-    "SystemMessageChunk",
-    "ToolCall",
-    "ToolCallChunk",
-    "ToolMessage",
-    "ToolMessageChunk",
-    "RemoveMessage",
-    "_message_from_dict",
-    "convert_to_messages",
-    "get_buffer_string",
-    "merge_content",
-    "message_chunk_to_message",
-    "message_to_dict",
-    "messages_from_dict",
-    "messages_to_dict",
-    "filter_messages",
-    "merge_message_runs",
-    "trim_messages",
-    "convert_to_openai_messages",
-]
Binary files .venv/lib/python3.12/site-packages/langchain_core/messages/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/messages/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/messages/__pycache__/ai.cpython-312.pyc and ./custom_langchain_core/messages/__pycache__/ai.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/messages/__pycache__/base.cpython-312.pyc and ./custom_langchain_core/messages/__pycache__/base.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/messages/__pycache__/chat.cpython-312.pyc and ./custom_langchain_core/messages/__pycache__/chat.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/messages/__pycache__/function.cpython-312.pyc and ./custom_langchain_core/messages/__pycache__/function.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/messages/__pycache__/human.cpython-312.pyc and ./custom_langchain_core/messages/__pycache__/human.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/messages/__pycache__/modifier.cpython-312.pyc and ./custom_langchain_core/messages/__pycache__/modifier.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/messages/__pycache__/system.cpython-312.pyc and ./custom_langchain_core/messages/__pycache__/system.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/messages/__pycache__/tool.cpython-312.pyc and ./custom_langchain_core/messages/__pycache__/tool.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/messages/__pycache__/utils.cpython-312.pyc and ./custom_langchain_core/messages/__pycache__/utils.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/messages/ai.py ./custom_langchain_core/messages/ai.py
--- .venv/lib/python3.12/site-packages/langchain_core/messages/ai.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/messages/ai.py	1970-01-01 09:00:00
@@ -1,566 +0,0 @@
-import json
-import operator
-from typing import Any, Literal, Optional, Union, cast
-
-from pydantic import model_validator
-from typing_extensions import NotRequired, Self, TypedDict
-
-from langchain_core.messages.base import (
-    BaseMessage,
-    BaseMessageChunk,
-    merge_content,
-)
-from langchain_core.messages.tool import (
-    InvalidToolCall,
-    ToolCall,
-    ToolCallChunk,
-    default_tool_chunk_parser,
-    default_tool_parser,
-)
-from langchain_core.messages.tool import (
-    invalid_tool_call as create_invalid_tool_call,
-)
-from langchain_core.messages.tool import (
-    tool_call as create_tool_call,
-)
-from langchain_core.messages.tool import (
-    tool_call_chunk as create_tool_call_chunk,
-)
-from langchain_core.utils._merge import merge_dicts, merge_lists
-from langchain_core.utils.json import parse_partial_json
-from langchain_core.utils.usage import _dict_int_op
-
-
-class InputTokenDetails(TypedDict, total=False):
-    """Breakdown of input token counts.
-
-    Does *not* need to sum to full input token count. Does *not* need to have all keys.
-
-    Example:
-
-        .. code-block:: python
-
-            {
-                "audio": 10,
-                "cache_creation": 200,
-                "cache_read": 100,
-            }
-
-    .. versionadded:: 0.3.9
-    """
-
-    audio: int
-    """Audio input tokens."""
-    cache_creation: int
-    """Input tokens that were cached and there was a cache miss.
-
-    Since there was a cache miss, the cache was created from these tokens.
-    """
-    cache_read: int
-    """Input tokens that were cached and there was a cache hit.
-
-    Since there was a cache hit, the tokens were read from the cache. More precisely,
-    the model state given these tokens was read from the cache.
-    """
-
-
-class OutputTokenDetails(TypedDict, total=False):
-    """Breakdown of output token counts.
-
-    Does *not* need to sum to full output token count. Does *not* need to have all keys.
-
-    Example:
-
-        .. code-block:: python
-
-            {
-                "audio": 10,
-                "reasoning": 200,
-            }
-
-    .. versionadded:: 0.3.9
-    """
-
-    audio: int
-    """Audio output tokens."""
-    reasoning: int
-    """Reasoning output tokens.
-
-    Tokens generated by the model in a chain of thought process (i.e. by OpenAI's o1
-    models) that are not returned as part of model output.
-    """
-
-
-class UsageMetadata(TypedDict):
-    """Usage metadata for a message, such as token counts.
-
-    This is a standard representation of token usage that is consistent across models.
-
-    Example:
-
-        .. code-block:: python
-
-            {
-                "input_tokens": 350,
-                "output_tokens": 240,
-                "total_tokens": 590,
-                "input_token_details": {
-                    "audio": 10,
-                    "cache_creation": 200,
-                    "cache_read": 100,
-                },
-                "output_token_details": {
-                    "audio": 10,
-                    "reasoning": 200,
-                }
-            }
-
-    .. versionchanged:: 0.3.9
-
-        Added ``input_token_details`` and ``output_token_details``.
-    """
-
-    input_tokens: int
-    """Count of input (or prompt) tokens. Sum of all input token types."""
-    output_tokens: int
-    """Count of output (or completion) tokens. Sum of all output token types."""
-    total_tokens: int
-    """Total token count. Sum of input_tokens + output_tokens."""
-    input_token_details: NotRequired[InputTokenDetails]
-    """Breakdown of input token counts.
-
-    Does *not* need to sum to full input token count. Does *not* need to have all keys.
-    """
-    output_token_details: NotRequired[OutputTokenDetails]
-    """Breakdown of output token counts.
-
-    Does *not* need to sum to full output token count. Does *not* need to have all keys.
-    """
-
-
-class AIMessage(BaseMessage):
-    """Message from an AI.
-
-    AIMessage is returned from a chat model as a response to a prompt.
-
-    This message represents the output of the model and consists of both
-    the raw output as returned by the model together standardized fields
-    (e.g., tool calls, usage metadata) added by the LangChain framework.
-    """
-
-    example: bool = False
-    """Use to denote that a message is part of an example conversation.
-
-    At the moment, this is ignored by most models. Usage is discouraged.
-    """
-
-    tool_calls: list[ToolCall] = []
-    """If provided, tool calls associated with the message."""
-    invalid_tool_calls: list[InvalidToolCall] = []
-    """If provided, tool calls with parsing errors associated with the message."""
-    usage_metadata: Optional[UsageMetadata] = None
-    """If provided, usage metadata for a message, such as token counts.
-
-    This is a standard representation of token usage that is consistent across models.
-    """
-
-    type: Literal["ai"] = "ai"
-    """The type of the message (used for deserialization). Defaults to "ai"."""
-
-    def __init__(
-        self, content: Union[str, list[Union[str, dict]]], **kwargs: Any
-    ) -> None:
-        """Pass in content as positional arg.
-
-        Args:
-            content: The content of the message.
-            kwargs: Additional arguments to pass to the parent class.
-        """
-        super().__init__(content=content, **kwargs)
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-
-        Returns:
-            The namespace of the langchain object.
-            Defaults to ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
-
-    @property
-    def lc_attributes(self) -> dict:
-        """Attrs to be serialized even if they are derived from other init args."""
-        return {
-            "tool_calls": self.tool_calls,
-            "invalid_tool_calls": self.invalid_tool_calls,
-        }
-
-    @model_validator(mode="before")
-    @classmethod
-    def _backwards_compat_tool_calls(cls, values: dict) -> Any:
-        check_additional_kwargs = not any(
-            values.get(k)
-            for k in ("tool_calls", "invalid_tool_calls", "tool_call_chunks")
-        )
-        if check_additional_kwargs and (
-            raw_tool_calls := values.get("additional_kwargs", {}).get("tool_calls")
-        ):
-            try:
-                if issubclass(cls, AIMessageChunk):  # type: ignore
-                    values["tool_call_chunks"] = default_tool_chunk_parser(
-                        raw_tool_calls
-                    )
-                else:
-                    parsed_tool_calls, parsed_invalid_tool_calls = default_tool_parser(
-                        raw_tool_calls
-                    )
-                    values["tool_calls"] = parsed_tool_calls
-                    values["invalid_tool_calls"] = parsed_invalid_tool_calls
-            except Exception:
-                pass
-
-        # Ensure "type" is properly set on all tool call-like dicts.
-        if tool_calls := values.get("tool_calls"):
-            updated: list = []
-            for tc in tool_calls:
-                updated.append(
-                    create_tool_call(**{k: v for k, v in tc.items() if k != "type"})
-                )
-            values["tool_calls"] = updated
-        if invalid_tool_calls := values.get("invalid_tool_calls"):
-            updated = []
-            for tc in invalid_tool_calls:
-                updated.append(
-                    create_invalid_tool_call(
-                        **{k: v for k, v in tc.items() if k != "type"}
-                    )
-                )
-            values["invalid_tool_calls"] = updated
-
-        if tool_call_chunks := values.get("tool_call_chunks"):
-            updated = []
-            for tc in tool_call_chunks:
-                updated.append(
-                    create_tool_call_chunk(
-                        **{k: v for k, v in tc.items() if k != "type"}
-                    )
-                )
-            values["tool_call_chunks"] = updated
-
-        return values
-
-    def pretty_repr(self, html: bool = False) -> str:
-        """Return a pretty representation of the message.
-
-        Args:
-            html: Whether to return an HTML-formatted string.
-                 Defaults to False.
-
-        Returns:
-            A pretty representation of the message.
-        """
-        base = super().pretty_repr(html=html)
-        lines = []
-
-        def _format_tool_args(tc: Union[ToolCall, InvalidToolCall]) -> list[str]:
-            lines = [
-                f"  {tc.get('name', 'Tool')} ({tc.get('id')})",
-                f" Call ID: {tc.get('id')}",
-            ]
-            if tc.get("error"):
-                lines.append(f"  Error: {tc.get('error')}")
-            lines.append("  Args:")
-            args = tc.get("args")
-            if isinstance(args, str):
-                lines.append(f"    {args}")
-            elif isinstance(args, dict):
-                for arg, value in args.items():
-                    lines.append(f"    {arg}: {value}")
-            return lines
-
-        if self.tool_calls:
-            lines.append("Tool Calls:")
-            for tc in self.tool_calls:
-                lines.extend(_format_tool_args(tc))
-        if self.invalid_tool_calls:
-            lines.append("Invalid Tool Calls:")
-            for itc in self.invalid_tool_calls:
-                lines.extend(_format_tool_args(itc))
-        return (base.strip() + "\n" + "\n".join(lines)).strip()
-
-
-AIMessage.model_rebuild()
-
-
-class AIMessageChunk(AIMessage, BaseMessageChunk):
-    """Message chunk from an AI."""
-
-    # Ignoring mypy re-assignment here since we're overriding the value
-    # to make sure that the chunk variant can be discriminated from the
-    # non-chunk variant.
-    type: Literal["AIMessageChunk"] = "AIMessageChunk"  # type: ignore
-    """The type of the message (used for deserialization).
-    Defaults to "AIMessageChunk"."""
-
-    tool_call_chunks: list[ToolCallChunk] = []
-    """If provided, tool call chunks associated with the message."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-
-        Returns:
-            The namespace of the langchain object.
-            Defaults to ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
-
-    @property
-    def lc_attributes(self) -> dict:
-        """Attrs to be serialized even if they are derived from other init args."""
-        return {
-            "tool_calls": self.tool_calls,
-            "invalid_tool_calls": self.invalid_tool_calls,
-        }
-
-    @model_validator(mode="after")
-    def init_tool_calls(self) -> Self:
-        """Initialize tool calls from tool call chunks.
-
-        Args:
-            values: The values to validate.
-
-        Returns:
-            The values with tool calls initialized.
-
-        Raises:
-            ValueError: If the tool call chunks are malformed.
-        """
-        if not self.tool_call_chunks:
-            if self.tool_calls:
-                self.tool_call_chunks = [
-                    create_tool_call_chunk(
-                        name=tc["name"],
-                        args=json.dumps(tc["args"]),
-                        id=tc["id"],
-                        index=None,
-                    )
-                    for tc in self.tool_calls
-                ]
-            if self.invalid_tool_calls:
-                tool_call_chunks = self.tool_call_chunks
-                tool_call_chunks.extend(
-                    [
-                        create_tool_call_chunk(
-                            name=tc["name"], args=tc["args"], id=tc["id"], index=None
-                        )
-                        for tc in self.invalid_tool_calls
-                    ]
-                )
-                self.tool_call_chunks = tool_call_chunks
-
-            return self
-        tool_calls = []
-        invalid_tool_calls = []
-
-        def add_chunk_to_invalid_tool_calls(chunk: ToolCallChunk) -> None:
-            invalid_tool_calls.append(
-                create_invalid_tool_call(
-                    name=chunk["name"],
-                    args=chunk["args"],
-                    id=chunk["id"],
-                    error=None,
-                )
-            )
-
-        for chunk in self.tool_call_chunks:
-            try:
-                args_ = parse_partial_json(chunk["args"]) if chunk["args"] != "" else {}  # type: ignore[arg-type]
-                if isinstance(args_, dict):
-                    tool_calls.append(
-                        create_tool_call(
-                            name=chunk["name"] or "",
-                            args=args_,
-                            id=chunk["id"],
-                        )
-                    )
-                else:
-                    add_chunk_to_invalid_tool_calls(chunk)
-            except Exception:
-                add_chunk_to_invalid_tool_calls(chunk)
-        self.tool_calls = tool_calls
-        self.invalid_tool_calls = invalid_tool_calls
-        return self
-
-    def __add__(self, other: Any) -> BaseMessageChunk:  # type: ignore
-        if isinstance(other, AIMessageChunk):
-            return add_ai_message_chunks(self, other)
-        elif isinstance(other, (list, tuple)) and all(
-            isinstance(o, AIMessageChunk) for o in other
-        ):
-            return add_ai_message_chunks(self, *other)
-        return super().__add__(other)
-
-
-def add_ai_message_chunks(
-    left: AIMessageChunk, *others: AIMessageChunk
-) -> AIMessageChunk:
-    """Add multiple AIMessageChunks together."""
-    if any(left.example != o.example for o in others):
-        msg = "Cannot concatenate AIMessageChunks with different example values."
-        raise ValueError(msg)
-
-    content = merge_content(left.content, *(o.content for o in others))
-    additional_kwargs = merge_dicts(
-        left.additional_kwargs, *(o.additional_kwargs for o in others)
-    )
-    response_metadata = merge_dicts(
-        left.response_metadata, *(o.response_metadata for o in others)
-    )
-
-    # Merge tool call chunks
-    if raw_tool_calls := merge_lists(
-        left.tool_call_chunks, *(o.tool_call_chunks for o in others)
-    ):
-        tool_call_chunks = [
-            create_tool_call_chunk(
-                name=rtc.get("name"),
-                args=rtc.get("args"),
-                index=rtc.get("index"),
-                id=rtc.get("id"),
-            )
-            for rtc in raw_tool_calls
-        ]
-    else:
-        tool_call_chunks = []
-
-    # Token usage
-    if left.usage_metadata or any(o.usage_metadata is not None for o in others):
-        usage_metadata: Optional[UsageMetadata] = left.usage_metadata
-        for other in others:
-            usage_metadata = add_usage(usage_metadata, other.usage_metadata)
-    else:
-        usage_metadata = None
-
-    return left.__class__(
-        example=left.example,
-        content=content,
-        additional_kwargs=additional_kwargs,
-        tool_call_chunks=tool_call_chunks,
-        response_metadata=response_metadata,
-        usage_metadata=usage_metadata,
-        id=left.id,
-    )
-
-
-def add_usage(
-    left: Optional[UsageMetadata], right: Optional[UsageMetadata]
-) -> UsageMetadata:
-    """Recursively add two UsageMetadata objects.
-
-    Example:
-        .. code-block:: python
-
-            from langchain_core.messages.ai import add_usage
-
-            left = UsageMetadata(
-                input_tokens=5,
-                output_tokens=0,
-                total_tokens=5,
-                input_token_details=InputTokenDetails(cache_read=3)
-            )
-            right = UsageMetadata(
-                input_tokens=0,
-                output_tokens=10,
-                total_tokens=10,
-                output_token_details=OutputTokenDetails(reasoning=4)
-            )
-
-            add_usage(left, right)
-
-        results in
-
-        .. code-block:: python
-
-            UsageMetadata(
-                input_tokens=5,
-                output_tokens=10,
-                total_tokens=15,
-                input_token_details=InputTokenDetails(cache_read=3),
-                output_token_details=OutputTokenDetails(reasoning=4)
-            )
-
-    """
-    if not (left or right):
-        return UsageMetadata(input_tokens=0, output_tokens=0, total_tokens=0)
-    if not (left and right):
-        return cast(UsageMetadata, left or right)
-
-    return UsageMetadata(
-        **cast(
-            UsageMetadata,
-            _dict_int_op(
-                cast(dict, left),
-                cast(dict, right),
-                operator.add,
-            ),
-        )
-    )
-
-
-def subtract_usage(
-    left: Optional[UsageMetadata], right: Optional[UsageMetadata]
-) -> UsageMetadata:
-    """Recursively subtract two UsageMetadata objects.
-
-    Token counts cannot be negative so the actual operation is max(left - right, 0).
-
-    Example:
-        .. code-block:: python
-
-            from langchain_core.messages.ai import subtract_usage
-
-            left = UsageMetadata(
-                input_tokens=5,
-                output_tokens=10,
-                total_tokens=15,
-                input_token_details=InputTokenDetails(cache_read=4)
-            )
-            right = UsageMetadata(
-                input_tokens=3,
-                output_tokens=8,
-                total_tokens=11,
-                output_token_details=OutputTokenDetails(reasoning=4)
-            )
-
-            subtract_usage(left, right)
-
-        results in
-
-        .. code-block:: python
-
-            UsageMetadata(
-                input_tokens=2,
-                output_tokens=2,
-                total_tokens=4,
-                input_token_details=InputTokenDetails(cache_read=4),
-                output_token_details=OutputTokenDetails(reasoning=0)
-            )
-
-    """
-    if not (left or right):
-        return UsageMetadata(input_tokens=0, output_tokens=0, total_tokens=0)
-    if not (left and right):
-        return cast(UsageMetadata, left or right)
-
-    return UsageMetadata(
-        **cast(
-            UsageMetadata,
-            _dict_int_op(
-                cast(dict, left),
-                cast(dict, right),
-                (lambda le, ri: max(le - ri, 0)),
-            ),
-        )
-    )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/messages/base.py ./custom_langchain_core/messages/base.py
--- .venv/lib/python3.12/site-packages/langchain_core/messages/base.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/messages/base.py	1970-01-01 09:00:00
@@ -1,297 +0,0 @@
-from __future__ import annotations
-
-from collections.abc import Sequence
-from typing import TYPE_CHECKING, Any, Optional, Union, cast
-
-from pydantic import ConfigDict, Field, field_validator
-
-from langchain_core.load.serializable import Serializable
-from langchain_core.utils import get_bolded_text
-from langchain_core.utils._merge import merge_dicts, merge_lists
-from langchain_core.utils.interactive_env import is_interactive_env
-
-if TYPE_CHECKING:
-    from langchain_core.prompts.chat import ChatPromptTemplate
-
-
-class BaseMessage(Serializable):
-    """Base abstract message class.
-
-    Messages are the inputs and outputs of ChatModels.
-    """
-
-    content: Union[str, list[Union[str, dict]]]
-    """The string contents of the message."""
-
-    additional_kwargs: dict = Field(default_factory=dict)
-    """Reserved for additional payload data associated with the message.
-
-    For example, for a message from an AI, this could include tool calls as
-    encoded by the model provider.
-    """
-
-    response_metadata: dict = Field(default_factory=dict)
-    """Response metadata. For example: response headers, logprobs, token counts."""
-
-    type: str
-    """The type of the message. Must be a string that is unique to the message type.
-
-    The purpose of this field is to allow for easy identification of the message type
-    when deserializing messages.
-    """
-
-    name: Optional[str] = None
-    """An optional name for the message.
-
-    This can be used to provide a human-readable name for the message.
-
-    Usage of this field is optional, and whether it's used or not is up to the
-    model implementation.
-    """
-
-    id: Optional[str] = None
-    """An optional unique identifier for the message. This should ideally be
-    provided by the provider/model which created the message."""
-
-    model_config = ConfigDict(
-        extra="allow",
-    )
-
-    @field_validator("id", mode="before")
-    def cast_id_to_str(cls, id_value: Any) -> Optional[str]:
-        if id_value is not None:
-            return str(id_value)
-        else:
-            return id_value
-
-    def __init__(
-        self, content: Union[str, list[Union[str, dict]]], **kwargs: Any
-    ) -> None:
-        """Pass in content as positional arg.
-
-        Args:
-            content: The string contents of the message.
-            kwargs: Additional fields to pass to the
-        """
-        super().__init__(content=content, **kwargs)
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Return whether this class is serializable. This is used to determine
-        whether the class should be included in the langchain schema.
-
-        Returns:
-            True if the class is serializable, False otherwise.
-        """
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Default is ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
-
-    def text(self) -> str:
-        """Get the text content of the message.
-
-        Returns:
-            The text content of the message.
-        """
-        if isinstance(self.content, str):
-            return self.content
-
-        # must be a list
-        blocks = [
-            block
-            for block in self.content
-            if isinstance(block, str)
-            or block.get("type") == "text"
-            and isinstance(block.get("text"), str)
-        ]
-        return "".join(
-            block if isinstance(block, str) else block["text"] for block in blocks
-        )
-
-    def __add__(self, other: Any) -> ChatPromptTemplate:
-        """Concatenate this message with another message."""
-        from langchain_core.prompts.chat import ChatPromptTemplate
-
-        prompt = ChatPromptTemplate(messages=[self])  # type: ignore[call-arg]
-        return prompt + other
-
-    def pretty_repr(self, html: bool = False) -> str:
-        """Get a pretty representation of the message.
-
-        Args:
-            html: Whether to format the message as HTML. If True, the message will be
-                formatted with HTML tags. Default is False.
-
-        Returns:
-            A pretty representation of the message.
-        """
-        title = get_msg_title_repr(self.type.title() + " Message", bold=html)
-        # TODO: handle non-string content.
-        if self.name is not None:
-            title += f"\nName: {self.name}"
-        return f"{title}\n\n{self.content}"
-
-    def pretty_print(self) -> None:
-        print(self.pretty_repr(html=is_interactive_env()))  # noqa: T201
-
-
-def merge_content(
-    first_content: Union[str, list[Union[str, dict]]],
-    *contents: Union[str, list[Union[str, dict]]],
-) -> Union[str, list[Union[str, dict]]]:
-    """Merge two message contents.
-
-    Args:
-        first_content: The first content. Can be a string or a list.
-        second_content: The second content. Can be a string or a list.
-
-    Returns:
-        The merged content.
-    """
-    merged = first_content
-    for content in contents:
-        # If current is a string
-        if isinstance(merged, str):
-            # If the next chunk is also a string, then merge them naively
-            if isinstance(content, str):
-                merged = cast(str, merged) + content
-            # If the next chunk is a list, add the current to the start of the list
-            else:
-                merged = [merged] + content  # type: ignore
-        elif isinstance(content, list):
-            # If both are lists
-            merged = merge_lists(cast(list, merged), content)  # type: ignore
-        # If the first content is a list, and the second content is a string
-        else:
-            # If the last element of the first content is a string
-            # Add the second content to the last element
-            if merged and isinstance(merged[-1], str):
-                merged[-1] += content
-            # If second content is an empty string, treat as a no-op
-            elif content == "":
-                pass
-            else:
-                # Otherwise, add the second content as a new element of the list
-                merged.append(content)
-    return merged
-
-
-class BaseMessageChunk(BaseMessage):
-    """Message chunk, which can be concatenated with other Message chunks."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Default is ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
-
-    def __add__(self, other: Any) -> BaseMessageChunk:  # type: ignore
-        """Message chunks support concatenation with other message chunks.
-
-        This functionality is useful to combine message chunks yielded from
-        a streaming model into a complete message.
-
-        Args:
-            other: Another message chunk to concatenate with this one.
-
-        Returns:
-            A new message chunk that is the concatenation of this message chunk
-            and the other message chunk.
-
-        Raises:
-            TypeError: If the other object is not a message chunk.
-
-        For example,
-
-        `AIMessageChunk(content="Hello") + AIMessageChunk(content=" World")`
-
-        will give `AIMessageChunk(content="Hello World")`
-        """
-        if isinstance(other, BaseMessageChunk):
-            # If both are (subclasses of) BaseMessageChunk,
-            # concat into a single BaseMessageChunk
-
-            return self.__class__(  # type: ignore[call-arg]
-                id=self.id,
-                type=self.type,
-                content=merge_content(self.content, other.content),
-                additional_kwargs=merge_dicts(
-                    self.additional_kwargs, other.additional_kwargs
-                ),
-                response_metadata=merge_dicts(
-                    self.response_metadata, other.response_metadata
-                ),
-            )
-        elif isinstance(other, list) and all(
-            isinstance(o, BaseMessageChunk) for o in other
-        ):
-            content = merge_content(self.content, *(o.content for o in other))
-            additional_kwargs = merge_dicts(
-                self.additional_kwargs, *(o.additional_kwargs for o in other)
-            )
-            response_metadata = merge_dicts(
-                self.response_metadata, *(o.response_metadata for o in other)
-            )
-            return self.__class__(  # type: ignore[call-arg]
-                id=self.id,
-                content=content,
-                additional_kwargs=additional_kwargs,
-                response_metadata=response_metadata,
-            )
-        else:
-            msg = (
-                'unsupported operand type(s) for +: "'
-                f"{self.__class__.__name__}"
-                f'" and "{other.__class__.__name__}"'
-            )
-            raise TypeError(msg)
-
-
-def message_to_dict(message: BaseMessage) -> dict:
-    """Convert a Message to a dictionary.
-
-    Args:
-        message: Message to convert.
-
-    Returns:
-        Message as a dict. The dict will have a "type" key with the message type
-        and a "data" key with the message data as a dict.
-    """
-    return {"type": message.type, "data": message.model_dump()}
-
-
-def messages_to_dict(messages: Sequence[BaseMessage]) -> list[dict]:
-    """Convert a sequence of Messages to a list of dictionaries.
-
-    Args:
-        messages: Sequence of messages (as BaseMessages) to convert.
-
-    Returns:
-        List of messages as dicts.
-    """
-    return [message_to_dict(m) for m in messages]
-
-
-def get_msg_title_repr(title: str, *, bold: bool = False) -> str:
-    """Get a title representation for a message.
-
-    Args:
-        title: The title.
-        bold: Whether to bold the title. Default is False.
-
-    Returns:
-        The title representation.
-    """
-    padded = " " + title + " "
-    sep_len = (80 - len(padded)) // 2
-    sep = "=" * sep_len
-    second_sep = sep + "=" if len(padded) % 2 else sep
-    if bold:
-        padded = get_bolded_text(padded)
-    return f"{sep}{padded}{second_sep}"
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/messages/chat.py ./custom_langchain_core/messages/chat.py
--- .venv/lib/python3.12/site-packages/langchain_core/messages/chat.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/messages/chat.py	1970-01-01 09:00:00
@@ -1,78 +0,0 @@
-from typing import Any, Literal
-
-from langchain_core.messages.base import (
-    BaseMessage,
-    BaseMessageChunk,
-    merge_content,
-)
-from langchain_core.utils._merge import merge_dicts
-
-
-class ChatMessage(BaseMessage):
-    """Message that can be assigned an arbitrary speaker (i.e. role)."""
-
-    role: str
-    """The speaker / role of the Message."""
-
-    type: Literal["chat"] = "chat"
-    """The type of the message (used during serialization). Defaults to "chat"."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Default is ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
-
-
-ChatMessage.model_rebuild()
-
-
-class ChatMessageChunk(ChatMessage, BaseMessageChunk):
-    """Chat Message chunk."""
-
-    # Ignoring mypy re-assignment here since we're overriding the value
-    # to make sure that the chunk variant can be discriminated from the
-    # non-chunk variant.
-    type: Literal["ChatMessageChunk"] = "ChatMessageChunk"  # type: ignore
-    """The type of the message (used during serialization).
-    Defaults to "ChatMessageChunk"."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Default is ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
-
-    def __add__(self, other: Any) -> BaseMessageChunk:  # type: ignore
-        if isinstance(other, ChatMessageChunk):
-            if self.role != other.role:
-                msg = "Cannot concatenate ChatMessageChunks with different roles."
-                raise ValueError(msg)
-
-            return self.__class__(
-                role=self.role,
-                content=merge_content(self.content, other.content),
-                additional_kwargs=merge_dicts(
-                    self.additional_kwargs, other.additional_kwargs
-                ),
-                response_metadata=merge_dicts(
-                    self.response_metadata, other.response_metadata
-                ),
-                id=self.id,
-            )
-        elif isinstance(other, BaseMessageChunk):
-            return self.__class__(
-                role=self.role,
-                content=merge_content(self.content, other.content),
-                additional_kwargs=merge_dicts(
-                    self.additional_kwargs, other.additional_kwargs
-                ),
-                response_metadata=merge_dicts(
-                    self.response_metadata, other.response_metadata
-                ),
-                id=self.id,
-            )
-        else:
-            return super().__add__(other)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/messages/function.py ./custom_langchain_core/messages/function.py
--- .venv/lib/python3.12/site-packages/langchain_core/messages/function.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/messages/function.py	1970-01-01 09:00:00
@@ -1,74 +0,0 @@
-from typing import Any, Literal
-
-from langchain_core.messages.base import (
-    BaseMessage,
-    BaseMessageChunk,
-    merge_content,
-)
-from langchain_core.utils._merge import merge_dicts
-
-
-class FunctionMessage(BaseMessage):
-    """Message for passing the result of executing a tool back to a model.
-
-    FunctionMessage are an older version of the ToolMessage schema, and
-    do not contain the tool_call_id field.
-
-    The tool_call_id field is used to associate the tool call request with the
-    tool call response. This is useful in situations where a chat model is able
-    to request multiple tool calls in parallel.
-    """
-
-    name: str
-    """The name of the function that was executed."""
-
-    type: Literal["function"] = "function"
-    """The type of the message (used for serialization). Defaults to "function"."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Default is ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
-
-
-FunctionMessage.model_rebuild()
-
-
-class FunctionMessageChunk(FunctionMessage, BaseMessageChunk):
-    """Function Message chunk."""
-
-    # Ignoring mypy re-assignment here since we're overriding the value
-    # to make sure that the chunk variant can be discriminated from the
-    # non-chunk variant.
-    type: Literal["FunctionMessageChunk"] = "FunctionMessageChunk"  # type: ignore[assignment]
-    """The type of the message (used for serialization).
-    Defaults to "FunctionMessageChunk"."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Default is ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
-
-    def __add__(self, other: Any) -> BaseMessageChunk:  # type: ignore
-        if isinstance(other, FunctionMessageChunk):
-            if self.name != other.name:
-                msg = "Cannot concatenate FunctionMessageChunks with different names."
-                raise ValueError(msg)
-
-            return self.__class__(
-                name=self.name,
-                content=merge_content(self.content, other.content),
-                additional_kwargs=merge_dicts(
-                    self.additional_kwargs, other.additional_kwargs
-                ),
-                response_metadata=merge_dicts(
-                    self.response_metadata, other.response_metadata
-                ),
-                id=self.id,
-            )
-
-        return super().__add__(other)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/messages/human.py ./custom_langchain_core/messages/human.py
--- .venv/lib/python3.12/site-packages/langchain_core/messages/human.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/messages/human.py	1970-01-01 09:00:00
@@ -1,78 +0,0 @@
-from typing import Any, Literal, Union
-
-from langchain_core.messages.base import BaseMessage, BaseMessageChunk
-
-
-class HumanMessage(BaseMessage):
-    """Message from a human.
-
-    HumanMessages are messages that are passed in from a human to the model.
-
-    Example:
-
-        .. code-block:: python
-
-            from langchain_core.messages import HumanMessage, SystemMessage
-
-            messages = [
-                SystemMessage(
-                    content="You are a helpful assistant! Your name is Bob."
-                ),
-                HumanMessage(
-                    content="What is your name?"
-                )
-            ]
-
-            # Instantiate a chat model and invoke it with the messages
-            model = ...
-            print(model.invoke(messages))
-    """
-
-    example: bool = False
-    """Use to denote that a message is part of an example conversation.
-
-    At the moment, this is ignored by most models. Usage is discouraged.
-    Defaults to False.
-    """
-
-    type: Literal["human"] = "human"
-    """The type of the message (used for serialization). Defaults to "human"."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Default is ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
-
-    def __init__(
-        self, content: Union[str, list[Union[str, dict]]], **kwargs: Any
-    ) -> None:
-        """Pass in content as positional arg.
-
-        Args:
-            content: The string contents of the message.
-            kwargs: Additional fields to pass to the message.
-        """
-        super().__init__(content=content, **kwargs)
-
-
-HumanMessage.model_rebuild()
-
-
-class HumanMessageChunk(HumanMessage, BaseMessageChunk):
-    """Human Message chunk."""
-
-    # Ignoring mypy re-assignment here since we're overriding the value
-    # to make sure that the chunk variant can be discriminated from the
-    # non-chunk variant.
-    type: Literal["HumanMessageChunk"] = "HumanMessageChunk"  # type: ignore[assignment]
-    """The type of the message (used for serialization).
-    Defaults to "HumanMessageChunk"."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Default is ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/messages/modifier.py ./custom_langchain_core/messages/modifier.py
--- .venv/lib/python3.12/site-packages/langchain_core/messages/modifier.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/messages/modifier.py	1970-01-01 09:00:00
@@ -1,36 +0,0 @@
-from typing import Any, Literal
-
-from langchain_core.messages.base import BaseMessage
-
-
-class RemoveMessage(BaseMessage):
-    """Message responsible for deleting other messages."""
-
-    type: Literal["remove"] = "remove"
-    """The type of the message (used for serialization). Defaults to "remove"."""
-
-    def __init__(self, id: str, **kwargs: Any) -> None:
-        """Create a RemoveMessage.
-
-        Args:
-            id: The ID of the message to remove.
-            kwargs: Additional fields to pass to the message.
-
-        Raises:
-            ValueError: If the 'content' field is passed in kwargs.
-        """
-        if kwargs.pop("content", None):
-            msg = "RemoveMessage does not support 'content' field."
-            raise ValueError(msg)
-
-        return super().__init__("", id=id, **kwargs)
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Default is ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
-
-
-RemoveMessage.model_rebuild()
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/messages/system.py ./custom_langchain_core/messages/system.py
--- .venv/lib/python3.12/site-packages/langchain_core/messages/system.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/messages/system.py	1970-01-01 09:00:00
@@ -1,72 +0,0 @@
-from typing import Any, Literal, Union
-
-from langchain_core.messages.base import BaseMessage, BaseMessageChunk
-
-
-class SystemMessage(BaseMessage):
-    """Message for priming AI behavior.
-
-    The system message is usually passed in as the first of a sequence
-    of input messages.
-
-    Example:
-
-        .. code-block:: python
-
-            from langchain_core.messages import HumanMessage, SystemMessage
-
-            messages = [
-                SystemMessage(
-                    content="You are a helpful assistant! Your name is Bob."
-                ),
-                HumanMessage(
-                    content="What is your name?"
-                )
-            ]
-
-            # Define a chat model and invoke it with the messages
-            print(model.invoke(messages))
-
-    """
-
-    type: Literal["system"] = "system"
-    """The type of the message (used for serialization). Defaults to "system"."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Default is ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
-
-    def __init__(
-        self, content: Union[str, list[Union[str, dict]]], **kwargs: Any
-    ) -> None:
-        """Pass in content as positional arg.
-
-        Args:
-               content: The string contents of the message.
-               kwargs: Additional fields to pass to the message.
-        """
-        super().__init__(content=content, **kwargs)
-
-
-SystemMessage.model_rebuild()
-
-
-class SystemMessageChunk(SystemMessage, BaseMessageChunk):
-    """System Message chunk."""
-
-    # Ignoring mypy re-assignment here since we're overriding the value
-    # to make sure that the chunk variant can be discriminated from the
-    # non-chunk variant.
-    type: Literal["SystemMessageChunk"] = "SystemMessageChunk"  # type: ignore[assignment]
-    """The type of the message (used for serialization).
-    Defaults to "SystemMessageChunk"."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Default is ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/messages/tool.py ./custom_langchain_core/messages/tool.py
--- .venv/lib/python3.12/site-packages/langchain_core/messages/tool.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/messages/tool.py	1970-01-01 09:00:00
@@ -1,343 +0,0 @@
-import json
-from typing import Any, Literal, Optional, Union
-from uuid import UUID
-
-from pydantic import Field, model_validator
-from typing_extensions import NotRequired, TypedDict
-
-from langchain_core.messages.base import BaseMessage, BaseMessageChunk, merge_content
-from langchain_core.utils._merge import merge_dicts, merge_obj
-
-
-class ToolOutputMixin:
-    """Mixin for objects that tools can return directly.
-
-    If a custom BaseTool is invoked with a ToolCall and the output of custom code is
-    not an instance of ToolOutputMixin, the output will automatically be coerced to a
-    string and wrapped in a ToolMessage.
-    """
-
-
-class ToolMessage(BaseMessage, ToolOutputMixin):
-    """Message for passing the result of executing a tool back to a model.
-
-    ToolMessages contain the result of a tool invocation. Typically, the result
-    is encoded inside the `content` field.
-
-    Example: A ToolMessage representing a result of 42 from a tool call with id
-
-        .. code-block:: python
-
-            from langchain_core.messages import ToolMessage
-
-            ToolMessage(content='42', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL')
-
-
-    Example: A ToolMessage where only part of the tool output is sent to the model
-        and the full output is passed in to artifact.
-
-        .. versionadded:: 0.2.17
-
-        .. code-block:: python
-
-            from langchain_core.messages import ToolMessage
-
-            tool_output = {
-                "stdout": "From the graph we can see that the correlation between x and y is ...",
-                "stderr": None,
-                "artifacts": {"type": "image", "base64_data": "/9j/4gIcSU..."},
-            }
-
-            ToolMessage(
-                content=tool_output["stdout"],
-                artifact=tool_output,
-                tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',
-            )
-
-    The tool_call_id field is used to associate the tool call request with the
-    tool call response. This is useful in situations where a chat model is able
-    to request multiple tool calls in parallel.
-    """  # noqa: E501
-
-    tool_call_id: str
-    """Tool call that this message is responding to."""
-
-    type: Literal["tool"] = "tool"
-    """The type of the message (used for serialization). Defaults to "tool"."""
-
-    artifact: Any = None
-    """Artifact of the Tool execution which is not meant to be sent to the model.
-
-    Should only be specified if it is different from the message content, e.g. if only
-    a subset of the full tool output is being passed as message content but the full
-    output is needed in other parts of the code.
-
-    .. versionadded:: 0.2.17
-    """
-
-    status: Literal["success", "error"] = "success"
-    """Status of the tool invocation.
-
-    .. versionadded:: 0.2.24
-    """
-
-    additional_kwargs: dict = Field(default_factory=dict, repr=False)
-    """Currently inherited from BaseMessage, but not used."""
-    response_metadata: dict = Field(default_factory=dict, repr=False)
-    """Currently inherited from BaseMessage, but not used."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Default is ["langchain", "schema", "messages"].
-        """
-        return ["langchain", "schema", "messages"]
-
-    @model_validator(mode="before")
-    @classmethod
-    def coerce_args(cls, values: dict) -> dict:
-        content = values["content"]
-        if isinstance(content, tuple):
-            content = list(content)
-
-        if not isinstance(content, (str, list)):
-            try:
-                values["content"] = str(content)
-            except ValueError as e:
-                msg = (
-                    "ToolMessage content should be a string or a list of string/dicts. "
-                    f"Received:\n\n{content=}\n\n which could not be coerced into a "
-                    "string."
-                )
-                raise ValueError(msg) from e
-        elif isinstance(content, list):
-            values["content"] = []
-            for i, x in enumerate(content):
-                if not isinstance(x, (str, dict)):
-                    try:
-                        values["content"].append(str(x))
-                    except ValueError as e:
-                        msg = (
-                            "ToolMessage content should be a string or a list of "
-                            "string/dicts. Received a list but "
-                            f"element ToolMessage.content[{i}] is not a dict and could "
-                            f"not be coerced to a string.:\n\n{x}"
-                        )
-                        raise ValueError(msg) from e
-                else:
-                    values["content"].append(x)
-        else:
-            pass
-
-        tool_call_id = values["tool_call_id"]
-        if isinstance(tool_call_id, (UUID, int, float)):
-            values["tool_call_id"] = str(tool_call_id)
-        return values
-
-    def __init__(
-        self, content: Union[str, list[Union[str, dict]]], **kwargs: Any
-    ) -> None:
-        super().__init__(content=content, **kwargs)
-
-
-ToolMessage.model_rebuild()
-
-
-class ToolMessageChunk(ToolMessage, BaseMessageChunk):
-    """Tool Message chunk."""
-
-    # Ignoring mypy re-assignment here since we're overriding the value
-    # to make sure that the chunk variant can be discriminated from the
-    # non-chunk variant.
-    type: Literal["ToolMessageChunk"] = "ToolMessageChunk"  # type: ignore[assignment]
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "messages"]
-
-    def __add__(self, other: Any) -> BaseMessageChunk:  # type: ignore
-        if isinstance(other, ToolMessageChunk):
-            if self.tool_call_id != other.tool_call_id:
-                msg = "Cannot concatenate ToolMessageChunks with different names."
-                raise ValueError(msg)
-
-            return self.__class__(
-                tool_call_id=self.tool_call_id,
-                content=merge_content(self.content, other.content),
-                artifact=merge_obj(self.artifact, other.artifact),
-                additional_kwargs=merge_dicts(
-                    self.additional_kwargs, other.additional_kwargs
-                ),
-                response_metadata=merge_dicts(
-                    self.response_metadata, other.response_metadata
-                ),
-                id=self.id,
-                status=_merge_status(self.status, other.status),
-            )
-
-        return super().__add__(other)
-
-
-class ToolCall(TypedDict):
-    """Represents a request to call a tool.
-
-    Example:
-
-        .. code-block:: python
-
-            {
-                "name": "foo",
-                "args": {"a": 1},
-                "id": "123"
-            }
-
-        This represents a request to call the tool named "foo" with arguments {"a": 1}
-        and an identifier of "123".
-    """
-
-    name: str
-    """The name of the tool to be called."""
-    args: dict[str, Any]
-    """The arguments to the tool call."""
-    id: Optional[str]
-    """An identifier associated with the tool call.
-
-    An identifier is needed to associate a tool call request with a tool
-    call result in events when multiple concurrent tool calls are made.
-    """
-    type: NotRequired[Literal["tool_call"]]
-
-
-def tool_call(*, name: str, args: dict[str, Any], id: Optional[str]) -> ToolCall:
-    return ToolCall(name=name, args=args, id=id, type="tool_call")
-
-
-class ToolCallChunk(TypedDict):
-    """A chunk of a tool call (e.g., as part of a stream).
-
-    When merging ToolCallChunks (e.g., via AIMessageChunk.__add__),
-    all string attributes are concatenated. Chunks are only merged if their
-    values of `index` are equal and not None.
-
-    Example:
-
-    .. code-block:: python
-
-        left_chunks = [ToolCallChunk(name="foo", args='{"a":', index=0)]
-        right_chunks = [ToolCallChunk(name=None, args='1}', index=0)]
-
-        (
-            AIMessageChunk(content="", tool_call_chunks=left_chunks)
-            + AIMessageChunk(content="", tool_call_chunks=right_chunks)
-        ).tool_call_chunks == [ToolCallChunk(name='foo', args='{"a":1}', index=0)]
-    """
-
-    name: Optional[str]
-    """The name of the tool to be called."""
-    args: Optional[str]
-    """The arguments to the tool call."""
-    id: Optional[str]
-    """An identifier associated with the tool call."""
-    index: Optional[int]
-    """The index of the tool call in a sequence."""
-    type: NotRequired[Literal["tool_call_chunk"]]
-
-
-def tool_call_chunk(
-    *,
-    name: Optional[str] = None,
-    args: Optional[str] = None,
-    id: Optional[str] = None,
-    index: Optional[int] = None,
-) -> ToolCallChunk:
-    return ToolCallChunk(
-        name=name, args=args, id=id, index=index, type="tool_call_chunk"
-    )
-
-
-class InvalidToolCall(TypedDict):
-    """Allowance for errors made by LLM.
-
-    Here we add an `error` key to surface errors made during generation
-    (e.g., invalid JSON arguments.)
-    """
-
-    name: Optional[str]
-    """The name of the tool to be called."""
-    args: Optional[str]
-    """The arguments to the tool call."""
-    id: Optional[str]
-    """An identifier associated with the tool call."""
-    error: Optional[str]
-    """An error message associated with the tool call."""
-    type: NotRequired[Literal["invalid_tool_call"]]
-
-
-def invalid_tool_call(
-    *,
-    name: Optional[str] = None,
-    args: Optional[str] = None,
-    id: Optional[str] = None,
-    error: Optional[str] = None,
-) -> InvalidToolCall:
-    return InvalidToolCall(
-        name=name, args=args, id=id, error=error, type="invalid_tool_call"
-    )
-
-
-def default_tool_parser(
-    raw_tool_calls: list[dict],
-) -> tuple[list[ToolCall], list[InvalidToolCall]]:
-    """Best-effort parsing of tools."""
-    tool_calls = []
-    invalid_tool_calls = []
-    for raw_tool_call in raw_tool_calls:
-        if "function" not in raw_tool_call:
-            continue
-        else:
-            function_name = raw_tool_call["function"]["name"]
-            try:
-                function_args = json.loads(raw_tool_call["function"]["arguments"])
-                parsed = tool_call(
-                    name=function_name or "",
-                    args=function_args or {},
-                    id=raw_tool_call.get("id"),
-                )
-                tool_calls.append(parsed)
-            except json.JSONDecodeError:
-                invalid_tool_calls.append(
-                    invalid_tool_call(
-                        name=function_name,
-                        args=raw_tool_call["function"]["arguments"],
-                        id=raw_tool_call.get("id"),
-                        error=None,
-                    )
-                )
-    return tool_calls, invalid_tool_calls
-
-
-def default_tool_chunk_parser(raw_tool_calls: list[dict]) -> list[ToolCallChunk]:
-    """Best-effort parsing of tool chunks."""
-    tool_call_chunks = []
-    for tool_call in raw_tool_calls:
-        if "function" not in tool_call:
-            function_args = None
-            function_name = None
-        else:
-            function_args = tool_call["function"]["arguments"]
-            function_name = tool_call["function"]["name"]
-        parsed = tool_call_chunk(
-            name=function_name,
-            args=function_args,
-            id=tool_call.get("id"),
-            index=tool_call.get("index"),
-        )
-        tool_call_chunks.append(parsed)
-    return tool_call_chunks
-
-
-def _merge_status(
-    left: Literal["success", "error"], right: Literal["success", "error"]
-) -> Literal["success", "error"]:
-    return "error" if "error" in (left, right) else "success"
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/messages/utils.py ./custom_langchain_core/messages/utils.py
--- .venv/lib/python3.12/site-packages/langchain_core/messages/utils.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/messages/utils.py	1970-01-01 09:00:00
@@ -1,1425 +0,0 @@
-"""Module contains utility functions for working with messages.
-
-Some examples of what you can do with these functions include:
-
-* Convert messages to strings (serialization)
-* Convert messages from dicts to Message objects (deserialization)
-* Filter messages from a list of messages based on name, type or id etc.
-"""
-
-from __future__ import annotations
-
-import base64
-import inspect
-import json
-from collections.abc import Iterable, Sequence
-from functools import partial
-from typing import (
-    TYPE_CHECKING,
-    Annotated,
-    Any,
-    Callable,
-    Literal,
-    Optional,
-    Union,
-    cast,
-    overload,
-)
-
-from pydantic import Discriminator, Field, Tag
-
-from langchain_core.exceptions import ErrorCode, create_message
-from langchain_core.messages.ai import AIMessage, AIMessageChunk
-from langchain_core.messages.base import BaseMessage, BaseMessageChunk
-from langchain_core.messages.chat import ChatMessage, ChatMessageChunk
-from langchain_core.messages.function import FunctionMessage, FunctionMessageChunk
-from langchain_core.messages.human import HumanMessage, HumanMessageChunk
-from langchain_core.messages.modifier import RemoveMessage
-from langchain_core.messages.system import SystemMessage, SystemMessageChunk
-from langchain_core.messages.tool import ToolCall, ToolMessage, ToolMessageChunk
-
-if TYPE_CHECKING:
-    from langchain_text_splitters import TextSplitter
-
-    from langchain_core.language_models import BaseLanguageModel
-    from langchain_core.prompt_values import PromptValue
-    from langchain_core.runnables.base import Runnable
-
-
-def _get_type(v: Any) -> str:
-    """Get the type associated with the object for serialization purposes."""
-    if isinstance(v, dict) and "type" in v:
-        return v["type"]
-    elif hasattr(v, "type"):
-        return v.type
-    else:
-        msg = (
-            f"Expected either a dictionary with a 'type' key or an object "
-            f"with a 'type' attribute. Instead got type {type(v)}."
-        )
-        raise TypeError(msg)
-
-
-AnyMessage = Annotated[
-    Union[
-        Annotated[AIMessage, Tag(tag="ai")],
-        Annotated[HumanMessage, Tag(tag="human")],
-        Annotated[ChatMessage, Tag(tag="chat")],
-        Annotated[SystemMessage, Tag(tag="system")],
-        Annotated[FunctionMessage, Tag(tag="function")],
-        Annotated[ToolMessage, Tag(tag="tool")],
-        Annotated[AIMessageChunk, Tag(tag="AIMessageChunk")],
-        Annotated[HumanMessageChunk, Tag(tag="HumanMessageChunk")],
-        Annotated[ChatMessageChunk, Tag(tag="ChatMessageChunk")],
-        Annotated[SystemMessageChunk, Tag(tag="SystemMessageChunk")],
-        Annotated[FunctionMessageChunk, Tag(tag="FunctionMessageChunk")],
-        Annotated[ToolMessageChunk, Tag(tag="ToolMessageChunk")],
-    ],
-    Field(discriminator=Discriminator(_get_type)),
-]
-
-
-def get_buffer_string(
-    messages: Sequence[BaseMessage], human_prefix: str = "Human", ai_prefix: str = "AI"
-) -> str:
-    """Convert a sequence of Messages to strings and concatenate them into one string.
-
-    Args:
-        messages: Messages to be converted to strings.
-        human_prefix: The prefix to prepend to contents of HumanMessages.
-            Default is "Human".
-        ai_prefix: THe prefix to prepend to contents of AIMessages. Default is "AI".
-
-    Returns:
-        A single string concatenation of all input messages.
-
-    Raises:
-        ValueError: If an unsupported message type is encountered.
-
-    Example:
-        .. code-block:: python
-
-            from langchain_core import AIMessage, HumanMessage
-
-            messages = [
-                HumanMessage(content="Hi, how are you?"),
-                AIMessage(content="Good, how are you?"),
-            ]
-            get_buffer_string(messages)
-            # -> "Human: Hi, how are you?\nAI: Good, how are you?"
-    """
-    string_messages = []
-    for m in messages:
-        if isinstance(m, HumanMessage):
-            role = human_prefix
-        elif isinstance(m, AIMessage):
-            role = ai_prefix
-        elif isinstance(m, SystemMessage):
-            role = "System"
-        elif isinstance(m, FunctionMessage):
-            role = "Function"
-        elif isinstance(m, ToolMessage):
-            role = "Tool"
-        elif isinstance(m, ChatMessage):
-            role = m.role
-        else:
-            msg = f"Got unsupported message type: {m}"
-            raise ValueError(msg)  # noqa: TRY004
-        message = f"{role}: {m.content}"
-        if isinstance(m, AIMessage) and "function_call" in m.additional_kwargs:
-            message += f"{m.additional_kwargs['function_call']}"
-        string_messages.append(message)
-
-    return "\n".join(string_messages)
-
-
-def _message_from_dict(message: dict) -> BaseMessage:
-    _type = message["type"]
-    if _type == "human":
-        return HumanMessage(**message["data"])
-    elif _type == "ai":
-        return AIMessage(**message["data"])
-    elif _type == "system":
-        return SystemMessage(**message["data"])
-    elif _type == "chat":
-        return ChatMessage(**message["data"])
-    elif _type == "function":
-        return FunctionMessage(**message["data"])
-    elif _type == "tool":
-        return ToolMessage(**message["data"])
-    elif _type == "remove":
-        return RemoveMessage(**message["data"])
-    elif _type == "AIMessageChunk":
-        return AIMessageChunk(**message["data"])
-    elif _type == "HumanMessageChunk":
-        return HumanMessageChunk(**message["data"])
-    elif _type == "FunctionMessageChunk":
-        return FunctionMessageChunk(**message["data"])
-    elif _type == "ToolMessageChunk":
-        return ToolMessageChunk(**message["data"])
-    elif _type == "SystemMessageChunk":
-        return SystemMessageChunk(**message["data"])
-    elif _type == "ChatMessageChunk":
-        return ChatMessageChunk(**message["data"])
-    else:
-        msg = f"Got unexpected message type: {_type}"
-        raise ValueError(msg)
-
-
-def messages_from_dict(messages: Sequence[dict]) -> list[BaseMessage]:
-    """Convert a sequence of messages from dicts to Message objects.
-
-    Args:
-        messages: Sequence of messages (as dicts) to convert.
-
-    Returns:
-        list of messages (BaseMessages).
-    """
-    return [_message_from_dict(m) for m in messages]
-
-
-def message_chunk_to_message(chunk: BaseMessageChunk) -> BaseMessage:
-    """Convert a message chunk to a message.
-
-    Args:
-        chunk: Message chunk to convert.
-
-    Returns:
-        Message.
-    """
-    if not isinstance(chunk, BaseMessageChunk):
-        return chunk
-    # chunk classes always have the equivalent non-chunk class as their first parent
-    ignore_keys = ["type"]
-    if isinstance(chunk, AIMessageChunk):
-        ignore_keys.append("tool_call_chunks")
-    return chunk.__class__.__mro__[1](
-        **{k: v for k, v in chunk.__dict__.items() if k not in ignore_keys}
-    )
-
-
-MessageLikeRepresentation = Union[
-    BaseMessage, list[str], tuple[str, str], str, dict[str, Any]
-]
-
-
-def _create_message_from_message_type(
-    message_type: str,
-    content: str,
-    name: Optional[str] = None,
-    tool_call_id: Optional[str] = None,
-    tool_calls: Optional[list[dict[str, Any]]] = None,
-    id: Optional[str] = None,
-    **additional_kwargs: Any,
-) -> BaseMessage:
-    """Create a message from a message type and content string.
-
-    Args:
-        message_type: (str) the type of the message (e.g., "human", "ai", etc.).
-        content: (str) the content string.
-        name: (str) the name of the message. Default is None.
-        tool_call_id: (str) the tool call id. Default is None.
-        tool_calls: (list[dict[str, Any]]) the tool calls. Default is None.
-        id: (str) the id of the message. Default is None.
-        additional_kwargs: (dict[str, Any]) additional keyword arguments.
-
-    Returns:
-        a message of the appropriate type.
-
-    Raises:
-        ValueError: if the message type is not one of "human", "user", "ai",
-            "assistant", "function", "tool", "system", or "developer".
-    """
-    kwargs: dict[str, Any] = {}
-    if name is not None:
-        kwargs["name"] = name
-    if tool_call_id is not None:
-        kwargs["tool_call_id"] = tool_call_id
-    if additional_kwargs:
-        if response_metadata := additional_kwargs.pop("response_metadata", None):
-            kwargs["response_metadata"] = response_metadata
-        kwargs["additional_kwargs"] = additional_kwargs  # type: ignore[assignment]
-        additional_kwargs.update(additional_kwargs.pop("additional_kwargs", {}))
-    if id is not None:
-        kwargs["id"] = id
-    if tool_calls is not None:
-        kwargs["tool_calls"] = []
-        for tool_call in tool_calls:
-            # Convert OpenAI-format tool call to LangChain format.
-            if "function" in tool_call:
-                args = tool_call["function"]["arguments"]
-                if isinstance(args, str):
-                    args = json.loads(args, strict=False)
-                kwargs["tool_calls"].append(
-                    {
-                        "name": tool_call["function"]["name"],
-                        "args": args,
-                        "id": tool_call["id"],
-                        "type": "tool_call",
-                    }
-                )
-            else:
-                kwargs["tool_calls"].append(tool_call)
-    if message_type in ("human", "user"):
-        if example := kwargs.get("additional_kwargs", {}).pop("example", False):
-            kwargs["example"] = example
-        message: BaseMessage = HumanMessage(content=content, **kwargs)
-    elif message_type in ("ai", "assistant"):
-        if example := kwargs.get("additional_kwargs", {}).pop("example", False):
-            kwargs["example"] = example
-        message = AIMessage(content=content, **kwargs)
-    elif message_type in ("system", "developer"):
-        if message_type == "developer":
-            kwargs["additional_kwargs"] = kwargs.get("additional_kwargs") or {}
-            kwargs["additional_kwargs"]["__openai_role__"] = "developer"
-        message = SystemMessage(content=content, **kwargs)
-    elif message_type == "function":
-        message = FunctionMessage(content=content, **kwargs)
-    elif message_type == "tool":
-        artifact = kwargs.get("additional_kwargs", {}).pop("artifact", None)
-        message = ToolMessage(content=content, artifact=artifact, **kwargs)
-    elif message_type == "remove":
-        message = RemoveMessage(**kwargs)
-    else:
-        msg = (
-            f"Unexpected message type: '{message_type}'. Use one of 'human',"
-            f" 'user', 'ai', 'assistant', 'function', 'tool', 'system', or 'developer'."
-        )
-        msg = create_message(message=msg, error_code=ErrorCode.MESSAGE_COERCION_FAILURE)
-        raise ValueError(msg)
-    return message
-
-
-def _convert_to_message(message: MessageLikeRepresentation) -> BaseMessage:
-    """Instantiate a message from a variety of message formats.
-
-    The message format can be one of the following:
-
-    - BaseMessagePromptTemplate
-    - BaseMessage
-    - 2-tuple of (role string, template); e.g., ("human", "{user_input}")
-    - dict: a message dict with role and content keys
-    - string: shorthand for ("human", template); e.g., "{user_input}"
-
-    Args:
-        message: a representation of a message in one of the supported formats.
-
-    Returns:
-        an instance of a message or a message template.
-
-    Raises:
-        NotImplementedError: if the message type is not supported.
-        ValueError: if the message dict does not contain the required keys.
-    """
-    if isinstance(message, BaseMessage):
-        _message = message
-    elif isinstance(message, str):
-        _message = _create_message_from_message_type("human", message)
-    elif isinstance(message, Sequence) and len(message) == 2:
-        # mypy doesn't realise this can't be a string given the previous branch
-        message_type_str, template = message  # type: ignore[misc]
-        _message = _create_message_from_message_type(message_type_str, template)
-    elif isinstance(message, dict):
-        msg_kwargs = message.copy()
-        try:
-            try:
-                msg_type = msg_kwargs.pop("role")
-            except KeyError:
-                msg_type = msg_kwargs.pop("type")
-            # None msg content is not allowed
-            msg_content = msg_kwargs.pop("content") or ""
-        except KeyError as e:
-            msg = f"Message dict must contain 'role' and 'content' keys, got {message}"
-            msg = create_message(
-                message=msg, error_code=ErrorCode.MESSAGE_COERCION_FAILURE
-            )
-            raise ValueError(msg) from e
-        _message = _create_message_from_message_type(
-            msg_type, msg_content, **msg_kwargs
-        )
-    else:
-        msg = f"Unsupported message type: {type(message)}"
-        msg = create_message(message=msg, error_code=ErrorCode.MESSAGE_COERCION_FAILURE)
-        raise NotImplementedError(msg)
-
-    return _message
-
-
-def convert_to_messages(
-    messages: Union[Iterable[MessageLikeRepresentation], PromptValue],
-) -> list[BaseMessage]:
-    """Convert a sequence of messages to a list of messages.
-
-    Args:
-        messages: Sequence of messages to convert.
-
-    Returns:
-        list of messages (BaseMessages).
-    """
-    # Import here to avoid circular imports
-    from langchain_core.prompt_values import PromptValue
-
-    if isinstance(messages, PromptValue):
-        return messages.to_messages()
-    return [_convert_to_message(m) for m in messages]
-
-
-def _runnable_support(func: Callable) -> Callable:
-    @overload
-    def wrapped(
-        messages: Literal[None] = None, **kwargs: Any
-    ) -> Runnable[Sequence[MessageLikeRepresentation], list[BaseMessage]]: ...
-
-    @overload
-    def wrapped(
-        messages: Sequence[MessageLikeRepresentation], **kwargs: Any
-    ) -> list[BaseMessage]: ...
-
-    def wrapped(
-        messages: Union[Sequence[MessageLikeRepresentation], None] = None,
-        **kwargs: Any,
-    ) -> Union[
-        list[BaseMessage],
-        Runnable[Sequence[MessageLikeRepresentation], list[BaseMessage]],
-    ]:
-        from langchain_core.runnables.base import RunnableLambda
-
-        if messages is not None:
-            return func(messages, **kwargs)
-        else:
-            return RunnableLambda(partial(func, **kwargs), name=func.__name__)
-
-    wrapped.__doc__ = func.__doc__
-    return wrapped
-
-
-@_runnable_support
-def filter_messages(
-    messages: Union[Iterable[MessageLikeRepresentation], PromptValue],
-    *,
-    include_names: Optional[Sequence[str]] = None,
-    exclude_names: Optional[Sequence[str]] = None,
-    include_types: Optional[Sequence[Union[str, type[BaseMessage]]]] = None,
-    exclude_types: Optional[Sequence[Union[str, type[BaseMessage]]]] = None,
-    include_ids: Optional[Sequence[str]] = None,
-    exclude_ids: Optional[Sequence[str]] = None,
-) -> list[BaseMessage]:
-    """Filter messages based on name, type or id.
-
-    Args:
-        messages: Sequence Message-like objects to filter.
-        include_names: Message names to include. Default is None.
-        exclude_names: Messages names to exclude. Default is None.
-        include_types: Message types to include. Can be specified as string names (e.g.
-            "system", "human", "ai", ...) or as BaseMessage classes (e.g.
-            SystemMessage, HumanMessage, AIMessage, ...). Default is None.
-        exclude_types: Message types to exclude. Can be specified as string names (e.g.
-            "system", "human", "ai", ...) or as BaseMessage classes (e.g.
-            SystemMessage, HumanMessage, AIMessage, ...). Default is None.
-        include_ids: Message IDs to include. Default is None.
-        exclude_ids: Message IDs to exclude. Default is None.
-
-    Returns:
-        A list of Messages that meets at least one of the incl_* conditions and none
-        of the excl_* conditions. If not incl_* conditions are specified then
-        anything that is not explicitly excluded will be included.
-
-    Raises:
-        ValueError if two incompatible arguments are provided.
-
-    Example:
-        .. code-block:: python
-
-            from langchain_core.messages import filter_messages, AIMessage, HumanMessage, SystemMessage
-
-            messages = [
-                SystemMessage("you're a good assistant."),
-                HumanMessage("what's your name", id="foo", name="example_user"),
-                AIMessage("steve-o", id="bar", name="example_assistant"),
-                HumanMessage("what's your favorite color", id="baz",),
-                AIMessage("silicon blue", id="blah",),
-            ]
-
-            filter_messages(
-                messages,
-                incl_names=("example_user", "example_assistant"),
-                incl_types=("system",),
-                excl_ids=("bar",),
-            )
-
-        .. code-block:: python
-
-            [
-                SystemMessage("you're a good assistant."),
-                HumanMessage("what's your name", id="foo", name="example_user"),
-            ]
-    """  # noqa: E501
-    messages = convert_to_messages(messages)
-    filtered: list[BaseMessage] = []
-    for msg in messages:
-        if (
-            (exclude_names and msg.name in exclude_names)
-            or (exclude_types and _is_message_type(msg, exclude_types))
-            or (exclude_ids and msg.id in exclude_ids)
-        ):
-            continue
-        else:
-            pass
-
-        # default to inclusion when no inclusion criteria given.
-        if (
-            not (include_types or include_ids or include_names)
-            or (include_names and msg.name in include_names)
-            or (include_types and _is_message_type(msg, include_types))
-            or (include_ids and msg.id in include_ids)
-        ):
-            filtered.append(msg)
-        else:
-            pass
-
-    return filtered
-
-
-@_runnable_support
-def merge_message_runs(
-    messages: Union[Iterable[MessageLikeRepresentation], PromptValue],
-    *,
-    chunk_separator: str = "\n",
-) -> list[BaseMessage]:
-    """Merge consecutive Messages of the same type.
-
-    **NOTE**: ToolMessages are not merged, as each has a distinct tool call id that
-    can't be merged.
-
-    Args:
-        messages: Sequence Message-like objects to merge.
-        chunk_separator: Specify the string to be inserted between message chunks.
-        Default is "\n".
-
-    Returns:
-        list of BaseMessages with consecutive runs of message types merged into single
-        messages. By default, if two messages being merged both have string contents,
-        the merged content is a concatenation of the two strings with a new-line separator.
-        The separator inserted between message chunks can be controlled by specifying
-        any string with ``chunk_separator``. If at least one of the messages has a list of
-        content blocks, the merged content is a list of content blocks.
-
-    Example:
-        .. code-block:: python
-
-            from langchain_core.messages import (
-                merge_message_runs,
-                AIMessage,
-                HumanMessage,
-                SystemMessage,
-                ToolCall,
-            )
-
-            messages = [
-                SystemMessage("you're a good assistant."),
-                HumanMessage("what's your favorite color", id="foo",),
-                HumanMessage("wait your favorite food", id="bar",),
-                AIMessage(
-                    "my favorite colo",
-                    tool_calls=[ToolCall(name="blah_tool", args={"x": 2}, id="123", type="tool_call")],
-                    id="baz",
-                ),
-                AIMessage(
-                    [{"type": "text", "text": "my favorite dish is lasagna"}],
-                    tool_calls=[ToolCall(name="blah_tool", args={"x": -10}, id="456", type="tool_call")],
-                    id="blur",
-                ),
-            ]
-
-            merge_message_runs(messages)
-
-        .. code-block:: python
-
-            [
-                SystemMessage("you're a good assistant."),
-                HumanMessage("what's your favorite color\\nwait your favorite food", id="foo",),
-                AIMessage(
-                    [
-                        "my favorite colo",
-                        {"type": "text", "text": "my favorite dish is lasagna"}
-                    ],
-                    tool_calls=[
-                        ToolCall({"name": "blah_tool", "args": {"x": 2}, "id": "123", "type": "tool_call"}),
-                        ToolCall({"name": "blah_tool", "args": {"x": -10}, "id": "456", "type": "tool_call"})
-                    ]
-                    id="baz"
-                ),
-            ]
-
-    """  # noqa: E501
-    if not messages:
-        return []
-    messages = convert_to_messages(messages)
-    merged: list[BaseMessage] = []
-    for msg in messages:
-        curr = msg.model_copy(deep=True)
-        last = merged.pop() if merged else None
-        if not last:
-            merged.append(curr)
-        elif isinstance(curr, ToolMessage) or not isinstance(curr, last.__class__):
-            merged.extend([last, curr])
-        else:
-            last_chunk = _msg_to_chunk(last)
-            curr_chunk = _msg_to_chunk(curr)
-            if curr_chunk.response_metadata:
-                curr_chunk.response_metadata.clear()
-            if (
-                isinstance(last_chunk.content, str)
-                and isinstance(curr_chunk.content, str)
-                and last_chunk.content
-                and curr_chunk.content
-            ):
-                last_chunk.content += chunk_separator
-            merged.append(_chunk_to_msg(last_chunk + curr_chunk))
-    return merged
-
-
-# TODO: Update so validation errors (for token_counter, for example) are raised on
-# init not at runtime.
-@_runnable_support
-def trim_messages(
-    messages: Union[Iterable[MessageLikeRepresentation], PromptValue],
-    *,
-    max_tokens: int,
-    token_counter: Union[
-        Callable[[list[BaseMessage]], int],
-        Callable[[BaseMessage], int],
-        BaseLanguageModel,
-    ],
-    strategy: Literal["first", "last"] = "last",
-    allow_partial: bool = False,
-    end_on: Optional[
-        Union[str, type[BaseMessage], Sequence[Union[str, type[BaseMessage]]]]
-    ] = None,
-    start_on: Optional[
-        Union[str, type[BaseMessage], Sequence[Union[str, type[BaseMessage]]]]
-    ] = None,
-    include_system: bool = False,
-    text_splitter: Optional[Union[Callable[[str], list[str]], TextSplitter]] = None,
-) -> list[BaseMessage]:
-    r"""Trim messages to be below a token count.
-
-    trim_messages can be used to reduce the size of a chat history to a specified token
-    count or specified message count.
-
-    In either case, if passing the trimmed chat history back into a chat model
-    directly, the resulting chat history should usually satisfy the following
-    properties:
-
-    1. The resulting chat history should be valid. Most chat models expect that chat
-       history starts with either (1) a `HumanMessage` or (2) a `SystemMessage` followed
-       by a `HumanMessage`. To achieve this, set `start_on="human"`.
-       In addition, generally a `ToolMessage` can only appear after an `AIMessage`
-       that involved a tool call.
-       Please see the following link for more information about messages:
-       https://python.langchain.com/docs/concepts/#messages
-    2. It includes recent messages and drops old messages in the chat history.
-       To achieve this set the `strategy="last"`.
-    3. Usually, the new chat history should include the `SystemMessage` if it
-       was present in the original chat history since the `SystemMessage` includes
-       special instructions to the chat model. The `SystemMessage` is almost always
-       the first message in the history if present. To achieve this set the
-       `include_system=True`.
-
-    **Note** The examples below show how to configure `trim_messages` to achieve
-        a behavior consistent with the above properties.
-
-    Args:
-        messages: Sequence of Message-like objects to trim.
-        max_tokens: Max token count of trimmed messages.
-        token_counter: Function or llm for counting tokens in a BaseMessage or a list of
-            BaseMessage. If a BaseLanguageModel is passed in then
-            BaseLanguageModel.get_num_tokens_from_messages() will be used.
-            Set to `len` to count the number of **messages** in the chat history.
-        strategy: Strategy for trimming.
-            - "first": Keep the first <= n_count tokens of the messages.
-            - "last": Keep the last <= n_count tokens of the messages.
-            Default is "last".
-        allow_partial: Whether to split a message if only part of the message can be
-            included. If ``strategy="last"`` then the last partial contents of a message
-            are included. If ``strategy="first"`` then the first partial contents of a
-            message are included.
-            Default is False.
-        end_on: The message type to end on. If specified then every message after the
-            last occurrence of this type is ignored. If ``strategy=="last"`` then this
-            is done before we attempt to get the last ``max_tokens``. If
-            ``strategy=="first"`` then this is done after we get the first
-            ``max_tokens``. Can be specified as string names (e.g. "system", "human",
-            "ai", ...) or as BaseMessage classes (e.g. SystemMessage, HumanMessage,
-            AIMessage, ...). Can be a single type or a list of types.
-            Default is None.
-        start_on: The message type to start on. Should only be specified if
-            ``strategy="last"``. If specified then every message before
-            the first occurrence of this type is ignored. This is done after we trim
-            the initial messages to the last ``max_tokens``. Does not
-            apply to a SystemMessage at index 0 if ``include_system=True``. Can be
-            specified as string names (e.g. "system", "human", "ai", ...) or as
-            BaseMessage classes (e.g. SystemMessage, HumanMessage, AIMessage, ...). Can
-            be a single type or a list of types.
-            Default is None.
-        include_system: Whether to keep the SystemMessage if there is one at index 0.
-            Should only be specified if ``strategy="last"``.
-            Default is False.
-        text_splitter: Function or ``langchain_text_splitters.TextSplitter`` for
-            splitting the string contents of a message. Only used if
-            ``allow_partial=True``. If ``strategy="last"`` then the last split tokens
-            from a partial message will be included. if ``strategy=="first"`` then the
-            first split tokens from a partial message will be included. Token splitter
-            assumes that separators are kept, so that split contents can be directly
-            concatenated to recreate the original text. Defaults to splitting on
-            newlines.
-
-    Returns:
-        list of trimmed BaseMessages.
-
-    Raises:
-        ValueError: if two incompatible arguments are specified or an unrecognized
-            ``strategy`` is specified.
-
-    Example:
-        Trim chat history based on token count, keeping the SystemMessage if
-        present, and ensuring that the chat history starts with a HumanMessage (
-        or a SystemMessage followed by a HumanMessage).
-
-        .. code-block:: python
-
-            from typing import list
-
-            from langchain_core.messages import (
-                AIMessage,
-                HumanMessage,
-                BaseMessage,
-                SystemMessage,
-                trim_messages,
-            )
-
-            messages = [
-                SystemMessage("you're a good assistant, you always respond with a joke."),
-                HumanMessage("i wonder why it's called langchain"),
-                AIMessage(
-                    'Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'
-                ),
-                HumanMessage("and who is harrison chasing anyways"),
-                AIMessage(
-                    "Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"
-                ),
-                HumanMessage("what do you call a speechless parrot"),
-            ]
-
-
-            trim_messages(
-                messages,
-                max_tokens=45,
-                strategy="last",
-                token_counter=ChatOpenAI(model="gpt-4o"),
-                # Most chat models expect that chat history starts with either:
-                # (1) a HumanMessage or
-                # (2) a SystemMessage followed by a HumanMessage
-                start_on="human",
-                # Usually, we want to keep the SystemMessage
-                # if it's present in the original history.
-                # The SystemMessage has special instructions for the model.
-                include_system=True,
-                allow_partial=False,
-            )
-
-        .. code-block:: python
-
-            [
-                SystemMessage(content="you're a good assistant, you always respond with a joke."),
-                HumanMessage(content='what do you call a speechless parrot'),
-            ]
-
-        Trim chat history based on the message count, keeping the SystemMessage if
-        present, and ensuring that the chat history starts with a HumanMessage (
-        or a SystemMessage followed by a HumanMessage).
-
-            trim_messages(
-                messages,
-                # When `len` is passed in as the token counter function,
-                # max_tokens will count the number of messages in the chat history.
-                max_tokens=4,
-                strategy="last",
-                # Passing in `len` as a token counter function will
-                # count the number of messages in the chat history.
-                token_counter=len,
-                # Most chat models expect that chat history starts with either:
-                # (1) a HumanMessage or
-                # (2) a SystemMessage followed by a HumanMessage
-                start_on="human",
-                # Usually, we want to keep the SystemMessage
-                # if it's present in the original history.
-                # The SystemMessage has special instructions for the model.
-                include_system=True,
-                allow_partial=False,
-            )
-
-        .. code-block:: python
-
-            [
-                SystemMessage(content="you're a good assistant, you always respond with a joke."),
-                HumanMessage(content='and who is harrison chasing anyways'),
-                AIMessage(content="Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"),
-                HumanMessage(content='what do you call a speechless parrot'),
-            ]
-
-
-        Trim chat history using a custom token counter function that counts the
-        number of tokens in each message.
-
-        .. code-block:: python
-
-            messages = [
-                SystemMessage("This is a 4 token text. The full message is 10 tokens."),
-                HumanMessage("This is a 4 token text. The full message is 10 tokens.", id="first"),
-                AIMessage(
-                    [
-                        {"type": "text", "text": "This is the FIRST 4 token block."},
-                        {"type": "text", "text": "This is the SECOND 4 token block."},
-                    ],
-                    id="second",
-                ),
-                HumanMessage("This is a 4 token text. The full message is 10 tokens.", id="third"),
-                AIMessage("This is a 4 token text. The full message is 10 tokens.", id="fourth"),
-            ]
-
-            def dummy_token_counter(messages: list[BaseMessage]) -> int:
-                # treat each message like it adds 3 default tokens at the beginning
-                # of the message and at the end of the message. 3 + 4 + 3 = 10 tokens
-                # per message.
-
-                default_content_len = 4
-                default_msg_prefix_len = 3
-                default_msg_suffix_len = 3
-
-                count = 0
-                for msg in messages:
-                    if isinstance(msg.content, str):
-                        count += default_msg_prefix_len + default_content_len + default_msg_suffix_len
-                    if isinstance(msg.content, list):
-                        count += default_msg_prefix_len + len(msg.content) *  default_content_len + default_msg_suffix_len
-                return count
-
-        First 30 tokens, allowing partial messages:
-            .. code-block:: python
-
-                trim_messages(
-                    messages,
-                    max_tokens=30,
-                    token_counter=dummy_token_counter,
-                    strategy="first",
-                    allow_partial=True,
-                )
-
-            .. code-block:: python
-
-                [
-                    SystemMessage("This is a 4 token text. The full message is 10 tokens."),
-                    HumanMessage("This is a 4 token text. The full message is 10 tokens.", id="first"),
-                    AIMessage( [{"type": "text", "text": "This is the FIRST 4 token block."}], id="second"),
-                ]
-    """  # noqa: E501
-    if start_on and strategy == "first":
-        raise ValueError
-    if include_system and strategy == "first":
-        raise ValueError
-    messages = convert_to_messages(messages)
-    if hasattr(token_counter, "get_num_tokens_from_messages"):
-        list_token_counter = token_counter.get_num_tokens_from_messages
-    elif callable(token_counter):
-        if (
-            list(inspect.signature(token_counter).parameters.values())[0].annotation
-            is BaseMessage
-        ):
-
-            def list_token_counter(messages: Sequence[BaseMessage]) -> int:
-                return sum(token_counter(msg) for msg in messages)  # type: ignore[arg-type, misc]
-
-        else:
-            list_token_counter = token_counter  # type: ignore[assignment]
-    else:
-        msg = (
-            f"'token_counter' expected to be a model that implements "
-            f"'get_num_tokens_from_messages()' or a function. Received object of type "
-            f"{type(token_counter)}."
-        )
-        raise ValueError(msg)
-
-    try:
-        from langchain_text_splitters import TextSplitter
-    except ImportError:
-        text_splitter_fn: Optional[Callable] = cast(Optional[Callable], text_splitter)
-    else:
-        if isinstance(text_splitter, TextSplitter):
-            text_splitter_fn = text_splitter.split_text
-        else:
-            text_splitter_fn = text_splitter
-
-    text_splitter_fn = text_splitter_fn or _default_text_splitter
-
-    if strategy == "first":
-        return _first_max_tokens(
-            messages,
-            max_tokens=max_tokens,
-            token_counter=list_token_counter,
-            text_splitter=text_splitter_fn,
-            partial_strategy="first" if allow_partial else None,
-            end_on=end_on,
-        )
-    elif strategy == "last":
-        return _last_max_tokens(
-            messages,
-            max_tokens=max_tokens,
-            token_counter=list_token_counter,
-            allow_partial=allow_partial,
-            include_system=include_system,
-            start_on=start_on,
-            end_on=end_on,
-            text_splitter=text_splitter_fn,
-        )
-    else:
-        msg = f"Unrecognized {strategy=}. Supported strategies are 'last' and 'first'."
-        raise ValueError(msg)
-
-
-def convert_to_openai_messages(
-    messages: Union[MessageLikeRepresentation, Sequence[MessageLikeRepresentation]],
-    *,
-    text_format: Literal["string", "block"] = "string",
-) -> Union[dict, list[dict]]:
-    """Convert LangChain messages into OpenAI message dicts.
-
-    Args:
-        messages: Message-like object or iterable of objects whose contents are
-            in OpenAI, Anthropic, Bedrock Converse, or VertexAI formats.
-        text_format: How to format string or text block contents:
-
-                - "string":
-                    If a message has a string content, this is left as a string. If
-                    a message has content blocks that are all of type 'text', these are
-                    joined with a newline to make a single string. If a message has
-                    content blocks and at least one isn't of type 'text', then
-                    all blocks are left as dicts.
-                - "block":
-                    If a message has a string content, this is turned into a list
-                    with a single content block of type 'text'. If a message has content
-                    blocks these are left as is.
-
-    Returns:
-        The return type depends on the input type:
-            - dict:
-                If a single message-like object is passed in, a single OpenAI message
-                dict is returned.
-            - list[dict]:
-                If a sequence of message-like objects are passed in, a list of OpenAI
-                message dicts is returned.
-
-    Example:
-
-        .. code-block:: python
-
-            from langchain_core.messages import (
-                convert_to_openai_messages,
-                AIMessage,
-                SystemMessage,
-                ToolMessage,
-            )
-
-            messages = [
-                SystemMessage([{"type": "text", "text": "foo"}]),
-                {"role": "user", "content": [{"type": "text", "text": "whats in this"}, {"type": "image_url", "image_url": {"url": "data:image/png;base64,'/9j/4AAQSk'"}}]},
-                AIMessage("", tool_calls=[{"name": "analyze", "args": {"baz": "buz"}, "id": "1", "type": "tool_call"}]),
-                ToolMessage("foobar", tool_call_id="1", name="bar"),
-                {"role": "assistant", "content": "thats nice"},
-            ]
-            oai_messages = convert_to_openai_messages(messages)
-            # -> [
-            #   {'role': 'system', 'content': 'foo'},
-            #   {'role': 'user', 'content': [{'type': 'text', 'text': 'whats in this'}, {'type': 'image_url', 'image_url': {'url': "data:image/png;base64,'/9j/4AAQSk'"}}]},
-            #   {'role': 'assistant', 'tool_calls': [{'type': 'function', 'id': '1','function': {'name': 'analyze', 'arguments': '{"baz": "buz"}'}}], 'content': ''},
-            #   {'role': 'tool', 'name': 'bar', 'content': 'foobar'},
-            #   {'role': 'assistant', 'content': 'thats nice'}
-            # ]
-
-    .. versionadded:: 0.3.11
-
-    """  # noqa: E501
-    if text_format not in ("string", "block"):
-        err = f"Unrecognized {text_format=}, expected one of 'string' or 'block'."
-        raise ValueError(err)
-
-    oai_messages: list = []
-
-    if is_single := isinstance(messages, (BaseMessage, dict, str)):
-        messages = [messages]
-
-    messages = convert_to_messages(messages)
-
-    for i, message in enumerate(messages):
-        oai_msg: dict = {"role": _get_message_openai_role(message)}
-        tool_messages: list = []
-        content: Union[str, list[dict]]
-
-        if message.name:
-            oai_msg["name"] = message.name
-        if isinstance(message, AIMessage) and message.tool_calls:
-            oai_msg["tool_calls"] = _convert_to_openai_tool_calls(message.tool_calls)
-        if message.additional_kwargs.get("refusal"):
-            oai_msg["refusal"] = message.additional_kwargs["refusal"]
-        if isinstance(message, ToolMessage):
-            oai_msg["tool_call_id"] = message.tool_call_id
-
-        if not message.content:
-            content = "" if text_format == "string" else []
-        elif isinstance(message.content, str):
-            if text_format == "string":
-                content = message.content
-            else:
-                content = [{"type": "text", "text": message.content}]
-        else:
-            if text_format == "string" and all(
-                isinstance(block, str) or block.get("type") == "text"
-                for block in message.content
-            ):
-                content = "\n".join(
-                    block if isinstance(block, str) else block["text"]
-                    for block in message.content
-                )
-            else:
-                content = []
-                for j, block in enumerate(message.content):
-                    # OpenAI format
-                    if isinstance(block, str):
-                        content.append({"type": "text", "text": block})
-                    elif block.get("type") == "text":
-                        if missing := [k for k in ("text",) if k not in block]:
-                            err = (
-                                f"Unrecognized content block at "
-                                f"messages[{i}].content[{j}] has 'type': 'text' "
-                                f"but is missing expected key(s) "
-                                f"{missing}. Full content block:\n\n{block}"
-                            )
-                            raise ValueError(err)
-                        content.append({"type": block["type"], "text": block["text"]})
-                    elif block.get("type") == "image_url":
-                        if missing := [k for k in ("image_url",) if k not in block]:
-                            err = (
-                                f"Unrecognized content block at "
-                                f"messages[{i}].content[{j}] has 'type': 'image_url' "
-                                f"but is missing expected key(s) "
-                                f"{missing}. Full content block:\n\n{block}"
-                            )
-                            raise ValueError(err)
-                        content.append(
-                            {
-                                "type": "image_url",
-                                "image_url": block["image_url"],
-                            }
-                        )
-                    # Anthropic and Bedrock converse format
-                    elif (block.get("type") == "image") or "image" in block:
-                        # Anthropic
-                        if source := block.get("source"):
-                            if missing := [
-                                k
-                                for k in ("media_type", "type", "data")
-                                if k not in source
-                            ]:
-                                err = (
-                                    f"Unrecognized content block at "
-                                    f"messages[{i}].content[{j}] has 'type': 'image' "
-                                    f"but 'source' is missing expected key(s) "
-                                    f"{missing}. Full content block:\n\n{block}"
-                                )
-                                raise ValueError(err)
-                            content.append(
-                                {
-                                    "type": "image_url",
-                                    "image_url": {
-                                        "url": (
-                                            f"data:{source['media_type']};"
-                                            f"{source['type']},{source['data']}"
-                                        )
-                                    },
-                                }
-                            )
-                        # Bedrock converse
-                        elif image := block.get("image"):
-                            if missing := [
-                                k for k in ("source", "format") if k not in image
-                            ]:
-                                err = (
-                                    f"Unrecognized content block at "
-                                    f"messages[{i}].content[{j}] has key 'image', "
-                                    f"but 'image' is missing expected key(s) "
-                                    f"{missing}. Full content block:\n\n{block}"
-                                )
-                                raise ValueError(err)
-                            b64_image = _bytes_to_b64_str(image["source"]["bytes"])
-                            content.append(
-                                {
-                                    "type": "image_url",
-                                    "image_url": {
-                                        "url": (
-                                            f"data:image/{image['format']};"
-                                            f"base64,{b64_image}"
-                                        )
-                                    },
-                                }
-                            )
-                        else:
-                            err = (
-                                f"Unrecognized content block at "
-                                f"messages[{i}].content[{j}] has 'type': 'image' "
-                                f"but does not have a 'source' or 'image' key. Full "
-                                f"content block:\n\n{block}"
-                            )
-                            raise ValueError(err)
-                    elif block.get("type") == "tool_use":
-                        if missing := [
-                            k for k in ("id", "name", "input") if k not in block
-                        ]:
-                            err = (
-                                f"Unrecognized content block at "
-                                f"messages[{i}].content[{j}] has 'type': "
-                                f"'tool_use', but is missing expected key(s) "
-                                f"{missing}. Full content block:\n\n{block}"
-                            )
-                            raise ValueError(err)
-                        if not any(
-                            tool_call["id"] == block["id"]
-                            for tool_call in cast(AIMessage, message).tool_calls
-                        ):
-                            oai_msg["tool_calls"] = oai_msg.get("tool_calls", [])
-                            oai_msg["tool_calls"].append(
-                                {
-                                    "type": "function",
-                                    "id": block["id"],
-                                    "function": {
-                                        "name": block["name"],
-                                        "arguments": json.dumps(block["input"]),
-                                    },
-                                }
-                            )
-                    elif block.get("type") == "tool_result":
-                        if missing := [
-                            k for k in ("content", "tool_use_id") if k not in block
-                        ]:
-                            msg = (
-                                f"Unrecognized content block at "
-                                f"messages[{i}].content[{j}] has 'type': "
-                                f"'tool_result', but is missing expected key(s) "
-                                f"{missing}. Full content block:\n\n{block}"
-                            )
-                            raise ValueError(msg)
-                        tool_message = ToolMessage(
-                            block["content"],
-                            tool_call_id=block["tool_use_id"],
-                            status="error" if block.get("is_error") else "success",
-                        )
-                        # Recurse to make sure tool message contents are OpenAI format.
-                        tool_messages.extend(
-                            convert_to_openai_messages(
-                                [tool_message], text_format=text_format
-                            )
-                        )
-                    elif (block.get("type") == "json") or "json" in block:
-                        if "json" not in block:
-                            msg = (
-                                f"Unrecognized content block at "
-                                f"messages[{i}].content[{j}] has 'type': 'json' "
-                                f"but does not have a 'json' key. Full "
-                                f"content block:\n\n{block}"
-                            )
-                            raise ValueError(msg)
-                        content.append(
-                            {
-                                "type": "text",
-                                "text": json.dumps(block["json"]),
-                            }
-                        )
-                    elif (
-                        block.get("type") == "guard_content"
-                    ) or "guard_content" in block:
-                        if (
-                            "guard_content" not in block
-                            or "text" not in block["guard_content"]
-                        ):
-                            msg = (
-                                f"Unrecognized content block at "
-                                f"messages[{i}].content[{j}] has 'type': "
-                                f"'guard_content' but does not have a "
-                                f"messages[{i}].content[{j}]['guard_content']['text'] "
-                                f"key. Full content block:\n\n{block}"
-                            )
-                            raise ValueError(msg)
-                        text = block["guard_content"]["text"]
-                        if isinstance(text, dict):
-                            text = text["text"]
-                        content.append({"type": "text", "text": text})
-                    # VertexAI format
-                    elif block.get("type") == "media":
-                        if missing := [
-                            k for k in ("mime_type", "data") if k not in block
-                        ]:
-                            err = (
-                                f"Unrecognized content block at "
-                                f"messages[{i}].content[{j}] has 'type': "
-                                f"'media' but does not have key(s) {missing}. Full "
-                                f"content block:\n\n{block}"
-                            )
-                            raise ValueError(err)
-                        if "image" not in block["mime_type"]:
-                            err = (
-                                f"OpenAI messages can only support text and image data."
-                                f" Received content block with media of type:"
-                                f" {block['mime_type']}"
-                            )
-                            raise ValueError(err)
-                        b64_image = _bytes_to_b64_str(block["data"])
-                        content.append(
-                            {
-                                "type": "image_url",
-                                "image_url": {
-                                    "url": (
-                                        f"data:{block['mime_type']};base64,{b64_image}"
-                                    )
-                                },
-                            }
-                        )
-                    else:
-                        err = (
-                            f"Unrecognized content block at "
-                            f"messages[{i}].content[{j}] does not match OpenAI, "
-                            f"Anthropic, Bedrock Converse, or VertexAI format. Full "
-                            f"content block:\n\n{block}"
-                        )
-                        raise ValueError(err)
-                if text_format == "string" and not any(
-                    block["type"] != "text" for block in content
-                ):
-                    content = "\n".join(block["text"] for block in content)
-        oai_msg["content"] = content
-        if message.content and not oai_msg["content"] and tool_messages:
-            oai_messages.extend(tool_messages)
-        else:
-            oai_messages.extend([oai_msg, *tool_messages])
-
-    if is_single:
-        return oai_messages[0]
-    else:
-        return oai_messages
-
-
-def _first_max_tokens(
-    messages: Sequence[BaseMessage],
-    *,
-    max_tokens: int,
-    token_counter: Callable[[list[BaseMessage]], int],
-    text_splitter: Callable[[str], list[str]],
-    partial_strategy: Optional[Literal["first", "last"]] = None,
-    end_on: Optional[
-        Union[str, type[BaseMessage], Sequence[Union[str, type[BaseMessage]]]]
-    ] = None,
-) -> list[BaseMessage]:
-    messages = list(messages)
-    if not messages:
-        return messages
-    idx = 0
-    for i in range(len(messages)):
-        if token_counter(messages[:-i] if i else messages) <= max_tokens:
-            idx = len(messages) - i
-            break
-    if partial_strategy and (idx < len(messages) - 1 or idx == 0):
-        included_partial = False
-        if isinstance(messages[idx].content, list):
-            excluded = messages[idx].model_copy(deep=True)
-            num_block = len(excluded.content)
-            if partial_strategy == "last":
-                excluded.content = list(reversed(excluded.content))
-            for _ in range(1, num_block):
-                excluded.content = excluded.content[:-1]
-                if token_counter(messages[:idx] + [excluded]) <= max_tokens:
-                    messages = messages[:idx] + [excluded]
-                    idx += 1
-                    included_partial = True
-                    break
-            if included_partial and partial_strategy == "last":
-                excluded.content = list(reversed(excluded.content))
-        if not included_partial:
-            excluded = messages[idx].model_copy(deep=True)
-            if isinstance(excluded.content, list) and any(
-                isinstance(block, str) or block["type"] == "text"
-                for block in messages[idx].content
-            ):
-                text_block = next(
-                    block
-                    for block in messages[idx].content
-                    if isinstance(block, str) or block["type"] == "text"
-                )
-                text = (
-                    text_block["text"] if isinstance(text_block, dict) else text_block
-                )
-            elif isinstance(excluded.content, str):
-                text = excluded.content
-            else:
-                text = None
-            if text:
-                split_texts = text_splitter(text)
-                num_splits = len(split_texts)
-                if partial_strategy == "last":
-                    split_texts = list(reversed(split_texts))
-                for _ in range(num_splits - 1):
-                    split_texts.pop()
-                    excluded.content = "".join(split_texts)
-                    if token_counter(messages[:idx] + [excluded]) <= max_tokens:
-                        if partial_strategy == "last":
-                            excluded.content = "".join(reversed(split_texts))
-                        messages = messages[:idx] + [excluded]
-                        idx += 1
-                        break
-
-    if end_on:
-        while idx > 0 and not _is_message_type(messages[idx - 1], end_on):
-            idx -= 1
-
-    return messages[:idx]
-
-
-def _last_max_tokens(
-    messages: Sequence[BaseMessage],
-    *,
-    max_tokens: int,
-    token_counter: Callable[[list[BaseMessage]], int],
-    text_splitter: Callable[[str], list[str]],
-    allow_partial: bool = False,
-    include_system: bool = False,
-    start_on: Optional[
-        Union[str, type[BaseMessage], Sequence[Union[str, type[BaseMessage]]]]
-    ] = None,
-    end_on: Optional[
-        Union[str, type[BaseMessage], Sequence[Union[str, type[BaseMessage]]]]
-    ] = None,
-) -> list[BaseMessage]:
-    messages = list(messages)
-    if len(messages) == 0:
-        return []
-    if end_on:
-        while messages and not _is_message_type(messages[-1], end_on):
-            messages.pop()
-    swapped_system = include_system and isinstance(messages[0], SystemMessage)
-    reversed_ = messages[:1] + messages[1:][::-1] if swapped_system else messages[::-1]
-
-    reversed_ = _first_max_tokens(
-        reversed_,
-        max_tokens=max_tokens,
-        token_counter=token_counter,
-        text_splitter=text_splitter,
-        partial_strategy="last" if allow_partial else None,
-        end_on=start_on,
-    )
-    if swapped_system:
-        return reversed_[:1] + reversed_[1:][::-1]
-    else:
-        return reversed_[::-1]
-
-
-_MSG_CHUNK_MAP: dict[type[BaseMessage], type[BaseMessageChunk]] = {
-    HumanMessage: HumanMessageChunk,
-    AIMessage: AIMessageChunk,
-    SystemMessage: SystemMessageChunk,
-    ToolMessage: ToolMessageChunk,
-    FunctionMessage: FunctionMessageChunk,
-    ChatMessage: ChatMessageChunk,
-}
-_CHUNK_MSG_MAP = {v: k for k, v in _MSG_CHUNK_MAP.items()}
-
-
-def _msg_to_chunk(message: BaseMessage) -> BaseMessageChunk:
-    if message.__class__ in _MSG_CHUNK_MAP:
-        return _MSG_CHUNK_MAP[message.__class__](**message.model_dump(exclude={"type"}))
-
-    for msg_cls, chunk_cls in _MSG_CHUNK_MAP.items():
-        if isinstance(message, msg_cls):
-            return chunk_cls(**message.model_dump(exclude={"type"}))
-
-    msg = (
-        f"Unrecognized message class {message.__class__}. Supported classes are "
-        f"{list(_MSG_CHUNK_MAP.keys())}"
-    )
-    msg = create_message(message=msg, error_code=ErrorCode.MESSAGE_COERCION_FAILURE)
-    raise ValueError(msg)
-
-
-def _chunk_to_msg(chunk: BaseMessageChunk) -> BaseMessage:
-    if chunk.__class__ in _CHUNK_MSG_MAP:
-        return _CHUNK_MSG_MAP[chunk.__class__](
-            **chunk.model_dump(exclude={"type", "tool_call_chunks"})
-        )
-    for chunk_cls, msg_cls in _CHUNK_MSG_MAP.items():
-        if isinstance(chunk, chunk_cls):
-            return msg_cls(**chunk.model_dump(exclude={"type", "tool_call_chunks"}))
-
-    msg = (
-        f"Unrecognized message chunk class {chunk.__class__}. Supported classes are "
-        f"{list(_CHUNK_MSG_MAP.keys())}"
-    )
-    msg = create_message(message=msg, error_code=ErrorCode.MESSAGE_COERCION_FAILURE)
-    raise ValueError(msg)
-
-
-def _default_text_splitter(text: str) -> list[str]:
-    splits = text.split("\n")
-    return [s + "\n" for s in splits[:-1]] + splits[-1:]
-
-
-def _is_message_type(
-    message: BaseMessage,
-    type_: Union[str, type[BaseMessage], Sequence[Union[str, type[BaseMessage]]]],
-) -> bool:
-    types = [type_] if isinstance(type_, (str, type)) else type_
-    types_str = [t for t in types if isinstance(t, str)]
-    types_types = tuple(t for t in types if isinstance(t, type))
-
-    return message.type in types_str or isinstance(message, types_types)
-
-
-def _bytes_to_b64_str(bytes_: bytes) -> str:
-    return base64.b64encode(bytes_).decode("utf-8")
-
-
-def _get_message_openai_role(message: BaseMessage) -> str:
-    if isinstance(message, AIMessage):
-        return "assistant"
-    elif isinstance(message, HumanMessage):
-        return "user"
-    elif isinstance(message, ToolMessage):
-        return "tool"
-    elif isinstance(message, SystemMessage):
-        return message.additional_kwargs.get("__openai_role__", "system")
-    elif isinstance(message, FunctionMessage):
-        return "function"
-    elif isinstance(message, ChatMessage):
-        return message.role
-    else:
-        msg = f"Unknown BaseMessage type {message.__class__}."
-        raise ValueError(msg)  # noqa: TRY004
-
-
-def _convert_to_openai_tool_calls(tool_calls: list[ToolCall]) -> list[dict]:
-    return [
-        {
-            "type": "function",
-            "id": tool_call["id"],
-            "function": {
-                "name": tool_call["name"],
-                "arguments": json.dumps(tool_call["args"]),
-            },
-        }
-        for tool_call in tool_calls
-    ]
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/output_parsers/__init__.py ./custom_langchain_core/output_parsers/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/output_parsers/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/output_parsers/__init__.py	1970-01-01 09:00:00
@@ -1,59 +0,0 @@
-"""**OutputParser** classes parse the output of an LLM call.
-
-**Class hierarchy:**
-
-.. code-block::
-
-    BaseLLMOutputParser --> BaseOutputParser --> <name>OutputParser  # ListOutputParser, PydanticOutputParser
-
-**Main helpers:**
-
-.. code-block::
-
-    Serializable, Generation, PromptValue
-"""  # noqa: E501
-
-from langchain_core.output_parsers.base import (
-    BaseGenerationOutputParser,
-    BaseLLMOutputParser,
-    BaseOutputParser,
-)
-from langchain_core.output_parsers.json import JsonOutputParser, SimpleJsonOutputParser
-from langchain_core.output_parsers.list import (
-    CommaSeparatedListOutputParser,
-    ListOutputParser,
-    MarkdownListOutputParser,
-    NumberedListOutputParser,
-)
-from langchain_core.output_parsers.openai_tools import (
-    JsonOutputKeyToolsParser,
-    JsonOutputToolsParser,
-    PydanticToolsParser,
-)
-from langchain_core.output_parsers.pydantic import PydanticOutputParser
-from langchain_core.output_parsers.string import StrOutputParser
-from langchain_core.output_parsers.transform import (
-    BaseCumulativeTransformOutputParser,
-    BaseTransformOutputParser,
-)
-from langchain_core.output_parsers.xml import XMLOutputParser
-
-__all__ = [
-    "BaseLLMOutputParser",
-    "BaseGenerationOutputParser",
-    "BaseOutputParser",
-    "ListOutputParser",
-    "CommaSeparatedListOutputParser",
-    "NumberedListOutputParser",
-    "MarkdownListOutputParser",
-    "StrOutputParser",
-    "BaseTransformOutputParser",
-    "BaseCumulativeTransformOutputParser",
-    "SimpleJsonOutputParser",
-    "XMLOutputParser",
-    "JsonOutputParser",
-    "PydanticOutputParser",
-    "JsonOutputToolsParser",
-    "JsonOutputKeyToolsParser",
-    "PydanticToolsParser",
-]
Binary files .venv/lib/python3.12/site-packages/langchain_core/output_parsers/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/output_parsers/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/output_parsers/__pycache__/base.cpython-312.pyc and ./custom_langchain_core/output_parsers/__pycache__/base.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/output_parsers/__pycache__/format_instructions.cpython-312.pyc and ./custom_langchain_core/output_parsers/__pycache__/format_instructions.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/output_parsers/__pycache__/json.cpython-312.pyc and ./custom_langchain_core/output_parsers/__pycache__/json.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/output_parsers/__pycache__/list.cpython-312.pyc and ./custom_langchain_core/output_parsers/__pycache__/list.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/output_parsers/__pycache__/openai_functions.cpython-312.pyc and ./custom_langchain_core/output_parsers/__pycache__/openai_functions.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/output_parsers/__pycache__/openai_tools.cpython-312.pyc and ./custom_langchain_core/output_parsers/__pycache__/openai_tools.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/output_parsers/__pycache__/pydantic.cpython-312.pyc and ./custom_langchain_core/output_parsers/__pycache__/pydantic.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/output_parsers/__pycache__/string.cpython-312.pyc and ./custom_langchain_core/output_parsers/__pycache__/string.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/output_parsers/__pycache__/transform.cpython-312.pyc and ./custom_langchain_core/output_parsers/__pycache__/transform.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/output_parsers/__pycache__/xml.cpython-312.pyc and ./custom_langchain_core/output_parsers/__pycache__/xml.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py ./custom_langchain_core/output_parsers/base.py
--- .venv/lib/python3.12/site-packages/langchain_core/output_parsers/base.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/output_parsers/base.py	1970-01-01 09:00:00
@@ -1,325 +0,0 @@
-from __future__ import annotations
-
-import contextlib
-from abc import ABC, abstractmethod
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Generic,
-    Optional,
-    TypeVar,
-    Union,
-)
-
-from typing_extensions import override
-
-from langchain_core.language_models import LanguageModelOutput
-from langchain_core.messages import AnyMessage, BaseMessage
-from langchain_core.outputs import ChatGeneration, Generation
-from langchain_core.runnables import Runnable, RunnableConfig, RunnableSerializable
-from langchain_core.runnables.config import run_in_executor
-
-if TYPE_CHECKING:
-    from langchain_core.prompt_values import PromptValue
-
-T = TypeVar("T")
-OutputParserLike = Runnable[LanguageModelOutput, T]
-
-
-class BaseLLMOutputParser(Generic[T], ABC):
-    """Abstract base class for parsing the outputs of a model."""
-
-    @abstractmethod
-    def parse_result(self, result: list[Generation], *, partial: bool = False) -> T:
-        """Parse a list of candidate model Generations into a specific format.
-
-        Args:
-            result: A list of Generations to be parsed. The Generations are assumed
-                to be different candidate outputs for a single model input.
-            partial: Whether to parse the output as a partial result. This is useful
-                for parsers that can parse partial results. Default is False.
-
-        Returns:
-            Structured output.
-        """
-
-    async def aparse_result(
-        self, result: list[Generation], *, partial: bool = False
-    ) -> T:
-        """Async parse a list of candidate model Generations into a specific format.
-
-        Args:
-            result: A list of Generations to be parsed. The Generations are assumed
-                to be different candidate outputs for a single model input.
-            partial: Whether to parse the output as a partial result. This is useful
-                for parsers that can parse partial results. Default is False.
-
-        Returns:
-            Structured output.
-        """
-        return await run_in_executor(None, self.parse_result, result)
-
-
-class BaseGenerationOutputParser(
-    BaseLLMOutputParser, RunnableSerializable[LanguageModelOutput, T]
-):
-    """Base class to parse the output of an LLM call."""
-
-    @property
-    @override
-    def InputType(self) -> Any:
-        """Return the input type for the parser."""
-        return Union[str, AnyMessage]
-
-    @property
-    @override
-    def OutputType(self) -> type[T]:
-        """Return the output type for the parser."""
-        # even though mypy complains this isn't valid,
-        # it is good enough for pydantic to build the schema from
-        return T  # type: ignore[misc]
-
-    def invoke(
-        self,
-        input: Union[str, BaseMessage],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> T:
-        if isinstance(input, BaseMessage):
-            return self._call_with_config(
-                lambda inner_input: self.parse_result(
-                    [ChatGeneration(message=inner_input)]
-                ),
-                input,
-                config,
-                run_type="parser",
-            )
-        else:
-            return self._call_with_config(
-                lambda inner_input: self.parse_result([Generation(text=inner_input)]),
-                input,
-                config,
-                run_type="parser",
-            )
-
-    async def ainvoke(
-        self,
-        input: Union[str, BaseMessage],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> T:
-        if isinstance(input, BaseMessage):
-            return await self._acall_with_config(
-                lambda inner_input: self.aparse_result(
-                    [ChatGeneration(message=inner_input)]
-                ),
-                input,
-                config,
-                run_type="parser",
-            )
-        else:
-            return await self._acall_with_config(
-                lambda inner_input: self.aparse_result([Generation(text=inner_input)]),
-                input,
-                config,
-                run_type="parser",
-            )
-
-
-class BaseOutputParser(
-    BaseLLMOutputParser, RunnableSerializable[LanguageModelOutput, T]
-):
-    """Base class to parse the output of an LLM call.
-
-    Output parsers help structure language model responses.
-
-    Example:
-        .. code-block:: python
-
-            class BooleanOutputParser(BaseOutputParser[bool]):
-                true_val: str = "YES"
-                false_val: str = "NO"
-
-                def parse(self, text: str) -> bool:
-                    cleaned_text = text.strip().upper()
-                    if cleaned_text not in (self.true_val.upper(), self.false_val.upper()):
-                        raise OutputParserException(
-                            f"BooleanOutputParser expected output value to either be "
-                            f"{self.true_val} or {self.false_val} (case-insensitive). "
-                            f"Received {cleaned_text}."
-                        )
-                    return cleaned_text == self.true_val.upper()
-
-                @property
-                def _type(self) -> str:
-                    return "boolean_output_parser"
-    """  # noqa: E501
-
-    @property
-    @override
-    def InputType(self) -> Any:
-        """Return the input type for the parser."""
-        return Union[str, AnyMessage]
-
-    @property
-    @override
-    def OutputType(self) -> type[T]:
-        """Return the output type for the parser.
-
-        This property is inferred from the first type argument of the class.
-
-        Raises:
-            TypeError: If the class doesn't have an inferable OutputType.
-        """
-        for base in self.__class__.mro():
-            if hasattr(base, "__pydantic_generic_metadata__"):
-                metadata = base.__pydantic_generic_metadata__
-                if "args" in metadata and len(metadata["args"]) > 0:
-                    return metadata["args"][0]
-
-        msg = (
-            f"Runnable {self.__class__.__name__} doesn't have an inferable OutputType. "
-            "Override the OutputType property to specify the output type."
-        )
-        raise TypeError(msg)
-
-    def invoke(
-        self,
-        input: Union[str, BaseMessage],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> T:
-        if isinstance(input, BaseMessage):
-            return self._call_with_config(
-                lambda inner_input: self.parse_result(
-                    [ChatGeneration(message=inner_input)]
-                ),
-                input,
-                config,
-                run_type="parser",
-            )
-        else:
-            return self._call_with_config(
-                lambda inner_input: self.parse_result([Generation(text=inner_input)]),
-                input,
-                config,
-                run_type="parser",
-            )
-
-    async def ainvoke(
-        self,
-        input: Union[str, BaseMessage],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> T:
-        if isinstance(input, BaseMessage):
-            return await self._acall_with_config(
-                lambda inner_input: self.aparse_result(
-                    [ChatGeneration(message=inner_input)]
-                ),
-                input,
-                config,
-                run_type="parser",
-            )
-        else:
-            return await self._acall_with_config(
-                lambda inner_input: self.aparse_result([Generation(text=inner_input)]),
-                input,
-                config,
-                run_type="parser",
-            )
-
-    def parse_result(self, result: list[Generation], *, partial: bool = False) -> T:
-        """Parse a list of candidate model Generations into a specific format.
-
-        The return value is parsed from only the first Generation in the result, which
-            is assumed to be the highest-likelihood Generation.
-
-        Args:
-            result: A list of Generations to be parsed. The Generations are assumed
-                to be different candidate outputs for a single model input.
-            partial: Whether to parse the output as a partial result. This is useful
-                for parsers that can parse partial results. Default is False.
-
-        Returns:
-            Structured output.
-        """
-        return self.parse(result[0].text)
-
-    @abstractmethod
-    def parse(self, text: str) -> T:
-        """Parse a single string model output into some structure.
-
-        Args:
-            text: String output of a language model.
-
-        Returns:
-            Structured output.
-        """
-
-    async def aparse_result(
-        self, result: list[Generation], *, partial: bool = False
-    ) -> T:
-        """Async parse a list of candidate model Generations into a specific format.
-
-        The return value is parsed from only the first Generation in the result, which
-            is assumed to be the highest-likelihood Generation.
-
-        Args:
-            result: A list of Generations to be parsed. The Generations are assumed
-                to be different candidate outputs for a single model input.
-            partial: Whether to parse the output as a partial result. This is useful
-                for parsers that can parse partial results. Default is False.
-
-        Returns:
-            Structured output.
-        """
-        return await run_in_executor(None, self.parse_result, result, partial=partial)
-
-    async def aparse(self, text: str) -> T:
-        """Async parse a single string model output into some structure.
-
-        Args:
-            text: String output of a language model.
-
-        Returns:
-            Structured output.
-        """
-        return await run_in_executor(None, self.parse, text)
-
-    # TODO: rename 'completion' -> 'text'.
-    def parse_with_prompt(self, completion: str, prompt: PromptValue) -> Any:
-        """Parse the output of an LLM call with the input prompt for context.
-
-        The prompt is largely provided in the event the OutputParser wants
-        to retry or fix the output in some way, and needs information from
-        the prompt to do so.
-
-        Args:
-            completion: String output of a language model.
-            prompt: Input PromptValue.
-
-        Returns:
-            Structured output.
-        """
-        return self.parse(completion)
-
-    def get_format_instructions(self) -> str:
-        """Instructions on how the LLM output should be formatted."""
-        raise NotImplementedError
-
-    @property
-    def _type(self) -> str:
-        """Return the output parser type for serialization."""
-        msg = (
-            f"_type property is not implemented in class {self.__class__.__name__}."
-            " This is required for serialization."
-        )
-        raise NotImplementedError(msg)
-
-    def dict(self, **kwargs: Any) -> dict:
-        """Return dictionary representation of output parser."""
-        output_parser_dict = super().dict(**kwargs)
-        with contextlib.suppress(NotImplementedError):
-            output_parser_dict["_type"] = self._type
-        return output_parser_dict
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/output_parsers/format_instructions.py ./custom_langchain_core/output_parsers/format_instructions.py
--- .venv/lib/python3.12/site-packages/langchain_core/output_parsers/format_instructions.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/output_parsers/format_instructions.py	1970-01-01 09:00:00
@@ -1,11 +0,0 @@
-# flake8: noqa
-
-JSON_FORMAT_INSTRUCTIONS = """The output should be formatted as a JSON instance that conforms to the JSON schema below.
-
-As an example, for the schema {{"properties": {{"foo": {{"title": "Foo", "description": "a list of strings", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}
-the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of the schema. The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.
-
-Here is the output schema:
-```
-{schema}
-```"""
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/output_parsers/json.py ./custom_langchain_core/output_parsers/json.py
--- .venv/lib/python3.12/site-packages/langchain_core/output_parsers/json.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/output_parsers/json.py	1970-01-01 09:00:00
@@ -1,129 +0,0 @@
-from __future__ import annotations
-
-import json
-from json import JSONDecodeError
-from typing import Annotated, Any, Optional, TypeVar, Union
-
-import jsonpatch  # type: ignore[import]
-import pydantic
-from pydantic import SkipValidation
-
-from langchain_core.exceptions import OutputParserException
-from langchain_core.output_parsers.format_instructions import JSON_FORMAT_INSTRUCTIONS
-from langchain_core.output_parsers.transform import BaseCumulativeTransformOutputParser
-from langchain_core.outputs import Generation
-from langchain_core.utils.json import (
-    parse_and_check_json_markdown,
-    parse_json_markdown,
-    parse_partial_json,
-)
-from langchain_core.utils.pydantic import PYDANTIC_MAJOR_VERSION
-
-if PYDANTIC_MAJOR_VERSION < 2:
-    PydanticBaseModel = pydantic.BaseModel
-
-else:
-    from pydantic.v1 import BaseModel
-
-    # Union type needs to be last assignment to PydanticBaseModel to make mypy happy.
-    PydanticBaseModel = Union[BaseModel, pydantic.BaseModel]  # type: ignore
-
-TBaseModel = TypeVar("TBaseModel", bound=PydanticBaseModel)
-
-
-class JsonOutputParser(BaseCumulativeTransformOutputParser[Any]):
-    """Parse the output of an LLM call to a JSON object.
-
-    When used in streaming mode, it will yield partial JSON objects containing
-    all the keys that have been returned so far.
-
-    In streaming, if `diff` is set to `True`, yields JSONPatch operations
-    describing the difference between the previous and the current object.
-    """
-
-    pydantic_object: Annotated[Optional[type[TBaseModel]], SkipValidation()] = None  # type: ignore
-    """The Pydantic object to use for validation.
-    If None, no validation is performed."""
-
-    def _diff(self, prev: Optional[Any], next: Any) -> Any:
-        return jsonpatch.make_patch(prev, next).patch
-
-    def _get_schema(self, pydantic_object: type[TBaseModel]) -> dict[str, Any]:
-        if issubclass(pydantic_object, pydantic.BaseModel):
-            return pydantic_object.model_json_schema()
-        elif issubclass(pydantic_object, pydantic.v1.BaseModel):
-            return pydantic_object.schema()
-
-    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
-        """Parse the result of an LLM call to a JSON object.
-
-        Args:
-            result: The result of the LLM call.
-            partial: Whether to parse partial JSON objects.
-                If True, the output will be a JSON object containing
-                all the keys that have been returned so far.
-                If False, the output will be the full JSON object.
-                Default is False.
-
-        Returns:
-            The parsed JSON object.
-
-        Raises:
-            OutputParserException: If the output is not valid JSON.
-        """
-        text = result[0].text
-        text = text.strip()
-        if partial:
-            try:
-                return parse_json_markdown(text)
-            except JSONDecodeError:
-                return None
-        else:
-            try:
-                return parse_json_markdown(text)
-            except JSONDecodeError as e:
-                msg = f"Invalid json output: {text}"
-                raise OutputParserException(msg, llm_output=text) from e
-
-    def parse(self, text: str) -> Any:
-        """Parse the output of an LLM call to a JSON object.
-
-        Args:
-            text: The output of the LLM call.
-
-        Returns:
-            The parsed JSON object.
-        """
-        return self.parse_result([Generation(text=text)])
-
-    def get_format_instructions(self) -> str:
-        """Return the format instructions for the JSON output.
-
-        Returns:
-            The format instructions for the JSON output.
-        """
-        if self.pydantic_object is None:
-            return "Return a JSON object."
-        else:
-            # Copy schema to avoid altering original Pydantic schema.
-            schema = dict(self._get_schema(self.pydantic_object).items())
-
-            # Remove extraneous fields.
-            reduced_schema = schema
-            if "title" in reduced_schema:
-                del reduced_schema["title"]
-            if "type" in reduced_schema:
-                del reduced_schema["type"]
-            # Ensure json in context is well-formed with double quotes.
-            schema_str = json.dumps(reduced_schema, ensure_ascii=False)
-            return JSON_FORMAT_INSTRUCTIONS.format(schema=schema_str)
-
-    @property
-    def _type(self) -> str:
-        return "simple_json_output_parser"
-
-
-# For backwards compatibility
-SimpleJsonOutputParser = JsonOutputParser
-parse_partial_json = parse_partial_json
-parse_and_check_json_markdown = parse_and_check_json_markdown
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/output_parsers/list.py ./custom_langchain_core/output_parsers/list.py
--- .venv/lib/python3.12/site-packages/langchain_core/output_parsers/list.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/output_parsers/list.py	1970-01-01 09:00:00
@@ -1,256 +0,0 @@
-from __future__ import annotations
-
-import csv
-import re
-from abc import abstractmethod
-from collections import deque
-from collections.abc import AsyncIterator, Iterator
-from io import StringIO
-from typing import Optional as Optional
-from typing import TypeVar, Union
-
-from langchain_core.messages import BaseMessage
-from langchain_core.output_parsers.transform import BaseTransformOutputParser
-
-T = TypeVar("T")
-
-
-def droplastn(iter: Iterator[T], n: int) -> Iterator[T]:
-    """Drop the last n elements of an iterator.
-
-    Args:
-        iter: The iterator to drop elements from.
-        n: The number of elements to drop.
-
-    Yields:
-        The elements of the iterator, except the last n elements.
-    """
-    buffer: deque[T] = deque()
-    for item in iter:
-        buffer.append(item)
-        if len(buffer) > n:
-            yield buffer.popleft()
-
-
-class ListOutputParser(BaseTransformOutputParser[list[str]]):
-    """Parse the output of an LLM call to a list."""
-
-    @property
-    def _type(self) -> str:
-        return "list"
-
-    @abstractmethod
-    def parse(self, text: str) -> list[str]:
-        """Parse the output of an LLM call.
-
-        Args:
-            text: The output of an LLM call.
-
-        Returns:
-            A list of strings.
-        """
-
-    def parse_iter(self, text: str) -> Iterator[re.Match]:
-        """Parse the output of an LLM call.
-
-        Args:
-            text: The output of an LLM call.
-
-        Yields:
-            A match object for each part of the output.
-        """
-        raise NotImplementedError
-
-    def _transform(
-        self, input: Iterator[Union[str, BaseMessage]]
-    ) -> Iterator[list[str]]:
-        buffer = ""
-        for chunk in input:
-            if isinstance(chunk, BaseMessage):
-                # extract text
-                chunk_content = chunk.content
-                if not isinstance(chunk_content, str):
-                    continue
-                chunk = chunk_content
-            # add current chunk to buffer
-            buffer += chunk
-            # parse buffer into a list of parts
-            try:
-                done_idx = 0
-                # yield only complete parts
-                for m in droplastn(self.parse_iter(buffer), 1):
-                    done_idx = m.end()
-                    yield [m.group(1)]
-                buffer = buffer[done_idx:]
-            except NotImplementedError:
-                parts = self.parse(buffer)
-                # yield only complete parts
-                if len(parts) > 1:
-                    for part in parts[:-1]:
-                        yield [part]
-                    buffer = parts[-1]
-        # yield the last part
-        for part in self.parse(buffer):
-            yield [part]
-
-    async def _atransform(
-        self, input: AsyncIterator[Union[str, BaseMessage]]
-    ) -> AsyncIterator[list[str]]:
-        buffer = ""
-        async for chunk in input:
-            if isinstance(chunk, BaseMessage):
-                # extract text
-                chunk_content = chunk.content
-                if not isinstance(chunk_content, str):
-                    continue
-                chunk = chunk_content
-            # add current chunk to buffer
-            buffer += chunk
-            # parse buffer into a list of parts
-            try:
-                done_idx = 0
-                # yield only complete parts
-                for m in droplastn(self.parse_iter(buffer), 1):
-                    done_idx = m.end()
-                    yield [m.group(1)]
-                buffer = buffer[done_idx:]
-            except NotImplementedError:
-                parts = self.parse(buffer)
-                # yield only complete parts
-                if len(parts) > 1:
-                    for part in parts[:-1]:
-                        yield [part]
-                    buffer = parts[-1]
-        # yield the last part
-        for part in self.parse(buffer):
-            yield [part]
-
-
-ListOutputParser.model_rebuild()
-
-
-class CommaSeparatedListOutputParser(ListOutputParser):
-    """Parse the output of an LLM call to a comma-separated list."""
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Check if the langchain object is serializable.
-
-        Returns True.
-        """
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-
-        Returns:
-            A list of strings.
-            Default is ["langchain", "output_parsers", "list"].
-        """
-        return ["langchain", "output_parsers", "list"]
-
-    def get_format_instructions(self) -> str:
-        """Return the format instructions for the comma-separated list output."""
-        return (
-            "Your response should be a list of comma separated values, "
-            "eg: `foo, bar, baz` or `foo,bar,baz`"
-        )
-
-    def parse(self, text: str) -> list[str]:
-        """Parse the output of an LLM call.
-
-        Args:
-            text: The output of an LLM call.
-
-        Returns:
-            A list of strings.
-        """
-        try:
-            reader = csv.reader(
-                StringIO(text), quotechar='"', delimiter=",", skipinitialspace=True
-            )
-            return [item for sublist in reader for item in sublist]
-        except csv.Error:
-            # keep old logic for backup
-            return [part.strip() for part in text.split(",")]
-
-    @property
-    def _type(self) -> str:
-        return "comma-separated-list"
-
-
-class NumberedListOutputParser(ListOutputParser):
-    """Parse a numbered list."""
-
-    pattern: str = r"\d+\.\s([^\n]+)"
-    """The pattern to match a numbered list item."""
-
-    def get_format_instructions(self) -> str:
-        return (
-            "Your response should be a numbered list with each item on a new line. "
-            "For example: \n\n1. foo\n\n2. bar\n\n3. baz"
-        )
-
-    def parse(self, text: str) -> list[str]:
-        """Parse the output of an LLM call.
-
-        Args:
-            text: The output of an LLM call.
-
-        Returns:
-            A list of strings.
-        """
-        return re.findall(self.pattern, text)
-
-    def parse_iter(self, text: str) -> Iterator[re.Match]:
-        """Parse the output of an LLM call.
-
-        Args:
-            text: The output of an LLM call.
-
-        Yields:
-            A match object for each part of the output.
-        """
-        return re.finditer(self.pattern, text)
-
-    @property
-    def _type(self) -> str:
-        return "numbered-list"
-
-
-class MarkdownListOutputParser(ListOutputParser):
-    """Parse a Markdown list."""
-
-    pattern: str = r"^\s*[-*]\s([^\n]+)$"
-    """The pattern to match a Markdown list item."""
-
-    def get_format_instructions(self) -> str:
-        """Return the format instructions for the Markdown list output."""
-        return "Your response should be a markdown list, eg: `- foo\n- bar\n- baz`"
-
-    def parse(self, text: str) -> list[str]:
-        """Parse the output of an LLM call.
-
-        Args:
-            text: The output of an LLM call.
-
-        Returns:
-            A list of strings.
-        """
-        return re.findall(self.pattern, text, re.MULTILINE)
-
-    def parse_iter(self, text: str) -> Iterator[re.Match]:
-        """Parse the output of an LLM call.
-
-        Args:
-            text: The output of an LLM call.
-
-        Yields:
-            A match object for each part of the output.
-        """
-        return re.finditer(self.pattern, text, re.MULTILINE)
-
-    @property
-    def _type(self) -> str:
-        return "markdown-list"
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/output_parsers/openai_functions.py ./custom_langchain_core/output_parsers/openai_functions.py
--- .venv/lib/python3.12/site-packages/langchain_core/output_parsers/openai_functions.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/output_parsers/openai_functions.py	1970-01-01 09:00:00
@@ -1,301 +0,0 @@
-import copy
-import json
-from types import GenericAlias
-from typing import Any, Optional, Union
-
-import jsonpatch  # type: ignore[import]
-from pydantic import BaseModel, model_validator
-
-from langchain_core.exceptions import OutputParserException
-from langchain_core.output_parsers import (
-    BaseCumulativeTransformOutputParser,
-    BaseGenerationOutputParser,
-)
-from langchain_core.output_parsers.json import parse_partial_json
-from langchain_core.outputs import ChatGeneration, Generation
-
-
-class OutputFunctionsParser(BaseGenerationOutputParser[Any]):
-    """Parse an output that is one of sets of values."""
-
-    args_only: bool = True
-    """Whether to only return the arguments to the function call."""
-
-    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
-        """Parse the result of an LLM call to a JSON object.
-
-        Args:
-            result: The result of the LLM call.
-            partial: Whether to parse partial JSON objects. Default is False.
-
-        Returns:
-            The parsed JSON object.
-
-        Raises:
-            OutputParserException: If the output is not valid JSON.
-        """
-        generation = result[0]
-        if not isinstance(generation, ChatGeneration):
-            msg = "This output parser can only be used with a chat generation."
-            raise OutputParserException(msg)
-        message = generation.message
-        try:
-            func_call = copy.deepcopy(message.additional_kwargs["function_call"])
-        except KeyError as exc:
-            msg = f"Could not parse function call: {exc}"
-            raise OutputParserException(msg) from exc
-
-        if self.args_only:
-            return func_call["arguments"]
-        return func_call
-
-
-class JsonOutputFunctionsParser(BaseCumulativeTransformOutputParser[Any]):
-    """Parse an output as the Json object."""
-
-    strict: bool = False
-    """Whether to allow non-JSON-compliant strings.
-
-    See: https://docs.python.org/3/library/json.html#encoders-and-decoders
-
-    Useful when the parsed output may include unicode characters or new lines.
-    """
-
-    args_only: bool = True
-    """Whether to only return the arguments to the function call."""
-
-    @property
-    def _type(self) -> str:
-        return "json_functions"
-
-    def _diff(self, prev: Optional[Any], next: Any) -> Any:
-        return jsonpatch.make_patch(prev, next).patch
-
-    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
-        """Parse the result of an LLM call to a JSON object.
-
-        Args:
-            result: The result of the LLM call.
-            partial: Whether to parse partial JSON objects. Default is False.
-
-        Returns:
-            The parsed JSON object.
-
-        Raises:
-            OutputParserException: If the output is not valid JSON.
-        """
-        if len(result) != 1:
-            msg = f"Expected exactly one result, but got {len(result)}"
-            raise OutputParserException(msg)
-        generation = result[0]
-        if not isinstance(generation, ChatGeneration):
-            msg = "This output parser can only be used with a chat generation."
-            raise OutputParserException(msg)
-        message = generation.message
-        try:
-            function_call = message.additional_kwargs["function_call"]
-        except KeyError as exc:
-            if partial:
-                return None
-            else:
-                msg = f"Could not parse function call: {exc}"
-                raise OutputParserException(msg) from exc
-        try:
-            if partial:
-                try:
-                    if self.args_only:
-                        return parse_partial_json(
-                            function_call["arguments"], strict=self.strict
-                        )
-                    else:
-                        return {
-                            **function_call,
-                            "arguments": parse_partial_json(
-                                function_call["arguments"], strict=self.strict
-                            ),
-                        }
-                except json.JSONDecodeError:
-                    return None
-            else:
-                if self.args_only:
-                    try:
-                        return json.loads(
-                            function_call["arguments"], strict=self.strict
-                        )
-                    except (json.JSONDecodeError, TypeError) as exc:
-                        msg = f"Could not parse function call data: {exc}"
-                        raise OutputParserException(msg) from exc
-                else:
-                    try:
-                        return {
-                            **function_call,
-                            "arguments": json.loads(
-                                function_call["arguments"], strict=self.strict
-                            ),
-                        }
-                    except (json.JSONDecodeError, TypeError) as exc:
-                        msg = f"Could not parse function call data: {exc}"
-                        raise OutputParserException(msg) from exc
-        except KeyError:
-            return None
-
-    # This method would be called by the default implementation of `parse_result`
-    # but we're overriding that method so it's not needed.
-    def parse(self, text: str) -> Any:
-        """Parse the output of an LLM call to a JSON object.
-
-        Args:
-            text: The output of the LLM call.
-
-        Returns:
-            The parsed JSON object.
-        """
-        raise NotImplementedError
-
-
-class JsonKeyOutputFunctionsParser(JsonOutputFunctionsParser):
-    """Parse an output as the element of the Json object."""
-
-    key_name: str
-    """The name of the key to return."""
-
-    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
-        """Parse the result of an LLM call to a JSON object.
-
-        Args:
-            result: The result of the LLM call.
-            partial: Whether to parse partial JSON objects. Default is False.
-
-        Returns:
-            The parsed JSON object.
-        """
-        res = super().parse_result(result, partial=partial)
-        if partial and res is None:
-            return None
-        return res.get(self.key_name) if partial else res[self.key_name]
-
-
-class PydanticOutputFunctionsParser(OutputFunctionsParser):
-    """Parse an output as a pydantic object.
-
-    This parser is used to parse the output of a ChatModel that uses
-    OpenAI function format to invoke functions.
-
-    The parser extracts the function call invocation and matches
-    them to the pydantic schema provided.
-
-    An exception will be raised if the function call does not match
-    the provided schema.
-
-    Example:
-        ... code-block:: python
-
-            message = AIMessage(
-                content="This is a test message",
-                additional_kwargs={
-                    "function_call": {
-                        "name": "cookie",
-                        "arguments": json.dumps({"name": "value", "age": 10}),
-                    }
-                },
-            )
-            chat_generation = ChatGeneration(message=message)
-
-            class Cookie(BaseModel):
-                name: str
-                age: int
-
-            class Dog(BaseModel):
-                species: str
-
-            # Full output
-            parser = PydanticOutputFunctionsParser(
-                pydantic_schema={"cookie": Cookie, "dog": Dog}
-            )
-            result = parser.parse_result([chat_generation])
-    """
-
-    pydantic_schema: Union[type[BaseModel], dict[str, type[BaseModel]]]
-    """The pydantic schema to parse the output with.
-
-    If multiple schemas are provided, then the function name will be used to
-    determine which schema to use.
-    """
-
-    @model_validator(mode="before")
-    @classmethod
-    def validate_schema(cls, values: dict) -> Any:
-        """Validate the pydantic schema.
-
-        Args:
-            values: The values to validate.
-
-        Returns:
-            The validated values.
-
-        Raises:
-            ValueError: If the schema is not a pydantic schema.
-        """
-        schema = values["pydantic_schema"]
-        if "args_only" not in values:
-            values["args_only"] = (
-                isinstance(schema, type)
-                and not isinstance(schema, GenericAlias)
-                and issubclass(schema, BaseModel)
-            )
-        elif values["args_only"] and isinstance(schema, dict):
-            msg = (
-                "If multiple pydantic schemas are provided then args_only should be"
-                " False."
-            )
-            raise ValueError(msg)
-        return values
-
-    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
-        """Parse the result of an LLM call to a JSON object.
-
-        Args:
-            result: The result of the LLM call.
-            partial: Whether to parse partial JSON objects. Default is False.
-
-        Returns:
-            The parsed JSON object.
-        """
-        _result = super().parse_result(result)
-        if self.args_only:
-            if hasattr(self.pydantic_schema, "model_validate_json"):
-                pydantic_args = self.pydantic_schema.model_validate_json(_result)
-            else:
-                pydantic_args = self.pydantic_schema.parse_raw(_result)  # type: ignore
-        else:
-            fn_name = _result["name"]
-            _args = _result["arguments"]
-            if isinstance(self.pydantic_schema, dict):
-                pydantic_schema = self.pydantic_schema[fn_name]
-            else:
-                pydantic_schema = self.pydantic_schema
-            if hasattr(pydantic_schema, "model_validate_json"):
-                pydantic_args = pydantic_schema.model_validate_json(_args)  # type: ignore
-            else:
-                pydantic_args = pydantic_schema.parse_raw(_args)  # type: ignore
-        return pydantic_args
-
-
-class PydanticAttrOutputFunctionsParser(PydanticOutputFunctionsParser):
-    """Parse an output as an attribute of a pydantic object."""
-
-    attr_name: str
-    """The name of the attribute to return."""
-
-    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
-        """Parse the result of an LLM call to a JSON object.
-
-        Args:
-            result: The result of the LLM call.
-            partial: Whether to parse partial JSON objects. Default is False.
-
-        Returns:
-            The parsed JSON object.
-        """
-        result = super().parse_result(result)
-        return getattr(result, self.attr_name)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/output_parsers/openai_tools.py ./custom_langchain_core/output_parsers/openai_tools.py
--- .venv/lib/python3.12/site-packages/langchain_core/output_parsers/openai_tools.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/output_parsers/openai_tools.py	1970-01-01 09:00:00
@@ -1,302 +0,0 @@
-import copy
-import json
-from json import JSONDecodeError
-from typing import Annotated, Any, Optional
-
-from pydantic import SkipValidation, ValidationError
-
-from langchain_core.exceptions import OutputParserException
-from langchain_core.messages import AIMessage, InvalidToolCall
-from langchain_core.messages.tool import invalid_tool_call
-from langchain_core.messages.tool import tool_call as create_tool_call
-from langchain_core.output_parsers.transform import BaseCumulativeTransformOutputParser
-from langchain_core.outputs import ChatGeneration, Generation
-from langchain_core.utils.json import parse_partial_json
-from langchain_core.utils.pydantic import TypeBaseModel
-
-
-def parse_tool_call(
-    raw_tool_call: dict[str, Any],
-    *,
-    partial: bool = False,
-    strict: bool = False,
-    return_id: bool = True,
-) -> Optional[dict[str, Any]]:
-    """Parse a single tool call.
-
-    Args:
-        raw_tool_call: The raw tool call to parse.
-        partial: Whether to parse partial JSON. Default is False.
-        strict: Whether to allow non-JSON-compliant strings.
-            Default is False.
-        return_id: Whether to return the tool call id. Default is True.
-
-    Returns:
-        The parsed tool call.
-
-    Raises:
-        OutputParserException: If the tool call is not valid JSON.
-    """
-    if "function" not in raw_tool_call:
-        return None
-    if partial:
-        try:
-            function_args = parse_partial_json(
-                raw_tool_call["function"]["arguments"], strict=strict
-            )
-        except (JSONDecodeError, TypeError):  # None args raise TypeError
-            return None
-    else:
-        try:
-            function_args = json.loads(
-                raw_tool_call["function"]["arguments"], strict=strict
-            )
-        except JSONDecodeError as e:
-            msg = (
-                f"Function {raw_tool_call['function']['name']} arguments:\n\n"
-                f"{raw_tool_call['function']['arguments']}\n\nare not valid JSON. "
-                f"Received JSONDecodeError {e}"
-            )
-            raise OutputParserException(msg) from e
-    parsed = {
-        "name": raw_tool_call["function"]["name"] or "",
-        "args": function_args or {},
-    }
-    if return_id:
-        parsed["id"] = raw_tool_call.get("id")
-        parsed = create_tool_call(**parsed)  # type: ignore
-    return parsed
-
-
-def make_invalid_tool_call(
-    raw_tool_call: dict[str, Any],
-    error_msg: Optional[str],
-) -> InvalidToolCall:
-    """Create an InvalidToolCall from a raw tool call.
-
-    Args:
-        raw_tool_call: The raw tool call.
-        error_msg: The error message.
-
-    Returns:
-        An InvalidToolCall instance with the error message.
-    """
-    return invalid_tool_call(
-        name=raw_tool_call["function"]["name"],
-        args=raw_tool_call["function"]["arguments"],
-        id=raw_tool_call.get("id"),
-        error=error_msg,
-    )
-
-
-def parse_tool_calls(
-    raw_tool_calls: list[dict],
-    *,
-    partial: bool = False,
-    strict: bool = False,
-    return_id: bool = True,
-) -> list[dict[str, Any]]:
-    """Parse a list of tool calls.
-
-    Args:
-        raw_tool_calls: The raw tool calls to parse.
-        partial: Whether to parse partial JSON. Default is False.
-        strict: Whether to allow non-JSON-compliant strings.
-            Default is False.
-        return_id: Whether to return the tool call id. Default is True.
-
-    Returns:
-        The parsed tool calls.
-
-    Raises:
-        OutputParserException: If any of the tool calls are not valid JSON.
-    """
-    final_tools: list[dict[str, Any]] = []
-    exceptions = []
-    for tool_call in raw_tool_calls:
-        try:
-            parsed = parse_tool_call(
-                tool_call, partial=partial, strict=strict, return_id=return_id
-            )
-            if parsed:
-                final_tools.append(parsed)
-        except OutputParserException as e:
-            exceptions.append(str(e))
-            continue
-    if exceptions:
-        raise OutputParserException("\n\n".join(exceptions))
-    return final_tools
-
-
-class JsonOutputToolsParser(BaseCumulativeTransformOutputParser[Any]):
-    """Parse tools from OpenAI response."""
-
-    strict: bool = False
-    """Whether to allow non-JSON-compliant strings.
-
-    See: https://docs.python.org/3/library/json.html#encoders-and-decoders
-
-    Useful when the parsed output may include unicode characters or new lines.
-    """
-    return_id: bool = False
-    """Whether to return the tool call id."""
-    first_tool_only: bool = False
-    """Whether to return only the first tool call.
-
-    If False, the result will be a list of tool calls, or an empty list
-    if no tool calls are found.
-
-    If true, and multiple tool calls are found, only the first one will be returned,
-    and the other tool calls will be ignored.
-    If no tool calls are found, None will be returned.
-    """
-
-    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
-        """Parse the result of an LLM call to a list of tool calls.
-
-        Args:
-            result: The result of the LLM call.
-            partial: Whether to parse partial JSON.
-                If True, the output will be a JSON object containing
-                all the keys that have been returned so far.
-                If False, the output will be the full JSON object.
-                Default is False.
-
-        Returns:
-            The parsed tool calls.
-
-        Raises:
-            OutputParserException: If the output is not valid JSON.
-        """
-        generation = result[0]
-        if not isinstance(generation, ChatGeneration):
-            msg = "This output parser can only be used with a chat generation."
-            raise OutputParserException(msg)
-        message = generation.message
-        if isinstance(message, AIMessage) and message.tool_calls:
-            tool_calls = [dict(tc) for tc in message.tool_calls]
-            for tool_call in tool_calls:
-                if not self.return_id:
-                    _ = tool_call.pop("id")
-        else:
-            try:
-                raw_tool_calls = copy.deepcopy(message.additional_kwargs["tool_calls"])
-            except KeyError:
-                return []
-            tool_calls = parse_tool_calls(
-                raw_tool_calls,
-                partial=partial,
-                strict=self.strict,
-                return_id=self.return_id,
-            )
-        # for backwards compatibility
-        for tc in tool_calls:
-            tc["type"] = tc.pop("name")
-
-        if self.first_tool_only:
-            return tool_calls[0] if tool_calls else None
-        return tool_calls
-
-    def parse(self, text: str) -> Any:
-        """Parse the output of an LLM call to a list of tool calls.
-
-        Args:
-            text: The output of the LLM call.
-
-        Returns:
-            The parsed tool calls.
-        """
-        raise NotImplementedError
-
-
-class JsonOutputKeyToolsParser(JsonOutputToolsParser):
-    """Parse tools from OpenAI response."""
-
-    key_name: str
-    """The type of tools to return."""
-
-    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
-        """Parse the result of an LLM call to a list of tool calls.
-
-        Args:
-            result: The result of the LLM call.
-            partial: Whether to parse partial JSON.
-                If True, the output will be a JSON object containing
-                all the keys that have been returned so far.
-                If False, the output will be the full JSON object.
-                Default is False.
-
-        Returns:
-            The parsed tool calls.
-        """
-        parsed_result = super().parse_result(result, partial=partial)
-
-        if self.first_tool_only:
-            single_result = (
-                parsed_result
-                if parsed_result and parsed_result["type"] == self.key_name
-                else None
-            )
-            if self.return_id:
-                return single_result
-            elif single_result:
-                return single_result["args"]
-            else:
-                return None
-        parsed_result = [res for res in parsed_result if res["type"] == self.key_name]
-        if not self.return_id:
-            parsed_result = [res["args"] for res in parsed_result]
-        return parsed_result
-
-
-class PydanticToolsParser(JsonOutputToolsParser):
-    """Parse tools from OpenAI response."""
-
-    tools: Annotated[list[TypeBaseModel], SkipValidation()]
-    """The tools to parse."""
-
-    # TODO: Support more granular streaming of objects. Currently only streams once all
-    # Pydantic object fields are present.
-    def parse_result(self, result: list[Generation], *, partial: bool = False) -> Any:
-        """Parse the result of an LLM call to a list of Pydantic objects.
-
-        Args:
-            result: The result of the LLM call.
-            partial: Whether to parse partial JSON.
-                If True, the output will be a JSON object containing
-                all the keys that have been returned so far.
-                If False, the output will be the full JSON object.
-                Default is False.
-
-        Returns:
-            The parsed Pydantic objects.
-
-        Raises:
-            OutputParserException: If the output is not valid JSON.
-        """
-        json_results = super().parse_result(result, partial=partial)
-        if not json_results:
-            return None if self.first_tool_only else []
-
-        json_results = [json_results] if self.first_tool_only else json_results
-        name_dict = {tool.__name__: tool for tool in self.tools}
-        pydantic_objects = []
-        for res in json_results:
-            if not isinstance(res["args"], dict):
-                if partial:
-                    continue
-                msg = (
-                    f"Tool arguments must be specified as a dict, received: "
-                    f"{res['args']}"
-                )
-                raise ValueError(msg)
-            try:
-                pydantic_objects.append(name_dict[res["type"]](**res["args"]))
-            except (ValidationError, ValueError):
-                if partial:
-                    continue
-                raise
-        if self.first_tool_only:
-            return pydantic_objects[0] if pydantic_objects else None
-        else:
-            return pydantic_objects
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/output_parsers/pydantic.py ./custom_langchain_core/output_parsers/pydantic.py
--- .venv/lib/python3.12/site-packages/langchain_core/output_parsers/pydantic.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/output_parsers/pydantic.py	1970-01-01 09:00:00
@@ -1,134 +0,0 @@
-import json
-from typing import Annotated, Generic, Optional
-
-import pydantic
-from pydantic import SkipValidation
-from typing_extensions import override
-
-from langchain_core.exceptions import OutputParserException
-from langchain_core.output_parsers import JsonOutputParser
-from langchain_core.outputs import Generation
-from langchain_core.utils.pydantic import (
-    PYDANTIC_MAJOR_VERSION,
-    PydanticBaseModel,
-    TBaseModel,
-)
-
-
-class PydanticOutputParser(JsonOutputParser, Generic[TBaseModel]):
-    """Parse an output using a pydantic model."""
-
-    pydantic_object: Annotated[type[TBaseModel], SkipValidation()]  # type: ignore
-    """The pydantic model to parse."""
-
-    def _parse_obj(self, obj: dict) -> TBaseModel:
-        if PYDANTIC_MAJOR_VERSION == 2:
-            try:
-                if issubclass(self.pydantic_object, pydantic.BaseModel):
-                    return self.pydantic_object.model_validate(obj)
-                elif issubclass(self.pydantic_object, pydantic.v1.BaseModel):
-                    return self.pydantic_object.parse_obj(obj)
-                else:
-                    msg = f"Unsupported model version for PydanticOutputParser: \
-                            {self.pydantic_object.__class__}"
-                    raise OutputParserException(msg)
-            except (pydantic.ValidationError, pydantic.v1.ValidationError) as e:
-                raise self._parser_exception(e, obj) from e
-        else:  # pydantic v1
-            try:
-                return self.pydantic_object.parse_obj(obj)
-            except pydantic.ValidationError as e:
-                raise self._parser_exception(e, obj) from e
-
-    def _parser_exception(
-        self, e: Exception, json_object: dict
-    ) -> OutputParserException:
-        json_string = json.dumps(json_object)
-        name = self.pydantic_object.__name__
-        msg = f"Failed to parse {name} from completion {json_string}. Got: {e}"
-        return OutputParserException(msg, llm_output=json_string)
-
-    def parse_result(
-        self, result: list[Generation], *, partial: bool = False
-    ) -> Optional[TBaseModel]:
-        """Parse the result of an LLM call to a pydantic object.
-
-        Args:
-            result: The result of the LLM call.
-            partial: Whether to parse partial JSON objects.
-                If True, the output will be a JSON object containing
-                all the keys that have been returned so far.
-                Defaults to False.
-
-        Returns:
-            The parsed pydantic object.
-        """
-        try:
-            json_object = super().parse_result(result)
-            return self._parse_obj(json_object)
-        except OutputParserException:
-            if partial:
-                return None
-            raise
-
-    def parse(self, text: str) -> TBaseModel:
-        """Parse the output of an LLM call to a pydantic object.
-
-        Args:
-            text: The output of the LLM call.
-
-        Returns:
-            The parsed pydantic object.
-        """
-        return super().parse(text)
-
-    def get_format_instructions(self) -> str:
-        """Return the format instructions for the JSON output.
-
-        Returns:
-            The format instructions for the JSON output.
-        """
-        # Copy schema to avoid altering original Pydantic schema.
-        schema = dict(self.pydantic_object.model_json_schema().items())
-
-        # Remove extraneous fields.
-        reduced_schema = schema
-        if "title" in reduced_schema:
-            del reduced_schema["title"]
-        if "type" in reduced_schema:
-            del reduced_schema["type"]
-        # Ensure json in context is well-formed with double quotes.
-        schema_str = json.dumps(reduced_schema, ensure_ascii=False)
-
-        return _PYDANTIC_FORMAT_INSTRUCTIONS.format(schema=schema_str)
-
-    @property
-    def _type(self) -> str:
-        return "pydantic"
-
-    @property
-    @override
-    def OutputType(self) -> type[TBaseModel]:
-        """Return the pydantic model."""
-        return self.pydantic_object
-
-
-PydanticOutputParser.model_rebuild()
-
-
-_PYDANTIC_FORMAT_INSTRUCTIONS = """The output should be formatted as a JSON instance that conforms to the JSON schema below.
-
-As an example, for the schema {{"properties": {{"foo": {{"title": "Foo", "description": "a list of strings", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}
-the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of the schema. The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.
-
-Here is the output schema:
-```
-{schema}
-```"""  # noqa: E501
-
-# Re-exporting types for backwards compatibility
-__all__ = [
-    "PydanticBaseModel",
-    "PydanticOutputParser",
-    "TBaseModel",
-]
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/output_parsers/string.py ./custom_langchain_core/output_parsers/string.py
--- .venv/lib/python3.12/site-packages/langchain_core/output_parsers/string.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/output_parsers/string.py	1970-01-01 09:00:00
@@ -1,29 +0,0 @@
-from typing import Optional as Optional
-
-from langchain_core.output_parsers.transform import BaseTransformOutputParser
-
-
-class StrOutputParser(BaseTransformOutputParser[str]):
-    """OutputParser that parses LLMResult into the top likely string."""
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Return whether this class is serializable."""
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "output_parser"]
-
-    @property
-    def _type(self) -> str:
-        """Return the output parser type for serialization."""
-        return "default"
-
-    def parse(self, text: str) -> str:
-        """Returns the input text with no changes."""
-        return text
-
-
-StrOutputParser.model_rebuild()
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/output_parsers/transform.py ./custom_langchain_core/output_parsers/transform.py
--- .venv/lib/python3.12/site-packages/langchain_core/output_parsers/transform.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/output_parsers/transform.py	1970-01-01 09:00:00
@@ -1,159 +0,0 @@
-from __future__ import annotations
-
-from collections.abc import AsyncIterator, Iterator
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Optional,
-    Union,
-)
-
-from langchain_core.messages import BaseMessage, BaseMessageChunk
-from langchain_core.output_parsers.base import BaseOutputParser, T
-from langchain_core.outputs import (
-    ChatGeneration,
-    ChatGenerationChunk,
-    Generation,
-    GenerationChunk,
-)
-from langchain_core.runnables.config import run_in_executor
-
-if TYPE_CHECKING:
-    from langchain_core.runnables import RunnableConfig
-
-
-class BaseTransformOutputParser(BaseOutputParser[T]):
-    """Base class for an output parser that can handle streaming input."""
-
-    def _transform(self, input: Iterator[Union[str, BaseMessage]]) -> Iterator[T]:
-        for chunk in input:
-            if isinstance(chunk, BaseMessage):
-                yield self.parse_result([ChatGeneration(message=chunk)])
-            else:
-                yield self.parse_result([Generation(text=chunk)])
-
-    async def _atransform(
-        self, input: AsyncIterator[Union[str, BaseMessage]]
-    ) -> AsyncIterator[T]:
-        async for chunk in input:
-            if isinstance(chunk, BaseMessage):
-                yield await run_in_executor(
-                    None, self.parse_result, [ChatGeneration(message=chunk)]
-                )
-            else:
-                yield await run_in_executor(
-                    None, self.parse_result, [Generation(text=chunk)]
-                )
-
-    def transform(
-        self,
-        input: Iterator[Union[str, BaseMessage]],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Iterator[T]:
-        """Transform the input into the output format.
-
-        Args:
-            input: The input to transform.
-            config: The configuration to use for the transformation.
-            kwargs: Additional keyword arguments.
-
-        Yields:
-            The transformed output.
-        """
-        yield from self._transform_stream_with_config(
-            input, self._transform, config, run_type="parser"
-        )
-
-    async def atransform(
-        self,
-        input: AsyncIterator[Union[str, BaseMessage]],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[T]:
-        """Async transform the input into the output format.
-
-        Args:
-            input: The input to transform.
-            config: The configuration to use for the transformation.
-            kwargs: Additional keyword arguments.
-
-        Yields:
-            The transformed output.
-        """
-        async for chunk in self._atransform_stream_with_config(
-            input, self._atransform, config, run_type="parser"
-        ):
-            yield chunk
-
-
-class BaseCumulativeTransformOutputParser(BaseTransformOutputParser[T]):
-    """Base class for an output parser that can handle streaming input."""
-
-    diff: bool = False
-    """In streaming mode, whether to yield diffs between the previous and current
-    parsed output, or just the current parsed output.
-    """
-
-    def _diff(self, prev: Optional[T], next: T) -> T:
-        """Convert parsed outputs into a diff format. The semantics of this are
-        up to the output parser.
-
-        Args:
-            prev: The previous parsed output.
-            next: The current parsed output.
-
-        Returns:
-            The diff between the previous and current parsed output.
-        """
-        raise NotImplementedError
-
-    def _transform(self, input: Iterator[Union[str, BaseMessage]]) -> Iterator[Any]:
-        prev_parsed = None
-        acc_gen: Union[GenerationChunk, ChatGenerationChunk, None] = None
-        for chunk in input:
-            chunk_gen: Union[GenerationChunk, ChatGenerationChunk]
-            if isinstance(chunk, BaseMessageChunk):
-                chunk_gen = ChatGenerationChunk(message=chunk)
-            elif isinstance(chunk, BaseMessage):
-                chunk_gen = ChatGenerationChunk(
-                    message=BaseMessageChunk(**chunk.dict())
-                )
-            else:
-                chunk_gen = GenerationChunk(text=chunk)
-
-            acc_gen = chunk_gen if acc_gen is None else acc_gen + chunk_gen  # type: ignore[operator]
-
-            parsed = self.parse_result([acc_gen], partial=True)
-            if parsed is not None and parsed != prev_parsed:
-                if self.diff:
-                    yield self._diff(prev_parsed, parsed)
-                else:
-                    yield parsed
-                prev_parsed = parsed
-
-    async def _atransform(
-        self, input: AsyncIterator[Union[str, BaseMessage]]
-    ) -> AsyncIterator[T]:
-        prev_parsed = None
-        acc_gen: Union[GenerationChunk, ChatGenerationChunk, None] = None
-        async for chunk in input:
-            chunk_gen: Union[GenerationChunk, ChatGenerationChunk]
-            if isinstance(chunk, BaseMessageChunk):
-                chunk_gen = ChatGenerationChunk(message=chunk)
-            elif isinstance(chunk, BaseMessage):
-                chunk_gen = ChatGenerationChunk(
-                    message=BaseMessageChunk(**chunk.dict())
-                )
-            else:
-                chunk_gen = GenerationChunk(text=chunk)
-
-            acc_gen = chunk_gen if acc_gen is None else acc_gen + chunk_gen  # type: ignore[operator]
-
-            parsed = await self.aparse_result([acc_gen], partial=True)
-            if parsed is not None and parsed != prev_parsed:
-                if self.diff:
-                    yield await run_in_executor(None, self._diff, prev_parsed, parsed)
-                else:
-                    yield parsed
-                prev_parsed = parsed
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/output_parsers/xml.py ./custom_langchain_core/output_parsers/xml.py
--- .venv/lib/python3.12/site-packages/langchain_core/output_parsers/xml.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/output_parsers/xml.py	1970-01-01 09:00:00
@@ -1,284 +0,0 @@
-import contextlib
-import re
-import xml
-import xml.etree.ElementTree as ET  # noqa: N817
-from collections.abc import AsyncIterator, Iterator
-from typing import Any, Literal, Optional, Union
-from xml.etree.ElementTree import TreeBuilder
-
-from langchain_core.exceptions import OutputParserException
-from langchain_core.messages import BaseMessage
-from langchain_core.output_parsers.transform import BaseTransformOutputParser
-from langchain_core.runnables.utils import AddableDict
-
-XML_FORMAT_INSTRUCTIONS = """The output should be formatted as a XML file.
-1. Output should conform to the tags below.
-2. If tags are not given, make them on your own.
-3. Remember to always open and close all the tags.
-
-As an example, for the tags ["foo", "bar", "baz"]:
-1. String "<foo>\n   <bar>\n      <baz></baz>\n   </bar>\n</foo>" is a well-formatted instance of the schema.
-2. String "<foo>\n   <bar>\n   </foo>" is a badly-formatted instance.
-3. String "<foo>\n   <tag>\n   </tag>\n</foo>" is a badly-formatted instance.
-
-Here are the output tags:
-```
-{tags}
-```"""  # noqa: E501
-
-
-class _StreamingParser:
-    """Streaming parser for XML.
-
-    This implementation is pulled into a class to avoid implementation
-    drift between transform and atransform of the XMLOutputParser.
-    """
-
-    def __init__(self, parser: Literal["defusedxml", "xml"]) -> None:
-        """Initialize the streaming parser.
-
-        Args:
-            parser: Parser to use for XML parsing. Can be either 'defusedxml' or 'xml'.
-              See documentation in XMLOutputParser for more information.
-
-        Raises:
-            ImportError: If defusedxml is not installed and the defusedxml
-                parser is requested.
-        """
-        if parser == "defusedxml":
-            try:
-                import defusedxml  # type: ignore
-            except ImportError as e:
-                msg = (
-                    "defusedxml is not installed. "
-                    "Please install it to use the defusedxml parser."
-                    "You can install it with `pip install defusedxml` "
-                )
-                raise ImportError(msg) from e
-            _parser = defusedxml.ElementTree.DefusedXMLParser(target=TreeBuilder())
-        else:
-            _parser = None
-        self.pull_parser = ET.XMLPullParser(["start", "end"], _parser=_parser)
-        self.xml_start_re = re.compile(r"<[a-zA-Z:_]")
-        self.current_path: list[str] = []
-        self.current_path_has_children = False
-        self.buffer = ""
-        self.xml_started = False
-
-    def parse(self, chunk: Union[str, BaseMessage]) -> Iterator[AddableDict]:
-        """Parse a chunk of text.
-
-        Args:
-            chunk: A chunk of text to parse. This can be a string or a BaseMessage.
-
-        Yields:
-            AddableDict: A dictionary representing the parsed XML element.
-
-        Raises:
-            xml.etree.ElementTree.ParseError: If the XML is not well-formed.
-        """
-        if isinstance(chunk, BaseMessage):
-            # extract text
-            chunk_content = chunk.content
-            if not isinstance(chunk_content, str):
-                # ignore non-string messages (e.g., function calls)
-                return
-            chunk = chunk_content
-        # add chunk to buffer of unprocessed text
-        self.buffer += chunk
-        # if xml string hasn't started yet, continue to next chunk
-        if not self.xml_started:
-            if match := self.xml_start_re.search(self.buffer):
-                # if xml string has started, remove all text before it
-                self.buffer = self.buffer[match.start() :]
-                self.xml_started = True
-            else:
-                return
-        # feed buffer to parser
-        self.pull_parser.feed(self.buffer)
-        self.buffer = ""
-        # yield all events
-        try:
-            for event, elem in self.pull_parser.read_events():
-                if event == "start":
-                    # update current path
-                    self.current_path.append(elem.tag)
-                    self.current_path_has_children = False
-                elif event == "end":
-                    # remove last element from current path
-                    #
-                    self.current_path.pop()
-                    # yield element
-                    if not self.current_path_has_children:
-                        yield nested_element(self.current_path, elem)
-                    # prevent yielding of parent element
-                    if self.current_path:
-                        self.current_path_has_children = True
-                    else:
-                        self.xml_started = False
-        except xml.etree.ElementTree.ParseError:
-            # This might be junk at the end of the XML input.
-            # Let's check whether the current path is empty.
-            if not self.current_path:
-                # If it is empty, we can ignore this error.
-                return
-            else:
-                raise
-
-    def close(self) -> None:
-        """Close the parser.
-
-        This should be called after all chunks have been parsed.
-
-        Raises:
-            xml.etree.ElementTree.ParseError: If the XML is not well-formed.
-        """
-        # Ignore ParseError. This will ignore any incomplete XML at the end of the input
-        with contextlib.suppress(xml.etree.ElementTree.ParseError):
-            self.pull_parser.close()
-
-
-class XMLOutputParser(BaseTransformOutputParser):
-    """Parse an output using xml format."""
-
-    tags: Optional[list[str]] = None
-    """Tags to tell the LLM to expect in the XML output.
-
-    Note this may not be perfect depending on the LLM implementation.
-
-    For example, with tags=["foo", "bar", "baz"]:
-            1. A well-formatted XML instance:
-                "<foo>\n   <bar>\n      <baz></baz>\n   </bar>\n</foo>"
-
-            2. A badly-formatted XML instance (missing closing tag for 'bar'):
-                "<foo>\n   <bar>\n   </foo>"
-
-            3. A badly-formatted XML instance (unexpected 'tag' element):
-                "<foo>\n   <tag>\n   </tag>\n</foo>"
-    """
-    encoding_matcher: re.Pattern = re.compile(
-        r"<([^>]*encoding[^>]*)>\n(.*)", re.MULTILINE | re.DOTALL
-    )
-    parser: Literal["defusedxml", "xml"] = "defusedxml"
-    """Parser to use for XML parsing. Can be either 'defusedxml' or 'xml'.
-
-    * 'defusedxml' is the default parser and is used to prevent XML vulnerabilities
-       present in some distributions of Python's standard library xml.
-       `defusedxml` is a wrapper around the standard library parser that
-       sets up the parser with secure defaults.
-    * 'xml' is the standard library parser.
-
-    Use `xml` only if you are sure that your distribution of the standard library
-    is not vulnerable to XML vulnerabilities.
-
-    Please review the following resources for more information:
-
-    * https://docs.python.org/3/library/xml.html#xml-vulnerabilities
-    * https://github.com/tiran/defusedxml
-
-    The standard library relies on libexpat for parsing XML:
-    https://github.com/libexpat/libexpat
-    """
-
-    def get_format_instructions(self) -> str:
-        """Return the format instructions for the XML output."""
-        return XML_FORMAT_INSTRUCTIONS.format(tags=self.tags)
-
-    def parse(self, text: str) -> dict[str, Union[str, list[Any]]]:
-        """Parse the output of an LLM call.
-
-        Args:
-            text: The output of an LLM call.
-
-        Returns:
-            A dictionary representing the parsed XML.
-
-        Raises:
-            OutputParserException: If the XML is not well-formed.
-            ImportError: If defusedxml is not installed and the defusedxml
-                parser is requested.
-        """
-        # Try to find XML string within triple backticks
-        # Imports are temporarily placed here to avoid issue with caching on CI
-        # likely if you're reading this you can move them to the top of the file
-        if self.parser == "defusedxml":
-            try:
-                from defusedxml import ElementTree  # type: ignore
-            except ImportError as e:
-                msg = (
-                    "defusedxml is not installed. "
-                    "Please install it to use the defusedxml parser."
-                    "You can install it with `pip install defusedxml`"
-                    "See https://github.com/tiran/defusedxml for more details"
-                )
-                raise ImportError(msg) from e
-            _et = ElementTree  # Use the defusedxml parser
-        else:
-            _et = ET  # Use the standard library parser
-
-        match = re.search(r"```(xml)?(.*)```", text, re.DOTALL)
-        if match is not None:
-            # If match found, use the content within the backticks
-            text = match.group(2)
-        encoding_match = self.encoding_matcher.search(text)
-        if encoding_match:
-            text = encoding_match.group(2)
-
-        text = text.strip()
-        try:
-            root = _et.fromstring(text)
-            return self._root_to_dict(root)
-        except _et.ParseError as e:
-            msg = f"Failed to parse XML format from completion {text}. Got: {e}"
-            raise OutputParserException(msg, llm_output=text) from e
-
-    def _transform(
-        self, input: Iterator[Union[str, BaseMessage]]
-    ) -> Iterator[AddableDict]:
-        streaming_parser = _StreamingParser(self.parser)
-        for chunk in input:
-            yield from streaming_parser.parse(chunk)
-        streaming_parser.close()
-
-    async def _atransform(
-        self, input: AsyncIterator[Union[str, BaseMessage]]
-    ) -> AsyncIterator[AddableDict]:
-        streaming_parser = _StreamingParser(self.parser)
-        async for chunk in input:
-            for output in streaming_parser.parse(chunk):
-                yield output
-        streaming_parser.close()
-
-    def _root_to_dict(self, root: ET.Element) -> dict[str, Union[str, list[Any]]]:
-        """Converts xml tree to python dictionary."""
-        if root.text and bool(re.search(r"\S", root.text)):
-            # If root text contains any non-whitespace character it
-            # returns {root.tag: root.text}
-            return {root.tag: root.text}
-        result: dict = {root.tag: []}
-        for child in root:
-            if len(child) == 0:
-                result[root.tag].append({child.tag: child.text})
-            else:
-                result[root.tag].append(self._root_to_dict(child))
-        return result
-
-    @property
-    def _type(self) -> str:
-        return "xml"
-
-
-def nested_element(path: list[str], elem: ET.Element) -> Any:
-    """Get nested element from path.
-
-    Args:
-        path: The path to the element.
-        elem: The element to extract.
-
-    Returns:
-        The nested element.
-    """
-    if len(path) == 0:
-        return AddableDict({elem.tag: elem.text})
-    else:
-        return AddableDict({path[0]: [nested_element(path[1:], elem)]})
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/outputs/__init__.py ./custom_langchain_core/outputs/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/outputs/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/outputs/__init__.py	1970-01-01 09:00:00
@@ -1,36 +0,0 @@
-"""**Output** classes are used to represent the output of a language model call
-and the output of a chat.
-
-The top container for information is the `LLMResult` object. `LLMResult` is used by
-both chat models and LLMs. This object contains the output of the language
-model and any additional information that the model provider wants to return.
-
-When invoking models via the standard runnable methods (e.g. invoke, batch, etc.):
-- Chat models will return `AIMessage` objects.
-- LLMs will return regular text strings.
-
-In addition, users can access the raw output of either LLMs or chat models via
-callbacks. The on_chat_model_end and on_llm_end callbacks will return an
-LLMResult object containing the generated outputs and any additional information
-returned by the model provider.
-
-In general, if information is already available
-in the AIMessage object, it is recommended to access it from there rather than
-from the `LLMResult` object.
-"""
-
-from langchain_core.outputs.chat_generation import ChatGeneration, ChatGenerationChunk
-from langchain_core.outputs.chat_result import ChatResult
-from langchain_core.outputs.generation import Generation, GenerationChunk
-from langchain_core.outputs.llm_result import LLMResult
-from langchain_core.outputs.run_info import RunInfo
-
-__all__ = [
-    "ChatGeneration",
-    "ChatGenerationChunk",
-    "ChatResult",
-    "Generation",
-    "GenerationChunk",
-    "LLMResult",
-    "RunInfo",
-]
Binary files .venv/lib/python3.12/site-packages/langchain_core/outputs/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/outputs/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/outputs/__pycache__/chat_generation.cpython-312.pyc and ./custom_langchain_core/outputs/__pycache__/chat_generation.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/outputs/__pycache__/chat_result.cpython-312.pyc and ./custom_langchain_core/outputs/__pycache__/chat_result.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/outputs/__pycache__/generation.cpython-312.pyc and ./custom_langchain_core/outputs/__pycache__/generation.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/outputs/__pycache__/llm_result.cpython-312.pyc and ./custom_langchain_core/outputs/__pycache__/llm_result.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/outputs/__pycache__/run_info.cpython-312.pyc and ./custom_langchain_core/outputs/__pycache__/run_info.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/outputs/chat_generation.py ./custom_langchain_core/outputs/chat_generation.py
--- .venv/lib/python3.12/site-packages/langchain_core/outputs/chat_generation.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/outputs/chat_generation.py	1970-01-01 09:00:00
@@ -1,121 +0,0 @@
-from __future__ import annotations
-
-from typing import Literal, Union
-
-from pydantic import model_validator
-from typing_extensions import Self
-
-from langchain_core.messages import BaseMessage, BaseMessageChunk
-from langchain_core.outputs.generation import Generation
-from langchain_core.utils._merge import merge_dicts
-
-
-class ChatGeneration(Generation):
-    """A single chat generation output.
-
-    A subclass of Generation that represents the response from a chat model
-    that generates chat messages.
-
-    The `message` attribute is a structured representation of the chat message.
-    Most of the time, the message will be of type `AIMessage`.
-
-    Users working with chat models will usually access information via either
-    `AIMessage` (returned from runnable interfaces) or `LLMResult` (available
-    via callbacks).
-    """
-
-    text: str = ""
-    """*SHOULD NOT BE SET DIRECTLY* The text contents of the output message."""
-    message: BaseMessage
-    """The message output by the chat model."""
-    # Override type to be ChatGeneration, ignore mypy error as this is intentional
-    type: Literal["ChatGeneration"] = "ChatGeneration"  # type: ignore[assignment]
-    """Type is used exclusively for serialization purposes."""
-
-    @model_validator(mode="after")
-    def set_text(self) -> Self:
-        """Set the text attribute to be the contents of the message.
-
-        Args:
-            values: The values of the object.
-
-        Returns:
-            The values of the object with the text attribute set.
-
-        Raises:
-            ValueError: If the message is not a string or a list.
-        """
-        try:
-            text = ""
-            if isinstance(self.message.content, str):
-                text = self.message.content
-            # HACK: Assumes text in content blocks in OpenAI format.
-            # Uses first text block.
-            elif isinstance(self.message.content, list):
-                for block in self.message.content:
-                    if isinstance(block, str):
-                        text = block
-                        break
-                    elif isinstance(block, dict) and "text" in block:
-                        text = block["text"]
-                        break
-                    else:
-                        pass
-            else:
-                pass
-            self.text = text
-        except (KeyError, AttributeError) as e:
-            msg = "Error while initializing ChatGeneration"
-            raise ValueError(msg) from e
-        return self
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "output"]
-
-
-class ChatGenerationChunk(ChatGeneration):
-    """ChatGeneration chunk, which can be concatenated with other
-    ChatGeneration chunks.
-    """
-
-    message: BaseMessageChunk
-    """The message chunk output by the chat model."""
-    # Override type to be ChatGeneration, ignore mypy error as this is intentional
-    type: Literal["ChatGenerationChunk"] = "ChatGenerationChunk"  # type: ignore[assignment]
-    """Type is used exclusively for serialization purposes."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "output"]
-
-    def __add__(
-        self, other: Union[ChatGenerationChunk, list[ChatGenerationChunk]]
-    ) -> ChatGenerationChunk:
-        if isinstance(other, ChatGenerationChunk):
-            generation_info = merge_dicts(
-                self.generation_info or {},
-                other.generation_info or {},
-            )
-            return ChatGenerationChunk(
-                message=self.message + other.message,
-                generation_info=generation_info or None,
-            )
-        elif isinstance(other, list) and all(
-            isinstance(x, ChatGenerationChunk) for x in other
-        ):
-            generation_info = merge_dicts(
-                self.generation_info or {},
-                *[chunk.generation_info for chunk in other if chunk.generation_info],
-            )
-            return ChatGenerationChunk(
-                message=self.message + [chunk.message for chunk in other],
-                generation_info=generation_info or None,
-            )
-        else:
-            msg = (
-                f"unsupported operand type(s) for +: '{type(self)}' and '{type(other)}'"
-            )
-            raise TypeError(msg)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/outputs/chat_result.py ./custom_langchain_core/outputs/chat_result.py
--- .venv/lib/python3.12/site-packages/langchain_core/outputs/chat_result.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/outputs/chat_result.py	1970-01-01 09:00:00
@@ -1,36 +0,0 @@
-from typing import Optional
-
-from pydantic import BaseModel
-
-from langchain_core.outputs.chat_generation import ChatGeneration
-
-
-class ChatResult(BaseModel):
-    """Use to represent the result of a chat model call with a single prompt.
-
-    This container is used internally by some implementations of chat model,
-    it will eventually be mapped to a more general `LLMResult` object, and
-    then projected into an `AIMessage` object.
-
-    LangChain users working with chat models will usually access information via
-    `AIMessage` (returned from runnable interfaces) or `LLMResult` (available
-    via callbacks). Please refer the `AIMessage` and `LLMResult` schema documentation
-    for more information.
-    """
-
-    generations: list[ChatGeneration]
-    """List of the chat generations.
-
-    Generations is a list to allow for multiple candidate generations for a single
-    input prompt.
-    """
-    llm_output: Optional[dict] = None
-    """For arbitrary LLM provider specific output.
-
-    This dictionary is a free-form dictionary that can contain any information that the
-    provider wants to return. It is not standardized and is provider-specific.
-
-    Users should generally avoid relying on this field and instead rely on
-    accessing relevant information from standardized fields present in
-    AIMessage.
-    """
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/outputs/generation.py ./custom_langchain_core/outputs/generation.py
--- .venv/lib/python3.12/site-packages/langchain_core/outputs/generation.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/outputs/generation.py	1970-01-01 09:00:00
@@ -1,70 +0,0 @@
-from __future__ import annotations
-
-from typing import Any, Literal, Optional
-
-from langchain_core.load import Serializable
-from langchain_core.utils._merge import merge_dicts
-
-
-class Generation(Serializable):
-    """A single text generation output.
-
-    Generation represents the response from an "old-fashioned" LLM that
-    generates regular text (not chat messages).
-
-    This model is used internally by chat model and will eventually
-    be mapped to a more general `LLMResult` object, and then projected into
-    an `AIMessage` object.
-
-    LangChain users working with chat models will usually access information via
-    `AIMessage` (returned from runnable interfaces) or `LLMResult` (available
-    via callbacks). Please refer the `AIMessage` and `LLMResult` schema documentation
-    for more information.
-    """
-
-    text: str
-    """Generated text output."""
-
-    generation_info: Optional[dict[str, Any]] = None
-    """Raw response from the provider.
-
-    May include things like the reason for finishing or token log probabilities.
-    """
-    type: Literal["Generation"] = "Generation"
-    """Type is used exclusively for serialization purposes.
-    Set to "Generation" for this class."""
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Return whether this class is serializable."""
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "output"]
-
-
-class GenerationChunk(Generation):
-    """Generation chunk, which can be concatenated with other Generation chunks."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "output"]
-
-    def __add__(self, other: GenerationChunk) -> GenerationChunk:
-        if isinstance(other, GenerationChunk):
-            generation_info = merge_dicts(
-                self.generation_info or {},
-                other.generation_info or {},
-            )
-            return GenerationChunk(
-                text=self.text + other.text,
-                generation_info=generation_info or None,
-            )
-        else:
-            msg = (
-                f"unsupported operand type(s) for +: '{type(self)}' and '{type(other)}'"
-            )
-            raise TypeError(msg)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/outputs/llm_result.py ./custom_langchain_core/outputs/llm_result.py
--- .venv/lib/python3.12/site-packages/langchain_core/outputs/llm_result.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/outputs/llm_result.py	1970-01-01 09:00:00
@@ -1,97 +0,0 @@
-from __future__ import annotations
-
-from copy import deepcopy
-from typing import Literal, Optional, Union
-
-from pydantic import BaseModel
-
-from langchain_core.outputs.chat_generation import ChatGeneration, ChatGenerationChunk
-from langchain_core.outputs.generation import Generation, GenerationChunk
-from langchain_core.outputs.run_info import RunInfo
-
-
-class LLMResult(BaseModel):
-    """A container for results of an LLM call.
-
-    Both chat models and LLMs generate an LLMResult object. This object contains
-    the generated outputs and any additional information that the model provider
-    wants to return.
-    """
-
-    generations: list[
-        list[Union[Generation, ChatGeneration, GenerationChunk, ChatGenerationChunk]]
-    ]
-    """Generated outputs.
-
-    The first dimension of the list represents completions for different input
-    prompts.
-
-    The second dimension of the list represents different candidate generations
-    for a given prompt.
-
-    When returned from an LLM the type is List[List[Generation]].
-    When returned from a chat model the type is List[List[ChatGeneration]].
-
-    ChatGeneration is a subclass of Generation that has a field for a structured
-    chat message.
-    """
-    llm_output: Optional[dict] = None
-    """For arbitrary LLM provider specific output.
-
-    This dictionary is a free-form dictionary that can contain any information that the
-    provider wants to return. It is not standardized and is provider-specific.
-
-    Users should generally avoid relying on this field and instead rely on
-    accessing relevant information from standardized fields present in
-    AIMessage.
-    """
-    run: Optional[list[RunInfo]] = None
-    """List of metadata info for model call for each input."""
-
-    type: Literal["LLMResult"] = "LLMResult"  # type: ignore[assignment]
-    """Type is used exclusively for serialization purposes."""
-
-    def flatten(self) -> list[LLMResult]:
-        """Flatten generations into a single list.
-
-        Unpack List[List[Generation]] -> List[LLMResult] where each returned LLMResult
-        contains only a single Generation. If token usage information is available,
-        it is kept only for the LLMResult corresponding to the top-choice
-        Generation, to avoid over-counting of token usage downstream.
-
-        Returns:
-            List of LLMResults where each returned LLMResult contains a single
-                Generation.
-        """
-        llm_results = []
-        for i, gen_list in enumerate(self.generations):
-            # Avoid double counting tokens in OpenAICallback
-            if i == 0:
-                llm_results.append(
-                    LLMResult(
-                        generations=[gen_list],
-                        llm_output=self.llm_output,
-                    )
-                )
-            else:
-                if self.llm_output is not None:
-                    llm_output = deepcopy(self.llm_output)
-                    llm_output["token_usage"] = {}
-                else:
-                    llm_output = None
-                llm_results.append(
-                    LLMResult(
-                        generations=[gen_list],
-                        llm_output=llm_output,
-                    )
-                )
-        return llm_results
-
-    def __eq__(self, other: object) -> bool:
-        """Check for LLMResult equality by ignoring any metadata related to runs."""
-        if not isinstance(other, LLMResult):
-            return NotImplemented
-        return (
-            self.generations == other.generations
-            and self.llm_output == other.llm_output
-        )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/outputs/run_info.py ./custom_langchain_core/outputs/run_info.py
--- .venv/lib/python3.12/site-packages/langchain_core/outputs/run_info.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/outputs/run_info.py	1970-01-01 09:00:00
@@ -1,20 +0,0 @@
-from __future__ import annotations
-
-from uuid import UUID
-
-from pydantic import BaseModel
-
-
-class RunInfo(BaseModel):
-    """Class that contains metadata for a single execution of a Chain or model.
-
-    Defined for backwards compatibility with older versions of langchain_core.
-
-    This model will likely be deprecated in the future.
-
-    Users can acquire the run_id information from callbacks or via run_id
-    information present in the astream_event API (depending on the use case).
-    """
-
-    run_id: UUID
-    """A unique identifier for the model or chain run."""
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/prompt_values.py ./custom_langchain_core/prompt_values.py
--- .venv/lib/python3.12/site-packages/langchain_core/prompt_values.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/prompt_values.py	1970-01-01 09:00:00
@@ -1,146 +0,0 @@
-"""**Prompt values** for language model prompts.
-
-Prompt values are used to represent different pieces of prompts.
-They can be used to represent text, images, or chat message pieces.
-"""
-
-from __future__ import annotations
-
-from abc import ABC, abstractmethod
-from collections.abc import Sequence
-from typing import Literal, cast
-
-from typing_extensions import TypedDict
-
-from langchain_core.load.serializable import Serializable
-from langchain_core.messages import (
-    AnyMessage,
-    BaseMessage,
-    HumanMessage,
-    get_buffer_string,
-)
-
-
-class PromptValue(Serializable, ABC):
-    """Base abstract class for inputs to any language model.
-
-    PromptValues can be converted to both LLM (pure text-generation) inputs and
-    ChatModel inputs.
-    """
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Return whether this class is serializable. Defaults to True."""
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        This is used to determine the namespace of the object when serializing.
-        Defaults to ["langchain", "schema", "prompt"].
-        """
-        return ["langchain", "schema", "prompt"]
-
-    @abstractmethod
-    def to_string(self) -> str:
-        """Return prompt value as string."""
-
-    @abstractmethod
-    def to_messages(self) -> list[BaseMessage]:
-        """Return prompt as a list of Messages."""
-
-
-class StringPromptValue(PromptValue):
-    """String prompt value."""
-
-    text: str
-    """Prompt text."""
-    type: Literal["StringPromptValue"] = "StringPromptValue"
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        This is used to determine the namespace of the object when serializing.
-        Defaults to ["langchain", "prompts", "base"].
-        """
-        return ["langchain", "prompts", "base"]
-
-    def to_string(self) -> str:
-        """Return prompt as string."""
-        return self.text
-
-    def to_messages(self) -> list[BaseMessage]:
-        """Return prompt as messages."""
-        return [HumanMessage(content=self.text)]
-
-
-class ChatPromptValue(PromptValue):
-    """Chat prompt value.
-
-    A type of a prompt value that is built from messages.
-    """
-
-    messages: Sequence[BaseMessage]
-    """List of messages."""
-
-    def to_string(self) -> str:
-        """Return prompt as string."""
-        return get_buffer_string(self.messages)
-
-    def to_messages(self) -> list[BaseMessage]:
-        """Return prompt as a list of messages."""
-        return list(self.messages)
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        This is used to determine the namespace of the object when serializing.
-        Defaults to ["langchain", "prompts", "chat"].
-        """
-        return ["langchain", "prompts", "chat"]
-
-
-class ImageURL(TypedDict, total=False):
-    """Image URL."""
-
-    detail: Literal["auto", "low", "high"]
-    """Specifies the detail level of the image. Defaults to "auto".
-    Can be "auto", "low", or "high"."""
-
-    url: str
-    """Either a URL of the image or the base64 encoded image data."""
-
-
-class ImagePromptValue(PromptValue):
-    """Image prompt value."""
-
-    image_url: ImageURL
-    """Image URL."""
-    type: Literal["ImagePromptValue"] = "ImagePromptValue"
-
-    def to_string(self) -> str:
-        """Return prompt (image URL) as string."""
-        return self.image_url["url"]
-
-    def to_messages(self) -> list[BaseMessage]:
-        """Return prompt (image URL) as messages."""
-        return [HumanMessage(content=[cast(dict, self.image_url)])]
-
-
-class ChatPromptValueConcrete(ChatPromptValue):
-    """Chat prompt value which explicitly lists out the message types it accepts.
-    For use in external schemas.
-    """
-
-    messages: Sequence[AnyMessage]
-    """Sequence of messages."""
-
-    type: Literal["ChatPromptValueConcrete"] = "ChatPromptValueConcrete"
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        This is used to determine the namespace of the object when serializing.
-        Defaults to ["langchain", "prompts", "chat"].
-        """
-        return ["langchain", "prompts", "chat"]
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/prompts/__init__.py ./custom_langchain_core/prompts/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/prompts/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/prompts/__init__.py	1970-01-01 09:00:00
@@ -1,80 +0,0 @@
-"""**Prompt** is the input to the model.
-
-Prompt is often constructed
-from multiple components and prompt values. Prompt classes and functions make constructing
- and working with prompts easy.
-
-**Class hierarchy:**
-
-.. code-block::
-
-    BasePromptTemplate --> PipelinePromptTemplate
-                           StringPromptTemplate --> PromptTemplate
-                                                    FewShotPromptTemplate
-                                                    FewShotPromptWithTemplates
-                           BaseChatPromptTemplate --> AutoGPTPrompt
-                                                      ChatPromptTemplate --> AgentScratchPadChatPromptTemplate
-
-
-
-    BaseMessagePromptTemplate --> MessagesPlaceholder
-                                  BaseStringMessagePromptTemplate --> ChatMessagePromptTemplate
-                                                                      HumanMessagePromptTemplate
-                                                                      AIMessagePromptTemplate
-                                                                      SystemMessagePromptTemplate
-
-"""  # noqa: E501
-
-from langchain_core.prompts.base import (
-    BasePromptTemplate,
-    aformat_document,
-    format_document,
-)
-from langchain_core.prompts.chat import (
-    AIMessagePromptTemplate,
-    BaseChatPromptTemplate,
-    ChatMessagePromptTemplate,
-    ChatPromptTemplate,
-    HumanMessagePromptTemplate,
-    MessagesPlaceholder,
-    SystemMessagePromptTemplate,
-)
-from langchain_core.prompts.few_shot import (
-    FewShotChatMessagePromptTemplate,
-    FewShotPromptTemplate,
-)
-from langchain_core.prompts.few_shot_with_templates import FewShotPromptWithTemplates
-from langchain_core.prompts.loading import load_prompt
-from langchain_core.prompts.pipeline import PipelinePromptTemplate
-from langchain_core.prompts.prompt import PromptTemplate
-from langchain_core.prompts.string import (
-    StringPromptTemplate,
-    check_valid_template,
-    get_template_variables,
-    jinja2_formatter,
-    validate_jinja2,
-)
-
-__all__ = [
-    "AIMessagePromptTemplate",
-    "BaseChatPromptTemplate",
-    "BasePromptTemplate",
-    "ChatMessagePromptTemplate",
-    "ChatPromptTemplate",
-    "FewShotPromptTemplate",
-    "FewShotPromptWithTemplates",
-    "FewShotChatMessagePromptTemplate",
-    "HumanMessagePromptTemplate",
-    "MessagesPlaceholder",
-    "PipelinePromptTemplate",
-    "PromptTemplate",
-    "StringPromptTemplate",
-    "SystemMessagePromptTemplate",
-    "load_prompt",
-    "format_document",
-    "aformat_document",
-    "check_valid_template",
-    "get_template_variables",
-    "jinja2_formatter",
-    "validate_jinja2",
-]
Binary files .venv/lib/python3.12/site-packages/langchain_core/prompts/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/prompts/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/prompts/__pycache__/base.cpython-312.pyc and ./custom_langchain_core/prompts/__pycache__/base.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/prompts/__pycache__/chat.cpython-312.pyc and ./custom_langchain_core/prompts/__pycache__/chat.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/prompts/__pycache__/few_shot.cpython-312.pyc and ./custom_langchain_core/prompts/__pycache__/few_shot.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/prompts/__pycache__/few_shot_with_templates.cpython-312.pyc and ./custom_langchain_core/prompts/__pycache__/few_shot_with_templates.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/prompts/__pycache__/image.cpython-312.pyc and ./custom_langchain_core/prompts/__pycache__/image.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/prompts/__pycache__/loading.cpython-312.pyc and ./custom_langchain_core/prompts/__pycache__/loading.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/prompts/__pycache__/pipeline.cpython-312.pyc and ./custom_langchain_core/prompts/__pycache__/pipeline.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/prompts/__pycache__/prompt.cpython-312.pyc and ./custom_langchain_core/prompts/__pycache__/prompt.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/prompts/__pycache__/string.cpython-312.pyc and ./custom_langchain_core/prompts/__pycache__/string.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/prompts/__pycache__/structured.cpython-312.pyc and ./custom_langchain_core/prompts/__pycache__/structured.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/prompts/base.py ./custom_langchain_core/prompts/base.py
--- .venv/lib/python3.12/site-packages/langchain_core/prompts/base.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/prompts/base.py	1970-01-01 09:00:00
@@ -1,465 +0,0 @@
-from __future__ import annotations
-
-import contextlib
-import json
-import typing
-from abc import ABC, abstractmethod
-from collections.abc import Mapping
-from functools import cached_property
-from pathlib import Path
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    Generic,
-    Optional,
-    TypeVar,
-    Union,
-)
-
-import yaml
-from pydantic import BaseModel, ConfigDict, Field, model_validator
-from typing_extensions import Self, override
-
-from langchain_core.exceptions import ErrorCode, create_message
-from langchain_core.load import dumpd
-from langchain_core.output_parsers.base import BaseOutputParser
-from langchain_core.prompt_values import (
-    ChatPromptValueConcrete,
-    PromptValue,
-    StringPromptValue,
-)
-from langchain_core.runnables import RunnableConfig, RunnableSerializable
-from langchain_core.runnables.config import ensure_config
-from langchain_core.utils.pydantic import create_model_v2
-
-if TYPE_CHECKING:
-    from langchain_core.documents import Document
-
-
-FormatOutputType = TypeVar("FormatOutputType")
-
-
-class BasePromptTemplate(
-    RunnableSerializable[dict, PromptValue], Generic[FormatOutputType], ABC
-):
-    """Base class for all prompt templates, returning a prompt."""
-
-    input_variables: list[str]
-    """A list of the names of the variables whose values are required as inputs to the
-    prompt."""
-    optional_variables: list[str] = Field(default=[])
-    """optional_variables: A list of the names of the variables for placeholder
-       or MessagePlaceholder that are optional. These variables are auto inferred
-       from the prompt and user need not provide them."""
-    input_types: typing.Dict[str, Any] = Field(default_factory=dict, exclude=True)  # noqa: UP006
-    """A dictionary of the types of the variables the prompt template expects.
-    If not provided, all variables are assumed to be strings."""
-    output_parser: Optional[BaseOutputParser] = None
-    """How to parse the output of calling an LLM on this formatted prompt."""
-    partial_variables: Mapping[str, Any] = Field(default_factory=dict)
-    """A dictionary of the partial variables the prompt template carries.
-
-    Partial variables populate the template so that you don't need to
-    pass them in every time you call the prompt."""
-    metadata: Optional[typing.Dict[str, Any]] = None  # noqa: UP006
-    """Metadata to be used for tracing."""
-    tags: Optional[list[str]] = None
-    """Tags to be used for tracing."""
-
-    @model_validator(mode="after")
-    def validate_variable_names(self) -> Self:
-        """Validate variable names do not include restricted names."""
-        if "stop" in self.input_variables:
-            msg = (
-                "Cannot have an input variable named 'stop', as it is used internally,"
-                " please rename."
-            )
-            raise ValueError(
-                create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)
-            )
-        if "stop" in self.partial_variables:
-            msg = (
-                "Cannot have an partial variable named 'stop', as it is used "
-                "internally, please rename."
-            )
-            raise ValueError(
-                create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)
-            )
-
-        overall = set(self.input_variables).intersection(self.partial_variables)
-        if overall:
-            msg = f"Found overlapping input and partial variables: {overall}"
-            raise ValueError(
-                create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)
-            )
-        return self
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-        Returns ["langchain", "schema", "prompt_template"].
-        """
-        return ["langchain", "schema", "prompt_template"]
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Return whether this class is serializable.
-        Returns True.
-        """
-        return True
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    @cached_property
-    def _serialized(self) -> dict[str, Any]:
-        return dumpd(self)
-
-    @property
-    @override
-    def OutputType(self) -> Any:
-        """Return the output type of the prompt."""
-        return Union[StringPromptValue, ChatPromptValueConcrete]
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        """Get the input schema for the prompt.
-
-        Args:
-            config: RunnableConfig, configuration for the prompt.
-
-        Returns:
-            Type[BaseModel]: The input schema for the prompt.
-        """
-        # This is correct, but pydantic typings/mypy don't think so.
-        required_input_variables = {
-            k: (self.input_types.get(k, str), ...) for k in self.input_variables
-        }
-        optional_input_variables = {
-            k: (self.input_types.get(k, str), None) for k in self.optional_variables
-        }
-        return create_model_v2(
-            "PromptInput",
-            field_definitions={**required_input_variables, **optional_input_variables},
-        )
-
-    def _validate_input(self, inner_input: Any) -> dict:
-        if not isinstance(inner_input, dict):
-            if len(self.input_variables) == 1:
-                var_name = self.input_variables[0]
-                inner_input = {var_name: inner_input}
-
-            else:
-                msg = (
-                    f"Expected mapping type as input to {self.__class__.__name__}. "
-                    f"Received {type(inner_input)}."
-                )
-                raise TypeError(
-                    create_message(
-                        message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT
-                    )
-                )
-        missing = set(self.input_variables).difference(inner_input)
-        if missing:
-            msg = (
-                f"Input to {self.__class__.__name__} is missing variables {missing}. "
-                f" Expected: {self.input_variables}"
-                f" Received: {list(inner_input.keys())}"
-            )
-            example_key = missing.pop()
-            msg += (
-                f"\nNote: if you intended {{{example_key}}} to be part of the string"
-                " and not a variable, please escape it with double curly braces like: "
-                f"'{{{{{example_key}}}}}'."
-            )
-            raise KeyError(
-                create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)
-            )
-        return inner_input
-
-    def _format_prompt_with_error_handling(self, inner_input: dict) -> PromptValue:
-        _inner_input = self._validate_input(inner_input)
-        return self.format_prompt(**_inner_input)
-
-    async def _aformat_prompt_with_error_handling(
-        self, inner_input: dict
-    ) -> PromptValue:
-        _inner_input = self._validate_input(inner_input)
-        return await self.aformat_prompt(**_inner_input)
-
-    def invoke(
-        self, input: dict, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> PromptValue:
-        """Invoke the prompt.
-
-        Args:
-            input: Dict, input to the prompt.
-            config: RunnableConfig, configuration for the prompt.
-
-        Returns:
-            PromptValue: The output of the prompt.
-        """
-        config = ensure_config(config)
-        if self.metadata:
-            config["metadata"] = {**config["metadata"], **self.metadata}
-        if self.tags:
-            config["tags"] = config["tags"] + self.tags
-        return self._call_with_config(
-            self._format_prompt_with_error_handling,
-            input,
-            config,
-            run_type="prompt",
-            serialized=self._serialized,
-        )
-
-    async def ainvoke(
-        self, input: dict, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> PromptValue:
-        """Async invoke the prompt.
-
-        Args:
-            input: Dict, input to the prompt.
-            config: RunnableConfig, configuration for the prompt.
-
-        Returns:
-            PromptValue: The output of the prompt.
-        """
-        config = ensure_config(config)
-        if self.metadata:
-            config["metadata"].update(self.metadata)
-        if self.tags:
-            config["tags"].extend(self.tags)
-        return await self._acall_with_config(
-            self._aformat_prompt_with_error_handling,
-            input,
-            config,
-            run_type="prompt",
-            serialized=self._serialized,
-        )
-
-    @abstractmethod
-    def format_prompt(self, **kwargs: Any) -> PromptValue:
-        """Create Prompt Value.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            PromptValue: The output of the prompt.
-        """
-
-    async def aformat_prompt(self, **kwargs: Any) -> PromptValue:
-        """Async create Prompt Value.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            PromptValue: The output of the prompt.
-        """
-        return self.format_prompt(**kwargs)
-
-    def partial(self, **kwargs: Union[str, Callable[[], str]]) -> BasePromptTemplate:
-        """Return a partial of the prompt template.
-
-        Args:
-            kwargs: Union[str, Callable[[], str]], partial variables to set.
-
-        Returns:
-            BasePromptTemplate: A partial of the prompt template.
-        """
-        prompt_dict = self.__dict__.copy()
-        prompt_dict["input_variables"] = list(
-            set(self.input_variables).difference(kwargs)
-        )
-        prompt_dict["partial_variables"] = {**self.partial_variables, **kwargs}
-        return type(self)(**prompt_dict)
-
-    def _merge_partial_and_user_variables(self, **kwargs: Any) -> dict[str, Any]:
-        # Get partial params:
-        partial_kwargs = {
-            k: v if not callable(v) else v() for k, v in self.partial_variables.items()
-        }
-        return {**partial_kwargs, **kwargs}
-
-    @abstractmethod
-    def format(self, **kwargs: Any) -> FormatOutputType:
-        """Format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-
-        Example:
-
-        .. code-block:: python
-
-            prompt.format(variable1="foo")
-        """
-
-    async def aformat(self, **kwargs: Any) -> FormatOutputType:
-        """Async format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-
-        Example:
-
-        .. code-block:: python
-
-            await prompt.aformat(variable1="foo")
-        """
-        return self.format(**kwargs)
-
-    @property
-    def _prompt_type(self) -> str:
-        """Return the prompt type key."""
-        raise NotImplementedError
-
-    def dict(self, **kwargs: Any) -> dict:
-        """Return dictionary representation of prompt.
-
-        Args:
-            kwargs: Any additional arguments to pass to the dictionary.
-
-        Returns:
-            Dict: Dictionary representation of the prompt.
-
-        Raises:
-            NotImplementedError: If the prompt type is not implemented.
-        """
-        prompt_dict = super().model_dump(**kwargs)
-        with contextlib.suppress(NotImplementedError):
-            prompt_dict["_type"] = self._prompt_type
-        return prompt_dict
-
-    def save(self, file_path: Union[Path, str]) -> None:
-        """Save the prompt.
-
-        Args:
-            file_path: Path to directory to save prompt to.
-
-        Raises:
-            ValueError: If the prompt has partial variables.
-            ValueError: If the file path is not json or yaml.
-            NotImplementedError: If the prompt type is not implemented.
-
-        Example:
-        .. code-block:: python
-
-            prompt.save(file_path="path/prompt.yaml")
-        """
-        if self.partial_variables:
-            msg = "Cannot save prompt with partial variables."
-            raise ValueError(msg)
-
-        # Fetch dictionary to save
-        prompt_dict = self.dict()
-        if "_type" not in prompt_dict:
-            msg = f"Prompt {self} does not support saving."
-            raise NotImplementedError(msg)
-
-        # Convert file to Path object.
-        save_path = Path(file_path) if isinstance(file_path, str) else file_path
-
-        directory_path = save_path.parent
-        directory_path.mkdir(parents=True, exist_ok=True)
-
-        if save_path.suffix == ".json":
-            with open(file_path, "w") as f:
-                json.dump(prompt_dict, f, indent=4)
-        elif save_path.suffix.endswith((".yaml", ".yml")):
-            with open(file_path, "w") as f:
-                yaml.dump(prompt_dict, f, default_flow_style=False)
-        else:
-            msg = f"{save_path} must be json or yaml"
-            raise ValueError(msg)
-
-
-def _get_document_info(doc: Document, prompt: BasePromptTemplate[str]) -> dict:
-    base_info = {"page_content": doc.page_content, **doc.metadata}
-    missing_metadata = set(prompt.input_variables).difference(base_info)
-    if len(missing_metadata) > 0:
-        required_metadata = [
-            iv for iv in prompt.input_variables if iv != "page_content"
-        ]
-        msg = (
-            f"Document prompt requires documents to have metadata variables: "
-            f"{required_metadata}. Received document with missing metadata: "
-            f"{list(missing_metadata)}."
-        )
-        raise ValueError(
-            create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)
-        )
-    return {k: base_info[k] for k in prompt.input_variables}
-
-
-def format_document(doc: Document, prompt: BasePromptTemplate[str]) -> str:
-    """Format a document into a string based on a prompt template.
-
-    First, this pulls information from the document from two sources:
-
-    1. page_content:
-        This takes the information from the `document.page_content`
-        and assigns it to a variable named `page_content`.
-    2. metadata:
-        This takes information from `document.metadata` and assigns
-        it to variables of the same name.
-
-    Those variables are then passed into the `prompt` to produce a formatted string.
-
-    Args:
-        doc: Document, the page_content and metadata will be used to create
-            the final string.
-        prompt: BasePromptTemplate, will be used to format the page_content
-            and metadata into the final string.
-
-    Returns:
-        string of the document formatted.
-
-    Example:
-        .. code-block:: python
-
-            from langchain_core.documents import Document
-            from langchain_core.prompts import PromptTemplate
-
-            doc = Document(page_content="This is a joke", metadata={"page": "1"})
-            prompt = PromptTemplate.from_template("Page {page}: {page_content}")
-            format_document(doc, prompt)
-            >>> "Page 1: This is a joke"
-    """
-    return prompt.format(**_get_document_info(doc, prompt))
-
-
-async def aformat_document(doc: Document, prompt: BasePromptTemplate[str]) -> str:
-    """Async format a document into a string based on a prompt template.
-
-    First, this pulls information from the document from two sources:
-
-    1. page_content:
-        This takes the information from the `document.page_content`
-        and assigns it to a variable named `page_content`.
-    2. metadata:
-        This takes information from `document.metadata` and assigns
-        it to variables of the same name.
-
-    Those variables are then passed into the `prompt` to produce a formatted string.
-
-    Args:
-        doc: Document, the page_content and metadata will be used to create
-            the final string.
-        prompt: BasePromptTemplate, will be used to format the page_content
-            and metadata into the final string.
-
-    Returns:
-        string of the document formatted.
-    """
-    return await prompt.aformat(**_get_document_info(doc, prompt))
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/prompts/chat.py ./custom_langchain_core/prompts/chat.py
--- .venv/lib/python3.12/site-packages/langchain_core/prompts/chat.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/prompts/chat.py	1970-01-01 09:00:00
@@ -1,1492 +0,0 @@
-"""Chat prompt template."""
-
-from __future__ import annotations
-
-from abc import ABC, abstractmethod
-from collections.abc import Sequence
-from pathlib import Path
-from typing import (
-    Annotated,
-    Any,
-    Optional,
-    TypedDict,
-    TypeVar,
-    Union,
-    cast,
-    overload,
-)
-
-from pydantic import (
-    Field,
-    PositiveInt,
-    SkipValidation,
-    model_validator,
-)
-
-from langchain_core._api import deprecated
-from langchain_core.load import Serializable
-from langchain_core.messages import (
-    AIMessage,
-    AnyMessage,
-    BaseMessage,
-    ChatMessage,
-    HumanMessage,
-    SystemMessage,
-    convert_to_messages,
-)
-from langchain_core.messages.base import get_msg_title_repr
-from langchain_core.prompt_values import ChatPromptValue, ImageURL, PromptValue
-from langchain_core.prompts.base import BasePromptTemplate
-from langchain_core.prompts.image import ImagePromptTemplate
-from langchain_core.prompts.prompt import PromptTemplate
-from langchain_core.prompts.string import (
-    PromptTemplateFormat,
-    StringPromptTemplate,
-    get_template_variables,
-)
-from langchain_core.utils import get_colored_text
-from langchain_core.utils.interactive_env import is_interactive_env
-
-
-class BaseMessagePromptTemplate(Serializable, ABC):
-    """Base class for message prompt templates."""
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Return whether or not the class is serializable.
-        Returns: True.
-        """
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "chat"]
-
-    @abstractmethod
-    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:
-        """Format messages from kwargs. Should return a list of BaseMessages.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            List of BaseMessages.
-        """
-
-    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:
-        """Async format messages from kwargs.
-        Should return a list of BaseMessages.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            List of BaseMessages.
-        """
-        return self.format_messages(**kwargs)
-
-    @property
-    @abstractmethod
-    def input_variables(self) -> list[str]:
-        """Input variables for this prompt template.
-
-        Returns:
-            List of input variables.
-        """
-
-    def pretty_repr(self, html: bool = False) -> str:
-        """Human-readable representation.
-
-        Args:
-            html: Whether to format as HTML. Defaults to False.
-
-        Returns:
-            Human-readable representation.
-        """
-        raise NotImplementedError
-
-    def pretty_print(self) -> None:
-        """Print a human-readable representation."""
-        print(self.pretty_repr(html=is_interactive_env()))  # noqa: T201
-
-    def __add__(self, other: Any) -> ChatPromptTemplate:
-        """Combine two prompt templates.
-
-        Args:
-            other: Another prompt template.
-
-        Returns:
-            Combined prompt template.
-        """
-        prompt = ChatPromptTemplate(messages=[self])  # type: ignore[call-arg]
-        return prompt + other
-
-
-class MessagesPlaceholder(BaseMessagePromptTemplate):
-    """Prompt template that assumes variable is already list of messages.
-
-    A placeholder which can be used to pass in a list of messages.
-
-    Direct usage:
-
-        .. code-block:: python
-
-            from langchain_core.prompts import MessagesPlaceholder
-
-            prompt = MessagesPlaceholder("history")
-            prompt.format_messages() # raises KeyError
-
-            prompt = MessagesPlaceholder("history", optional=True)
-            prompt.format_messages() # returns empty list []
-
-            prompt.format_messages(
-                history=[
-                    ("system", "You are an AI assistant."),
-                    ("human", "Hello!"),
-                ]
-            )
-            # -> [
-            #     SystemMessage(content="You are an AI assistant."),
-            #     HumanMessage(content="Hello!"),
-            # ]
-
-    Building a prompt with chat history:
-
-        .. code-block:: python
-
-            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
-
-            prompt = ChatPromptTemplate.from_messages(
-                [
-                    ("system", "You are a helpful assistant."),
-                    MessagesPlaceholder("history"),
-                    ("human", "{question}")
-                ]
-            )
-            prompt.invoke(
-               {
-                   "history": [("human", "what's 5 + 2"), ("ai", "5 + 2 is 7")],
-                   "question": "now multiply that by 4"
-               }
-            )
-            # -> ChatPromptValue(messages=[
-            #     SystemMessage(content="You are a helpful assistant."),
-            #     HumanMessage(content="what's 5 + 2"),
-            #     AIMessage(content="5 + 2 is 7"),
-            #     HumanMessage(content="now multiply that by 4"),
-            # ])
-
-    Limiting the number of messages:
-
-        .. code-block:: python
-
-            from langchain_core.prompts import MessagesPlaceholder
-
-            prompt = MessagesPlaceholder("history", n_messages=1)
-
-            prompt.format_messages(
-                history=[
-                    ("system", "You are an AI assistant."),
-                    ("human", "Hello!"),
-                ]
-            )
-            # -> [
-            #     HumanMessage(content="Hello!"),
-            # ]
-    """
-
-    variable_name: str
-    """Name of variable to use as messages."""
-
-    optional: bool = False
-    """If True format_messages can be called with no arguments and will return an empty
-        list. If False then a named argument with name `variable_name` must be passed
-        in, even if the value is an empty list."""
-
-    n_messages: Optional[PositiveInt] = None
-    """Maximum number of messages to include. If None, then will include all.
-    Defaults to None."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "chat"]
-
-    def __init__(
-        self, variable_name: str, *, optional: bool = False, **kwargs: Any
-    ) -> None:
-        # mypy can't detect the init which is defined in the parent class
-        # b/c these are BaseModel classes.
-        super().__init__(  # type: ignore
-            variable_name=variable_name, optional=optional, **kwargs
-        )
-
-    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:
-        """Format messages from kwargs.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            List of BaseMessage.
-
-        Raises:
-            ValueError: If variable is not a list of messages.
-        """
-        value = (
-            kwargs.get(self.variable_name, [])
-            if self.optional
-            else kwargs[self.variable_name]
-        )
-        if not isinstance(value, list):
-            msg = (
-                f"variable {self.variable_name} should be a list of base messages, "
-                f"got {value} of type {type(value)}"
-            )
-            raise ValueError(msg)  # noqa: TRY004
-        value = convert_to_messages(value)
-        if self.n_messages:
-            value = value[-self.n_messages :]
-        return value
-
-    @property
-    def input_variables(self) -> list[str]:
-        """Input variables for this prompt template.
-
-        Returns:
-            List of input variable names.
-        """
-        return [self.variable_name] if not self.optional else []
-
-    def pretty_repr(self, html: bool = False) -> str:
-        """Human-readable representation.
-
-        Args:
-            html: Whether to format as HTML. Defaults to False.
-
-        Returns:
-            Human-readable representation.
-        """
-        var = "{" + self.variable_name + "}"
-        if html:
-            title = get_msg_title_repr("Messages Placeholder", bold=True)
-            var = get_colored_text(var, "yellow")
-        else:
-            title = get_msg_title_repr("Messages Placeholder")
-        return f"{title}\n\n{var}"
-
-
-MessagePromptTemplateT = TypeVar(
-    "MessagePromptTemplateT", bound="BaseStringMessagePromptTemplate"
-)
-"""Type variable for message prompt templates."""
-
-
-class BaseStringMessagePromptTemplate(BaseMessagePromptTemplate, ABC):
-    """Base class for message prompt templates that use a string prompt template."""
-
-    prompt: StringPromptTemplate
-    """String prompt template."""
-    additional_kwargs: dict = Field(default_factory=dict)
-    """Additional keyword arguments to pass to the prompt template."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "chat"]
-
-    @classmethod
-    def from_template(
-        cls: type[MessagePromptTemplateT],
-        template: str,
-        template_format: PromptTemplateFormat = "f-string",
-        partial_variables: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> MessagePromptTemplateT:
-        """Create a class from a string template.
-
-        Args:
-            template: a template.
-            template_format: format of the template. Defaults to "f-string".
-            partial_variables: A dictionary of variables that can be used to partially
-                               fill in the template. For example, if the template is
-                              `"{variable1} {variable2}"`, and `partial_variables` is
-                              `{"variable1": "foo"}`, then the final prompt will be
-                              `"foo {variable2}"`.
-                              Defaults to None.
-            **kwargs: keyword arguments to pass to the constructor.
-
-        Returns:
-            A new instance of this class.
-        """
-        prompt = PromptTemplate.from_template(
-            template,
-            template_format=template_format,
-            partial_variables=partial_variables,
-        )
-        return cls(prompt=prompt, **kwargs)
-
-    @classmethod
-    def from_template_file(
-        cls: type[MessagePromptTemplateT],
-        template_file: Union[str, Path],
-        input_variables: list[str],
-        **kwargs: Any,
-    ) -> MessagePromptTemplateT:
-        """Create a class from a template file.
-
-        Args:
-            template_file: path to a template file. String or Path.
-            input_variables: list of input variables.
-            **kwargs: keyword arguments to pass to the constructor.
-
-        Returns:
-            A new instance of this class.
-        """
-        prompt = PromptTemplate.from_file(template_file, input_variables)
-        return cls(prompt=prompt, **kwargs)
-
-    @abstractmethod
-    def format(self, **kwargs: Any) -> BaseMessage:
-        """Format the prompt template.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            Formatted message.
-        """
-
-    async def aformat(self, **kwargs: Any) -> BaseMessage:
-        """Async format the prompt template.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            Formatted message.
-        """
-        return self.format(**kwargs)
-
-    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:
-        """Format messages from kwargs.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            List of BaseMessages.
-        """
-        return [self.format(**kwargs)]
-
-    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:
-        """Async format messages from kwargs.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            List of BaseMessages.
-        """
-        return [await self.aformat(**kwargs)]
-
-    @property
-    def input_variables(self) -> list[str]:
-        """Input variables for this prompt template.
-
-        Returns:
-            List of input variable names.
-        """
-        return self.prompt.input_variables
-
-    def pretty_repr(self, html: bool = False) -> str:
-        """Human-readable representation.
-
-        Args:
-            html: Whether to format as HTML. Defaults to False.
-
-        Returns:
-            Human-readable representation.
-        """
-        # TODO: Handle partials
-        title = self.__class__.__name__.replace("MessagePromptTemplate", " Message")
-        title = get_msg_title_repr(title, bold=html)
-        return f"{title}\n\n{self.prompt.pretty_repr(html=html)}"
-
-
-class ChatMessagePromptTemplate(BaseStringMessagePromptTemplate):
-    """Chat message prompt template."""
-
-    role: str
-    """Role of the message."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "chat"]
-
-    def format(self, **kwargs: Any) -> BaseMessage:
-        """Format the prompt template.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            Formatted message.
-        """
-        text = self.prompt.format(**kwargs)
-        return ChatMessage(
-            content=text, role=self.role, additional_kwargs=self.additional_kwargs
-        )
-
-    async def aformat(self, **kwargs: Any) -> BaseMessage:
-        """Async format the prompt template.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            Formatted message.
-        """
-        text = await self.prompt.aformat(**kwargs)
-        return ChatMessage(
-            content=text, role=self.role, additional_kwargs=self.additional_kwargs
-        )
-
-
-_StringImageMessagePromptTemplateT = TypeVar(
-    "_StringImageMessagePromptTemplateT", bound="_StringImageMessagePromptTemplate"
-)
-
-
-class _TextTemplateParam(TypedDict, total=False):
-    text: Union[str, dict]
-
-
-class _ImageTemplateParam(TypedDict, total=False):
-    image_url: Union[str, dict]
-
-
-class _StringImageMessagePromptTemplate(BaseMessagePromptTemplate):
-    """Human message prompt template. This is a message sent from the user."""
-
-    prompt: Union[
-        StringPromptTemplate, list[Union[StringPromptTemplate, ImagePromptTemplate]]
-    ]
-    """Prompt template."""
-    additional_kwargs: dict = Field(default_factory=dict)
-    """Additional keyword arguments to pass to the prompt template."""
-
-    _msg_class: type[BaseMessage]
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "chat"]
-
-    @classmethod
-    def from_template(
-        cls: type[_StringImageMessagePromptTemplateT],
-        template: Union[str, list[Union[str, _TextTemplateParam, _ImageTemplateParam]]],
-        template_format: PromptTemplateFormat = "f-string",
-        *,
-        partial_variables: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> _StringImageMessagePromptTemplateT:
-        """Create a class from a string template.
-
-        Args:
-            template: a template.
-            template_format: format of the template.
-                Options are: 'f-string', 'mustache', 'jinja2'. Defaults to "f-string".
-            partial_variables: A dictionary of variables that can be used too partially.
-                Defaults to None.
-            **kwargs: keyword arguments to pass to the constructor.
-
-        Returns:
-            A new instance of this class.
-
-        Raises:
-            ValueError: If the template is not a string or list of strings.
-        """
-        if isinstance(template, str):
-            prompt: Union[StringPromptTemplate, list] = PromptTemplate.from_template(
-                template,
-                template_format=template_format,
-                partial_variables=partial_variables,
-            )
-            return cls(prompt=prompt, **kwargs)
-        elif isinstance(template, list):
-            if (partial_variables is not None) and len(partial_variables) > 0:
-                msg = "Partial variables are not supported for list of templates."
-                raise ValueError(msg)
-            prompt = []
-            for tmpl in template:
-                if isinstance(tmpl, str) or isinstance(tmpl, dict) and "text" in tmpl:
-                    if isinstance(tmpl, str):
-                        text: str = tmpl
-                    else:
-                        text = cast(_TextTemplateParam, tmpl)["text"]  # type: ignore[assignment]
-                    prompt.append(
-                        PromptTemplate.from_template(
-                            text, template_format=template_format
-                        )
-                    )
-                elif isinstance(tmpl, dict) and "image_url" in tmpl:
-                    img_template = cast(_ImageTemplateParam, tmpl)["image_url"]
-                    input_variables = []
-                    if isinstance(img_template, str):
-                        vars = get_template_variables(img_template, template_format)
-                        if vars:
-                            if len(vars) > 1:
-                                msg = (
-                                    "Only one format variable allowed per image"
-                                    f" template.\nGot: {vars}"
-                                    f"\nFrom: {tmpl}"
-                                )
-                                raise ValueError(msg)
-                            input_variables = [vars[0]]
-                        img_template = {"url": img_template}
-                        img_template_obj = ImagePromptTemplate(
-                            input_variables=input_variables,
-                            template=img_template,
-                            template_format=template_format,
-                        )
-                    elif isinstance(img_template, dict):
-                        img_template = dict(img_template)
-                        for key in ["url", "path", "detail"]:
-                            if key in img_template:
-                                input_variables.extend(
-                                    get_template_variables(
-                                        img_template[key], template_format
-                                    )
-                                )
-                        img_template_obj = ImagePromptTemplate(
-                            input_variables=input_variables,
-                            template=img_template,
-                            template_format=template_format,
-                        )
-                    else:
-                        msg = f"Invalid image template: {tmpl}"
-                        raise ValueError(msg)
-                    prompt.append(img_template_obj)
-                else:
-                    msg = f"Invalid template: {tmpl}"
-                    raise ValueError(msg)
-            return cls(prompt=prompt, **kwargs)
-        else:
-            msg = f"Invalid template: {template}"
-            raise ValueError(msg)  # noqa: TRY004
-
-    @classmethod
-    def from_template_file(
-        cls: type[_StringImageMessagePromptTemplateT],
-        template_file: Union[str, Path],
-        input_variables: list[str],
-        **kwargs: Any,
-    ) -> _StringImageMessagePromptTemplateT:
-        """Create a class from a template file.
-
-        Args:
-            template_file: path to a template file. String or Path.
-            input_variables: list of input variables.
-            **kwargs: keyword arguments to pass to the constructor.
-
-        Returns:
-            A new instance of this class.
-        """
-        with open(str(template_file)) as f:
-            template = f.read()
-        return cls.from_template(template, input_variables=input_variables, **kwargs)
-
-    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:
-        """Format messages from kwargs.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            List of BaseMessages.
-        """
-        return [self.format(**kwargs)]
-
-    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:
-        """Async format messages from kwargs.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            List of BaseMessages.
-        """
-        return [await self.aformat(**kwargs)]
-
-    @property
-    def input_variables(self) -> list[str]:
-        """Input variables for this prompt template.
-
-        Returns:
-            List of input variable names.
-        """
-        prompts = self.prompt if isinstance(self.prompt, list) else [self.prompt]
-        input_variables = [iv for prompt in prompts for iv in prompt.input_variables]
-        return input_variables
-
-    def format(self, **kwargs: Any) -> BaseMessage:
-        """Format the prompt template.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            Formatted message.
-        """
-        if isinstance(self.prompt, StringPromptTemplate):
-            text = self.prompt.format(**kwargs)
-            return self._msg_class(
-                content=text, additional_kwargs=self.additional_kwargs
-            )
-        else:
-            content: list = []
-            for prompt in self.prompt:
-                inputs = {var: kwargs[var] for var in prompt.input_variables}
-                if isinstance(prompt, StringPromptTemplate):
-                    formatted: Union[str, ImageURL] = prompt.format(**inputs)
-                    content.append({"type": "text", "text": formatted})
-                elif isinstance(prompt, ImagePromptTemplate):
-                    formatted = prompt.format(**inputs)
-                    content.append({"type": "image_url", "image_url": formatted})
-            return self._msg_class(
-                content=content, additional_kwargs=self.additional_kwargs
-            )
-
-    async def aformat(self, **kwargs: Any) -> BaseMessage:
-        """Async format the prompt template.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            Formatted message.
-        """
-        if isinstance(self.prompt, StringPromptTemplate):
-            text = await self.prompt.aformat(**kwargs)
-            return self._msg_class(
-                content=text, additional_kwargs=self.additional_kwargs
-            )
-        else:
-            content: list = []
-            for prompt in self.prompt:
-                inputs = {var: kwargs[var] for var in prompt.input_variables}
-                if isinstance(prompt, StringPromptTemplate):
-                    formatted: Union[str, ImageURL] = await prompt.aformat(**inputs)
-                    content.append({"type": "text", "text": formatted})
-                elif isinstance(prompt, ImagePromptTemplate):
-                    formatted = await prompt.aformat(**inputs)
-                    content.append({"type": "image_url", "image_url": formatted})
-            return self._msg_class(
-                content=content, additional_kwargs=self.additional_kwargs
-            )
-
-    def pretty_repr(self, html: bool = False) -> str:
-        """Human-readable representation.
-
-        Args:
-            html: Whether to format as HTML. Defaults to False.
-
-        Returns:
-            Human-readable representation.
-        """
-        # TODO: Handle partials
-        title = self.__class__.__name__.replace("MessagePromptTemplate", " Message")
-        title = get_msg_title_repr(title, bold=html)
-        prompts = self.prompt if isinstance(self.prompt, list) else [self.prompt]
-        prompt_reprs = "\n\n".join(prompt.pretty_repr(html=html) for prompt in prompts)
-        return f"{title}\n\n{prompt_reprs}"
-
-
-class HumanMessagePromptTemplate(_StringImageMessagePromptTemplate):
-    """Human message prompt template. This is a message sent from the user."""
-
-    _msg_class: type[BaseMessage] = HumanMessage
-
-
-class AIMessagePromptTemplate(_StringImageMessagePromptTemplate):
-    """AI message prompt template. This is a message sent from the AI."""
-
-    _msg_class: type[BaseMessage] = AIMessage
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "chat"]
-
-
-class SystemMessagePromptTemplate(_StringImageMessagePromptTemplate):
-    """System message prompt template.
-    This is a message that is not sent to the user.
-    """
-
-    _msg_class: type[BaseMessage] = SystemMessage
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "chat"]
-
-
-class BaseChatPromptTemplate(BasePromptTemplate, ABC):
-    """Base class for chat prompt templates."""
-
-    @property
-    def lc_attributes(self) -> dict:
-        """Return a list of attribute names that should be included in the
-        serialized kwargs. These attributes must be accepted by the
-        constructor.
-        """
-        return {"input_variables": self.input_variables}
-
-    def format(self, **kwargs: Any) -> str:
-        """Format the chat template into a string.
-
-        Args:
-            **kwargs: keyword arguments to use for filling in template variables
-                      in all the template messages in this chat template.
-
-        Returns:
-            formatted string.
-        """
-        return self.format_prompt(**kwargs).to_string()
-
-    async def aformat(self, **kwargs: Any) -> str:
-        """Async format the chat template into a string.
-
-        Args:
-            **kwargs: keyword arguments to use for filling in template variables
-                      in all the template messages in this chat template.
-
-        Returns:
-            formatted string.
-        """
-        return (await self.aformat_prompt(**kwargs)).to_string()
-
-    def format_prompt(self, **kwargs: Any) -> PromptValue:
-        """Format prompt. Should return a PromptValue.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            PromptValue.
-        """
-        messages = self.format_messages(**kwargs)
-        return ChatPromptValue(messages=messages)
-
-    async def aformat_prompt(self, **kwargs: Any) -> PromptValue:
-        """Async format prompt. Should return a PromptValue.
-
-        Args:
-            **kwargs: Keyword arguments to use for formatting.
-
-        Returns:
-            PromptValue.
-        """
-        messages = await self.aformat_messages(**kwargs)
-        return ChatPromptValue(messages=messages)
-
-    @abstractmethod
-    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:
-        """Format kwargs into a list of messages."""
-
-    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:
-        """Async format kwargs into a list of messages."""
-        return self.format_messages(**kwargs)
-
-    def pretty_repr(self, html: bool = False) -> str:
-        """Human-readable representation.
-
-        Args:
-            html: Whether to format as HTML. Defaults to False.
-
-        Returns:
-            Human-readable representation.
-        """
-        raise NotImplementedError
-
-    def pretty_print(self) -> None:
-        """Print a human-readable representation."""
-        print(self.pretty_repr(html=is_interactive_env()))  # noqa: T201
-
-
-MessageLike = Union[BaseMessagePromptTemplate, BaseMessage, BaseChatPromptTemplate]
-
-MessageLikeRepresentation = Union[
-    MessageLike,
-    tuple[
-        Union[str, type],
-        Union[str, list[dict], list[object]],
-    ],
-    str,
-    dict,
-]
-
-
-class ChatPromptTemplate(BaseChatPromptTemplate):
-    """Prompt template for chat models.
-
-    Use to create flexible templated prompts for chat models.
-
-    Examples:
-
-        .. versionchanged:: 0.2.24
-
-            You can pass any Message-like formats supported by
-            ``ChatPromptTemplate.from_messages()`` directly to ``ChatPromptTemplate()``
-            init.
-
-        .. code-block:: python
-
-            from langchain_core.prompts import ChatPromptTemplate
-
-            template = ChatPromptTemplate([
-                ("system", "You are a helpful AI bot. Your name is {name}."),
-                ("human", "Hello, how are you doing?"),
-                ("ai", "I'm doing well, thanks!"),
-                ("human", "{user_input}"),
-            ])
-
-            prompt_value = template.invoke(
-                {
-                    "name": "Bob",
-                    "user_input": "What is your name?"
-                }
-            )
-            # Output:
-            # ChatPromptValue(
-            #    messages=[
-            #        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),
-            #        HumanMessage(content='Hello, how are you doing?'),
-            #        AIMessage(content="I'm doing well, thanks!"),
-            #        HumanMessage(content='What is your name?')
-            #    ]
-            #)
-
-    Messages Placeholder:
-
-        .. code-block:: python
-
-            # In addition to Human/AI/Tool/Function messages,
-            # you can initialize the template with a MessagesPlaceholder
-            # either using the class directly or with the shorthand tuple syntax:
-
-            template = ChatPromptTemplate([
-                ("system", "You are a helpful AI bot."),
-                # Means the template will receive an optional list of messages under
-                # the "conversation" key
-                ("placeholder", "{conversation}")
-                # Equivalently:
-                # MessagesPlaceholder(variable_name="conversation", optional=True)
-            ])
-
-            prompt_value = template.invoke(
-                {
-                    "conversation": [
-                        ("human", "Hi!"),
-                        ("ai", "How can I assist you today?"),
-                        ("human", "Can you make me an ice cream sundae?"),
-                        ("ai", "No.")
-                    ]
-                }
-            )
-
-            # Output:
-            # ChatPromptValue(
-            #    messages=[
-            #        SystemMessage(content='You are a helpful AI bot.'),
-            #        HumanMessage(content='Hi!'),
-            #        AIMessage(content='How can I assist you today?'),
-            #        HumanMessage(content='Can you make me an ice cream sundae?'),
-            #        AIMessage(content='No.'),
-            #    ]
-            #)
-
-    Single-variable template:
-
-        If your prompt has only a single input variable (i.e., 1 instance of "{variable_nams}"),
-        and you invoke the template with a non-dict object, the prompt template will
-        inject the provided argument into that variable location.
-
-
-        .. code-block:: python
-
-            from langchain_core.prompts import ChatPromptTemplate
-
-            template = ChatPromptTemplate([
-                ("system", "You are a helpful AI bot. Your name is Carl."),
-                ("human", "{user_input}"),
-            ])
-
-            prompt_value = template.invoke("Hello, there!")
-            # Equivalent to
-            # prompt_value = template.invoke({"user_input": "Hello, there!"})
-
-            # Output:
-            #  ChatPromptValue(
-            #     messages=[
-            #         SystemMessage(content='You are a helpful AI bot. Your name is Carl.'),
-            #         HumanMessage(content='Hello, there!'),
-            #     ]
-            # )
-
-    """  # noqa: E501
-
-    messages: Annotated[list[MessageLike], SkipValidation()]
-    """List of messages consisting of either message prompt templates or messages."""
-    validate_template: bool = False
-    """Whether or not to try validating the template."""
-
-    def __init__(
-        self,
-        messages: Sequence[MessageLikeRepresentation],
-        *,
-        template_format: PromptTemplateFormat = "f-string",
-        **kwargs: Any,
-    ) -> None:
-        """Create a chat prompt template from a variety of message formats.
-
-        Args:
-            messages: sequence of message representations.
-                  A message can be represented using the following formats:
-                  (1) BaseMessagePromptTemplate, (2) BaseMessage, (3) 2-tuple of
-                  (message type, template); e.g., ("human", "{user_input}"),
-                  (4) 2-tuple of (message class, template), (5) a string which is
-                  shorthand for ("human", template); e.g., "{user_input}".
-            template_format: format of the template. Defaults to "f-string".
-            input_variables: A list of the names of the variables whose values are
-                required as inputs to the prompt.
-            optional_variables: A list of the names of the variables for placeholder
-            or MessagePlaceholder that are optional. These variables are auto inferred
-            from the prompt and user need not provide them.
-            partial_variables: A dictionary of the partial variables the prompt
-                template carries. Partial variables populate the template so that you
-                don't need to pass them in every time you call the prompt.
-            validate_template: Whether to validate the template.
-            input_types: A dictionary of the types of the variables the prompt template
-                expects. If not provided, all variables are assumed to be strings.
-
-        Returns:
-            A chat prompt template.
-
-        Examples:
-            Instantiation from a list of message templates:
-
-            .. code-block:: python
-
-                template = ChatPromptTemplate([
-                    ("human", "Hello, how are you?"),
-                    ("ai", "I'm doing well, thanks!"),
-                    ("human", "That's good to hear."),
-                ])
-
-            Instantiation from mixed message formats:
-
-            .. code-block:: python
-
-                template = ChatPromptTemplate([
-                    SystemMessage(content="hello"),
-                    ("human", "Hello, how are you?"),
-                ])
-
-        """
-        _messages = [
-            _convert_to_message(message, template_format) for message in messages
-        ]
-
-        # Automatically infer input variables from messages
-        input_vars: set[str] = set()
-        optional_variables: set[str] = set()
-        partial_vars: dict[str, Any] = {}
-        for _message in _messages:
-            if isinstance(_message, MessagesPlaceholder) and _message.optional:
-                partial_vars[_message.variable_name] = []
-                optional_variables.add(_message.variable_name)
-            elif isinstance(
-                _message, (BaseChatPromptTemplate, BaseMessagePromptTemplate)
-            ):
-                input_vars.update(_message.input_variables)
-
-        kwargs = {
-            "input_variables": sorted(input_vars),
-            "optional_variables": sorted(optional_variables),
-            "partial_variables": partial_vars,
-            **kwargs,
-        }
-        cast(type[ChatPromptTemplate], super()).__init__(messages=_messages, **kwargs)
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "chat"]
-
-    def __add__(self, other: Any) -> ChatPromptTemplate:
-        """Combine two prompt templates.
-
-        Args:
-            other: Another prompt template.
-
-        Returns:
-            Combined prompt template.
-        """
-        # Allow for easy combining
-        if isinstance(other, ChatPromptTemplate):
-            return ChatPromptTemplate(messages=self.messages + other.messages)  # type: ignore[call-arg]
-        elif isinstance(
-            other, (BaseMessagePromptTemplate, BaseMessage, BaseChatPromptTemplate)
-        ):
-            return ChatPromptTemplate(messages=self.messages + [other])  # type: ignore[call-arg]
-        elif isinstance(other, (list, tuple)):
-            _other = ChatPromptTemplate.from_messages(other)
-            return ChatPromptTemplate(messages=self.messages + _other.messages)  # type: ignore[call-arg]
-        elif isinstance(other, str):
-            prompt = HumanMessagePromptTemplate.from_template(other)
-            return ChatPromptTemplate(messages=self.messages + [prompt])  # type: ignore[call-arg]
-        else:
-            msg = f"Unsupported operand type for +: {type(other)}"
-            raise NotImplementedError(msg)
-
-    @model_validator(mode="before")
-    @classmethod
-    def validate_input_variables(cls, values: dict) -> Any:
-        """Validate input variables.
-
-        If input_variables is not set, it will be set to the union of
-        all input variables in the messages.
-
-        Args:
-            values: values to validate.
-
-        Returns:
-            Validated values.
-
-        Raises:
-            ValueError: If input variables do not match.
-        """
-        messages = values["messages"]
-        input_vars = set()
-        optional_variables = set()
-        input_types: dict[str, Any] = values.get("input_types", {})
-        for message in messages:
-            if isinstance(message, (BaseMessagePromptTemplate, BaseChatPromptTemplate)):
-                input_vars.update(message.input_variables)
-            if isinstance(message, MessagesPlaceholder):
-                if "partial_variables" not in values:
-                    values["partial_variables"] = {}
-                if (
-                    message.optional
-                    and message.variable_name not in values["partial_variables"]
-                ):
-                    values["partial_variables"][message.variable_name] = []
-                    optional_variables.add(message.variable_name)
-                if message.variable_name not in input_types:
-                    input_types[message.variable_name] = list[AnyMessage]
-        if "partial_variables" in values:
-            input_vars = input_vars - set(values["partial_variables"])
-        if optional_variables:
-            input_vars = input_vars - optional_variables
-        if "input_variables" in values and values.get("validate_template"):
-            if input_vars != set(values["input_variables"]):
-                msg = (
-                    "Got mismatched input_variables. "
-                    f"Expected: {input_vars}. "
-                    f"Got: {values['input_variables']}"
-                )
-                raise ValueError(msg)
-        else:
-            values["input_variables"] = sorted(input_vars)
-        if optional_variables:
-            values["optional_variables"] = sorted(optional_variables)
-        values["input_types"] = input_types
-        return values
-
-    @classmethod
-    def from_template(cls, template: str, **kwargs: Any) -> ChatPromptTemplate:
-        """Create a chat prompt template from a template string.
-
-        Creates a chat template consisting of a single message assumed to be from
-        the human.
-
-        Args:
-            template: template string
-            **kwargs: keyword arguments to pass to the constructor.
-
-        Returns:
-            A new instance of this class.
-        """
-        prompt_template = PromptTemplate.from_template(template, **kwargs)
-        message = HumanMessagePromptTemplate(prompt=prompt_template)
-        return cls.from_messages([message])
-
-    @classmethod
-    @deprecated("0.0.1", alternative="from_messages classmethod", pending=True)
-    def from_role_strings(
-        cls, string_messages: list[tuple[str, str]]
-    ) -> ChatPromptTemplate:
-        """Create a chat prompt template from a list of (role, template) tuples.
-
-        Args:
-            string_messages: list of (role, template) tuples.
-
-        Returns:
-            a chat prompt template.
-        """
-        return cls(  # type: ignore[call-arg]
-            messages=[
-                ChatMessagePromptTemplate.from_template(template, role=role)
-                for role, template in string_messages
-            ]
-        )
-
-    @classmethod
-    @deprecated("0.0.1", alternative="from_messages classmethod", pending=True)
-    def from_strings(
-        cls, string_messages: list[tuple[type[BaseMessagePromptTemplate], str]]
-    ) -> ChatPromptTemplate:
-        """Create a chat prompt template from a list of (role class, template) tuples.
-
-        Args:
-            string_messages: list of (role class, template) tuples.
-
-        Returns:
-            a chat prompt template.
-        """
-        return cls.from_messages(string_messages)
-
-    @classmethod
-    def from_messages(
-        cls,
-        messages: Sequence[MessageLikeRepresentation],
-        template_format: PromptTemplateFormat = "f-string",
-    ) -> ChatPromptTemplate:
-        """Create a chat prompt template from a variety of message formats.
-
-        Examples:
-            Instantiation from a list of message templates:
-
-            .. code-block:: python
-
-                template = ChatPromptTemplate.from_messages([
-                    ("human", "Hello, how are you?"),
-                    ("ai", "I'm doing well, thanks!"),
-                    ("human", "That's good to hear."),
-                ])
-
-            Instantiation from mixed message formats:
-
-            .. code-block:: python
-
-                template = ChatPromptTemplate.from_messages([
-                    SystemMessage(content="hello"),
-                    ("human", "Hello, how are you?"),
-                ])
-
-        Args:
-            messages: sequence of message representations.
-                  A message can be represented using the following formats:
-                  (1) BaseMessagePromptTemplate, (2) BaseMessage, (3) 2-tuple of
-                  (message type, template); e.g., ("human", "{user_input}"),
-                  (4) 2-tuple of (message class, template), (5) a string which is
-                  shorthand for ("human", template); e.g., "{user_input}".
-            template_format: format of the template. Defaults to "f-string".
-
-        Returns:
-            a chat prompt template.
-        """
-        return cls(messages, template_format=template_format)
-
-    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:
-        """Format the chat template into a list of finalized messages.
-
-        Args:
-            **kwargs: keyword arguments to use for filling in template variables
-                      in all the template messages in this chat template.
-
-        Returns:
-            list of formatted messages.
-        """
-        kwargs = self._merge_partial_and_user_variables(**kwargs)
-        result = []
-        for message_template in self.messages:
-            if isinstance(message_template, BaseMessage):
-                result.extend([message_template])
-            elif isinstance(
-                message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)
-            ):
-                message = message_template.format_messages(**kwargs)
-                result.extend(message)
-            else:
-                msg = f"Unexpected input: {message_template}"
-                raise ValueError(msg)  # noqa: TRY004
-        return result
-
-    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:
-        """Async format the chat template into a list of finalized messages.
-
-        Args:
-            **kwargs: keyword arguments to use for filling in template variables
-                      in all the template messages in this chat template.
-
-        Returns:
-            list of formatted messages.
-
-        Raises:
-            ValueError: If unexpected input.
-        """
-        kwargs = self._merge_partial_and_user_variables(**kwargs)
-        result = []
-        for message_template in self.messages:
-            if isinstance(message_template, BaseMessage):
-                result.extend([message_template])
-            elif isinstance(
-                message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)
-            ):
-                message = await message_template.aformat_messages(**kwargs)
-                result.extend(message)
-            else:
-                msg = f"Unexpected input: {message_template}"
-                raise ValueError(msg)  # noqa:TRY004
-        return result
-
-    def partial(self, **kwargs: Any) -> ChatPromptTemplate:
-        """Get a new ChatPromptTemplate with some input variables already filled in.
-
-        Args:
-            **kwargs: keyword arguments to use for filling in template variables. Ought
-                        to be a subset of the input variables.
-
-        Returns:
-            A new ChatPromptTemplate.
-
-
-        Example:
-
-            .. code-block:: python
-
-                from langchain_core.prompts import ChatPromptTemplate
-
-                template = ChatPromptTemplate.from_messages(
-                    [
-                        ("system", "You are an AI assistant named {name}."),
-                        ("human", "Hi I'm {user}"),
-                        ("ai", "Hi there, {user}, I'm {name}."),
-                        ("human", "{input}"),
-                    ]
-                )
-                template2 = template.partial(user="Lucy", name="R2D2")
-
-                template2.format_messages(input="hello")
-        """
-        prompt_dict = self.__dict__.copy()
-        prompt_dict["input_variables"] = list(
-            set(self.input_variables).difference(kwargs)
-        )
-        prompt_dict["partial_variables"] = {**self.partial_variables, **kwargs}
-        return type(self)(**prompt_dict)
-
-    def append(self, message: MessageLikeRepresentation) -> None:
-        """Append a message to the end of the chat template.
-
-        Args:
-            message: representation of a message to append.
-        """
-        self.messages.append(_convert_to_message(message))
-
-    def extend(self, messages: Sequence[MessageLikeRepresentation]) -> None:
-        """Extend the chat template with a sequence of messages.
-
-        Args:
-            messages: sequence of message representations to append.
-        """
-        self.messages.extend([_convert_to_message(message) for message in messages])
-
-    @overload
-    def __getitem__(self, index: int) -> MessageLike: ...
-
-    @overload
-    def __getitem__(self, index: slice) -> ChatPromptTemplate: ...
-
-    def __getitem__(
-        self, index: Union[int, slice]
-    ) -> Union[MessageLike, ChatPromptTemplate]:
-        """Use to index into the chat template."""
-        if isinstance(index, slice):
-            start, stop, step = index.indices(len(self.messages))
-            messages = self.messages[start:stop:step]
-            return ChatPromptTemplate.from_messages(messages)
-        else:
-            return self.messages[index]
-
-    def __len__(self) -> int:
-        """Get the length of the chat template."""
-        return len(self.messages)
-
-    @property
-    def _prompt_type(self) -> str:
-        """Name of prompt type. Used for serialization."""
-        return "chat"
-
-    def save(self, file_path: Union[Path, str]) -> None:
-        """Save prompt to file.
-
-        Args:
-            file_path: path to file.
-        """
-        raise NotImplementedError
-
-    def pretty_repr(self, html: bool = False) -> str:
-        """Human-readable representation.
-
-        Args:
-            html: Whether to format as HTML. Defaults to False.
-
-        Returns:
-            Human-readable representation.
-        """
-        # TODO: handle partials
-        return "\n\n".join(msg.pretty_repr(html=html) for msg in self.messages)
-
-
-def _create_template_from_message_type(
-    message_type: str,
-    template: Union[str, list],
-    template_format: PromptTemplateFormat = "f-string",
-) -> BaseMessagePromptTemplate:
-    """Create a message prompt template from a message type and template string.
-
-    Args:
-        message_type: str the type of the message template (e.g., "human", "ai", etc.)
-        template: str the template string.
-        template_format: format of the template. Defaults to "f-string".
-
-    Returns:
-        a message prompt template of the appropriate type.
-
-    Raises:
-        ValueError: If unexpected message type.
-    """
-    if message_type in ("human", "user"):
-        message: BaseMessagePromptTemplate = HumanMessagePromptTemplate.from_template(
-            template, template_format=template_format
-        )
-    elif message_type in ("ai", "assistant"):
-        message = AIMessagePromptTemplate.from_template(
-            cast(str, template), template_format=template_format
-        )
-    elif message_type == "system":
-        message = SystemMessagePromptTemplate.from_template(
-            cast(str, template), template_format=template_format
-        )
-    elif message_type == "placeholder":
-        if isinstance(template, str):
-            if template[0] != "{" or template[-1] != "}":
-                msg = (
-                    f"Invalid placeholder template: {template}."
-                    " Expected a variable name surrounded by curly braces."
-                )
-                raise ValueError(msg)
-            var_name = template[1:-1]
-            message = MessagesPlaceholder(variable_name=var_name, optional=True)
-        elif len(template) == 2 and isinstance(template[1], bool):
-            var_name_wrapped, is_optional = template
-            if not isinstance(var_name_wrapped, str):
-                msg = f"Expected variable name to be a string. Got: {var_name_wrapped}"
-                raise ValueError(msg)  # noqa:TRY004
-            if var_name_wrapped[0] != "{" or var_name_wrapped[-1] != "}":
-                msg = (
-                    f"Invalid placeholder template: {var_name_wrapped}."
-                    " Expected a variable name surrounded by curly braces."
-                )
-                raise ValueError(msg)
-            var_name = var_name_wrapped[1:-1]
-
-            message = MessagesPlaceholder(variable_name=var_name, optional=is_optional)
-        else:
-            msg = (
-                "Unexpected arguments for placeholder message type."
-                " Expected either a single string variable name"
-                " or a list of [variable_name: str, is_optional: bool]."
-                f" Got: {template}"
-            )
-            raise ValueError(msg)
-    else:
-        msg = (
-            f"Unexpected message type: {message_type}. Use one of 'human',"
-            f" 'user', 'ai', 'assistant', or 'system'."
-        )
-        raise ValueError(msg)
-    return message
-
-
-def _convert_to_message(
-    message: MessageLikeRepresentation,
-    template_format: PromptTemplateFormat = "f-string",
-) -> Union[BaseMessage, BaseMessagePromptTemplate, BaseChatPromptTemplate]:
-    """Instantiate a message from a variety of message formats.
-
-    The message format can be one of the following:
-
-    - BaseMessagePromptTemplate
-    - BaseMessage
-    - 2-tuple of (role string, template); e.g., ("human", "{user_input}")
-    - 2-tuple of (message class, template)
-    - string: shorthand for ("human", template); e.g., "{user_input}"
-
-    Args:
-        message: a representation of a message in one of the supported formats.
-        template_format: format of the template. Defaults to "f-string".
-
-    Returns:
-        an instance of a message or a message template.
-
-    Raises:
-        ValueError: If unexpected message type.
-        ValueError: If 2-tuple does not have 2 elements.
-    """
-    if isinstance(message, (BaseMessagePromptTemplate, BaseChatPromptTemplate)):
-        _message: Union[
-            BaseMessage, BaseMessagePromptTemplate, BaseChatPromptTemplate
-        ] = message
-    elif isinstance(message, BaseMessage):
-        _message = message
-    elif isinstance(message, str):
-        _message = _create_template_from_message_type(
-            "human", message, template_format=template_format
-        )
-    elif isinstance(message, (tuple, dict)):
-        if isinstance(message, dict):
-            if set(message.keys()) != {"content", "role"}:
-                msg = (
-                    "Expected dict to have exact keys 'role' and 'content'."
-                    f" Got: {message}"
-                )
-                raise ValueError(msg)
-            message = (message["role"], message["content"])
-        if len(message) != 2:
-            msg = f"Expected 2-tuple of (role, template), got {message}"
-            raise ValueError(msg)
-        message_type_str, template = message
-        if isinstance(message_type_str, str):
-            _message = _create_template_from_message_type(
-                message_type_str, template, template_format=template_format
-            )
-        else:
-            _message = message_type_str(
-                prompt=PromptTemplate.from_template(
-                    cast(str, template), template_format=template_format
-                )
-            )
-    else:
-        msg = f"Unsupported message type: {type(message)}"
-        raise NotImplementedError(msg)
-
-    return _message
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/prompts/few_shot.py ./custom_langchain_core/prompts/few_shot.py
--- .venv/lib/python3.12/site-packages/langchain_core/prompts/few_shot.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/prompts/few_shot.py	1970-01-01 09:00:00
@@ -1,466 +0,0 @@
-"""Prompt template that contains few shot examples."""
-
-from __future__ import annotations
-
-from pathlib import Path
-from typing import Any, Literal, Optional, Union
-
-from pydantic import (
-    BaseModel,
-    ConfigDict,
-    Field,
-    model_validator,
-)
-from typing_extensions import Self
-
-from langchain_core.example_selectors import BaseExampleSelector
-from langchain_core.messages import BaseMessage, get_buffer_string
-from langchain_core.prompts.chat import (
-    BaseChatPromptTemplate,
-    BaseMessagePromptTemplate,
-)
-from langchain_core.prompts.prompt import PromptTemplate
-from langchain_core.prompts.string import (
-    DEFAULT_FORMATTER_MAPPING,
-    StringPromptTemplate,
-    check_valid_template,
-    get_template_variables,
-)
-
-
-class _FewShotPromptTemplateMixin(BaseModel):
-    """Prompt template that contains few shot examples."""
-
-    examples: Optional[list[dict]] = None
-    """Examples to format into the prompt.
-    Either this or example_selector should be provided."""
-
-    example_selector: Optional[BaseExampleSelector] = None
-    """ExampleSelector to choose the examples to format into the prompt.
-    Either this or examples should be provided."""
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-        extra="forbid",
-    )
-
-    @model_validator(mode="before")
-    @classmethod
-    def check_examples_and_selector(cls, values: dict) -> Any:
-        """Check that one and only one of examples/example_selector are provided.
-
-        Args:
-            values: The values to check.
-
-        Returns:
-            The values if they are valid.
-
-        Raises:
-            ValueError: If neither or both examples and example_selector are provided.
-            ValueError: If both examples and example_selector are provided.
-        """
-        examples = values.get("examples")
-        example_selector = values.get("example_selector")
-        if examples and example_selector:
-            msg = "Only one of 'examples' and 'example_selector' should be provided"
-            raise ValueError(msg)
-
-        if examples is None and example_selector is None:
-            msg = "One of 'examples' and 'example_selector' should be provided"
-            raise ValueError(msg)
-
-        return values
-
-    def _get_examples(self, **kwargs: Any) -> list[dict]:
-        """Get the examples to use for formatting the prompt.
-
-        Args:
-            **kwargs: Keyword arguments to be passed to the example selector.
-
-        Returns:
-            List of examples.
-
-        Raises:
-            ValueError: If neither examples nor example_selector are provided.
-        """
-        if self.examples is not None:
-            return self.examples
-        elif self.example_selector is not None:
-            return self.example_selector.select_examples(kwargs)
-        else:
-            msg = "One of 'examples' and 'example_selector' should be provided"
-            raise ValueError(msg)
-
-    async def _aget_examples(self, **kwargs: Any) -> list[dict]:
-        """Async get the examples to use for formatting the prompt.
-
-        Args:
-            **kwargs: Keyword arguments to be passed to the example selector.
-
-        Returns:
-            List of examples.
-
-        Raises:
-            ValueError: If neither examples nor example_selector are provided.
-        """
-        if self.examples is not None:
-            return self.examples
-        elif self.example_selector is not None:
-            return await self.example_selector.aselect_examples(kwargs)
-        else:
-            msg = "One of 'examples' and 'example_selector' should be provided"
-            raise ValueError(msg)
-
-
-class FewShotPromptTemplate(_FewShotPromptTemplateMixin, StringPromptTemplate):
-    """Prompt template that contains few shot examples."""
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Return whether or not the class is serializable."""
-        return False
-
-    validate_template: bool = False
-    """Whether or not to try validating the template."""
-
-    example_prompt: PromptTemplate
-    """PromptTemplate used to format an individual example."""
-
-    suffix: str
-    """A prompt template string to put after the examples."""
-
-    example_separator: str = "\n\n"
-    """String separator used to join the prefix, the examples, and suffix."""
-
-    prefix: str = ""
-    """A prompt template string to put before the examples."""
-
-    template_format: Literal["f-string", "jinja2"] = "f-string"
-    """The format of the prompt template. Options are: 'f-string', 'jinja2'."""
-
-    def __init__(self, **kwargs: Any) -> None:
-        """Initialize the few shot prompt template."""
-        if "input_variables" not in kwargs and "example_prompt" in kwargs:
-            kwargs["input_variables"] = kwargs["example_prompt"].input_variables
-        super().__init__(**kwargs)
-
-    @model_validator(mode="after")
-    def template_is_valid(self) -> Self:
-        """Check that prefix, suffix, and input variables are consistent."""
-        if self.validate_template:
-            check_valid_template(
-                self.prefix + self.suffix,
-                self.template_format,
-                self.input_variables + list(self.partial_variables),
-            )
-        elif self.template_format or None:
-            self.input_variables = [
-                var
-                for var in get_template_variables(
-                    self.prefix + self.suffix, self.template_format
-                )
-                if var not in self.partial_variables
-            ]
-        return self
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-        extra="forbid",
-    )
-
-    def format(self, **kwargs: Any) -> str:
-        """Format the prompt with inputs generating a string.
-
-        Use this method to generate a string representation of a prompt.
-
-        Args:
-            **kwargs: keyword arguments to use for formatting.
-
-        Returns:
-            A string representation of the prompt.
-        """
-        kwargs = self._merge_partial_and_user_variables(**kwargs)
-        # Get the examples to use.
-        examples = self._get_examples(**kwargs)
-        examples = [
-            {k: e[k] for k in self.example_prompt.input_variables} for e in examples
-        ]
-        # Format the examples.
-        example_strings = [
-            self.example_prompt.format(**example) for example in examples
-        ]
-        # Create the overall template.
-        pieces = [self.prefix, *example_strings, self.suffix]
-        template = self.example_separator.join([piece for piece in pieces if piece])
-
-        # Format the template with the input variables.
-        return DEFAULT_FORMATTER_MAPPING[self.template_format](template, **kwargs)
-
-    async def aformat(self, **kwargs: Any) -> str:
-        """Async format the prompt with inputs generating a string.
-
-        Use this method to generate a string representation of a prompt.
-
-        Args:
-            **kwargs: keyword arguments to use for formatting.
-
-        Returns:
-            A string representation of the prompt.
-        """
-        kwargs = self._merge_partial_and_user_variables(**kwargs)
-        # Get the examples to use.
-        examples = await self._aget_examples(**kwargs)
-        examples = [
-            {k: e[k] for k in self.example_prompt.input_variables} for e in examples
-        ]
-        # Format the examples.
-        example_strings = [
-            await self.example_prompt.aformat(**example) for example in examples
-        ]
-        # Create the overall template.
-        pieces = [self.prefix, *example_strings, self.suffix]
-        template = self.example_separator.join([piece for piece in pieces if piece])
-
-        # Format the template with the input variables.
-        return DEFAULT_FORMATTER_MAPPING[self.template_format](template, **kwargs)
-
-    @property
-    def _prompt_type(self) -> str:
-        """Return the prompt type key."""
-        return "few_shot"
-
-    def save(self, file_path: Union[Path, str]) -> None:
-        """Save the prompt template to a file.
-
-        Args:
-            file_path: The path to save the prompt template to.
-
-        Raises:
-            ValueError: If example_selector is provided.
-        """
-        if self.example_selector:
-            msg = "Saving an example selector is not currently supported"
-            raise ValueError(msg)
-        return super().save(file_path)
-
-
-class FewShotChatMessagePromptTemplate(
-    BaseChatPromptTemplate, _FewShotPromptTemplateMixin
-):
-    """Chat prompt template that supports few-shot examples.
-
-    The high level structure of produced by this prompt template is a list of messages
-    consisting of prefix message(s), example message(s), and suffix message(s).
-
-    This structure enables creating a conversation with intermediate examples like:
-
-        System: You are a helpful AI Assistant
-        Human: What is 2+2?
-        AI: 4
-        Human: What is 2+3?
-        AI: 5
-        Human: What is 4+4?
-
-    This prompt template can be used to generate a fixed list of examples or else
-    to dynamically select examples based on the input.
-
-    Examples:
-        Prompt template with a fixed list of examples (matching the sample
-        conversation above):
-
-        .. code-block:: python
-
-            from langchain_core.prompts import (
-                FewShotChatMessagePromptTemplate,
-                ChatPromptTemplate
-            )
-
-            examples = [
-                {"input": "2+2", "output": "4"},
-                {"input": "2+3", "output": "5"},
-            ]
-
-            example_prompt = ChatPromptTemplate.from_messages(
-            [('human', 'What is {input}?'), ('ai', '{output}')]
-            )
-
-            few_shot_prompt = FewShotChatMessagePromptTemplate(
-                examples=examples,
-                # This is a prompt template used to format each individual example.
-                example_prompt=example_prompt,
-            )
-
-            final_prompt = ChatPromptTemplate.from_messages(
-                [
-                    ('system', 'You are a helpful AI Assistant'),
-                    few_shot_prompt,
-                    ('human', '{input}'),
-                ]
-            )
-            final_prompt.format(input="What is 4+4?")
-
-        Prompt template with dynamically selected examples:
-
-        .. code-block:: python
-
-            from langchain_core.prompts import SemanticSimilarityExampleSelector
-            from langchain_core.embeddings import OpenAIEmbeddings
-            from langchain_core.vectorstores import Chroma
-
-            examples = [
-                {"input": "2+2", "output": "4"},
-                {"input": "2+3", "output": "5"},
-                {"input": "2+4", "output": "6"},
-                # ...
-            ]
-
-            to_vectorize = [
-                " ".join(example.values())
-                for example in examples
-            ]
-            embeddings = OpenAIEmbeddings()
-            vectorstore = Chroma.from_texts(
-                to_vectorize, embeddings, metadatas=examples
-            )
-            example_selector = SemanticSimilarityExampleSelector(
-                vectorstore=vectorstore
-            )
-
-            from langchain_core import SystemMessage
-            from langchain_core.prompts import HumanMessagePromptTemplate
-            from langchain_core.prompts.few_shot import FewShotChatMessagePromptTemplate
-
-            few_shot_prompt = FewShotChatMessagePromptTemplate(
-                # Which variable(s) will be passed to the example selector.
-                input_variables=["input"],
-                example_selector=example_selector,
-                # Define how each example will be formatted.
-                # In this case, each example will become 2 messages:
-                # 1 human, and 1 AI
-                example_prompt=(
-                    HumanMessagePromptTemplate.from_template("{input}")
-                    + AIMessagePromptTemplate.from_template("{output}")
-                ),
-            )
-            # Define the overall prompt.
-            final_prompt = (
-                SystemMessagePromptTemplate.from_template(
-                    "You are a helpful AI Assistant"
-                )
-                + few_shot_prompt
-                + HumanMessagePromptTemplate.from_template("{input}")
-            )
-            # Show the prompt
-            print(final_prompt.format_messages(input="What's 3+3?"))  # noqa: T201
-
-            # Use within an LLM
-            from langchain_core.chat_models import ChatAnthropic
-            chain = final_prompt | ChatAnthropic(model="claude-3-haiku-20240307")
-            chain.invoke({"input": "What's 3+3?"})
-    """
-
-    input_variables: list[str] = Field(default_factory=list)
-    """A list of the names of the variables the prompt template will use
-    to pass to the example_selector, if provided."""
-
-    example_prompt: Union[BaseMessagePromptTemplate, BaseChatPromptTemplate]
-    """The class to format each example."""
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Return whether or not the class is serializable."""
-        return False
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-        extra="forbid",
-    )
-
-    def format_messages(self, **kwargs: Any) -> list[BaseMessage]:
-        """Format kwargs into a list of messages.
-
-        Args:
-            **kwargs: keyword arguments to use for filling in templates in messages.
-
-        Returns:
-            A list of formatted messages with all template variables filled in.
-        """
-        # Get the examples to use.
-        examples = self._get_examples(**kwargs)
-        examples = [
-            {k: e[k] for k in self.example_prompt.input_variables} for e in examples
-        ]
-        # Format the examples.
-        messages = [
-            message
-            for example in examples
-            for message in self.example_prompt.format_messages(**example)
-        ]
-        return messages
-
-    async def aformat_messages(self, **kwargs: Any) -> list[BaseMessage]:
-        """Async format kwargs into a list of messages.
-
-        Args:
-            **kwargs: keyword arguments to use for filling in templates in messages.
-
-        Returns:
-            A list of formatted messages with all template variables filled in.
-        """
-        # Get the examples to use.
-        examples = await self._aget_examples(**kwargs)
-        examples = [
-            {k: e[k] for k in self.example_prompt.input_variables} for e in examples
-        ]
-        # Format the examples.
-        messages = [
-            message
-            for example in examples
-            for message in await self.example_prompt.aformat_messages(**example)
-        ]
-        return messages
-
-    def format(self, **kwargs: Any) -> str:
-        """Format the prompt with inputs generating a string.
-
-        Use this method to generate a string representation of a prompt consisting
-        of chat messages.
-
-        Useful for feeding into a string-based completion language model or debugging.
-
-        Args:
-            **kwargs: keyword arguments to use for formatting.
-
-        Returns:
-            A string representation of the prompt
-        """
-        messages = self.format_messages(**kwargs)
-        return get_buffer_string(messages)
-
-    async def aformat(self, **kwargs: Any) -> str:
-        """Async format the prompt with inputs generating a string.
-
-        Use this method to generate a string representation of a prompt consisting
-        of chat messages.
-
-        Useful for feeding into a string-based completion language model or debugging.
-
-        Args:
-            **kwargs: keyword arguments to use for formatting.
-
-        Returns:
-            A string representation of the prompt
-        """
-        messages = await self.aformat_messages(**kwargs)
-        return get_buffer_string(messages)
-
-    def pretty_repr(self, html: bool = False) -> str:
-        """Return a pretty representation of the prompt template.
-
-        Args:
-            html: Whether or not to return an HTML formatted string.
-
-        Returns:
-            A pretty representation of the prompt template.
-        """
-        raise NotImplementedError
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/prompts/few_shot_with_templates.py ./custom_langchain_core/prompts/few_shot_with_templates.py
--- .venv/lib/python3.12/site-packages/langchain_core/prompts/few_shot_with_templates.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/prompts/few_shot_with_templates.py	1970-01-01 09:00:00
@@ -1,222 +0,0 @@
-"""Prompt template that contains few shot examples."""
-
-from pathlib import Path
-from typing import Any, Optional, Union
-
-from pydantic import ConfigDict, model_validator
-from typing_extensions import Self
-
-from langchain_core.prompts.prompt import PromptTemplate
-from langchain_core.prompts.string import (
-    DEFAULT_FORMATTER_MAPPING,
-    PromptTemplateFormat,
-    StringPromptTemplate,
-)
-
-
-class FewShotPromptWithTemplates(StringPromptTemplate):
-    """Prompt template that contains few shot examples."""
-
-    examples: Optional[list[dict]] = None
-    """Examples to format into the prompt.
-    Either this or example_selector should be provided."""
-
-    example_selector: Any = None
-    """ExampleSelector to choose the examples to format into the prompt.
-    Either this or examples should be provided."""
-
-    example_prompt: PromptTemplate
-    """PromptTemplate used to format an individual example."""
-
-    suffix: StringPromptTemplate
-    """A PromptTemplate to put after the examples."""
-
-    example_separator: str = "\n\n"
-    """String separator used to join the prefix, the examples, and suffix."""
-
-    prefix: Optional[StringPromptTemplate] = None
-    """A PromptTemplate to put before the examples."""
-
-    template_format: PromptTemplateFormat = "f-string"
-    """The format of the prompt template.
-    Options are: 'f-string', 'jinja2', 'mustache'."""
-
-    validate_template: bool = False
-    """Whether or not to try validating the template."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "few_shot_with_templates"]
-
-    @model_validator(mode="before")
-    @classmethod
-    def check_examples_and_selector(cls, values: dict) -> Any:
-        """Check that one and only one of examples/example_selector are provided."""
-        examples = values.get("examples")
-        example_selector = values.get("example_selector")
-        if examples and example_selector:
-            msg = "Only one of 'examples' and 'example_selector' should be provided"
-            raise ValueError(msg)
-
-        if examples is None and example_selector is None:
-            msg = "One of 'examples' and 'example_selector' should be provided"
-            raise ValueError(msg)
-
-        return values
-
-    @model_validator(mode="after")
-    def template_is_valid(self) -> Self:
-        """Check that prefix, suffix, and input variables are consistent."""
-        if self.validate_template:
-            input_variables = self.input_variables
-            expected_input_variables = set(self.suffix.input_variables)
-            expected_input_variables |= set(self.partial_variables)
-            if self.prefix is not None:
-                expected_input_variables |= set(self.prefix.input_variables)
-            missing_vars = expected_input_variables.difference(input_variables)
-            if missing_vars:
-                msg = (
-                    f"Got input_variables={input_variables}, but based on "
-                    f"prefix/suffix expected {expected_input_variables}"
-                )
-                raise ValueError(msg)
-        else:
-            self.input_variables = sorted(
-                set(self.suffix.input_variables)
-                | set(self.prefix.input_variables if self.prefix else [])
-                - set(self.partial_variables)
-            )
-        return self
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-        extra="forbid",
-    )
-
-    def _get_examples(self, **kwargs: Any) -> list[dict]:
-        if self.examples is not None:
-            return self.examples
-        elif self.example_selector is not None:
-            return self.example_selector.select_examples(kwargs)
-        else:
-            raise ValueError
-
-    async def _aget_examples(self, **kwargs: Any) -> list[dict]:
-        if self.examples is not None:
-            return self.examples
-        elif self.example_selector is not None:
-            return await self.example_selector.aselect_examples(kwargs)
-        else:
-            raise ValueError
-
-    def format(self, **kwargs: Any) -> str:
-        """Format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-
-        Example:
-
-        .. code-block:: python
-
-            prompt.format(variable1="foo")
-        """
-        kwargs = self._merge_partial_and_user_variables(**kwargs)
-        # Get the examples to use.
-        examples = self._get_examples(**kwargs)
-        # Format the examples.
-        example_strings = [
-            self.example_prompt.format(**example) for example in examples
-        ]
-        # Create the overall prefix.
-        if self.prefix is None:
-            prefix = ""
-        else:
-            prefix_kwargs = {
-                k: v for k, v in kwargs.items() if k in self.prefix.input_variables
-            }
-            for k in prefix_kwargs:
-                kwargs.pop(k)
-            prefix = self.prefix.format(**prefix_kwargs)
-
-        # Create the overall suffix
-        suffix_kwargs = {
-            k: v for k, v in kwargs.items() if k in self.suffix.input_variables
-        }
-        for k in suffix_kwargs:
-            kwargs.pop(k)
-        suffix = self.suffix.format(
-            **suffix_kwargs,
-        )
-
-        pieces = [prefix, *example_strings, suffix]
-        template = self.example_separator.join([piece for piece in pieces if piece])
-        # Format the template with the input variables.
-        return DEFAULT_FORMATTER_MAPPING[self.template_format](template, **kwargs)
-
-    async def aformat(self, **kwargs: Any) -> str:
-        """Async format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-        """
-        kwargs = self._merge_partial_and_user_variables(**kwargs)
-        # Get the examples to use.
-        examples = await self._aget_examples(**kwargs)
-        # Format the examples.
-        example_strings = [
-            # We can use the sync method here as PromptTemplate doesn't block
-            self.example_prompt.format(**example)
-            for example in examples
-        ]
-        # Create the overall prefix.
-        if self.prefix is None:
-            prefix = ""
-        else:
-            prefix_kwargs = {
-                k: v for k, v in kwargs.items() if k in self.prefix.input_variables
-            }
-            for k in prefix_kwargs:
-                kwargs.pop(k)
-            prefix = await self.prefix.aformat(**prefix_kwargs)
-
-        # Create the overall suffix
-        suffix_kwargs = {
-            k: v for k, v in kwargs.items() if k in self.suffix.input_variables
-        }
-        for k in suffix_kwargs:
-            kwargs.pop(k)
-        suffix = await self.suffix.aformat(
-            **suffix_kwargs,
-        )
-
-        pieces = [prefix, *example_strings, suffix]
-        template = self.example_separator.join([piece for piece in pieces if piece])
-        # Format the template with the input variables.
-        return DEFAULT_FORMATTER_MAPPING[self.template_format](template, **kwargs)
-
-    @property
-    def _prompt_type(self) -> str:
-        """Return the prompt type key."""
-        return "few_shot_with_templates"
-
-    def save(self, file_path: Union[Path, str]) -> None:
-        """Save the prompt to a file.
-
-        Args:
-            file_path: The path to save the prompt to.
-
-        Raises:
-            ValueError: If example_selector is provided.
-        """
-        if self.example_selector:
-            msg = "Saving an example selector is not currently supported"
-            raise ValueError(msg)
-        return super().save(file_path)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/prompts/image.py ./custom_langchain_core/prompts/image.py
--- .venv/lib/python3.12/site-packages/langchain_core/prompts/image.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/prompts/image.py	1970-01-01 09:00:00
@@ -1,143 +0,0 @@
-from typing import Any
-
-from pydantic import Field
-
-from langchain_core.prompt_values import ImagePromptValue, ImageURL, PromptValue
-from langchain_core.prompts.base import BasePromptTemplate
-from langchain_core.prompts.string import (
-    DEFAULT_FORMATTER_MAPPING,
-    PromptTemplateFormat,
-)
-from langchain_core.runnables import run_in_executor
-
-
-class ImagePromptTemplate(BasePromptTemplate[ImageURL]):
-    """Image prompt template for a multimodal model."""
-
-    template: dict = Field(default_factory=dict)
-    """Template for the prompt."""
-    template_format: PromptTemplateFormat = "f-string"
-    """The format of the prompt template.
-    Options are: 'f-string', 'mustache', 'jinja2'."""
-
-    def __init__(self, **kwargs: Any) -> None:
-        if "input_variables" not in kwargs:
-            kwargs["input_variables"] = []
-
-        overlap = set(kwargs["input_variables"]) & {"url", "path", "detail"}
-        if overlap:
-            msg = (
-                "input_variables for the image template cannot contain"
-                " any of 'url', 'path', or 'detail'."
-                f" Found: {overlap}"
-            )
-            raise ValueError(msg)
-        super().__init__(**kwargs)
-
-    @property
-    def _prompt_type(self) -> str:
-        """Return the prompt type key."""
-        return "image-prompt"
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "image"]
-
-    def format_prompt(self, **kwargs: Any) -> PromptValue:
-        """Format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-        """
-        return ImagePromptValue(image_url=self.format(**kwargs))
-
-    async def aformat_prompt(self, **kwargs: Any) -> PromptValue:
-        """Async format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-        """
-        return ImagePromptValue(image_url=await self.aformat(**kwargs))
-
-    def format(
-        self,
-        **kwargs: Any,
-    ) -> ImageURL:
-        """Format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-
-        Raises:
-            ValueError: If the url is not provided.
-            ValueError: If the url is not a string.
-
-        Example:
-
-            .. code-block:: python
-
-                prompt.format(variable1="foo")
-        """
-        formatted = {}
-        for k, v in self.template.items():
-            if isinstance(v, str):
-                formatted[k] = DEFAULT_FORMATTER_MAPPING[self.template_format](
-                    v, **kwargs
-                )
-            else:
-                formatted[k] = v
-        url = kwargs.get("url") or formatted.get("url")
-        if kwargs.get("path") or formatted.get("path"):
-            msg = (
-                "Loading images from 'path' has been removed as of 0.3.15 for security "
-                "reasons. Please specify images by 'url'."
-            )
-            raise ValueError(msg)
-        detail = kwargs.get("detail") or formatted.get("detail")
-        if not url:
-            msg = "Must provide url."
-            raise ValueError(msg)
-        elif not isinstance(url, str):
-            msg = "url must be a string."
-            raise ValueError(msg)
-        else:
-            output: ImageURL = {"url": url}
-            if detail:
-                # Don't check literal values here: let the API check them
-                output["detail"] = detail  # type: ignore[typeddict-item]
-        return output
-
-    async def aformat(self, **kwargs: Any) -> ImageURL:
-        """Async format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-
-        Raises:
-            ValueError: If the path or url is not a string.
-        """
-        return await run_in_executor(None, self.format, **kwargs)
-
-    def pretty_repr(self, html: bool = False) -> str:
-        """Return a pretty representation of the prompt.
-
-        Args:
-            html: Whether to return an html formatted string.
-
-        Returns:
-            A pretty representation of the prompt.
-        """
-        raise NotImplementedError
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/prompts/loading.py ./custom_langchain_core/prompts/loading.py
--- .venv/lib/python3.12/site-packages/langchain_core/prompts/loading.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/prompts/loading.py	1970-01-01 09:00:00
@@ -1,203 +0,0 @@
-"""Load prompts."""
-
-import json
-import logging
-from pathlib import Path
-from typing import Callable, Optional, Union
-
-import yaml
-
-from langchain_core.output_parsers.string import StrOutputParser
-from langchain_core.prompts.base import BasePromptTemplate
-from langchain_core.prompts.chat import ChatPromptTemplate
-from langchain_core.prompts.few_shot import FewShotPromptTemplate
-from langchain_core.prompts.prompt import PromptTemplate
-
-URL_BASE = "https://raw.githubusercontent.com/hwchase17/langchain-hub/master/prompts/"
-logger = logging.getLogger(__name__)
-
-
-def load_prompt_from_config(config: dict) -> BasePromptTemplate:
-    """Load prompt from Config Dict.
-
-    Args:
-        config: Dict containing the prompt configuration.
-
-    Returns:
-        A PromptTemplate object.
-
-    Raises:
-        ValueError: If the prompt type is not supported.
-    """
-    if "_type" not in config:
-        logger.warning("No `_type` key found, defaulting to `prompt`.")
-    config_type = config.pop("_type", "prompt")
-
-    if config_type not in type_to_loader_dict:
-        msg = f"Loading {config_type} prompt not supported"
-        raise ValueError(msg)
-
-    prompt_loader = type_to_loader_dict[config_type]
-    return prompt_loader(config)
-
-
-def _load_template(var_name: str, config: dict) -> dict:
-    """Load template from the path if applicable."""
-    # Check if template_path exists in config.
-    if f"{var_name}_path" in config:
-        # If it does, make sure template variable doesn't also exist.
-        if var_name in config:
-            msg = f"Both `{var_name}_path` and `{var_name}` cannot be provided."
-            raise ValueError(msg)
-        # Pop the template path from the config.
-        template_path = Path(config.pop(f"{var_name}_path"))
-        # Load the template.
-        if template_path.suffix == ".txt":
-            with open(template_path) as f:
-                template = f.read()
-        else:
-            raise ValueError
-        # Set the template variable to the extracted variable.
-        config[var_name] = template
-    return config
-
-
-def _load_examples(config: dict) -> dict:
-    """Load examples if necessary."""
-    if isinstance(config["examples"], list):
-        pass
-    elif isinstance(config["examples"], str):
-        with open(config["examples"]) as f:
-            if config["examples"].endswith(".json"):
-                examples = json.load(f)
-            elif config["examples"].endswith((".yaml", ".yml")):
-                examples = yaml.safe_load(f)
-            else:
-                msg = "Invalid file format. Only json or yaml formats are supported."
-                raise ValueError(msg)
-        config["examples"] = examples
-    else:
-        msg = "Invalid examples format. Only list or string are supported."
-        raise ValueError(msg)  # noqa:TRY004
-    return config
-
-
-def _load_output_parser(config: dict) -> dict:
-    """Load output parser."""
-    if "output_parser" in config and config["output_parser"]:
-        _config = config.pop("output_parser")
-        output_parser_type = _config.pop("_type")
-        if output_parser_type == "default":
-            output_parser = StrOutputParser(**_config)
-        else:
-            msg = f"Unsupported output parser {output_parser_type}"
-            raise ValueError(msg)
-        config["output_parser"] = output_parser
-    return config
-
-
-def _load_few_shot_prompt(config: dict) -> FewShotPromptTemplate:
-    """Load the "few shot" prompt from the config."""
-    # Load the suffix and prefix templates.
-    config = _load_template("suffix", config)
-    config = _load_template("prefix", config)
-    # Load the example prompt.
-    if "example_prompt_path" in config:
-        if "example_prompt" in config:
-            msg = (
-                "Only one of example_prompt and example_prompt_path should "
-                "be specified."
-            )
-            raise ValueError(msg)
-        config["example_prompt"] = load_prompt(config.pop("example_prompt_path"))
-    else:
-        config["example_prompt"] = load_prompt_from_config(config["example_prompt"])
-    # Load the examples.
-    config = _load_examples(config)
-    config = _load_output_parser(config)
-    return FewShotPromptTemplate(**config)
-
-
-def _load_prompt(config: dict) -> PromptTemplate:
-    """Load the prompt template from config."""
-    # Load the template from disk if necessary.
-    config = _load_template("template", config)
-    config = _load_output_parser(config)
-
-    template_format = config.get("template_format", "f-string")
-    if template_format == "jinja2":
-        # Disabled due to:
-        # https://github.com/langchain-ai/langchain/issues/4394
-        msg = (
-            f"Loading templates with '{template_format}' format is no longer supported "
-            f"since it can lead to arbitrary code execution. Please migrate to using "
-            f"the 'f-string' template format, which does not suffer from this issue."
-        )
-        raise ValueError(msg)
-
-    return PromptTemplate(**config)
-
-
-def load_prompt(
-    path: Union[str, Path], encoding: Optional[str] = None
-) -> BasePromptTemplate:
-    """Unified method for loading a prompt from LangChainHub or local fs.
-
-    Args:
-        path: Path to the prompt file.
-        encoding: Encoding of the file. Defaults to None.
-
-    Returns:
-        A PromptTemplate object.
-
-    Raises:
-        RuntimeError: If the path is a Lang Chain Hub path.
-    """
-    if isinstance(path, str) and path.startswith("lc://"):
-        msg = (
-            "Loading from the deprecated github-based Hub is no longer supported. "
-            "Please use the new LangChain Hub at https://smith.langchain.com/hub "
-            "instead."
-        )
-        raise RuntimeError(msg)
-    return _load_prompt_from_file(path, encoding)
-
-
-def _load_prompt_from_file(
-    file: Union[str, Path], encoding: Optional[str] = None
-) -> BasePromptTemplate:
-    """Load prompt from file."""
-    # Convert file to a Path object.
-    file_path = Path(file) if isinstance(file, str) else file
-    # Load from either json or yaml.
-    if file_path.suffix == ".json":
-        with open(file_path, encoding=encoding) as f:
-            config = json.load(f)
-    elif file_path.suffix.endswith((".yaml", ".yml")):
-        with open(file_path, encoding=encoding) as f:
-            config = yaml.safe_load(f)
-    else:
-        msg = f"Got unsupported file type {file_path.suffix}"
-        raise ValueError(msg)
-    # Load the prompt from the config now.
-    return load_prompt_from_config(config)
-
-
-def _load_chat_prompt(config: dict) -> ChatPromptTemplate:
-    """Load chat prompt from config."""
-    messages = config.pop("messages")
-    template = messages[0]["prompt"].pop("template") if messages else None
-    config.pop("input_variables")
-
-    if not template:
-        msg = "Can't load chat prompt without template"
-        raise ValueError(msg)
-
-    return ChatPromptTemplate.from_template(template=template, **config)
-
-
-type_to_loader_dict: dict[str, Callable[[dict], BasePromptTemplate]] = {
-    "prompt": _load_prompt,
-    "few_shot": _load_few_shot_prompt,
-    "chat": _load_chat_prompt,
-}
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/prompts/pipeline.py ./custom_langchain_core/prompts/pipeline.py
--- .venv/lib/python3.12/site-packages/langchain_core/prompts/pipeline.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/prompts/pipeline.py	1970-01-01 09:00:00
@@ -1,134 +0,0 @@
-from typing import Any
-from typing import Optional as Optional
-
-from pydantic import model_validator
-
-from langchain_core._api.deprecation import deprecated
-from langchain_core.prompt_values import PromptValue
-from langchain_core.prompts.base import BasePromptTemplate
-from langchain_core.prompts.chat import BaseChatPromptTemplate
-
-
-def _get_inputs(inputs: dict, input_variables: list[str]) -> dict:
-    return {k: inputs[k] for k in input_variables}
-
-
-@deprecated(
-    since="0.3.22",
-    removal="1.0",
-    message=(
-        "This class is deprecated. Please see the docstring below or at the link"
-        " for a replacement option: "
-        "https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.pipeline.PipelinePromptTemplate.html"
-    ),
-)
-class PipelinePromptTemplate(BasePromptTemplate):
-    """This has been deprecated in favor of chaining individual prompts together in your
-    code. E.g. using a for loop, you could do:
-
-    .. code-block:: python
-
-        my_input = {"key": "value"}
-        for name, prompt in pipeline_prompts:
-            my_input[name] = prompt.invoke(my_input).to_string()
-        my_output = final_prompt.invoke(my_input)
-
-    Prompt template for composing multiple prompt templates together.
-
-    This can be useful when you want to reuse parts of prompts.
-
-    A PipelinePrompt consists of two main parts:
-        - final_prompt: This is the final prompt that is returned
-        - pipeline_prompts: This is a list of tuples, consisting
-          of a string (`name`) and a Prompt Template.
-          Each PromptTemplate will be formatted and then passed
-          to future prompt templates as a variable with
-          the same name as `name`
-    """
-
-    final_prompt: BasePromptTemplate
-    """The final prompt that is returned."""
-    pipeline_prompts: list[tuple[str, BasePromptTemplate]]
-    """A list of tuples, consisting of a string (`name`) and a Prompt Template."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "pipeline"]
-
-    @model_validator(mode="before")
-    @classmethod
-    def get_input_variables(cls, values: dict) -> Any:
-        """Get input variables."""
-        created_variables = set()
-        all_variables = set()
-        for k, prompt in values["pipeline_prompts"]:
-            created_variables.add(k)
-            all_variables.update(prompt.input_variables)
-        values["input_variables"] = list(all_variables.difference(created_variables))
-        return values
-
-    def format_prompt(self, **kwargs: Any) -> PromptValue:
-        """Format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-        """
-        for k, prompt in self.pipeline_prompts:
-            _inputs = _get_inputs(kwargs, prompt.input_variables)
-            if isinstance(prompt, BaseChatPromptTemplate):
-                kwargs[k] = prompt.format_messages(**_inputs)
-            else:
-                kwargs[k] = prompt.format(**_inputs)
-        _inputs = _get_inputs(kwargs, self.final_prompt.input_variables)
-        return self.final_prompt.format_prompt(**_inputs)
-
-    async def aformat_prompt(self, **kwargs: Any) -> PromptValue:
-        """Async format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-        """
-        for k, prompt in self.pipeline_prompts:
-            _inputs = _get_inputs(kwargs, prompt.input_variables)
-            if isinstance(prompt, BaseChatPromptTemplate):
-                kwargs[k] = await prompt.aformat_messages(**_inputs)
-            else:
-                kwargs[k] = await prompt.aformat(**_inputs)
-        _inputs = _get_inputs(kwargs, self.final_prompt.input_variables)
-        return await self.final_prompt.aformat_prompt(**_inputs)
-
-    def format(self, **kwargs: Any) -> str:
-        """Format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-        """
-        return self.format_prompt(**kwargs).to_string()
-
-    async def aformat(self, **kwargs: Any) -> str:
-        """Async format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-        """
-        return (await self.aformat_prompt(**kwargs)).to_string()
-
-    @property
-    def _prompt_type(self) -> str:
-        raise ValueError
-
-
-PipelinePromptTemplate.model_rebuild()
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/prompts/prompt.py ./custom_langchain_core/prompts/prompt.py
--- .venv/lib/python3.12/site-packages/langchain_core/prompts/prompt.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/prompts/prompt.py	1970-01-01 09:00:00
@@ -1,301 +0,0 @@
-"""Prompt schema definition."""
-
-from __future__ import annotations
-
-import warnings
-from pathlib import Path
-from typing import Any, Optional, Union
-
-from pydantic import BaseModel, model_validator
-
-from langchain_core.prompts.string import (
-    DEFAULT_FORMATTER_MAPPING,
-    PromptTemplateFormat,
-    StringPromptTemplate,
-    check_valid_template,
-    get_template_variables,
-    mustache_schema,
-)
-from langchain_core.runnables.config import RunnableConfig
-
-
-class PromptTemplate(StringPromptTemplate):
-    """Prompt template for a language model.
-
-    A prompt template consists of a string template. It accepts a set of parameters
-    from the user that can be used to generate a prompt for a language model.
-
-    The template can be formatted using either f-strings (default), jinja2,
-    or mustache syntax.
-
-    *Security warning*:
-        Prefer using `template_format="f-string"` instead of
-        `template_format="jinja2"`, or make sure to NEVER accept jinja2 templates
-        from untrusted sources as they may lead to arbitrary Python code execution.
-
-        As of LangChain 0.0.329, Jinja2 templates will be rendered using
-        Jinja2's SandboxedEnvironment by default. This sand-boxing should
-        be treated as a best-effort approach rather than a guarantee of security,
-        as it is an opt-out rather than opt-in approach.
-
-        Despite the sand-boxing, we recommend to never use jinja2 templates
-        from untrusted sources.
-
-    Example:
-
-        .. code-block:: python
-
-            from langchain_core.prompts import PromptTemplate
-
-            # Instantiation using from_template (recommended)
-            prompt = PromptTemplate.from_template("Say {foo}")
-            prompt.format(foo="bar")
-
-            # Instantiation using initializer
-            prompt = PromptTemplate(template="Say {foo}")
-    """
-
-    @property
-    def lc_attributes(self) -> dict[str, Any]:
-        return {
-            "template_format": self.template_format,
-        }
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "prompt"]
-
-    template: str
-    """The prompt template."""
-
-    template_format: PromptTemplateFormat = "f-string"
-    """The format of the prompt template.
-    Options are: 'f-string', 'mustache', 'jinja2'."""
-
-    validate_template: bool = False
-    """Whether or not to try validating the template."""
-
-    @model_validator(mode="before")
-    @classmethod
-    def pre_init_validation(cls, values: dict) -> Any:
-        """Check that template and input variables are consistent."""
-        if values.get("template") is None:
-            # Will let pydantic fail with a ValidationError if template
-            # is not provided.
-            return values
-
-        # Set some default values based on the field defaults
-        values.setdefault("template_format", "f-string")
-        values.setdefault("partial_variables", {})
-
-        if values.get("validate_template"):
-            if values["template_format"] == "mustache":
-                msg = "Mustache templates cannot be validated."
-                raise ValueError(msg)
-
-            if "input_variables" not in values:
-                msg = "Input variables must be provided to validate the template."
-                raise ValueError(msg)
-
-            all_inputs = values["input_variables"] + list(values["partial_variables"])
-            check_valid_template(
-                values["template"], values["template_format"], all_inputs
-            )
-
-        if values["template_format"]:
-            values["input_variables"] = [
-                var
-                for var in get_template_variables(
-                    values["template"], values["template_format"]
-                )
-                if var not in values["partial_variables"]
-            ]
-
-        return values
-
-    def get_input_schema(self, config: RunnableConfig | None = None) -> type[BaseModel]:
-        """Get the input schema for the prompt.
-
-        Args:
-            config: The runnable configuration.
-
-        Returns:
-            The input schema for the prompt.
-        """
-        if self.template_format != "mustache":
-            return super().get_input_schema(config)
-
-        return mustache_schema(self.template)
-
-    def __add__(self, other: Any) -> PromptTemplate:
-        """Override the + operator to allow for combining prompt templates."""
-        # Allow for easy combining
-        if isinstance(other, PromptTemplate):
-            if self.template_format != "f-string":
-                msg = "Adding prompt templates only supported for f-strings."
-                raise ValueError(msg)
-            if other.template_format != "f-string":
-                msg = "Adding prompt templates only supported for f-strings."
-                raise ValueError(msg)
-            input_variables = list(
-                set(self.input_variables) | set(other.input_variables)
-            )
-            template = self.template + other.template
-            # If any do not want to validate, then don't
-            validate_template = self.validate_template and other.validate_template
-            partial_variables = dict(self.partial_variables.items())
-            for k, v in other.partial_variables.items():
-                if k in partial_variables:
-                    msg = "Cannot have same variable partialed twice."
-                    raise ValueError(msg)
-                else:
-                    partial_variables[k] = v
-            return PromptTemplate(
-                template=template,
-                input_variables=input_variables,
-                partial_variables=partial_variables,
-                template_format="f-string",
-                validate_template=validate_template,
-            )
-        elif isinstance(other, str):
-            prompt = PromptTemplate.from_template(other)
-            return self + prompt
-        else:
-            msg = f"Unsupported operand type for +: {type(other)}"
-            raise NotImplementedError(msg)
-
-    @property
-    def _prompt_type(self) -> str:
-        """Return the prompt type key."""
-        return "prompt"
-
-    def format(self, **kwargs: Any) -> str:
-        """Format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-        """
-        kwargs = self._merge_partial_and_user_variables(**kwargs)
-        return DEFAULT_FORMATTER_MAPPING[self.template_format](self.template, **kwargs)
-
-    @classmethod
-    def from_examples(
-        cls,
-        examples: list[str],
-        suffix: str,
-        input_variables: list[str],
-        example_separator: str = "\n\n",
-        prefix: str = "",
-        **kwargs: Any,
-    ) -> PromptTemplate:
-        """Take examples in list format with prefix and suffix to create a prompt.
-
-        Intended to be used as a way to dynamically create a prompt from examples.
-
-        Args:
-            examples: List of examples to use in the prompt.
-            suffix: String to go after the list of examples. Should generally
-                set up the user's input.
-            input_variables: A list of variable names the final prompt template
-                will expect.
-            example_separator: The separator to use in between examples. Defaults
-                to two new line characters.
-            prefix: String that should go before any examples. Generally includes
-                examples. Default to an empty string.
-
-        Returns:
-            The final prompt generated.
-        """
-        template = example_separator.join([prefix, *examples, suffix])
-        return cls(input_variables=input_variables, template=template, **kwargs)
-
-    @classmethod
-    def from_file(
-        cls,
-        template_file: Union[str, Path],
-        input_variables: Optional[list[str]] = None,
-        encoding: Optional[str] = None,
-        **kwargs: Any,
-    ) -> PromptTemplate:
-        """Load a prompt from a file.
-
-        Args:
-            template_file: The path to the file containing the prompt template.
-            input_variables: [DEPRECATED] A list of variable names the final prompt
-                template will expect. Defaults to None.
-            encoding: The encoding system for opening the template file.
-                If not provided, will use the OS default.
-
-        input_variables is ignored as from_file now delegates to from_template().
-
-        Returns:
-            The prompt loaded from the file.
-        """
-        with open(str(template_file), encoding=encoding) as f:
-            template = f.read()
-        if input_variables:
-            warnings.warn(
-                "`input_variables' is deprecated and ignored.",
-                DeprecationWarning,
-                stacklevel=2,
-            )
-        return cls.from_template(template=template, **kwargs)
-
-    @classmethod
-    def from_template(
-        cls,
-        template: str,
-        *,
-        template_format: PromptTemplateFormat = "f-string",
-        partial_variables: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> PromptTemplate:
-        """Load a prompt template from a template.
-
-        *Security warning*:
-            Prefer using `template_format="f-string"` instead of
-            `template_format="jinja2"`, or make sure to NEVER accept jinja2 templates
-            from untrusted sources as they may lead to arbitrary Python code execution.
-
-            As of LangChain 0.0.329, Jinja2 templates will be rendered using
-            Jinja2's SandboxedEnvironment by default. This sand-boxing should
-            be treated as a best-effort approach rather than a guarantee of security,
-            as it is an opt-out rather than opt-in approach.
-
-            Despite the sand-boxing, we recommend never using jinja2 templates
-            from untrusted sources.
-
-        Args:
-            template: The template to load.
-            template_format: The format of the template. Use `jinja2` for jinja2,
-                             `mustache` for mustache, and `f-string` for f-strings.
-                             Defaults to `f-string`.
-            partial_variables: A dictionary of variables that can be used to partially
-                               fill in the template. For example, if the template is
-                              `"{variable1} {variable2}"`, and `partial_variables` is
-                              `{"variable1": "foo"}`, then the final prompt will be
-                              `"foo {variable2}"`. Defaults to None.
-            kwargs: Any other arguments to pass to the prompt template.
-
-        Returns:
-            The prompt template loaded from the template.
-        """
-        input_variables = get_template_variables(template, template_format)
-        _partial_variables = partial_variables or {}
-
-        if _partial_variables:
-            input_variables = [
-                var for var in input_variables if var not in _partial_variables
-            ]
-
-        return cls(
-            input_variables=input_variables,
-            template=template,
-            template_format=template_format,  # type: ignore[arg-type]
-            partial_variables=_partial_variables,
-            **kwargs,
-        )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/prompts/string.py ./custom_langchain_core/prompts/string.py
--- .venv/lib/python3.12/site-packages/langchain_core/prompts/string.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/prompts/string.py	1970-01-01 09:00:00
@@ -1,319 +0,0 @@
-"""BasePrompt schema definition."""
-
-from __future__ import annotations
-
-import warnings
-from abc import ABC
-from string import Formatter
-from typing import Any, Callable, Literal
-
-from pydantic import BaseModel, create_model
-
-import langchain_core.utils.mustache as mustache
-from langchain_core.prompt_values import PromptValue, StringPromptValue
-from langchain_core.prompts.base import BasePromptTemplate
-from langchain_core.utils import get_colored_text
-from langchain_core.utils.formatting import formatter
-from langchain_core.utils.interactive_env import is_interactive_env
-
-PromptTemplateFormat = Literal["f-string", "mustache", "jinja2"]
-
-
-def jinja2_formatter(template: str, /, **kwargs: Any) -> str:
-    """Format a template using jinja2.
-
-    *Security warning*:
-        As of LangChain 0.0.329, this method uses Jinja2's
-        SandboxedEnvironment by default. However, this sand-boxing should
-        be treated as a best-effort approach rather than a guarantee of security.
-        Do not accept jinja2 templates from untrusted sources as they may lead
-        to arbitrary Python code execution.
-
-        https://jinja.palletsprojects.com/en/3.1.x/sandbox/
-
-    Args:
-        template: The template string.
-        **kwargs: The variables to format the template with.
-
-    Returns:
-        The formatted string.
-
-    Raises:
-        ImportError: If jinja2 is not installed.
-    """
-    try:
-        from jinja2.sandbox import SandboxedEnvironment
-    except ImportError as e:
-        msg = (
-            "jinja2 not installed, which is needed to use the jinja2_formatter. "
-            "Please install it with `pip install jinja2`."
-            "Please be cautious when using jinja2 templates. "
-            "Do not expand jinja2 templates using unverified or user-controlled "
-            "inputs as that can result in arbitrary Python code execution."
-        )
-        raise ImportError(msg) from e
-
-    # This uses a sandboxed environment to prevent arbitrary code execution.
-    # Jinja2 uses an opt-out rather than opt-in approach for sand-boxing.
-    # Please treat this sand-boxing as a best-effort approach rather than
-    # a guarantee of security.
-    # We recommend to never use jinja2 templates with untrusted inputs.
-    # https://jinja.palletsprojects.com/en/3.1.x/sandbox/
-    # approach not a guarantee of security.
-    return SandboxedEnvironment().from_string(template).render(**kwargs)
-
-
-def validate_jinja2(template: str, input_variables: list[str]) -> None:
-    """Validate that the input variables are valid for the template.
-    Issues a warning if missing or extra variables are found.
-
-    Args:
-        template: The template string.
-        input_variables: The input variables.
-    """
-    input_variables_set = set(input_variables)
-    valid_variables = _get_jinja2_variables_from_template(template)
-    missing_variables = valid_variables - input_variables_set
-    extra_variables = input_variables_set - valid_variables
-
-    warning_message = ""
-    if missing_variables:
-        warning_message += f"Missing variables: {missing_variables} "
-
-    if extra_variables:
-        warning_message += f"Extra variables: {extra_variables}"
-
-    if warning_message:
-        warnings.warn(warning_message.strip(), stacklevel=7)
-
-
-def _get_jinja2_variables_from_template(template: str) -> set[str]:
-    try:
-        from jinja2 import Environment, meta
-    except ImportError as e:
-        msg = (
-            "jinja2 not installed, which is needed to use the jinja2_formatter. "
-            "Please install it with `pip install jinja2`."
-        )
-        raise ImportError(msg) from e
-    # noqa for insecure warning elsewhere
-    env = Environment()  # noqa: S701
-    ast = env.parse(template)
-    variables = meta.find_undeclared_variables(ast)
-    return variables
-
-
-def mustache_formatter(template: str, /, **kwargs: Any) -> str:
-    """Format a template using mustache.
-
-    Args:
-        template: The template string.
-        **kwargs: The variables to format the template with.
-
-    Returns:
-        The formatted string.
-    """
-    return mustache.render(template, kwargs)
-
-
-def mustache_template_vars(
-    template: str,
-) -> set[str]:
-    """Get the variables from a mustache template.
-
-    Args:
-        template: The template string.
-
-    Returns:
-        The variables from the template.
-    """
-    vars: set[str] = set()
-    section_depth = 0
-    for type, key in mustache.tokenize(template):
-        if type == "end":
-            section_depth -= 1
-        elif (
-            type in ("variable", "section", "inverted section", "no escape")
-            and key != "."
-            and section_depth == 0
-        ):
-            vars.add(key.split(".")[0])
-        if type in ("section", "inverted section"):
-            section_depth += 1
-    return vars
-
-
-Defs = dict[str, "Defs"]
-
-
-def mustache_schema(
-    template: str,
-) -> type[BaseModel]:
-    """Get the variables from a mustache template.
-
-    Args:
-        template: The template string.
-
-    Returns:
-        The variables from the template as a Pydantic model.
-    """
-    fields = {}
-    prefix: tuple[str, ...] = ()
-    section_stack: list[tuple[str, ...]] = []
-    for type, key in mustache.tokenize(template):
-        if key == ".":
-            continue
-        if type == "end":
-            if section_stack:
-                prefix = section_stack.pop()
-        elif type in ("section", "inverted section"):
-            section_stack.append(prefix)
-            prefix = prefix + tuple(key.split("."))
-            fields[prefix] = False
-        elif type in ("variable", "no escape"):
-            fields[prefix + tuple(key.split("."))] = True
-    defs: Defs = {}  # None means leaf node
-    while fields:
-        field, is_leaf = fields.popitem()
-        current = defs
-        for part in field[:-1]:
-            current = current.setdefault(part, {})
-        current.setdefault(field[-1], "" if is_leaf else {})  # type: ignore[arg-type]
-    return _create_model_recursive("PromptInput", defs)
-
-
-def _create_model_recursive(name: str, defs: Defs) -> type:
-    return create_model(  # type: ignore[call-overload]
-        name,
-        **{
-            k: (_create_model_recursive(k, v), None) if v else (type(v), None)
-            for k, v in defs.items()
-        },
-    )
-
-
-DEFAULT_FORMATTER_MAPPING: dict[str, Callable] = {
-    "f-string": formatter.format,
-    "mustache": mustache_formatter,
-    "jinja2": jinja2_formatter,
-}
-
-DEFAULT_VALIDATOR_MAPPING: dict[str, Callable] = {
-    "f-string": formatter.validate_input_variables,
-    "jinja2": validate_jinja2,
-}
-
-
-def check_valid_template(
-    template: str, template_format: str, input_variables: list[str]
-) -> None:
-    """Check that template string is valid.
-
-    Args:
-        template: The template string.
-        template_format: The template format. Should be one of "f-string" or "jinja2".
-        input_variables: The input variables.
-
-    Raises:
-        ValueError: If the template format is not supported.
-        ValueError: If the prompt schema is invalid.
-    """
-    try:
-        validator_func = DEFAULT_VALIDATOR_MAPPING[template_format]
-    except KeyError as exc:
-        msg = (
-            f"Invalid template format {template_format!r}, should be one of"
-            f" {list(DEFAULT_FORMATTER_MAPPING)}."
-        )
-        raise ValueError(msg) from exc
-    try:
-        validator_func(template, input_variables)
-    except (KeyError, IndexError) as exc:
-        msg = (
-            "Invalid prompt schema; check for mismatched or missing input parameters"
-            f" from {input_variables}."
-        )
-        raise ValueError(msg) from exc
-
-
-def get_template_variables(template: str, template_format: str) -> list[str]:
-    """Get the variables from the template.
-
-    Args:
-        template: The template string.
-        template_format: The template format. Should be one of "f-string" or "jinja2".
-
-    Returns:
-        The variables from the template.
-
-    Raises:
-        ValueError: If the template format is not supported.
-    """
-    if template_format == "jinja2":
-        # Get the variables for the template
-        input_variables = _get_jinja2_variables_from_template(template)
-    elif template_format == "f-string":
-        input_variables = {
-            v for _, v, _, _ in Formatter().parse(template) if v is not None
-        }
-    elif template_format == "mustache":
-        input_variables = mustache_template_vars(template)
-    else:
-        msg = f"Unsupported template format: {template_format}"
-        raise ValueError(msg)
-
-    return sorted(input_variables)
-
-
-class StringPromptTemplate(BasePromptTemplate, ABC):
-    """String prompt that exposes the format method, returning a prompt."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "base"]
-
-    def format_prompt(self, **kwargs: Any) -> PromptValue:
-        """Format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-        """
-        return StringPromptValue(text=self.format(**kwargs))
-
-    async def aformat_prompt(self, **kwargs: Any) -> PromptValue:
-        """Async format the prompt with the inputs.
-
-        Args:
-            kwargs: Any arguments to be passed to the prompt template.
-
-        Returns:
-            A formatted string.
-        """
-        return StringPromptValue(text=await self.aformat(**kwargs))
-
-    def pretty_repr(self, html: bool = False) -> str:
-        """Get a pretty representation of the prompt.
-
-        Args:
-            html: Whether to return an HTML-formatted string.
-
-        Returns:
-            A pretty representation of the prompt.
-        """
-        # TODO: handle partials
-        dummy_vars = {
-            input_var: "{" + f"{input_var}" + "}" for input_var in self.input_variables
-        }
-        if html:
-            dummy_vars = {
-                k: get_colored_text(v, "yellow") for k, v in dummy_vars.items()
-            }
-        return self.format(**dummy_vars)
-
-    def pretty_print(self) -> None:
-        """Print a pretty representation of the prompt."""
-        print(self.pretty_repr(html=is_interactive_env()))  # noqa: T201
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/prompts/structured.py ./custom_langchain_core/prompts/structured.py
--- .venv/lib/python3.12/site-packages/langchain_core/prompts/structured.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/prompts/structured.py	1970-01-01 09:00:00
@@ -1,159 +0,0 @@
-from collections.abc import Iterator, Mapping, Sequence
-from typing import (
-    Any,
-    Callable,
-    Optional,
-    Union,
-)
-
-from pydantic import BaseModel, Field
-
-from langchain_core._api.beta_decorator import beta
-from langchain_core.language_models.base import BaseLanguageModel
-from langchain_core.prompts.chat import (
-    ChatPromptTemplate,
-    MessageLikeRepresentation,
-)
-from langchain_core.prompts.string import PromptTemplateFormat
-from langchain_core.runnables.base import (
-    Other,
-    Runnable,
-    RunnableSequence,
-    RunnableSerializable,
-)
-from langchain_core.utils import get_pydantic_field_names
-
-
-@beta()
-class StructuredPrompt(ChatPromptTemplate):
-    """Structured prompt template for a language model."""
-
-    schema_: Union[dict, type]
-    """Schema for the structured prompt."""
-    structured_output_kwargs: dict[str, Any] = Field(default_factory=dict)
-
-    def __init__(
-        self,
-        messages: Sequence[MessageLikeRepresentation],
-        schema_: Optional[Union[dict, type[BaseModel]]] = None,
-        *,
-        structured_output_kwargs: Optional[dict[str, Any]] = None,
-        template_format: PromptTemplateFormat = "f-string",
-        **kwargs: Any,
-    ) -> None:
-        schema_ = schema_ or kwargs.pop("schema")
-        structured_output_kwargs = structured_output_kwargs or {}
-        for k in set(kwargs).difference(get_pydantic_field_names(self.__class__)):
-            structured_output_kwargs[k] = kwargs.pop(k)
-        super().__init__(
-            messages=messages,
-            schema_=schema_,
-            structured_output_kwargs=structured_output_kwargs,
-            template_format=template_format,
-            **kwargs,
-        )
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object.
-
-        For example, if the class is `langchain.llms.openai.OpenAI`, then the
-        namespace is ["langchain", "llms", "openai"]
-        """
-        return cls.__module__.split(".")
-
-    @classmethod
-    def from_messages_and_schema(
-        cls,
-        messages: Sequence[MessageLikeRepresentation],
-        schema: Union[dict, type],
-        **kwargs: Any,
-    ) -> ChatPromptTemplate:
-        """Create a chat prompt template from a variety of message formats.
-
-        Examples:
-            Instantiation from a list of message templates:
-
-            .. code-block:: python
-
-                from langchain_core.prompts import StructuredPrompt
-
-                class OutputSchema(BaseModel):
-                    name: str
-                    value: int
-
-                template = StructuredPrompt(
-                    [
-                        ("human", "Hello, how are you?"),
-                        ("ai", "I'm doing well, thanks!"),
-                        ("human", "That's good to hear."),
-                    ],
-                    OutputSchema,
-                )
-
-        Args:
-            messages: sequence of message representations.
-                  A message can be represented using the following formats:
-                  (1) BaseMessagePromptTemplate, (2) BaseMessage, (3) 2-tuple of
-                  (message type, template); e.g., ("human", "{user_input}"),
-                  (4) 2-tuple of (message class, template), (5) a string which is
-                  shorthand for ("human", template); e.g., "{user_input}"
-            schema: a dictionary representation of function call, or a Pydantic model.
-            kwargs: Any additional kwargs to pass through to
-                ``ChatModel.with_structured_output(schema, **kwargs)``.
-
-        Returns:
-            a structured prompt template
-        """
-        return cls(messages, schema, **kwargs)
-
-    def __or__(
-        self,
-        other: Union[
-            Runnable[Any, Other],
-            Callable[[Any], Other],
-            Callable[[Iterator[Any]], Iterator[Other]],
-            Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],
-        ],
-    ) -> RunnableSerializable[dict, Other]:
-        return self.pipe(other)
-
-    def pipe(
-        self,
-        *others: Union[
-            Runnable[Any, Other],
-            Callable[[Any], Other],
-            Callable[[Iterator[Any]], Iterator[Other]],
-            Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],
-        ],
-        name: Optional[str] = None,
-    ) -> RunnableSerializable[dict, Other]:
-        """Pipe the structured prompt to a language model.
-
-        Args:
-            others: The language model to pipe the structured prompt to.
-            name: The name of the pipeline. Defaults to None.
-
-        Returns:
-            A RunnableSequence object.
-
-        Raises:
-            NotImplementedError: If the first element of `others`
-            is not a language model.
-        """
-        if (
-            others
-            and isinstance(others[0], BaseLanguageModel)
-            or hasattr(others[0], "with_structured_output")
-        ):
-            return RunnableSequence(
-                self,
-                others[0].with_structured_output(
-                    self.schema_, **self.structured_output_kwargs
-                ),
-                *others[1:],
-                name=name,
-            )
-        else:
-            msg = "Structured prompts need to be piped to a language model."
-            raise NotImplementedError(msg)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/pydantic_v1/__init__.py ./custom_langchain_core/pydantic_v1/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/pydantic_v1/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/pydantic_v1/__init__.py	1970-01-01 09:00:00
@@ -1,43 +0,0 @@
-from importlib import metadata
-
-from langchain_core._api.deprecation import warn_deprecated
-
-# Create namespaces for pydantic v1 and v2.
-# This code must stay at the top of the file before other modules may
-# attempt to import pydantic since it adds pydantic_v1 and pydantic_v2 to sys.modules.
-#
-# This hack is done for the following reasons:
-# * Langchain will attempt to remain compatible with both pydantic v1 and v2 since
-#   both dependencies and dependents may be stuck on either version of v1 or v2.
-# * Creating namespaces for pydantic v1 and v2 should allow us to write code that
-#   unambiguously uses either v1 or v2 API.
-# * This change is easier to roll out and roll back.
-
-try:
-    from pydantic.v1 import *  # noqa: F403
-except ImportError:
-    from pydantic import *  # type: ignore # noqa: F403
-
-
-try:
-    _PYDANTIC_MAJOR_VERSION: int = int(metadata.version("pydantic").split(".")[0])
-except metadata.PackageNotFoundError:
-    _PYDANTIC_MAJOR_VERSION = 0
-
-warn_deprecated(
-    "0.3.0",
-    removal="1.0.0",
-    alternative="pydantic.v1 or pydantic",
-    message=(
-        "As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. "
-        "The langchain_core.pydantic_v1 module was a "
-        "compatibility shim for pydantic v1, and should no longer be used. "
-        "Please update the code to import from Pydantic directly.\n\n"
-        "For example, replace imports like: "
-        "`from langchain_core.pydantic_v1 import BaseModel`\n"
-        "with: `from pydantic import BaseModel`\n"
-        "or the v1 compatibility namespace if you are working in a code base "
-        "that has not been fully upgraded to pydantic 2 yet. "
-        "\tfrom pydantic.v1 import BaseModel\n"
-    ),
-)
Binary files .venv/lib/python3.12/site-packages/langchain_core/pydantic_v1/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/pydantic_v1/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/pydantic_v1/__pycache__/dataclasses.cpython-312.pyc and ./custom_langchain_core/pydantic_v1/__pycache__/dataclasses.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/pydantic_v1/__pycache__/main.cpython-312.pyc and ./custom_langchain_core/pydantic_v1/__pycache__/main.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/pydantic_v1/dataclasses.py ./custom_langchain_core/pydantic_v1/dataclasses.py
--- .venv/lib/python3.12/site-packages/langchain_core/pydantic_v1/dataclasses.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/pydantic_v1/dataclasses.py	1970-01-01 09:00:00
@@ -1,24 +0,0 @@
-from langchain_core._api import warn_deprecated
-
-try:
-    from pydantic.v1.dataclasses import *  # noqa: F403
-except ImportError:
-    from pydantic.dataclasses import *  # type: ignore # noqa: F403
-
-warn_deprecated(
-    "0.3.0",
-    removal="1.0.0",
-    alternative="pydantic.v1 or pydantic",
-    message=(
-        "As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. "
-        "The langchain_core.pydantic_v1 module was a "
-        "compatibility shim for pydantic v1, and should no longer be used. "
-        "Please update the code to import from Pydantic directly.\n\n"
-        "For example, replace imports like: "
-        "`from langchain_core.pydantic_v1 import BaseModel`\n"
-        "with: `from pydantic import BaseModel`\n"
-        "or the v1 compatibility namespace if you are working in a code base "
-        "that has not been fully upgraded to pydantic 2 yet. "
-        "\tfrom pydantic.v1 import BaseModel\n"
-    ),
-)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/pydantic_v1/main.py ./custom_langchain_core/pydantic_v1/main.py
--- .venv/lib/python3.12/site-packages/langchain_core/pydantic_v1/main.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/pydantic_v1/main.py	1970-01-01 09:00:00
@@ -1,24 +0,0 @@
-from langchain_core._api import warn_deprecated
-
-try:
-    from pydantic.v1.main import *  # noqa: F403
-except ImportError:
-    from pydantic.main import *  # type: ignore # noqa: F403
-
-warn_deprecated(
-    "0.3.0",
-    removal="1.0.0",
-    alternative="pydantic.v1 or pydantic",
-    message=(
-        "As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. "
-        "The langchain_core.pydantic_v1 module was a "
-        "compatibility shim for pydantic v1, and should no longer be used. "
-        "Please update the code to import from Pydantic directly.\n\n"
-        "For example, replace imports like: "
-        "`from langchain_core.pydantic_v1 import BaseModel`\n"
-        "with: `from pydantic import BaseModel`\n"
-        "or the v1 compatibility namespace if you are working in a code base "
-        "that has not been fully upgraded to pydantic 2 yet. "
-        "\tfrom pydantic.v1 import BaseModel\n"
-    ),
-)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/rate_limiters.py ./custom_langchain_core/rate_limiters.py
--- .venv/lib/python3.12/site-packages/langchain_core/rate_limiters.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/rate_limiters.py	1970-01-01 09:00:00
@@ -1,261 +0,0 @@
-"""Interface for a rate limiter and an in-memory rate limiter."""
-
-from __future__ import annotations
-
-import abc
-import asyncio
-import threading
-import time
-from typing import (
-    Optional,
-)
-
-
-class BaseRateLimiter(abc.ABC):
-    """Base class for rate limiters.
-
-    Usage of the base limiter is through the acquire and aacquire methods depending
-    on whether running in a sync or async context.
-
-    Implementations are free to add a timeout parameter to their initialize method
-    to allow users to specify a timeout for acquiring the necessary tokens when
-    using a blocking call.
-
-    Current limitations:
-
-    - Rate limiting information is not surfaced in tracing or callbacks. This means
-      that the total time it takes to invoke a chat model will encompass both
-      the time spent waiting for tokens and the time spent making the request.
-
-
-    .. versionadded:: 0.2.24
-    """
-
-    @abc.abstractmethod
-    def acquire(self, *, blocking: bool = True) -> bool:
-        """Attempt to acquire the necessary tokens for the rate limiter.
-
-        This method blocks until the required tokens are available if `blocking`
-        is set to True.
-
-        If `blocking` is set to False, the method will immediately return the result
-        of the attempt to acquire the tokens.
-
-        Args:
-            blocking: If True, the method will block until the tokens are available.
-                If False, the method will return immediately with the result of
-                the attempt. Defaults to True.
-
-        Returns:
-           True if the tokens were successfully acquired, False otherwise.
-        """
-
-    @abc.abstractmethod
-    async def aacquire(self, *, blocking: bool = True) -> bool:
-        """Attempt to acquire the necessary tokens for the rate limiter.
-
-        This method blocks until the required tokens are available if `blocking`
-        is set to True.
-
-        If `blocking` is set to False, the method will immediately return the result
-        of the attempt to acquire the tokens.
-
-        Args:
-            blocking: If True, the method will block until the tokens are available.
-                If False, the method will return immediately with the result of
-                the attempt. Defaults to True.
-
-        Returns:
-           True if the tokens were successfully acquired, False otherwise.
-        """
-
-
-class InMemoryRateLimiter(BaseRateLimiter):
-    """An in memory rate limiter based on a token bucket algorithm.
-
-    This is an in memory rate limiter, so it cannot rate limit across
-    different processes.
-
-    The rate limiter only allows time-based rate limiting and does not
-    take into account any information about the input or the output, so it
-    cannot be used to rate limit based on the size of the request.
-
-    It is thread safe and can be used in either a sync or async context.
-
-    The in memory rate limiter is based on a token bucket. The bucket is filled
-    with tokens at a given rate. Each request consumes a token. If there are
-    not enough tokens in the bucket, the request is blocked until there are
-    enough tokens.
-
-    These *tokens* have NOTHING to do with LLM tokens. They are just
-    a way to keep track of how many requests can be made at a given time.
-
-    Current limitations:
-
-    - The rate limiter is not designed to work across different processes. It is
-      an in-memory rate limiter, but it is thread safe.
-    - The rate limiter only supports time-based rate limiting. It does not take
-      into account the size of the request or any other factors.
-
-    Example:
-
-        .. code-block:: python
-
-            import time
-
-            from langchain_core.rate_limiters import InMemoryRateLimiter
-
-            rate_limiter = InMemoryRateLimiter(
-                requests_per_second=0.1,  # <-- Can only make a request once every 10 seconds!!
-                check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,
-                max_bucket_size=10,  # Controls the maximum burst size.
-            )
-
-            from langchain_anthropic import ChatAnthropic
-            model = ChatAnthropic(
-                model_name="claude-3-opus-20240229",
-                rate_limiter=rate_limiter
-            )
-
-            for _ in range(5):
-                tic = time.time()
-                model.invoke("hello")
-                toc = time.time()
-                print(toc - tic)
-
-
-    .. versionadded:: 0.2.24
-    """  # noqa: E501
-
-    def __init__(
-        self,
-        *,
-        requests_per_second: float = 1,
-        check_every_n_seconds: float = 0.1,
-        max_bucket_size: float = 1,
-    ) -> None:
-        """A rate limiter based on a token bucket.
-
-        These *tokens* have NOTHING to do with LLM tokens. They are just
-        a way to keep track of how many requests can be made at a given time.
-
-        This rate limiter is designed to work in a threaded environment.
-
-        It works by filling up a bucket with tokens at a given rate. Each
-        request consumes a given number of tokens. If there are not enough
-        tokens in the bucket, the request is blocked until there are enough
-        tokens.
-
-        Args:
-            requests_per_second: The number of tokens to add per second to the bucket.
-                Must be at least 1. The tokens represent "credit" that can be used
-                to make requests.
-            check_every_n_seconds: check whether the tokens are available
-                every this many seconds. Can be a float to represent
-                fractions of a second.
-            max_bucket_size: The maximum number of tokens that can be in the bucket.
-                This is used to prevent bursts of requests.
-        """
-        # Number of requests that we can make per second.
-        self.requests_per_second = requests_per_second
-        # Number of tokens in the bucket.
-        self.available_tokens = 0.0
-        self.max_bucket_size = max_bucket_size
-        # A lock to ensure that tokens can only be consumed by one thread
-        # at a given time.
-        self._consume_lock = threading.Lock()
-        # The last time we tried to consume tokens.
-        self.last: Optional[float] = None
-        self.check_every_n_seconds = check_every_n_seconds
-
-    def _consume(self) -> bool:
-        """Try to consume a token.
-
-        Returns:
-            True means that the tokens were consumed, and the caller can proceed to
-            make the request. A False means that the tokens were not consumed, and
-            the caller should try again later.
-        """
-        with self._consume_lock:
-            now = time.monotonic()
-
-            # initialize on first call to avoid a burst
-            if self.last is None:
-                self.last = now
-
-            elapsed = now - self.last
-
-            if elapsed * self.requests_per_second >= 1:
-                self.available_tokens += elapsed * self.requests_per_second
-                self.last = now
-
-            # Make sure that we don't exceed the bucket size.
-            # This is used to prevent bursts of requests.
-            self.available_tokens = min(self.available_tokens, self.max_bucket_size)
-
-            # As long as we have at least one token, we can proceed.
-            if self.available_tokens >= 1:
-                self.available_tokens -= 1
-                return True
-
-            return False
-
-    def acquire(self, *, blocking: bool = True) -> bool:
-        """Attempt to acquire a token from the rate limiter.
-
-        This method blocks until the required tokens are available if `blocking`
-        is set to True.
-
-        If `blocking` is set to False, the method will immediately return the result
-        of the attempt to acquire the tokens.
-
-        Args:
-            blocking: If True, the method will block until the tokens are available.
-                If False, the method will return immediately with the result of
-                the attempt. Defaults to True.
-
-        Returns:
-           True if the tokens were successfully acquired, False otherwise.
-        """
-        if not blocking:
-            return self._consume()
-
-        while not self._consume():
-            time.sleep(self.check_every_n_seconds)
-        return True
-
-    async def aacquire(self, *, blocking: bool = True) -> bool:
-        """Attempt to acquire a token from the rate limiter. Async version.
-
-        This method blocks until the required tokens are available if `blocking`
-        is set to True.
-
-        If `blocking` is set to False, the method will immediately return the result
-        of the attempt to acquire the tokens.
-
-        Args:
-            blocking: If True, the method will block until the tokens are available.
-                If False, the method will return immediately with the result of
-                the attempt. Defaults to True.
-
-        Returns:
-           True if the tokens were successfully acquired, False otherwise.
-        """
-        if not blocking:
-            return self._consume()
-
-        while not self._consume():  # noqa: ASYNC110
-            # This code ignores the ASYNC110 warning which is a false positive in this
-            # case.
-            # There is no external actor that can mark that the Event is done
-            # since the tokens are managed by the rate limiter itself.
-            # It needs to wake up to re-fill the tokens.
-            # https://docs.astral.sh/ruff/rules/async-busy-wait/
-            await asyncio.sleep(self.check_every_n_seconds)
-        return True
-
-
-__all__ = [
-    "BaseRateLimiter",
-    "InMemoryRateLimiter",
-]
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/retrievers.py ./custom_langchain_core/retrievers.py
--- .venv/lib/python3.12/site-packages/langchain_core/retrievers.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/retrievers.py	1970-01-01 09:00:00
@@ -1,454 +0,0 @@
-"""**Retriever** class returns Documents given a text **query**.
-
-It is more general than a vector store. A retriever does not need to be able to
-store documents, only to return (or retrieve) it. Vector stores can be used as
-the backbone of a retriever, but there are other types of retrievers as well.
-
-**Class hierarchy:**
-
-.. code-block::
-
-    BaseRetriever --> <name>Retriever  # Examples: ArxivRetriever, MergerRetriever
-
-**Main helpers:**
-
-.. code-block::
-
-    RetrieverInput, RetrieverOutput, RetrieverLike, RetrieverOutputLike,
-    Document, Serializable, Callbacks,
-    CallbackManagerForRetrieverRun, AsyncCallbackManagerForRetrieverRun
-"""
-
-from __future__ import annotations
-
-import warnings
-from abc import ABC, abstractmethod
-from inspect import signature
-from typing import TYPE_CHECKING, Any, Optional
-
-from pydantic import ConfigDict
-from typing_extensions import Self, TypedDict
-
-from langchain_core._api import deprecated
-from langchain_core.documents import Document
-from langchain_core.runnables import (
-    Runnable,
-    RunnableConfig,
-    RunnableSerializable,
-    ensure_config,
-)
-from langchain_core.runnables.config import run_in_executor
-
-if TYPE_CHECKING:
-    from langchain_core.callbacks.manager import (
-        AsyncCallbackManagerForRetrieverRun,
-        CallbackManagerForRetrieverRun,
-        Callbacks,
-    )
-
-RetrieverInput = str
-RetrieverOutput = list[Document]
-RetrieverLike = Runnable[RetrieverInput, RetrieverOutput]
-RetrieverOutputLike = Runnable[Any, RetrieverOutput]
-
-
-class LangSmithRetrieverParams(TypedDict, total=False):
-    """LangSmith parameters for tracing."""
-
-    ls_retriever_name: str
-    """Retriever name."""
-    ls_vector_store_provider: Optional[str]
-    """Vector store provider."""
-    ls_embedding_provider: Optional[str]
-    """Embedding provider."""
-    ls_embedding_model: Optional[str]
-    """Embedding model."""
-
-
-class BaseRetriever(RunnableSerializable[RetrieverInput, RetrieverOutput], ABC):
-    """Abstract base class for a Document retrieval system.
-
-    A retrieval system is defined as something that can take string queries and return
-    the most 'relevant' Documents from some source.
-
-    Usage:
-
-    A retriever follows the standard Runnable interface, and should be used
-    via the standard Runnable methods of `invoke`, `ainvoke`, `batch`, `abatch`.
-
-    Implementation:
-
-    When implementing a custom retriever, the class should implement
-    the `_get_relevant_documents` method to define the logic for retrieving documents.
-
-    Optionally, an async native implementations can be provided by overriding the
-    `_aget_relevant_documents` method.
-
-    Example: A retriever that returns the first 5 documents from a list of documents
-
-        .. code-block:: python
-
-            from langchain_core.documents import Document
-            from langchain_core.retrievers import BaseRetriever
-            from typing import List
-
-            class SimpleRetriever(BaseRetriever):
-                docs: List[Document]
-                k: int = 5
-
-                def _get_relevant_documents(self, query: str) -> List[Document]:
-                    \"\"\"Return the first k documents from the list of documents\"\"\"
-                    return self.docs[:self.k]
-
-                async def _aget_relevant_documents(self, query: str) -> List[Document]:
-                    \"\"\"(Optional) async native implementation.\"\"\"
-                    return self.docs[:self.k]
-
-    Example: A simple retriever based on a scikit-learn vectorizer
-
-        .. code-block:: python
-
-            from sklearn.metrics.pairwise import cosine_similarity
-
-            class TFIDFRetriever(BaseRetriever, BaseModel):
-                vectorizer: Any
-                docs: List[Document]
-                tfidf_array: Any
-                k: int = 4
-
-                class Config:
-                    arbitrary_types_allowed = True
-
-                def _get_relevant_documents(self, query: str) -> List[Document]:
-                    # Ip -- (n_docs,x), Op -- (n_docs,n_Feats)
-                    query_vec = self.vectorizer.transform([query])
-                    # Op -- (n_docs,1) -- Cosine Sim with each doc
-                    results = cosine_similarity(self.tfidf_array, query_vec).reshape((-1,))
-                    return [self.docs[i] for i in results.argsort()[-self.k :][::-1]]
-    """  # noqa: E501
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    _new_arg_supported: bool = False
-    _expects_other_args: bool = False
-    tags: Optional[list[str]] = None
-    """Optional list of tags associated with the retriever. Defaults to None.
-    These tags will be associated with each call to this retriever,
-    and passed as arguments to the handlers defined in `callbacks`.
-    You can use these to eg identify a specific instance of a retriever with its
-    use case.
-    """
-    metadata: Optional[dict[str, Any]] = None
-    """Optional metadata associated with the retriever. Defaults to None.
-    This metadata will be associated with each call to this retriever,
-    and passed as arguments to the handlers defined in `callbacks`.
-    You can use these to eg identify a specific instance of a retriever with its
-    use case.
-    """
-
-    def __init_subclass__(cls, **kwargs: Any) -> None:
-        super().__init_subclass__(**kwargs)
-        # Version upgrade for old retrievers that implemented the public
-        # methods directly.
-        if cls.get_relevant_documents != BaseRetriever.get_relevant_documents:
-            warnings.warn(
-                "Retrievers must implement abstract `_get_relevant_documents` method"
-                " instead of `get_relevant_documents`",
-                DeprecationWarning,
-                stacklevel=4,
-            )
-            swap = cls.get_relevant_documents
-            cls.get_relevant_documents = (  # type: ignore[assignment]
-                BaseRetriever.get_relevant_documents
-            )
-            cls._get_relevant_documents = swap  # type: ignore[assignment]
-        if (
-            hasattr(cls, "aget_relevant_documents")
-            and cls.aget_relevant_documents != BaseRetriever.aget_relevant_documents
-        ):
-            warnings.warn(
-                "Retrievers must implement abstract `_aget_relevant_documents` method"
-                " instead of `aget_relevant_documents`",
-                DeprecationWarning,
-                stacklevel=4,
-            )
-            aswap = cls.aget_relevant_documents
-            cls.aget_relevant_documents = (  # type: ignore[assignment]
-                BaseRetriever.aget_relevant_documents
-            )
-            cls._aget_relevant_documents = aswap  # type: ignore[assignment]
-        parameters = signature(cls._get_relevant_documents).parameters
-        cls._new_arg_supported = parameters.get("run_manager") is not None
-        if (
-            not cls._new_arg_supported
-            and cls._aget_relevant_documents == BaseRetriever._aget_relevant_documents
-        ):
-            # we need to tolerate no run_manager in _aget_relevant_documents signature
-            async def _aget_relevant_documents(
-                self: Self, query: str
-            ) -> list[Document]:
-                return await run_in_executor(None, self._get_relevant_documents, query)  # type: ignore
-
-            cls._aget_relevant_documents = _aget_relevant_documents  # type: ignore[assignment]
-
-        # If a V1 retriever broke the interface and expects additional arguments
-        cls._expects_other_args = (
-            len(set(parameters.keys()) - {"self", "query", "run_manager"}) > 0
-        )
-
-    def _get_ls_params(self, **kwargs: Any) -> LangSmithRetrieverParams:
-        """Get standard params for tracing."""
-        default_retriever_name = self.get_name()
-        if default_retriever_name.startswith("Retriever"):
-            default_retriever_name = default_retriever_name[9:]
-        elif default_retriever_name.endswith("Retriever"):
-            default_retriever_name = default_retriever_name[:-9]
-        default_retriever_name = default_retriever_name.lower()
-
-        ls_params = LangSmithRetrieverParams(ls_retriever_name=default_retriever_name)
-        return ls_params
-
-    def invoke(
-        self, input: str, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> list[Document]:
-        """Invoke the retriever to get relevant documents.
-
-        Main entry point for synchronous retriever invocations.
-
-        Args:
-            input: The query string.
-            config: Configuration for the retriever. Defaults to None.
-            kwargs: Additional arguments to pass to the retriever.
-
-        Returns:
-            List of relevant documents.
-
-        Examples:
-
-        .. code-block:: python
-
-            retriever.invoke("query")
-        """
-        from langchain_core.callbacks.manager import CallbackManager
-
-        config = ensure_config(config)
-        inheritable_metadata = {
-            **(config.get("metadata") or {}),
-            **self._get_ls_params(**kwargs),
-        }
-        callback_manager = CallbackManager.configure(
-            config.get("callbacks"),
-            None,
-            verbose=kwargs.get("verbose", False),
-            inheritable_tags=config.get("tags"),
-            local_tags=self.tags,
-            inheritable_metadata=inheritable_metadata,
-            local_metadata=self.metadata,
-        )
-        run_manager = callback_manager.on_retriever_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=kwargs.pop("run_id", None),
-        )
-        try:
-            _kwargs = kwargs if self._expects_other_args else {}
-            if self._new_arg_supported:
-                result = self._get_relevant_documents(
-                    input, run_manager=run_manager, **_kwargs
-                )
-            else:
-                result = self._get_relevant_documents(input, **_kwargs)
-        except Exception as e:
-            run_manager.on_retriever_error(e)
-            raise
-        else:
-            run_manager.on_retriever_end(
-                result,
-            )
-            return result
-
-    async def ainvoke(
-        self,
-        input: str,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> list[Document]:
-        """Asynchronously invoke the retriever to get relevant documents.
-
-        Main entry point for asynchronous retriever invocations.
-
-        Args:
-            input: The query string.
-            config: Configuration for the retriever. Defaults to None.
-            kwargs: Additional arguments to pass to the retriever.
-
-        Returns:
-            List of relevant documents.
-
-        Examples:
-
-        .. code-block:: python
-
-            await retriever.ainvoke("query")
-        """
-        from langchain_core.callbacks.manager import AsyncCallbackManager
-
-        config = ensure_config(config)
-        inheritable_metadata = {
-            **(config.get("metadata") or {}),
-            **self._get_ls_params(**kwargs),
-        }
-        callback_manager = AsyncCallbackManager.configure(
-            config.get("callbacks"),
-            None,
-            verbose=kwargs.get("verbose", False),
-            inheritable_tags=config.get("tags"),
-            local_tags=self.tags,
-            inheritable_metadata=inheritable_metadata,
-            local_metadata=self.metadata,
-        )
-        run_manager = await callback_manager.on_retriever_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=kwargs.pop("run_id", None),
-        )
-        try:
-            _kwargs = kwargs if self._expects_other_args else {}
-            if self._new_arg_supported:
-                result = await self._aget_relevant_documents(
-                    input, run_manager=run_manager, **_kwargs
-                )
-            else:
-                result = await self._aget_relevant_documents(input, **_kwargs)
-        except Exception as e:
-            await run_manager.on_retriever_error(e)
-            raise
-        else:
-            await run_manager.on_retriever_end(
-                result,
-            )
-            return result
-
-    @abstractmethod
-    def _get_relevant_documents(
-        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
-    ) -> list[Document]:
-        """Get documents relevant to a query.
-
-        Args:
-            query: String to find relevant documents for.
-            run_manager: The callback handler to use.
-
-        Returns:
-            List of relevant documents.
-        """
-
-    async def _aget_relevant_documents(
-        self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun
-    ) -> list[Document]:
-        """Asynchronously get documents relevant to a query.
-
-        Args:
-            query: String to find relevant documents for
-            run_manager: The callback handler to use
-        Returns:
-            List of relevant documents
-        """
-        return await run_in_executor(
-            None,
-            self._get_relevant_documents,
-            query,
-            run_manager=run_manager.get_sync(),
-        )
-
-    @deprecated(since="0.1.46", alternative="invoke", removal="1.0")
-    def get_relevant_documents(
-        self,
-        query: str,
-        *,
-        callbacks: Callbacks = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        run_name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> list[Document]:
-        """Retrieve documents relevant to a query.
-
-        Users should favor using `.invoke` or `.batch` rather than
-        `get_relevant_documents directly`.
-
-        Args:
-            query: string to find relevant documents for.
-            callbacks: Callback manager or list of callbacks. Defaults to None.
-            tags: Optional list of tags associated with the retriever.
-                These tags will be associated with each call to this retriever,
-                and passed as arguments to the handlers defined in `callbacks`.
-                Defaults to None.
-            metadata: Optional metadata associated with the retriever.
-                This metadata will be associated with each call to this retriever,
-                and passed as arguments to the handlers defined in `callbacks`.
-                Defaults to None.
-            run_name: Optional name for the run. Defaults to None.
-            kwargs: Additional arguments to pass to the retriever.
-
-        Returns:
-            List of relevant documents.
-        """
-        config: RunnableConfig = {}
-        if callbacks:
-            config["callbacks"] = callbacks
-        if tags:
-            config["tags"] = tags
-        if metadata:
-            config["metadata"] = metadata
-        if run_name:
-            config["run_name"] = run_name
-        return self.invoke(query, config, **kwargs)
-
-    @deprecated(since="0.1.46", alternative="ainvoke", removal="1.0")
-    async def aget_relevant_documents(
-        self,
-        query: str,
-        *,
-        callbacks: Callbacks = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        run_name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> list[Document]:
-        """Asynchronously get documents relevant to a query.
-
-        Users should favor using `.ainvoke` or `.abatch` rather than
-        `aget_relevant_documents directly`.
-
-        Args:
-            query: string to find relevant documents for.
-            callbacks: Callback manager or list of callbacks.
-            tags: Optional list of tags associated with the retriever.
-                These tags will be associated with each call to this retriever,
-                and passed as arguments to the handlers defined in `callbacks`.
-                Defaults to None.
-            metadata: Optional metadata associated with the retriever.
-                This metadata will be associated with each call to this retriever,
-                and passed as arguments to the handlers defined in `callbacks`.
-                Defaults to None.
-            run_name: Optional name for the run. Defaults to None.
-            kwargs: Additional arguments to pass to the retriever.
-
-        Returns:
-            List of relevant documents.
-        """
-        config: RunnableConfig = {}
-        if callbacks:
-            config["callbacks"] = callbacks
-        if tags:
-            config["tags"] = tags
-        if metadata:
-            config["metadata"] = metadata
-        if run_name:
-            config["run_name"] = run_name
-        return await self.ainvoke(query, config, **kwargs)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/__init__.py ./custom_langchain_core/runnables/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/__init__.py	1970-01-01 09:00:00
@@ -1,87 +0,0 @@
-"""LangChain **Runnable** and the **LangChain Expression Language (LCEL)**.
-
-The LangChain Expression Language (LCEL) offers a declarative method to build
-production-grade programs that harness the power of LLMs.
-
-Programs created using LCEL and LangChain Runnables inherently support
-synchronous, asynchronous, batch, and streaming operations.
-
-Support for **async** allows servers hosting LCEL based programs to scale better
-for higher concurrent loads.
-
-**Batch** operations allow for processing multiple inputs in parallel.
-
-**Streaming** of intermediate outputs, as they're being generated, allows for
-creating more responsive UX.
-
-This module contains schema and implementation of LangChain Runnables primitives.
-"""
-
-from langchain_core.runnables.base import (
-    Runnable,
-    RunnableBinding,
-    RunnableGenerator,
-    RunnableLambda,
-    RunnableMap,
-    RunnableParallel,
-    RunnableSequence,
-    RunnableSerializable,
-    chain,
-)
-from langchain_core.runnables.branch import RunnableBranch
-from langchain_core.runnables.config import (
-    RunnableConfig,
-    ensure_config,
-    get_config_list,
-    patch_config,
-    run_in_executor,
-)
-from langchain_core.runnables.fallbacks import RunnableWithFallbacks
-from langchain_core.runnables.history import RunnableWithMessageHistory
-from langchain_core.runnables.passthrough import (
-    RunnableAssign,
-    RunnablePassthrough,
-    RunnablePick,
-)
-from langchain_core.runnables.router import RouterInput, RouterRunnable
-from langchain_core.runnables.utils import (
-    AddableDict,
-    ConfigurableField,
-    ConfigurableFieldMultiOption,
-    ConfigurableFieldSingleOption,
-    ConfigurableFieldSpec,
-    aadd,
-    add,
-)
-
-__all__ = [
-    "chain",
-    "AddableDict",
-    "ConfigurableField",
-    "ConfigurableFieldSingleOption",
-    "ConfigurableFieldMultiOption",
-    "ConfigurableFieldSpec",
-    "ensure_config",
-    "run_in_executor",
-    "patch_config",
-    "RouterInput",
-    "RouterRunnable",
-    "Runnable",
-    "RunnableSerializable",
-    "RunnableBinding",
-    "RunnableBranch",
-    "RunnableConfig",
-    "RunnableGenerator",
-    "RunnableLambda",
-    "RunnableMap",
-    "RunnableParallel",
-    "RunnablePassthrough",
-    "RunnableAssign",
-    "RunnablePick",
-    "RunnableSequence",
-    "RunnableWithFallbacks",
-    "RunnableWithMessageHistory",
-    "get_config_list",
-    "aadd",
-    "add",
-]
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/base.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/base.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/branch.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/branch.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/config.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/config.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/configurable.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/configurable.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/fallbacks.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/fallbacks.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/graph.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/graph.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/graph_ascii.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/graph_ascii.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/graph_mermaid.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/graph_mermaid.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/graph_png.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/graph_png.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/history.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/history.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/learnable.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/learnable.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/passthrough.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/passthrough.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/retry.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/retry.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/router.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/router.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/schema.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/schema.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/runnables/__pycache__/utils.cpython-312.pyc and ./custom_langchain_core/runnables/__pycache__/utils.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/base.py ./custom_langchain_core/runnables/base.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/base.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/base.py	1970-01-01 09:00:00
@@ -1,5910 +0,0 @@
-from __future__ import annotations
-
-import asyncio
-import collections
-import contextlib
-import functools
-import inspect
-import threading
-from abc import ABC, abstractmethod
-from collections.abc import (
-    AsyncGenerator,
-    AsyncIterator,
-    Awaitable,
-    Coroutine,
-    Iterator,
-    Mapping,
-    Sequence,
-)
-from concurrent.futures import FIRST_COMPLETED, wait
-from contextvars import copy_context
-from functools import wraps
-from itertools import groupby, tee
-from operator import itemgetter
-from types import GenericAlias
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    Generic,
-    Optional,
-    Protocol,
-    TypeVar,
-    Union,
-    cast,
-    get_type_hints,
-    overload,
-)
-
-from pydantic import BaseModel, ConfigDict, Field, RootModel
-from typing_extensions import Literal, get_args, override
-
-from langchain_core._api import beta_decorator
-from langchain_core.load.serializable import (
-    Serializable,
-    SerializedConstructor,
-    SerializedNotImplemented,
-)
-from langchain_core.runnables.config import (
-    RunnableConfig,
-    _set_config_context,
-    acall_func_with_variable_args,
-    call_func_with_variable_args,
-    ensure_config,
-    get_async_callback_manager_for_config,
-    get_callback_manager_for_config,
-    get_config_list,
-    get_executor_for_config,
-    merge_configs,
-    patch_config,
-    run_in_executor,
-)
-from langchain_core.runnables.graph import Graph
-from langchain_core.runnables.schema import StreamEvent
-from langchain_core.runnables.utils import (
-    AddableDict,
-    AnyConfigurableField,
-    ConfigurableField,
-    ConfigurableFieldSpec,
-    Input,
-    Output,
-    accepts_config,
-    accepts_run_manager,
-    asyncio_accepts_context,
-    gated_coro,
-    gather_with_concurrency,
-    get_function_first_arg_dict_keys,
-    get_function_nonlocals,
-    get_lambda_source,
-    get_unique_config_specs,
-    indent_lines_after_first,
-    is_async_callable,
-    is_async_generator,
-)
-from langchain_core.utils.aiter import aclosing, atee, py_anext
-from langchain_core.utils.iter import safetee
-from langchain_core.utils.pydantic import create_model_v2
-
-if TYPE_CHECKING:
-    from langchain_core.callbacks.manager import (
-        AsyncCallbackManagerForChainRun,
-        CallbackManagerForChainRun,
-    )
-    from langchain_core.prompts.base import BasePromptTemplate
-    from langchain_core.runnables.fallbacks import (
-        RunnableWithFallbacks as RunnableWithFallbacksT,
-    )
-    from langchain_core.tools import BaseTool
-    from langchain_core.tracers.log_stream import (
-        RunLog,
-        RunLogPatch,
-    )
-    from langchain_core.tracers.root_listeners import AsyncListener
-    from langchain_core.tracers.schemas import Run
-
-
-Other = TypeVar("Other")
-
-
-class Runnable(Generic[Input, Output], ABC):
-    """A unit of work that can be invoked, batched, streamed, transformed and composed.
-
-    Key Methods
-    ===========
-
-    - **invoke/ainvoke**: Transforms a single input into an output.
-    - **batch/abatch**: Efficiently transforms multiple inputs into outputs.
-    - **stream/astream**: Streams output from a single input as it's produced.
-    - **astream_log**: Streams output and selected intermediate results from an input.
-
-    Built-in optimizations:
-
-    - **Batch**: By default, batch runs invoke() in parallel using a thread pool executor.
-      Override to optimize batching.
-
-    - **Async**: Methods with "a" suffix are asynchronous. By default, they execute
-      the sync counterpart using asyncio's thread pool.
-      Override for native async.
-
-    All methods accept an optional config argument, which can be used to configure
-    execution, add tags and metadata for tracing and debugging etc.
-
-    Runnables expose schematic information about their input, output and config via
-    the input_schema property, the output_schema property and config_schema method.
-
-    LCEL and Composition
-    ====================
-
-    The LangChain Expression Language (LCEL) is a declarative way to compose Runnables
-    into chains. Any chain constructed this way will automatically have sync, async,
-    batch, and streaming support.
-
-    The main composition primitives are RunnableSequence and RunnableParallel.
-
-    **RunnableSequence** invokes a series of runnables sequentially, with
-    one Runnable's output serving as the next's input. Construct using
-    the `|` operator or by passing a list of runnables to RunnableSequence.
-
-    **RunnableParallel** invokes runnables concurrently, providing the same input
-    to each. Construct it using a dict literal within a sequence or by passing a
-    dict to RunnableParallel.
-
-
-    For example,
-
-    .. code-block:: python
-
-        from langchain_core.runnables import RunnableLambda
-
-        # A RunnableSequence constructed using the `|` operator
-        sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)
-        sequence.invoke(1) # 4
-        sequence.batch([1, 2, 3]) # [4, 6, 8]
-
-
-        # A sequence that contains a RunnableParallel constructed using a dict literal
-        sequence = RunnableLambda(lambda x: x + 1) | {
-            'mul_2': RunnableLambda(lambda x: x * 2),
-            'mul_5': RunnableLambda(lambda x: x * 5)
-        }
-        sequence.invoke(1) # {'mul_2': 4, 'mul_5': 10}
-
-    Standard Methods
-    ================
-
-    All Runnables expose additional methods that can be used to modify their behavior
-    (e.g., add a retry policy, add lifecycle listeners, make them configurable, etc.).
-
-    These methods will work on any Runnable, including Runnable chains constructed
-    by composing other Runnables. See the individual methods for details.
-
-    For example,
-
-    .. code-block:: python
-
-        from langchain_core.runnables import RunnableLambda
-
-        import random
-
-        def add_one(x: int) -> int:
-            return x + 1
-
-
-        def buggy_double(y: int) -> int:
-            \"\"\"Buggy code that will fail 70% of the time\"\"\"
-            if random.random() > 0.3:
-                print('This code failed, and will probably be retried!')  # noqa: T201
-                raise ValueError('Triggered buggy code')
-            return y * 2
-
-        sequence = (
-            RunnableLambda(add_one) |
-            RunnableLambda(buggy_double).with_retry( # Retry on failure
-                stop_after_attempt=10,
-                wait_exponential_jitter=False
-            )
-        )
-
-        print(sequence.input_schema.model_json_schema()) # Show inferred input schema
-        print(sequence.output_schema.model_json_schema()) # Show inferred output schema
-        print(sequence.invoke(2)) # invoke the sequence (note the retry above!!)
-
-    Debugging and tracing
-    =====================
-
-    As the chains get longer, it can be useful to be able to see intermediate results
-    to debug and trace the chain.
-
-    You can set the global debug flag to True to enable debug output for all chains:
-
-        .. code-block:: python
-
-            from langchain_core.globals import set_debug
-            set_debug(True)
-
-    Alternatively, you can pass existing or custom callbacks to any given chain:
-
-        .. code-block:: python
-
-            from langchain_core.tracers import ConsoleCallbackHandler
-
-            chain.invoke(
-                ...,
-                config={'callbacks': [ConsoleCallbackHandler()]}
-            )
-
-    For a UI (and much more) checkout LangSmith: https://docs.smith.langchain.com/
-    """  # noqa: E501
-
-    name: Optional[str]
-    """The name of the Runnable. Used for debugging and tracing."""
-
-    def get_name(
-        self, suffix: Optional[str] = None, *, name: Optional[str] = None
-    ) -> str:
-        """Get the name of the Runnable."""
-        if name:
-            name_ = name
-        elif hasattr(self, "name") and self.name:
-            name_ = self.name
-        else:
-            # Here we handle a case where the runnable subclass is also a pydantic
-            # model.
-            cls = self.__class__
-            # Then it's a pydantic sub-class, and we have to check
-            # whether it's a generic, and if so recover the original name.
-            if (
-                hasattr(
-                    cls,
-                    "__pydantic_generic_metadata__",
-                )
-                and "origin" in cls.__pydantic_generic_metadata__
-                and cls.__pydantic_generic_metadata__["origin"] is not None
-            ):
-                name_ = cls.__pydantic_generic_metadata__["origin"].__name__
-            else:
-                name_ = cls.__name__
-
-        if suffix:
-            if name_[0].isupper():
-                return name_ + suffix.title()
-            else:
-                return name_ + "_" + suffix.lower()
-        else:
-            return name_
-
-    @property
-    def InputType(self) -> type[Input]:  # noqa: N802
-        """The type of input this Runnable accepts specified as a type annotation."""
-        # First loop through all parent classes and if any of them is
-        # a pydantic model, we will pick up the generic parameterization
-        # from that model via the __pydantic_generic_metadata__ attribute.
-        for base in self.__class__.mro():
-            if hasattr(base, "__pydantic_generic_metadata__"):
-                metadata = base.__pydantic_generic_metadata__
-                if "args" in metadata and len(metadata["args"]) == 2:
-                    return metadata["args"][0]
-
-        # If we didn't find a pydantic model in the parent classes,
-        # then loop through __orig_bases__. This corresponds to
-        # Runnables that are not pydantic models.
-        for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]
-            type_args = get_args(cls)
-            if type_args and len(type_args) == 2:
-                return type_args[0]
-
-        msg = (
-            f"Runnable {self.get_name()} doesn't have an inferable InputType. "
-            "Override the InputType property to specify the input type."
-        )
-        raise TypeError(msg)
-
-    @property
-    def OutputType(self) -> type[Output]:  # noqa: N802
-        """The type of output this Runnable produces specified as a type annotation."""
-        # First loop through bases -- this will help generic
-        # any pydantic models.
-        for base in self.__class__.mro():
-            if hasattr(base, "__pydantic_generic_metadata__"):
-                metadata = base.__pydantic_generic_metadata__
-                if "args" in metadata and len(metadata["args"]) == 2:
-                    return metadata["args"][1]
-
-        for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]
-            type_args = get_args(cls)
-            if type_args and len(type_args) == 2:
-                return type_args[1]
-
-        msg = (
-            f"Runnable {self.get_name()} doesn't have an inferable OutputType. "
-            "Override the OutputType property to specify the output type."
-        )
-        raise TypeError(msg)
-
-    @property
-    def input_schema(self) -> type[BaseModel]:
-        """The type of input this Runnable accepts specified as a pydantic model."""
-        return self.get_input_schema()
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        """Get a pydantic model that can be used to validate input to the Runnable.
-
-        Runnables that leverage the configurable_fields and configurable_alternatives
-        methods will have a dynamic input schema that depends on which
-        configuration the Runnable is invoked with.
-
-        This method allows to get an input schema for a specific configuration.
-
-        Args:
-            config: A config to use when generating the schema.
-
-        Returns:
-            A pydantic model that can be used to validate input.
-        """
-        root_type = self.InputType
-
-        if (
-            inspect.isclass(root_type)
-            and not isinstance(root_type, GenericAlias)
-            and issubclass(root_type, BaseModel)
-        ):
-            return root_type
-
-        return create_model_v2(
-            self.get_name("Input"),
-            root=root_type,
-            # create model needs access to appropriate type annotations to be
-            # able to construct the pydantic model.
-            # When we create the model, we pass information about the namespace
-            # where the model is being created, so the type annotations can
-            # be resolved correctly as well.
-            # self.__class__.__module__ handles the case when the Runnable is
-            # being sub-classed in a different module.
-            module_name=self.__class__.__module__,
-        )
-
-    def get_input_jsonschema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> dict[str, Any]:
-        """Get a JSON schema that represents the input to the Runnable.
-
-        Args:
-            config: A config to use when generating the schema.
-
-        Returns:
-            A JSON schema that represents the input to the Runnable.
-
-        Example:
-
-            .. code-block:: python
-
-                from langchain_core.runnables import RunnableLambda
-
-                def add_one(x: int) -> int:
-                    return x + 1
-
-                runnable = RunnableLambda(add_one)
-
-                print(runnable.get_input_jsonschema())
-
-        .. versionadded:: 0.3.0
-        """
-        return self.get_input_schema(config).model_json_schema()
-
-    @property
-    def output_schema(self) -> type[BaseModel]:
-        """The type of output this Runnable produces specified as a pydantic model."""
-        return self.get_output_schema()
-
-    def get_output_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        """Get a pydantic model that can be used to validate output to the Runnable.
-
-        Runnables that leverage the configurable_fields and configurable_alternatives
-        methods will have a dynamic output schema that depends on which
-        configuration the Runnable is invoked with.
-
-        This method allows to get an output schema for a specific configuration.
-
-        Args:
-            config: A config to use when generating the schema.
-
-        Returns:
-            A pydantic model that can be used to validate output.
-        """
-        root_type = self.OutputType
-
-        if (
-            inspect.isclass(root_type)
-            and not isinstance(root_type, GenericAlias)
-            and issubclass(root_type, BaseModel)
-        ):
-            return root_type
-
-        return create_model_v2(
-            self.get_name("Output"),
-            root=root_type,
-            # create model needs access to appropriate type annotations to be
-            # able to construct the pydantic model.
-            # When we create the model, we pass information about the namespace
-            # where the model is being created, so the type annotations can
-            # be resolved correctly as well.
-            # self.__class__.__module__ handles the case when the Runnable is
-            # being sub-classed in a different module.
-            module_name=self.__class__.__module__,
-        )
-
-    def get_output_jsonschema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> dict[str, Any]:
-        """Get a JSON schema that represents the output of the Runnable.
-
-        Args:
-            config: A config to use when generating the schema.
-
-        Returns:
-            A JSON schema that represents the output of the Runnable.
-
-        Example:
-
-            .. code-block:: python
-
-                from langchain_core.runnables import RunnableLambda
-
-                def add_one(x: int) -> int:
-                    return x + 1
-
-                runnable = RunnableLambda(add_one)
-
-                print(runnable.get_output_jsonschema())
-
-        .. versionadded:: 0.3.0
-        """
-        return self.get_output_schema(config).model_json_schema()
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        """List configurable fields for this Runnable."""
-        return []
-
-    def config_schema(
-        self, *, include: Optional[Sequence[str]] = None
-    ) -> type[BaseModel]:
-        """The type of config this Runnable accepts specified as a pydantic model.
-
-        To mark a field as configurable, see the `configurable_fields`
-        and `configurable_alternatives` methods.
-
-        Args:
-            include: A list of fields to include in the config schema.
-
-        Returns:
-            A pydantic model that can be used to validate config.
-        """
-        include = include or []
-        config_specs = self.config_specs
-        configurable = (
-            create_model_v2(  # type: ignore[call-overload]
-                "Configurable",
-                field_definitions={
-                    spec.id: (
-                        spec.annotation,
-                        Field(
-                            spec.default, title=spec.name, description=spec.description
-                        ),
-                    )
-                    for spec in config_specs
-                },
-            )
-            if config_specs
-            else None
-        )
-
-        # Many need to create a typed dict instead to implement NotRequired!
-        all_fields = {
-            **({"configurable": (configurable, None)} if configurable else {}),
-            **{
-                field_name: (field_type, None)
-                for field_name, field_type in get_type_hints(RunnableConfig).items()
-                if field_name in [i for i in include if i != "configurable"]
-            },
-        }
-        model = create_model_v2(  # type: ignore[call-overload]
-            self.get_name("Config"), field_definitions=all_fields
-        )
-        return model
-
-    def get_config_jsonschema(
-        self, *, include: Optional[Sequence[str]] = None
-    ) -> dict[str, Any]:
-        """Get a JSON schema that represents the config of the Runnable.
-
-        Args:
-            include: A list of fields to include in the config schema.
-
-        Returns:
-            A JSON schema that represents the config of the Runnable.
-
-        .. versionadded:: 0.3.0
-        """
-        return self.config_schema(include=include).model_json_schema()
-
-    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
-        """Return a graph representation of this Runnable."""
-        graph = Graph()
-        try:
-            input_node = graph.add_node(self.get_input_schema(config))
-        except TypeError:
-            input_node = graph.add_node(create_model_v2(self.get_name("Input")))
-        runnable_node = graph.add_node(
-            self, metadata=config.get("metadata") if config else None
-        )
-        try:
-            output_node = graph.add_node(self.get_output_schema(config))
-        except TypeError:
-            output_node = graph.add_node(create_model_v2(self.get_name("Output")))
-        graph.add_edge(input_node, runnable_node)
-        graph.add_edge(runnable_node, output_node)
-        return graph
-
-    def get_prompts(
-        self, config: Optional[RunnableConfig] = None
-    ) -> list[BasePromptTemplate]:
-        """Return a list of prompts used by this Runnable."""
-        from langchain_core.prompts.base import BasePromptTemplate
-
-        prompts = []
-        for _, node in self.get_graph(config=config).nodes.items():
-            if isinstance(node.data, BasePromptTemplate):
-                prompts.append(node.data)
-        return prompts
-
-    def __or__(
-        self,
-        other: Union[
-            Runnable[Any, Other],
-            Callable[[Any], Other],
-            Callable[[Iterator[Any]], Iterator[Other]],
-            Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],
-        ],
-    ) -> RunnableSerializable[Input, Other]:
-        """Compose this Runnable with another object to create a RunnableSequence."""
-        return RunnableSequence(self, coerce_to_runnable(other))
-
-    def __ror__(
-        self,
-        other: Union[
-            Runnable[Other, Any],
-            Callable[[Other], Any],
-            Callable[[Iterator[Other]], Iterator[Any]],
-            Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],
-        ],
-    ) -> RunnableSerializable[Other, Output]:
-        """Compose this Runnable with another object to create a RunnableSequence."""
-        return RunnableSequence(coerce_to_runnable(other), self)
-
-    def pipe(
-        self,
-        *others: Union[Runnable[Any, Other], Callable[[Any], Other]],
-        name: Optional[str] = None,
-    ) -> RunnableSerializable[Input, Other]:
-        """Compose this Runnable with Runnable-like objects to make a RunnableSequence.
-
-        Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`
-
-        Example:
-            .. code-block:: python
-
-                from langchain_core.runnables import RunnableLambda
-
-                def add_one(x: int) -> int:
-                    return x + 1
-
-                def mul_two(x: int) -> int:
-                    return x * 2
-
-                runnable_1 = RunnableLambda(add_one)
-                runnable_2 = RunnableLambda(mul_two)
-                sequence = runnable_1.pipe(runnable_2)
-                # Or equivalently:
-                # sequence = runnable_1 | runnable_2
-                # sequence = RunnableSequence(first=runnable_1, last=runnable_2)
-                sequence.invoke(1)
-                await sequence.ainvoke(1)
-                # -> 4
-
-                sequence.batch([1, 2, 3])
-                await sequence.abatch([1, 2, 3])
-                # -> [4, 6, 8]
-        """
-        return RunnableSequence(self, *others, name=name)
-
-    def pick(self, keys: Union[str, list[str]]) -> RunnableSerializable[Any, Any]:
-        """Pick keys from the output dict of this Runnable.
-
-        Pick single key:
-            .. code-block:: python
-
-                import json
-
-                from langchain_core.runnables import RunnableLambda, RunnableMap
-
-                as_str = RunnableLambda(str)
-                as_json = RunnableLambda(json.loads)
-                chain = RunnableMap(str=as_str, json=as_json)
-
-                chain.invoke("[1, 2, 3]")
-                # -> {"str": "[1, 2, 3]", "json": [1, 2, 3]}
-
-                json_only_chain = chain.pick("json")
-                json_only_chain.invoke("[1, 2, 3]")
-                # -> [1, 2, 3]
-
-        Pick list of keys:
-            .. code-block:: python
-
-                from typing import Any
-
-                import json
-
-                from langchain_core.runnables import RunnableLambda, RunnableMap
-
-                as_str = RunnableLambda(str)
-                as_json = RunnableLambda(json.loads)
-                def as_bytes(x: Any) -> bytes:
-                    return bytes(x, "utf-8")
-
-                chain = RunnableMap(
-                    str=as_str,
-                    json=as_json,
-                    bytes=RunnableLambda(as_bytes)
-                )
-
-                chain.invoke("[1, 2, 3]")
-                # -> {"str": "[1, 2, 3]", "json": [1, 2, 3], "bytes": b"[1, 2, 3]"}
-
-                json_and_bytes_chain = chain.pick(["json", "bytes"])
-                json_and_bytes_chain.invoke("[1, 2, 3]")
-                # -> {"json": [1, 2, 3], "bytes": b"[1, 2, 3]"}
-
-        """
-        from langchain_core.runnables.passthrough import RunnablePick
-
-        return self | RunnablePick(keys)
-
-    def assign(
-        self,
-        **kwargs: Union[
-            Runnable[dict[str, Any], Any],
-            Callable[[dict[str, Any]], Any],
-            Mapping[
-                str,
-                Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]],
-            ],
-        ],
-    ) -> RunnableSerializable[Any, Any]:
-        """Assigns new fields to the dict output of this Runnable.
-        Returns a new Runnable.
-
-        .. code-block:: python
-
-            from langchain_community.llms.fake import FakeStreamingListLLM
-            from langchain_core.output_parsers import StrOutputParser
-            from langchain_core.prompts import SystemMessagePromptTemplate
-            from langchain_core.runnables import Runnable
-            from operator import itemgetter
-
-            prompt = (
-                SystemMessagePromptTemplate.from_template("You are a nice assistant.")
-                + "{question}"
-            )
-            llm = FakeStreamingListLLM(responses=["foo-lish"])
-
-            chain: Runnable = prompt | llm | {"str": StrOutputParser()}
-
-            chain_with_assign = chain.assign(hello=itemgetter("str") | llm)
-
-            print(chain_with_assign.input_schema.model_json_schema())
-            # {'title': 'PromptInput', 'type': 'object', 'properties':
-            {'question': {'title': 'Question', 'type': 'string'}}}
-            print(chain_with_assign.output_schema.model_json_schema())
-            # {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':
-            {'str': {'title': 'Str',
-            'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}
-
-        """
-        from langchain_core.runnables.passthrough import RunnableAssign
-
-        return self | RunnableAssign(RunnableParallel[dict[str, Any]](kwargs))
-
-    """ --- Public API --- """
-
-    @abstractmethod
-    def invoke(
-        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Output:
-        """Transform a single input into an output. Override to implement.
-
-        Args:
-            input: The input to the Runnable.
-            config: A config to use when invoking the Runnable.
-               The config supports standard keys like 'tags', 'metadata' for tracing
-               purposes, 'max_concurrency' for controlling how much work to do
-               in parallel, and other keys. Please refer to the RunnableConfig
-               for more details.
-
-        Returns:
-            The output of the Runnable.
-        """
-
-    async def ainvoke(
-        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Output:
-        """Default implementation of ainvoke, calls invoke from a thread.
-
-        The default implementation allows usage of async code even if
-        the Runnable did not implement a native async version of invoke.
-
-        Subclasses should override this method if they can run asynchronously.
-        """
-        return await run_in_executor(config, self.invoke, input, config, **kwargs)
-
-    def batch(
-        self,
-        inputs: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        """Default implementation runs invoke in parallel using a thread pool executor.
-
-        The default implementation of batch works well for IO bound runnables.
-
-        Subclasses should override this method if they can batch more efficiently;
-        e.g., if the underlying Runnable uses an API which supports a batch mode.
-        """
-        if not inputs:
-            return []
-
-        configs = get_config_list(config, len(inputs))
-
-        def invoke(input: Input, config: RunnableConfig) -> Union[Output, Exception]:
-            if return_exceptions:
-                try:
-                    return self.invoke(input, config, **kwargs)
-                except Exception as e:
-                    return e
-            else:
-                return self.invoke(input, config, **kwargs)
-
-        # If there's only one input, don't bother with the executor
-        if len(inputs) == 1:
-            return cast(list[Output], [invoke(inputs[0], configs[0])])
-
-        with get_executor_for_config(configs[0]) as executor:
-            return cast(list[Output], list(executor.map(invoke, inputs, configs)))
-
-    @overload
-    def batch_as_completed(
-        self,
-        inputs: Sequence[Input],
-        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,
-        *,
-        return_exceptions: Literal[False] = False,
-        **kwargs: Any,
-    ) -> Iterator[tuple[int, Output]]: ...
-
-    @overload
-    def batch_as_completed(
-        self,
-        inputs: Sequence[Input],
-        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,
-        *,
-        return_exceptions: Literal[True],
-        **kwargs: Any,
-    ) -> Iterator[tuple[int, Union[Output, Exception]]]: ...
-
-    def batch_as_completed(
-        self,
-        inputs: Sequence[Input],
-        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> Iterator[tuple[int, Union[Output, Exception]]]:
-        """Run invoke in parallel on a list of inputs,
-        yielding results as they complete.
-        """
-        if not inputs:
-            return
-
-        configs = get_config_list(config, len(inputs))
-
-        def invoke(
-            i: int, input: Input, config: RunnableConfig
-        ) -> tuple[int, Union[Output, Exception]]:
-            if return_exceptions:
-                try:
-                    out: Union[Output, Exception] = self.invoke(input, config, **kwargs)
-                except Exception as e:
-                    out = e
-            else:
-                out = self.invoke(input, config, **kwargs)
-
-            return (i, out)
-
-        if len(inputs) == 1:
-            yield invoke(0, inputs[0], configs[0])
-            return
-
-        with get_executor_for_config(configs[0]) as executor:
-            futures = {
-                executor.submit(invoke, i, input, config)
-                for i, (input, config) in enumerate(zip(inputs, configs))
-            }
-
-            try:
-                while futures:
-                    done, futures = wait(futures, return_when=FIRST_COMPLETED)
-                    while done:
-                        yield done.pop().result()
-            finally:
-                for future in futures:
-                    future.cancel()
-
-    async def abatch(
-        self,
-        inputs: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        """Default implementation runs ainvoke in parallel using asyncio.gather.
-
-        The default implementation of batch works well for IO bound runnables.
-
-        Subclasses should override this method if they can batch more efficiently;
-        e.g., if the underlying Runnable uses an API which supports a batch mode.
-
-        Args:
-            inputs: A list of inputs to the Runnable.
-            config: A config to use when invoking the Runnable.
-               The config supports standard keys like 'tags', 'metadata' for tracing
-               purposes, 'max_concurrency' for controlling how much work to do
-               in parallel, and other keys. Please refer to the RunnableConfig
-               for more details. Defaults to None.
-            return_exceptions: Whether to return exceptions instead of raising them.
-                Defaults to False.
-            kwargs: Additional keyword arguments to pass to the Runnable.
-
-        Returns:
-            A list of outputs from the Runnable.
-        """
-        if not inputs:
-            return []
-
-        configs = get_config_list(config, len(inputs))
-
-        async def ainvoke(
-            input: Input, config: RunnableConfig
-        ) -> Union[Output, Exception]:
-            if return_exceptions:
-                try:
-                    return await self.ainvoke(input, config, **kwargs)
-                except Exception as e:
-                    return e
-            else:
-                return await self.ainvoke(input, config, **kwargs)
-
-        coros = map(ainvoke, inputs, configs)
-        return await gather_with_concurrency(configs[0].get("max_concurrency"), *coros)
-
-    @overload
-    def abatch_as_completed(
-        self,
-        inputs: Sequence[Input],
-        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,
-        *,
-        return_exceptions: Literal[False] = False,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[tuple[int, Output]]: ...
-
-    @overload
-    def abatch_as_completed(
-        self,
-        inputs: Sequence[Input],
-        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,
-        *,
-        return_exceptions: Literal[True],
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]: ...
-
-    async def abatch_as_completed(
-        self,
-        inputs: Sequence[Input],
-        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]:
-        """Run ainvoke in parallel on a list of inputs,
-        yielding results as they complete.
-
-        Args:
-            inputs: A list of inputs to the Runnable.
-            config: A config to use when invoking the Runnable.
-               The config supports standard keys like 'tags', 'metadata' for tracing
-               purposes, 'max_concurrency' for controlling how much work to do
-               in parallel, and other keys. Please refer to the RunnableConfig
-               for more details. Defaults to None. Defaults to None.
-            return_exceptions: Whether to return exceptions instead of raising them.
-                Defaults to False.
-            kwargs: Additional keyword arguments to pass to the Runnable.
-
-        Yields:
-            A tuple of the index of the input and the output from the Runnable.
-        """
-        if not inputs:
-            return
-
-        configs = get_config_list(config, len(inputs))
-        # Get max_concurrency from first config, defaulting to None (unlimited)
-        max_concurrency = configs[0].get("max_concurrency") if configs else None
-        semaphore = asyncio.Semaphore(max_concurrency) if max_concurrency else None
-
-        async def ainvoke_task(
-            i: int, input: Input, config: RunnableConfig
-        ) -> tuple[int, Union[Output, Exception]]:
-            if return_exceptions:
-                try:
-                    out: Union[Output, Exception] = await self.ainvoke(
-                        input, config, **kwargs
-                    )
-                except Exception as e:
-                    out = e
-            else:
-                out = await self.ainvoke(input, config, **kwargs)
-            return (i, out)
-
-        coros = [
-            gated_coro(semaphore, ainvoke_task(i, input, config))
-            if semaphore
-            else ainvoke_task(i, input, config)
-            for i, (input, config) in enumerate(zip(inputs, configs))
-        ]
-
-        for coro in asyncio.as_completed(coros):
-            yield await coro
-
-    def stream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        """Default implementation of stream, which calls invoke.
-        Subclasses should override this method if they support streaming output.
-
-        Args:
-            input: The input to the Runnable.
-            config: The config to use for the Runnable. Defaults to None.
-            kwargs: Additional keyword arguments to pass to the Runnable.
-
-        Yields:
-            The output of the Runnable.
-        """
-        yield self.invoke(input, config, **kwargs)
-
-    async def astream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        """Default implementation of astream, which calls ainvoke.
-        Subclasses should override this method if they support streaming output.
-
-        Args:
-            input: The input to the Runnable.
-            config: The config to use for the Runnable. Defaults to None.
-            kwargs: Additional keyword arguments to pass to the Runnable.
-
-        Yields:
-            The output of the Runnable.
-        """
-        yield await self.ainvoke(input, config, **kwargs)
-
-    @overload
-    def astream_log(
-        self,
-        input: Any,
-        config: Optional[RunnableConfig] = None,
-        *,
-        diff: Literal[True] = True,
-        with_streamed_output_list: bool = True,
-        include_names: Optional[Sequence[str]] = None,
-        include_types: Optional[Sequence[str]] = None,
-        include_tags: Optional[Sequence[str]] = None,
-        exclude_names: Optional[Sequence[str]] = None,
-        exclude_types: Optional[Sequence[str]] = None,
-        exclude_tags: Optional[Sequence[str]] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[RunLogPatch]: ...
-
-    @overload
-    def astream_log(
-        self,
-        input: Any,
-        config: Optional[RunnableConfig] = None,
-        *,
-        diff: Literal[False],
-        with_streamed_output_list: bool = True,
-        include_names: Optional[Sequence[str]] = None,
-        include_types: Optional[Sequence[str]] = None,
-        include_tags: Optional[Sequence[str]] = None,
-        exclude_names: Optional[Sequence[str]] = None,
-        exclude_types: Optional[Sequence[str]] = None,
-        exclude_tags: Optional[Sequence[str]] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[RunLog]: ...
-
-    async def astream_log(
-        self,
-        input: Any,
-        config: Optional[RunnableConfig] = None,
-        *,
-        diff: bool = True,
-        with_streamed_output_list: bool = True,
-        include_names: Optional[Sequence[str]] = None,
-        include_types: Optional[Sequence[str]] = None,
-        include_tags: Optional[Sequence[str]] = None,
-        exclude_names: Optional[Sequence[str]] = None,
-        exclude_types: Optional[Sequence[str]] = None,
-        exclude_tags: Optional[Sequence[str]] = None,
-        **kwargs: Any,
-    ) -> Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]:
-        """Stream all output from a Runnable, as reported to the callback system.
-        This includes all inner runs of LLMs, Retrievers, Tools, etc.
-
-        Output is streamed as Log objects, which include a list of
-        Jsonpatch ops that describe how the state of the run has changed in each
-        step, and the final state of the run.
-
-        The Jsonpatch ops can be applied in order to construct state.
-
-        Args:
-            input: The input to the Runnable.
-            config: The config to use for the Runnable.
-            diff: Whether to yield diffs between each step or the current state.
-            with_streamed_output_list: Whether to yield the streamed_output list.
-            include_names: Only include logs with these names.
-            include_types: Only include logs with these types.
-            include_tags: Only include logs with these tags.
-            exclude_names: Exclude logs with these names.
-            exclude_types: Exclude logs with these types.
-            exclude_tags: Exclude logs with these tags.
-            kwargs: Additional keyword arguments to pass to the Runnable.
-
-        Yields:
-            A RunLogPatch or RunLog object.
-        """
-        from langchain_core.tracers.log_stream import (
-            LogStreamCallbackHandler,
-            _astream_log_implementation,
-        )
-
-        stream = LogStreamCallbackHandler(
-            auto_close=False,
-            include_names=include_names,
-            include_types=include_types,
-            include_tags=include_tags,
-            exclude_names=exclude_names,
-            exclude_types=exclude_types,
-            exclude_tags=exclude_tags,
-            _schema_format="original",
-        )
-
-        # Mypy isn't resolving the overloads here
-        # Likely an issue b/c `self` is being passed through
-        # and it's can't map it to Runnable[Input,Output]?
-        async for item in _astream_log_implementation(  # type: ignore
-            self,
-            input,
-            config,
-            diff=diff,
-            stream=stream,
-            with_streamed_output_list=with_streamed_output_list,
-            **kwargs,
-        ):
-            yield item
-
-    async def astream_events(
-        self,
-        input: Any,
-        config: Optional[RunnableConfig] = None,
-        *,
-        version: Literal["v1", "v2"],
-        include_names: Optional[Sequence[str]] = None,
-        include_types: Optional[Sequence[str]] = None,
-        include_tags: Optional[Sequence[str]] = None,
-        exclude_names: Optional[Sequence[str]] = None,
-        exclude_types: Optional[Sequence[str]] = None,
-        exclude_tags: Optional[Sequence[str]] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[StreamEvent]:
-        """Generate a stream of events.
-
-        Use to create an iterator over StreamEvents that provide real-time information
-        about the progress of the Runnable, including StreamEvents from intermediate
-        results.
-
-        A StreamEvent is a dictionary with the following schema:
-
-        - ``event``: **str** - Event names are of the
-            format: on_[runnable_type]_(start|stream|end).
-        - ``name``: **str** - The name of the Runnable that generated the event.
-        - ``run_id``: **str** - randomly generated ID associated with the given execution of
-            the Runnable that emitted the event.
-            A child Runnable that gets invoked as part of the execution of a
-            parent Runnable is assigned its own unique ID.
-        - ``parent_ids``: **List[str]** - The IDs of the parent runnables that
-            generated the event. The root Runnable will have an empty list.
-            The order of the parent IDs is from the root to the immediate parent.
-            Only available for v2 version of the API. The v1 version of the API
-            will return an empty list.
-        - ``tags``: **Optional[List[str]]** - The tags of the Runnable that generated
-            the event.
-        - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the Runnable
-            that generated the event.
-        - ``data``: **Dict[str, Any]**
-
-
-        Below is a table that illustrates some events that might be emitted by various
-        chains. Metadata fields have been omitted from the table for brevity.
-        Chain definitions have been included after the table.
-
-        **ATTENTION** This reference table is for the V2 version of the schema.
-
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | event                | name             | chunk                           | input                                         | output                                          |
-        +======================+==================+=================================+===============================================+=================================================+
-        | on_chat_model_start  | [model name]     |                                 | {"messages": [[SystemMessage, HumanMessage]]} |                                                 |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_chat_model_stream | [model name]     | AIMessageChunk(content="hello") |                                               |                                                 |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_chat_model_end    | [model name]     |                                 | {"messages": [[SystemMessage, HumanMessage]]} | AIMessageChunk(content="hello world")           |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_chain_start       | format_docs      |                                 |                                               |                                                 |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_chain_stream      | format_docs      | "hello world!, goodbye world!"  |                                               |                                                 |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_chain_end         | format_docs      |                                 | [Document(...)]                               | "hello world!, goodbye world!"                  |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_tool_start        | some_tool        |                                 | {"x": 1, "y": "2"}                            |                                                 |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_tool_end          | some_tool        |                                 |                                               | {"x": 1, "y": "2"}                              |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_retriever_start   | [retriever name] |                                 | {"query": "hello"}                            |                                                 |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_retriever_end     | [retriever name] |                                 | {"query": "hello"}                            | [Document(...), ..]                             |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_prompt_start      | [template_name]  |                                 | {"question": "hello"}                         |                                                 |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-        | on_prompt_end        | [template_name]  |                                 | {"question": "hello"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |
-        +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+
-
-        In addition to the standard events, users can also dispatch custom events (see example below).
-
-        Custom events will be only be surfaced with in the `v2` version of the API!
-
-        A custom event has following format:
-
-        +-----------+------+-----------------------------------------------------------------------------------------------------------+
-        | Attribute | Type | Description                                                                                               |
-        +===========+======+===========================================================================================================+
-        | name      | str  | A user defined name for the event.                                                                        |
-        +-----------+------+-----------------------------------------------------------------------------------------------------------+
-        | data      | Any  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |
-        +-----------+------+-----------------------------------------------------------------------------------------------------------+
-
-        Here are declarations associated with the standard events shown above:
-
-        `format_docs`:
-
-        .. code-block:: python
-
-            def format_docs(docs: List[Document]) -> str:
-                '''Format the docs.'''
-                return ", ".join([doc.page_content for doc in docs])
-
-            format_docs = RunnableLambda(format_docs)
-
-        `some_tool`:
-
-        .. code-block:: python
-
-            @tool
-            def some_tool(x: int, y: str) -> dict:
-                '''Some_tool.'''
-                return {"x": x, "y": y}
-
-        `prompt`:
-
-        .. code-block:: python
-
-            template = ChatPromptTemplate.from_messages(
-                [("system", "You are Cat Agent 007"), ("human", "{question}")]
-            ).with_config({"run_name": "my_template", "tags": ["my_template"]})
-
-
-        Example:
-
-        .. code-block:: python
-
-            from langchain_core.runnables import RunnableLambda
-
-            async def reverse(s: str) -> str:
-                return s[::-1]
-
-            chain = RunnableLambda(func=reverse)
-
-            events = [
-                event async for event in chain.astream_events("hello", version="v2")
-            ]
-
-            # will produce the following events (run_id, and parent_ids
-            # has been omitted for brevity):
-            [
-                {
-                    "data": {"input": "hello"},
-                    "event": "on_chain_start",
-                    "metadata": {},
-                    "name": "reverse",
-                    "tags": [],
-                },
-                {
-                    "data": {"chunk": "olleh"},
-                    "event": "on_chain_stream",
-                    "metadata": {},
-                    "name": "reverse",
-                    "tags": [],
-                },
-                {
-                    "data": {"output": "olleh"},
-                    "event": "on_chain_end",
-                    "metadata": {},
-                    "name": "reverse",
-                    "tags": [],
-                },
-            ]
-
-
-        Example: Dispatch Custom Event
-
-        .. code-block:: python
-
-            from langchain_core.callbacks.manager import (
-                adispatch_custom_event,
-            )
-            from langchain_core.runnables import RunnableLambda, RunnableConfig
-            import asyncio
-
-
-            async def slow_thing(some_input: str, config: RunnableConfig) -> str:
-                \"\"\"Do something that takes a long time.\"\"\"
-                await asyncio.sleep(1) # Placeholder for some slow operation
-                await adispatch_custom_event(
-                    "progress_event",
-                    {"message": "Finished step 1 of 3"},
-                    config=config # Must be included for python < 3.10
-                )
-                await asyncio.sleep(1) # Placeholder for some slow operation
-                await adispatch_custom_event(
-                    "progress_event",
-                    {"message": "Finished step 2 of 3"},
-                    config=config # Must be included for python < 3.10
-                )
-                await asyncio.sleep(1) # Placeholder for some slow operation
-                return "Done"
-
-            slow_thing = RunnableLambda(slow_thing)
-
-            async for event in slow_thing.astream_events("some_input", version="v2"):
-                print(event)
-
-        Args:
-            input: The input to the Runnable.
-            config: The config to use for the Runnable.
-            version: The version of the schema to use either `v2` or `v1`.
-                     Users should use `v2`.
-                     `v1` is for backwards compatibility and will be deprecated
-                     in 0.4.0.
-                     No default will be assigned until the API is stabilized.
-                     custom events will only be surfaced in `v2`.
-            include_names: Only include events from runnables with matching names.
-            include_types: Only include events from runnables with matching types.
-            include_tags: Only include events from runnables with matching tags.
-            exclude_names: Exclude events from runnables with matching names.
-            exclude_types: Exclude events from runnables with matching types.
-            exclude_tags: Exclude events from runnables with matching tags.
-            kwargs: Additional keyword arguments to pass to the Runnable.
-                These will be passed to astream_log as this implementation
-                of astream_events is built on top of astream_log.
-
-        Yields:
-            An async stream of StreamEvents.
-
-        Raises:
-            NotImplementedError: If the version is not `v1` or `v2`.
-        """  # noqa: E501
-        from langchain_core.tracers.event_stream import (
-            _astream_events_implementation_v1,
-            _astream_events_implementation_v2,
-        )
-
-        if version == "v2":
-            event_stream = _astream_events_implementation_v2(
-                self,
-                input,
-                config=config,
-                include_names=include_names,
-                include_types=include_types,
-                include_tags=include_tags,
-                exclude_names=exclude_names,
-                exclude_types=exclude_types,
-                exclude_tags=exclude_tags,
-                **kwargs,
-            )
-        elif version == "v1":
-            # First implementation, built on top of astream_log API
-            # This implementation will be deprecated as of 0.2.0
-            event_stream = _astream_events_implementation_v1(
-                self,
-                input,
-                config=config,
-                include_names=include_names,
-                include_types=include_types,
-                include_tags=include_tags,
-                exclude_names=exclude_names,
-                exclude_types=exclude_types,
-                exclude_tags=exclude_tags,
-                **kwargs,
-            )
-        else:
-            msg = 'Only versions "v1" and "v2" of the schema is currently supported.'
-            raise NotImplementedError(msg)
-
-        async with aclosing(event_stream):
-            async for event in event_stream:
-                yield event
-
-    def transform(
-        self,
-        input: Iterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        """Default implementation of transform, which buffers input and calls astream.
-
-        Subclasses should override this method if they can start producing output while
-        input is still being generated.
-
-        Args:
-            input: An iterator of inputs to the Runnable.
-            config: The config to use for the Runnable. Defaults to None.
-            kwargs: Additional keyword arguments to pass to the Runnable.
-
-        Yields:
-            The output of the Runnable.
-        """
-        final: Input
-        got_first_val = False
-
-        for ichunk in input:
-            # The default implementation of transform is to buffer input and
-            # then call stream.
-            # It'll attempt to gather all input into a single chunk using
-            # the `+` operator.
-            # If the input is not addable, then we'll assume that we can
-            # only operate on the last chunk,
-            # and we'll iterate until we get to the last chunk.
-            if not got_first_val:
-                final = ichunk
-                got_first_val = True
-            else:
-                try:
-                    final = final + ichunk  # type: ignore[operator]
-                except TypeError:
-                    final = ichunk
-
-        if got_first_val:
-            yield from self.stream(final, config, **kwargs)
-
-    async def atransform(
-        self,
-        input: AsyncIterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        """Default implementation of atransform, which buffers input and calls astream.
-        Subclasses should override this method if they can start producing output while
-        input is still being generated.
-
-        Args:
-            input: An async iterator of inputs to the Runnable.
-            config: The config to use for the Runnable. Defaults to None.
-            kwargs: Additional keyword arguments to pass to the Runnable.
-
-        Yields:
-            The output of the Runnable.
-        """
-        final: Input
-        got_first_val = False
-
-        async for ichunk in input:
-            # The default implementation of transform is to buffer input and
-            # then call stream.
-            # It'll attempt to gather all input into a single chunk using
-            # the `+` operator.
-            # If the input is not addable, then we'll assume that we can
-            # only operate on the last chunk,
-            # and we'll iterate until we get to the last chunk.
-            if not got_first_val:
-                final = ichunk
-                got_first_val = True
-            else:
-                try:
-                    final = final + ichunk  # type: ignore[operator]
-                except TypeError:
-                    final = ichunk
-
-        if got_first_val:
-            async for output in self.astream(final, config, **kwargs):
-                yield output
-
-    def bind(self, **kwargs: Any) -> Runnable[Input, Output]:
-        """Bind arguments to a Runnable, returning a new Runnable.
-
-        Useful when a Runnable in a chain requires an argument that is not
-        in the output of the previous Runnable or included in the user input.
-
-        Args:
-            kwargs: The arguments to bind to the Runnable.
-
-        Returns:
-            A new Runnable with the arguments bound.
-
-        Example:
-
-        .. code-block:: python
-
-            from langchain_community.chat_models import ChatOllama
-            from langchain_core.output_parsers import StrOutputParser
-
-            llm = ChatOllama(model='llama2')
-
-            # Without bind.
-            chain = (
-                llm
-                | StrOutputParser()
-            )
-
-            chain.invoke("Repeat quoted words exactly: 'One two three four five.'")
-            # Output is 'One two three four five.'
-
-            # With bind.
-            chain = (
-                llm.bind(stop=["three"])
-                | StrOutputParser()
-            )
-
-            chain.invoke("Repeat quoted words exactly: 'One two three four five.'")
-            # Output is 'One two'
-
-        """
-        return RunnableBinding(bound=self, kwargs=kwargs, config={})
-
-    def with_config(
-        self,
-        config: Optional[RunnableConfig] = None,
-        # Sadly Unpack is not well-supported by mypy so this will have to be untyped
-        **kwargs: Any,
-    ) -> Runnable[Input, Output]:
-        """Bind config to a Runnable, returning a new Runnable.
-
-        Args:
-            config: The config to bind to the Runnable.
-            kwargs: Additional keyword arguments to pass to the Runnable.
-
-        Returns:
-            A new Runnable with the config bound.
-        """
-        return RunnableBinding(
-            bound=self,
-            config=cast(
-                RunnableConfig,
-                {**(config or {}), **kwargs},
-            ),  # type: ignore[misc]
-            kwargs={},
-        )
-
-    def with_listeners(
-        self,
-        *,
-        on_start: Optional[
-            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]
-        ] = None,
-        on_end: Optional[
-            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]
-        ] = None,
-        on_error: Optional[
-            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]
-        ] = None,
-    ) -> Runnable[Input, Output]:
-        """Bind lifecycle listeners to a Runnable, returning a new Runnable.
-
-        on_start: Called before the Runnable starts running, with the Run object.
-        on_end: Called after the Runnable finishes running, with the Run object.
-        on_error: Called if the Runnable throws an error, with the Run object.
-
-        The Run object contains information about the run, including its id,
-        type, input, output, error, start_time, end_time, and any tags or metadata
-        added to the run.
-
-        Args:
-            on_start: Called before the Runnable starts running. Defaults to None.
-            on_end: Called after the Runnable finishes running. Defaults to None.
-            on_error: Called if the Runnable throws an error. Defaults to None.
-
-        Returns:
-            A new Runnable with the listeners bound.
-
-        Example:
-
-        .. code-block:: python
-
-            from langchain_core.runnables import RunnableLambda
-            from langchain_core.tracers.schemas import Run
-
-            import time
-
-            def test_runnable(time_to_sleep : int):
-                time.sleep(time_to_sleep)
-
-            def fn_start(run_obj: Run):
-                print("start_time:", run_obj.start_time)
-
-            def fn_end(run_obj: Run):
-                print("end_time:", run_obj.end_time)
-
-            chain = RunnableLambda(test_runnable).with_listeners(
-                on_start=fn_start,
-                on_end=fn_end
-            )
-            chain.invoke(2)
-        """
-        from langchain_core.tracers.root_listeners import RootListenersTracer
-
-        return RunnableBinding(
-            bound=self,
-            config_factories=[
-                lambda config: {
-                    "callbacks": [
-                        RootListenersTracer(
-                            config=config,
-                            on_start=on_start,
-                            on_end=on_end,
-                            on_error=on_error,
-                        )
-                    ],
-                }
-            ],
-        )
-
-    def with_alisteners(
-        self,
-        *,
-        on_start: Optional[AsyncListener] = None,
-        on_end: Optional[AsyncListener] = None,
-        on_error: Optional[AsyncListener] = None,
-    ) -> Runnable[Input, Output]:
-        """Bind async lifecycle listeners to a Runnable, returning a new Runnable.
-
-        on_start: Asynchronously called before the Runnable starts running.
-        on_end: Asynchronously called after the Runnable finishes running.
-        on_error: Asynchronously called if the Runnable throws an error.
-
-        The Run object contains information about the run, including its id,
-        type, input, output, error, start_time, end_time, and any tags or metadata
-        added to the run.
-
-        Args:
-            on_start: Asynchronously called before the Runnable starts running.
-                Defaults to None.
-            on_end: Asynchronously called after the Runnable finishes running.
-                Defaults to None.
-            on_error: Asynchronously called if the Runnable throws an error.
-                Defaults to None.
-
-        Returns:
-            A new Runnable with the listeners bound.
-
-        Example:
-
-        .. code-block:: python
-
-            from langchain_core.runnables import RunnableLambda
-            import time
-
-            async def test_runnable(time_to_sleep : int):
-                print(f"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}")
-                await asyncio.sleep(time_to_sleep)
-                print(f"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}")
-
-            async def fn_start(run_obj : Runnable):
-                print(f"on start callback starts at {format_t(time.time())}
-                await asyncio.sleep(3)
-                print(f"on start callback ends at {format_t(time.time())}")
-
-            async def fn_end(run_obj : Runnable):
-                print(f"on end callback starts at {format_t(time.time())}
-                await asyncio.sleep(2)
-                print(f"on end callback ends at {format_t(time.time())}")
-
-            runnable = RunnableLambda(test_runnable).with_alisteners(
-                on_start=fn_start,
-                on_end=fn_end
-            )
-            async def concurrent_runs():
-                await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))
-
-            asyncio.run(concurrent_runs())
-            Result:
-            on start callback starts at 2024-05-16T14:20:29.637053+00:00
-            on start callback starts at 2024-05-16T14:20:29.637150+00:00
-            on start callback ends at 2024-05-16T14:20:32.638305+00:00
-            on start callback ends at 2024-05-16T14:20:32.638383+00:00
-            Runnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00
-            Runnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00
-            Runnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00
-            on end callback starts at 2024-05-16T14:20:35.640534+00:00
-            Runnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00
-            on end callback starts at 2024-05-16T14:20:37.640574+00:00
-            on end callback ends at 2024-05-16T14:20:37.640654+00:00
-            on end callback ends at 2024-05-16T14:20:39.641751+00:00
-
-        """
-        from langchain_core.tracers.root_listeners import AsyncRootListenersTracer
-
-        return RunnableBinding(
-            bound=self,
-            config_factories=[
-                lambda config: {
-                    "callbacks": [
-                        AsyncRootListenersTracer(
-                            config=config,
-                            on_start=on_start,
-                            on_end=on_end,
-                            on_error=on_error,
-                        )
-                    ],
-                }
-            ],
-        )
-
-    def with_types(
-        self,
-        *,
-        input_type: Optional[type[Input]] = None,
-        output_type: Optional[type[Output]] = None,
-    ) -> Runnable[Input, Output]:
-        """Bind input and output types to a Runnable, returning a new Runnable.
-
-        Args:
-            input_type: The input type to bind to the Runnable. Defaults to None.
-            output_type: The output type to bind to the Runnable. Defaults to None.
-
-        Returns:
-            A new Runnable with the types bound.
-        """
-        return RunnableBinding(
-            bound=self,
-            custom_input_type=input_type,
-            custom_output_type=output_type,
-            kwargs={},
-        )
-
-    def with_retry(
-        self,
-        *,
-        retry_if_exception_type: tuple[type[BaseException], ...] = (Exception,),
-        wait_exponential_jitter: bool = True,
-        stop_after_attempt: int = 3,
-    ) -> Runnable[Input, Output]:
-        """Create a new Runnable that retries the original Runnable on exceptions.
-
-        Args:
-            retry_if_exception_type: A tuple of exception types to retry on.
-                Defaults to (Exception,).
-            wait_exponential_jitter: Whether to add jitter to the wait
-                time between retries. Defaults to True.
-            stop_after_attempt: The maximum number of attempts to make before
-                giving up. Defaults to 3.
-
-        Returns:
-            A new Runnable that retries the original Runnable on exceptions.
-
-        Example:
-
-        .. code-block:: python
-
-            from langchain_core.runnables import RunnableLambda
-
-            count = 0
-
-
-            def _lambda(x: int) -> None:
-                global count
-                count = count + 1
-                if x == 1:
-                    raise ValueError("x is 1")
-                else:
-                     pass
-
-
-            runnable = RunnableLambda(_lambda)
-            try:
-                runnable.with_retry(
-                    stop_after_attempt=2,
-                    retry_if_exception_type=(ValueError,),
-                ).invoke(1)
-            except ValueError:
-                pass
-
-            assert (count == 2)
-
-
-        Args:
-            retry_if_exception_type: A tuple of exception types to retry on
-            wait_exponential_jitter: Whether to add jitter to the wait time
-                                     between retries
-            stop_after_attempt: The maximum number of attempts to make before giving up
-
-        Returns:
-            A new Runnable that retries the original Runnable on exceptions.
-        """
-        from langchain_core.runnables.retry import RunnableRetry
-
-        return RunnableRetry(
-            bound=self,
-            kwargs={},
-            config={},
-            retry_exception_types=retry_if_exception_type,
-            wait_exponential_jitter=wait_exponential_jitter,
-            max_attempt_number=stop_after_attempt,
-        )
-
-    def map(self) -> Runnable[list[Input], list[Output]]:
-        """Return a new Runnable that maps a list of inputs to a list of outputs,
-        by calling invoke() with each input.
-
-        Returns:
-            A new Runnable that maps a list of inputs to a list of outputs.
-
-        Example:
-
-            .. code-block:: python
-
-                    from langchain_core.runnables import RunnableLambda
-
-                    def _lambda(x: int) -> int:
-                        return x + 1
-
-                    runnable = RunnableLambda(_lambda)
-                    print(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]
-        """
-        return RunnableEach(bound=self)
-
-    def with_fallbacks(
-        self,
-        fallbacks: Sequence[Runnable[Input, Output]],
-        *,
-        exceptions_to_handle: tuple[type[BaseException], ...] = (Exception,),
-        exception_key: Optional[str] = None,
-    ) -> RunnableWithFallbacksT[Input, Output]:
-        """Add fallbacks to a Runnable, returning a new Runnable.
-
-        The new Runnable will try the original Runnable, and then each fallback
-        in order, upon failures.
-
-        Args:
-            fallbacks: A sequence of runnables to try if the original Runnable fails.
-            exceptions_to_handle: A tuple of exception types to handle.
-                Defaults to (Exception,).
-            exception_key: If string is specified then handled exceptions will be passed
-                to fallbacks as part of the input under the specified key. If None,
-                exceptions will not be passed to fallbacks. If used, the base Runnable
-                and its fallbacks must accept a dictionary as input. Defaults to None.
-
-        Returns:
-            A new Runnable that will try the original Runnable, and then each
-            fallback in order, upon failures.
-
-        Example:
-
-            .. code-block:: python
-
-                from typing import Iterator
-
-                from langchain_core.runnables import RunnableGenerator
-
-
-                def _generate_immediate_error(input: Iterator) -> Iterator[str]:
-                    raise ValueError()
-                    yield ""
-
-
-                def _generate(input: Iterator) -> Iterator[str]:
-                    yield from "foo bar"
-
-
-                runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(
-                    [RunnableGenerator(_generate)]
-                    )
-                print(''.join(runnable.stream({}))) #foo bar
-
-        Args:
-            fallbacks: A sequence of runnables to try if the original Runnable fails.
-            exceptions_to_handle: A tuple of exception types to handle.
-            exception_key: If string is specified then handled exceptions will be passed
-                to fallbacks as part of the input under the specified key. If None,
-                exceptions will not be passed to fallbacks. If used, the base Runnable
-                and its fallbacks must accept a dictionary as input.
-
-        Returns:
-            A new Runnable that will try the original Runnable, and then each
-            fallback in order, upon failures.
-
-        """
-        from langchain_core.runnables.fallbacks import RunnableWithFallbacks
-
-        return RunnableWithFallbacks(
-            runnable=self,
-            fallbacks=fallbacks,
-            exceptions_to_handle=exceptions_to_handle,
-            exception_key=exception_key,
-        )
-
-    """ --- Helper methods for Subclasses --- """
-
-    def _call_with_config(
-        self,
-        func: Union[
-            Callable[[Input], Output],
-            Callable[[Input, CallbackManagerForChainRun], Output],
-            Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],
-        ],
-        input: Input,
-        config: Optional[RunnableConfig],
-        run_type: Optional[str] = None,
-        serialized: Optional[dict[str, Any]] = None,
-        **kwargs: Optional[Any],
-    ) -> Output:
-        """Helper method to transform an Input value to an Output value,
-        with callbacks. Use this method to implement invoke() in subclasses.
-        """
-        config = ensure_config(config)
-        callback_manager = get_callback_manager_for_config(config)
-        run_manager = callback_manager.on_chain_start(
-            serialized,
-            input,
-            run_type=run_type,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-        try:
-            child_config = patch_config(config, callbacks=run_manager.get_child())
-            context = copy_context()
-            context.run(_set_config_context, child_config)
-            output = cast(
-                Output,
-                context.run(
-                    call_func_with_variable_args,  # type: ignore[arg-type]
-                    func,  # type: ignore[arg-type]
-                    input,  # type: ignore[arg-type]
-                    config,
-                    run_manager,
-                    **kwargs,
-                ),
-            )
-        except BaseException as e:
-            run_manager.on_chain_error(e)
-            raise
-        else:
-            run_manager.on_chain_end(output)
-            return output
-
-    async def _acall_with_config(
-        self,
-        func: Union[
-            Callable[[Input], Awaitable[Output]],
-            Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],
-            Callable[
-                [Input, AsyncCallbackManagerForChainRun, RunnableConfig],
-                Awaitable[Output],
-            ],
-        ],
-        input: Input,
-        config: Optional[RunnableConfig],
-        run_type: Optional[str] = None,
-        serialized: Optional[dict[str, Any]] = None,
-        **kwargs: Optional[Any],
-    ) -> Output:
-        """Helper method to transform an Input value to an Output value,
-        with callbacks. Use this method to implement ainvoke() in subclasses.
-        """
-        config = ensure_config(config)
-        callback_manager = get_async_callback_manager_for_config(config)
-        run_manager = await callback_manager.on_chain_start(
-            serialized,
-            input,
-            run_type=run_type,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-        try:
-            child_config = patch_config(config, callbacks=run_manager.get_child())
-            context = copy_context()
-            context.run(_set_config_context, child_config)
-            coro = acall_func_with_variable_args(
-                func, input, config, run_manager, **kwargs
-            )
-            if asyncio_accepts_context():
-                output: Output = await asyncio.create_task(coro, context=context)  # type: ignore
-            else:
-                output = await coro
-        except BaseException as e:
-            await run_manager.on_chain_error(e)
-            raise
-        else:
-            await run_manager.on_chain_end(output)
-            return output
-
-    def _batch_with_config(
-        self,
-        func: Union[
-            Callable[[list[Input]], list[Union[Exception, Output]]],
-            Callable[
-                [list[Input], list[CallbackManagerForChainRun]],
-                list[Union[Exception, Output]],
-            ],
-            Callable[
-                [list[Input], list[CallbackManagerForChainRun], list[RunnableConfig]],
-                list[Union[Exception, Output]],
-            ],
-        ],
-        input: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        run_type: Optional[str] = None,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        """Helper method to transform an Input value to an Output value,
-        with callbacks. Use this method to implement invoke() in subclasses.
-        """
-        if not input:
-            return []
-
-        configs = get_config_list(config, len(input))
-        callback_managers = [get_callback_manager_for_config(c) for c in configs]
-        run_managers = [
-            callback_manager.on_chain_start(
-                None,
-                input,
-                run_type=run_type,
-                name=config.get("run_name") or self.get_name(),
-                run_id=config.pop("run_id", None),
-            )
-            for callback_manager, input, config in zip(
-                callback_managers, input, configs
-            )
-        ]
-        try:
-            if accepts_config(func):
-                kwargs["config"] = [
-                    patch_config(c, callbacks=rm.get_child())
-                    for c, rm in zip(configs, run_managers)
-                ]
-            if accepts_run_manager(func):
-                kwargs["run_manager"] = run_managers
-            output = func(input, **kwargs)  # type: ignore[call-arg]
-        except BaseException as e:
-            for run_manager in run_managers:
-                run_manager.on_chain_error(e)
-            if return_exceptions:
-                return cast(list[Output], [e for _ in input])
-            else:
-                raise
-        else:
-            first_exception: Optional[Exception] = None
-            for run_manager, out in zip(run_managers, output):
-                if isinstance(out, Exception):
-                    first_exception = first_exception or out
-                    run_manager.on_chain_error(out)
-                else:
-                    run_manager.on_chain_end(out)
-            if return_exceptions or first_exception is None:
-                return cast(list[Output], output)
-            else:
-                raise first_exception
-
-    async def _abatch_with_config(
-        self,
-        func: Union[
-            Callable[[list[Input]], Awaitable[list[Union[Exception, Output]]]],
-            Callable[
-                [list[Input], list[AsyncCallbackManagerForChainRun]],
-                Awaitable[list[Union[Exception, Output]]],
-            ],
-            Callable[
-                [
-                    list[Input],
-                    list[AsyncCallbackManagerForChainRun],
-                    list[RunnableConfig],
-                ],
-                Awaitable[list[Union[Exception, Output]]],
-            ],
-        ],
-        input: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        run_type: Optional[str] = None,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        """Helper method to transform an Input value to an Output value,
-        with callbacks. Use this method to implement invoke() in subclasses.
-        """
-        if not input:
-            return []
-
-        configs = get_config_list(config, len(input))
-        callback_managers = [get_async_callback_manager_for_config(c) for c in configs]
-        run_managers: list[AsyncCallbackManagerForChainRun] = await asyncio.gather(
-            *(
-                callback_manager.on_chain_start(
-                    None,
-                    input,
-                    run_type=run_type,
-                    name=config.get("run_name") or self.get_name(),
-                    run_id=config.pop("run_id", None),
-                )
-                for callback_manager, input, config in zip(
-                    callback_managers, input, configs
-                )
-            )
-        )
-        try:
-            if accepts_config(func):
-                kwargs["config"] = [
-                    patch_config(c, callbacks=rm.get_child())
-                    for c, rm in zip(configs, run_managers)
-                ]
-            if accepts_run_manager(func):
-                kwargs["run_manager"] = run_managers
-            output = await func(input, **kwargs)  # type: ignore[call-arg]
-        except BaseException as e:
-            await asyncio.gather(
-                *(run_manager.on_chain_error(e) for run_manager in run_managers)
-            )
-            if return_exceptions:
-                return cast(list[Output], [e for _ in input])
-            else:
-                raise
-        else:
-            first_exception: Optional[Exception] = None
-            coros: list[Awaitable[None]] = []
-            for run_manager, out in zip(run_managers, output):
-                if isinstance(out, Exception):
-                    first_exception = first_exception or out
-                    coros.append(run_manager.on_chain_error(out))
-                else:
-                    coros.append(run_manager.on_chain_end(out))
-            await asyncio.gather(*coros)
-            if return_exceptions or first_exception is None:
-                return cast(list[Output], output)
-            else:
-                raise first_exception
-
-    def _transform_stream_with_config(
-        self,
-        input: Iterator[Input],
-        transformer: Union[
-            Callable[[Iterator[Input]], Iterator[Output]],
-            Callable[[Iterator[Input], CallbackManagerForChainRun], Iterator[Output]],
-            Callable[
-                [
-                    Iterator[Input],
-                    CallbackManagerForChainRun,
-                    RunnableConfig,
-                ],
-                Iterator[Output],
-            ],
-        ],
-        config: Optional[RunnableConfig],
-        run_type: Optional[str] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        """Helper method to transform an Iterator of Input values into an Iterator of
-        Output values, with callbacks.
-        Use this to implement `stream()` or `transform()` in Runnable subclasses.
-        """
-        # Mixin that is used by both astream log and astream events implementation
-        from langchain_core.tracers._streaming import _StreamingCallbackHandler
-
-        # tee the input so we can iterate over it twice
-        input_for_tracing, input_for_transform = tee(input, 2)
-        # Start the input iterator to ensure the input Runnable starts before this one
-        final_input: Optional[Input] = next(input_for_tracing, None)
-        final_input_supported = True
-        final_output: Optional[Output] = None
-        final_output_supported = True
-
-        config = ensure_config(config)
-        callback_manager = get_callback_manager_for_config(config)
-        run_manager = callback_manager.on_chain_start(
-            None,
-            {"input": ""},
-            run_type=run_type,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-        try:
-            child_config = patch_config(config, callbacks=run_manager.get_child())
-            if accepts_config(transformer):
-                kwargs["config"] = child_config
-            if accepts_run_manager(transformer):
-                kwargs["run_manager"] = run_manager
-            context = copy_context()
-            context.run(_set_config_context, child_config)
-            iterator = context.run(transformer, input_for_transform, **kwargs)  # type: ignore[arg-type]
-            if stream_handler := next(
-                (
-                    cast(_StreamingCallbackHandler, h)
-                    for h in run_manager.handlers
-                    # instance check OK here, it's a mixin
-                    if isinstance(h, _StreamingCallbackHandler)  # type: ignore[misc]
-                ),
-                None,
-            ):
-                # populates streamed_output in astream_log() output if needed
-                iterator = stream_handler.tap_output_iter(run_manager.run_id, iterator)
-            try:
-                while True:
-                    chunk: Output = context.run(next, iterator)  # type: ignore
-                    yield chunk
-                    if final_output_supported:
-                        if final_output is None:
-                            final_output = chunk
-                        else:
-                            try:
-                                final_output = final_output + chunk  # type: ignore
-                            except TypeError:
-                                final_output = chunk
-                                final_output_supported = False
-                    else:
-                        final_output = chunk
-            except (StopIteration, GeneratorExit):
-                pass
-            for ichunk in input_for_tracing:
-                if final_input_supported:
-                    if final_input is None:
-                        final_input = ichunk
-                    else:
-                        try:
-                            final_input = final_input + ichunk  # type: ignore
-                        except TypeError:
-                            final_input = ichunk
-                            final_input_supported = False
-                else:
-                    final_input = ichunk
-        except BaseException as e:
-            run_manager.on_chain_error(e, inputs=final_input)
-            raise
-        else:
-            run_manager.on_chain_end(final_output, inputs=final_input)
-
-    async def _atransform_stream_with_config(
-        self,
-        input: AsyncIterator[Input],
-        transformer: Union[
-            Callable[[AsyncIterator[Input]], AsyncIterator[Output]],
-            Callable[
-                [AsyncIterator[Input], AsyncCallbackManagerForChainRun],
-                AsyncIterator[Output],
-            ],
-            Callable[
-                [
-                    AsyncIterator[Input],
-                    AsyncCallbackManagerForChainRun,
-                    RunnableConfig,
-                ],
-                AsyncIterator[Output],
-            ],
-        ],
-        config: Optional[RunnableConfig],
-        run_type: Optional[str] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        """Helper method to transform an Async Iterator of Input values into an Async
-        Iterator of Output values, with callbacks.
-        Use this to implement `astream()` or `atransform()` in Runnable subclasses.
-        """
-        # Mixin that is used by both astream log and astream events implementation
-        from langchain_core.tracers._streaming import _StreamingCallbackHandler
-
-        # tee the input so we can iterate over it twice
-        input_for_tracing, input_for_transform = atee(input, 2)
-        # Start the input iterator to ensure the input Runnable starts before this one
-        final_input: Optional[Input] = await py_anext(input_for_tracing, None)
-        final_input_supported = True
-        final_output: Optional[Output] = None
-        final_output_supported = True
-
-        config = ensure_config(config)
-        callback_manager = get_async_callback_manager_for_config(config)
-        run_manager = await callback_manager.on_chain_start(
-            None,
-            {"input": ""},
-            run_type=run_type,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-        try:
-            child_config = patch_config(config, callbacks=run_manager.get_child())
-            if accepts_config(transformer):
-                kwargs["config"] = child_config
-            if accepts_run_manager(transformer):
-                kwargs["run_manager"] = run_manager
-            context = copy_context()
-            context.run(_set_config_context, child_config)
-            iterator_ = context.run(transformer, input_for_transform, **kwargs)  # type: ignore[arg-type]
-
-            if stream_handler := next(
-                (
-                    cast(_StreamingCallbackHandler, h)
-                    for h in run_manager.handlers
-                    # instance check OK here, it's a mixin
-                    if isinstance(h, _StreamingCallbackHandler)  # type: ignore[misc]
-                ),
-                None,
-            ):
-                # populates streamed_output in astream_log() output if needed
-                iterator = stream_handler.tap_output_aiter(
-                    run_manager.run_id, iterator_
-                )
-            else:
-                iterator = iterator_
-            try:
-                while True:
-                    if asyncio_accepts_context():
-                        chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
-                            py_anext(iterator),  # type: ignore[arg-type]
-                            context=context,
-                        )
-                    else:
-                        chunk = cast(Output, await py_anext(iterator))
-                    yield chunk
-                    if final_output_supported:
-                        if final_output is None:
-                            final_output = chunk
-                        else:
-                            try:
-                                final_output = final_output + chunk  # type: ignore
-                            except TypeError:
-                                final_output = chunk
-                                final_output_supported = False
-                    else:
-                        final_output = chunk
-            except StopAsyncIteration:
-                pass
-            async for ichunk in input_for_tracing:
-                if final_input_supported:
-                    if final_input is None:
-                        final_input = ichunk
-                    else:
-                        try:
-                            final_input = final_input + ichunk  # type: ignore[operator]
-                        except TypeError:
-                            final_input = ichunk
-                            final_input_supported = False
-                else:
-                    final_input = ichunk
-        except BaseException as e:
-            await run_manager.on_chain_error(e, inputs=final_input)
-            raise
-        else:
-            await run_manager.on_chain_end(final_output, inputs=final_input)
-        finally:
-            if iterator_ is not None and hasattr(iterator_, "aclose"):
-                await iterator_.aclose()
-
-    @beta_decorator.beta(message="This API is in beta and may change in the future.")
-    def as_tool(
-        self,
-        args_schema: Optional[type[BaseModel]] = None,
-        *,
-        name: Optional[str] = None,
-        description: Optional[str] = None,
-        arg_types: Optional[dict[str, type]] = None,
-    ) -> BaseTool:
-        """Create a BaseTool from a Runnable.
-
-        ``as_tool`` will instantiate a BaseTool with a name, description, and
-        ``args_schema`` from a Runnable. Where possible, schemas are inferred
-        from ``runnable.get_input_schema``. Alternatively (e.g., if the
-        Runnable takes a dict as input and the specific dict keys are not typed),
-        the schema can be specified directly with ``args_schema``. You can also
-        pass ``arg_types`` to just specify the required arguments and their types.
-
-        Args:
-            args_schema: The schema for the tool. Defaults to None.
-            name: The name of the tool. Defaults to None.
-            description: The description of the tool. Defaults to None.
-            arg_types: A dictionary of argument names to types. Defaults to None.
-
-        Returns:
-            A BaseTool instance.
-
-        Typed dict input:
-
-        .. code-block:: python
-
-            from typing import List
-            from typing_extensions import TypedDict
-            from langchain_core.runnables import RunnableLambda
-
-            class Args(TypedDict):
-                a: int
-                b: List[int]
-
-            def f(x: Args) -> str:
-                return str(x["a"] * max(x["b"]))
-
-            runnable = RunnableLambda(f)
-            as_tool = runnable.as_tool()
-            as_tool.invoke({"a": 3, "b": [1, 2]})
-
-        ``dict`` input, specifying schema via ``args_schema``:
-
-        .. code-block:: python
-
-            from typing import Any, Dict, List
-            from pydantic import BaseModel, Field
-            from langchain_core.runnables import RunnableLambda
-
-            def f(x: Dict[str, Any]) -> str:
-                return str(x["a"] * max(x["b"]))
-
-            class FSchema(BaseModel):
-                \"\"\"Apply a function to an integer and list of integers.\"\"\"
-
-                a: int = Field(..., description="Integer")
-                b: List[int] = Field(..., description="List of ints")
-
-            runnable = RunnableLambda(f)
-            as_tool = runnable.as_tool(FSchema)
-            as_tool.invoke({"a": 3, "b": [1, 2]})
-
-        ``dict`` input, specifying schema via ``arg_types``:
-
-        .. code-block:: python
-
-            from typing import Any, Dict, List
-            from langchain_core.runnables import RunnableLambda
-
-            def f(x: Dict[str, Any]) -> str:
-                return str(x["a"] * max(x["b"]))
-
-            runnable = RunnableLambda(f)
-            as_tool = runnable.as_tool(arg_types={"a": int, "b": List[int]})
-            as_tool.invoke({"a": 3, "b": [1, 2]})
-
-        String input:
-
-        .. code-block:: python
-
-            from langchain_core.runnables import RunnableLambda
-
-            def f(x: str) -> str:
-                return x + "a"
-
-            def g(x: str) -> str:
-                return x + "z"
-
-            runnable = RunnableLambda(f) | g
-            as_tool = runnable.as_tool()
-            as_tool.invoke("b")
-
-        .. versionadded:: 0.2.14
-        """
-        # Avoid circular import
-        from langchain_core.tools import convert_runnable_to_tool
-
-        return convert_runnable_to_tool(
-            self,
-            args_schema=args_schema,
-            name=name,
-            description=description,
-            arg_types=arg_types,
-        )
-
-
-class RunnableSerializable(Serializable, Runnable[Input, Output]):
-    """Runnable that can be serialized to JSON."""
-
-    name: Optional[str] = None
-
-    model_config = ConfigDict(
-        # Suppress warnings from pydantic protected namespaces
-        # (e.g., `model_`)
-        protected_namespaces=(),
-    )
-
-    def to_json(self) -> Union[SerializedConstructor, SerializedNotImplemented]:
-        """Serialize the Runnable to JSON.
-
-        Returns:
-            A JSON-serializable representation of the Runnable.
-        """
-        dumped = super().to_json()
-        with contextlib.suppress(Exception):
-            dumped["name"] = self.get_name()
-        return dumped
-
-    def configurable_fields(
-        self, **kwargs: AnyConfigurableField
-    ) -> RunnableSerializable[Input, Output]:
-        """Configure particular Runnable fields at runtime.
-
-        Args:
-            **kwargs: A dictionary of ConfigurableField instances to configure.
-
-        Returns:
-            A new Runnable with the fields configured.
-
-        .. code-block:: python
-
-            from langchain_core.runnables import ConfigurableField
-            from langchain_openai import ChatOpenAI
-
-            model = ChatOpenAI(max_tokens=20).configurable_fields(
-                max_tokens=ConfigurableField(
-                    id="output_token_number",
-                    name="Max tokens in the output",
-                    description="The maximum number of tokens in the output",
-                )
-            )
-
-            # max_tokens = 20
-            print(
-                "max_tokens_20: ",
-                model.invoke("tell me something about chess").content
-            )
-
-            # max_tokens = 200
-            print("max_tokens_200: ", model.with_config(
-                configurable={"output_token_number": 200}
-                ).invoke("tell me something about chess").content
-            )
-        """
-        from langchain_core.runnables.configurable import RunnableConfigurableFields
-
-        for key in kwargs:
-            if key not in self.model_fields:
-                msg = (
-                    f"Configuration key {key} not found in {self}: "
-                    f"available keys are {self.model_fields.keys()}"
-                )
-                raise ValueError(msg)
-
-        return RunnableConfigurableFields(default=self, fields=kwargs)
-
-    def configurable_alternatives(
-        self,
-        which: ConfigurableField,
-        *,
-        default_key: str = "default",
-        prefix_keys: bool = False,
-        **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]],
-    ) -> RunnableSerializable[Input, Output]:
-        """Configure alternatives for Runnables that can be set at runtime.
-
-        Args:
-            which: The ConfigurableField instance that will be used to select the
-                alternative.
-            default_key: The default key to use if no alternative is selected.
-                Defaults to "default".
-            prefix_keys: Whether to prefix the keys with the ConfigurableField id.
-                Defaults to False.
-            **kwargs: A dictionary of keys to Runnable instances or callables that
-                return Runnable instances.
-
-        Returns:
-            A new Runnable with the alternatives configured.
-
-        .. code-block:: python
-
-            from langchain_anthropic import ChatAnthropic
-            from langchain_core.runnables.utils import ConfigurableField
-            from langchain_openai import ChatOpenAI
-
-            model = ChatAnthropic(
-                model_name="claude-3-sonnet-20240229"
-            ).configurable_alternatives(
-                ConfigurableField(id="llm"),
-                default_key="anthropic",
-                openai=ChatOpenAI()
-            )
-
-            # uses the default model ChatAnthropic
-            print(model.invoke("which organization created you?").content)
-
-            # uses ChatOpenAI
-            print(
-                model.with_config(
-                    configurable={"llm": "openai"}
-                ).invoke("which organization created you?").content
-            )
-        """
-        from langchain_core.runnables.configurable import (
-            RunnableConfigurableAlternatives,
-        )
-
-        return RunnableConfigurableAlternatives(
-            which=which,
-            default=self,
-            alternatives=kwargs,
-            default_key=default_key,
-            prefix_keys=prefix_keys,
-        )
-
-
-def _seq_input_schema(
-    steps: list[Runnable[Any, Any]], config: Optional[RunnableConfig]
-) -> type[BaseModel]:
-    from langchain_core.runnables.passthrough import RunnableAssign, RunnablePick
-
-    first = steps[0]
-    if len(steps) == 1:
-        return first.get_input_schema(config)
-    elif isinstance(first, RunnableAssign):
-        next_input_schema = _seq_input_schema(steps[1:], config)
-        if not issubclass(next_input_schema, RootModel):
-            # it's a dict as expected
-            return create_model_v2(  # type: ignore[call-overload]
-                "RunnableSequenceInput",
-                field_definitions={
-                    k: (v.annotation, v.default)
-                    for k, v in next_input_schema.model_fields.items()
-                    if k not in first.mapper.steps__
-                },
-            )
-    elif isinstance(first, RunnablePick):
-        return _seq_input_schema(steps[1:], config)
-
-    return first.get_input_schema(config)
-
-
-def _seq_output_schema(
-    steps: list[Runnable[Any, Any]], config: Optional[RunnableConfig]
-) -> type[BaseModel]:
-    from langchain_core.runnables.passthrough import RunnableAssign, RunnablePick
-
-    last = steps[-1]
-    if len(steps) == 1:
-        return last.get_input_schema(config)
-    elif isinstance(last, RunnableAssign):
-        mapper_output_schema = last.mapper.get_output_schema(config)
-        prev_output_schema = _seq_output_schema(steps[:-1], config)
-        if not issubclass(prev_output_schema, RootModel):
-            # it's a dict as expected
-            return create_model_v2(  # type: ignore[call-overload]
-                "RunnableSequenceOutput",
-                field_definitions={
-                    **{
-                        k: (v.annotation, v.default)
-                        for k, v in prev_output_schema.model_fields.items()
-                    },
-                    **{
-                        k: (v.annotation, v.default)
-                        for k, v in mapper_output_schema.model_fields.items()
-                    },
-                },
-            )
-    elif isinstance(last, RunnablePick):
-        prev_output_schema = _seq_output_schema(steps[:-1], config)
-        if not issubclass(prev_output_schema, RootModel):
-            # it's a dict as expected
-            if isinstance(last.keys, list):
-                return create_model_v2(  # type: ignore[call-overload]
-                    "RunnableSequenceOutput",
-                    field_definitions={
-                        k: (v.annotation, v.default)
-                        for k, v in prev_output_schema.model_fields.items()
-                        if k in last.keys
-                    },
-                )
-            else:
-                field = prev_output_schema.model_fields[last.keys]
-                return create_model_v2(  # type: ignore[call-overload]
-                    "RunnableSequenceOutput", root=(field.annotation, field.default)
-                )
-
-    return last.get_output_schema(config)
-
-
-class RunnableSequence(RunnableSerializable[Input, Output]):
-    """Sequence of Runnables, where the output of each is the input of the next.
-
-    **RunnableSequence** is the most important composition operator in LangChain
-    as it is used in virtually every chain.
-
-    A RunnableSequence can be instantiated directly or more commonly by using the `|`
-    operator where either the left or right operands (or both) must be a Runnable.
-
-    Any RunnableSequence automatically supports sync, async, batch.
-
-    The default implementations of `batch` and `abatch` utilize threadpools and
-    asyncio gather and will be faster than naive invocation of invoke or ainvoke
-    for IO bound Runnables.
-
-    Batching is implemented by invoking the batch method on each component of the
-    RunnableSequence in order.
-
-    A RunnableSequence preserves the streaming properties of its components, so if all
-    components of the sequence implement a `transform` method -- which
-    is the method that implements the logic to map a streaming input to a streaming
-    output -- then the sequence will be able to stream input to output!
-
-    If any component of the sequence does not implement transform then the
-    streaming will only begin after this component is run. If there are
-    multiple blocking components, streaming begins after the last one.
-
-    Please note: RunnableLambdas do not support `transform` by default! So if
-        you need to use a RunnableLambdas be careful about where you place them in a
-        RunnableSequence (if you need to use the .stream()/.astream() methods).
-
-        If you need arbitrary logic and need streaming, you can subclass
-        Runnable, and implement `transform` for whatever logic you need.
-
-    Here is a simple example that uses simple functions to illustrate the use of
-    RunnableSequence:
-
-        .. code-block:: python
-
-            from langchain_core.runnables import RunnableLambda
-
-            def add_one(x: int) -> int:
-                return x + 1
-
-            def mul_two(x: int) -> int:
-                return x * 2
-
-            runnable_1 = RunnableLambda(add_one)
-            runnable_2 = RunnableLambda(mul_two)
-            sequence = runnable_1 | runnable_2
-            # Or equivalently:
-            # sequence = RunnableSequence(first=runnable_1, last=runnable_2)
-            sequence.invoke(1)
-            await sequence.ainvoke(1)
-
-            sequence.batch([1, 2, 3])
-            await sequence.abatch([1, 2, 3])
-
-    Here's an example that uses streams JSON output generated by an LLM:
-
-        .. code-block:: python
-
-            from langchain_core.output_parsers.json import SimpleJsonOutputParser
-            from langchain_openai import ChatOpenAI
-
-            prompt = PromptTemplate.from_template(
-                'In JSON format, give me a list of {topic} and their '
-                'corresponding names in French, Spanish and in a '
-                'Cat Language.'
-            )
-
-            model = ChatOpenAI()
-            chain = prompt | model | SimpleJsonOutputParser()
-
-            async for chunk in chain.astream({'topic': 'colors'}):
-                print('-')  # noqa: T201
-                print(chunk, sep='', flush=True)  # noqa: T201
-    """
-
-    # The steps are broken into first, middle and last, solely for type checking
-    # purposes. It allows specifying the `Input` on the first type, the `Output` of
-    # the last type.
-    first: Runnable[Input, Any]
-    """The first Runnable in the sequence."""
-    middle: list[Runnable[Any, Any]] = Field(default_factory=list)
-    """The middle Runnables in the sequence."""
-    last: Runnable[Any, Output]
-    """The last Runnable in the sequence."""
-
-    def __init__(
-        self,
-        *steps: RunnableLike,
-        name: Optional[str] = None,
-        first: Optional[Runnable[Any, Any]] = None,
-        middle: Optional[list[Runnable[Any, Any]]] = None,
-        last: Optional[Runnable[Any, Any]] = None,
-    ) -> None:
-        """Create a new RunnableSequence.
-
-        Args:
-            steps: The steps to include in the sequence.
-            name: The name of the Runnable. Defaults to None.
-            first: The first Runnable in the sequence. Defaults to None.
-            middle: The middle Runnables in the sequence. Defaults to None.
-            last: The last Runnable in the sequence. Defaults to None.
-
-        Raises:
-            ValueError: If the sequence has less than 2 steps.
-        """
-        steps_flat: list[Runnable] = []
-        if not steps and first is not None and last is not None:
-            steps_flat = [first] + (middle or []) + [last]
-        for step in steps:
-            if isinstance(step, RunnableSequence):
-                steps_flat.extend(step.steps)
-            else:
-                steps_flat.append(coerce_to_runnable(step))
-        if len(steps_flat) < 2:
-            msg = f"RunnableSequence must have at least 2 steps, got {len(steps_flat)}"
-            raise ValueError(msg)
-        super().__init__(  # type: ignore[call-arg]
-            first=steps_flat[0],
-            middle=list(steps_flat[1:-1]),
-            last=steps_flat[-1],
-            name=name,
-        )
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    @property
-    def steps(self) -> list[Runnable[Any, Any]]:
-        """All the Runnables that make up the sequence in order.
-
-        Returns:
-            A list of Runnables.
-        """
-        return [self.first] + self.middle + [self.last]
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Check if the object is serializable.
-
-        Returns:
-            True if the object is serializable, False otherwise.
-                Defaults to True.
-        """
-        return True
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    @property
-    @override
-    def InputType(self) -> type[Input]:
-        """The type of the input to the Runnable."""
-        return self.first.InputType
-
-    @property
-    @override
-    def OutputType(self) -> type[Output]:
-        """The type of the output of the Runnable."""
-        return self.last.OutputType
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        """Get the input schema of the Runnable.
-
-        Args:
-            config: The config to use. Defaults to None.
-
-        Returns:
-            The input schema of the Runnable.
-        """
-        return _seq_input_schema(self.steps, config)
-
-    def get_output_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        """Get the output schema of the Runnable.
-
-        Args:
-            config: The config to use. Defaults to None.
-
-        Returns:
-            The output schema of the Runnable.
-        """
-        return _seq_output_schema(self.steps, config)
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        """Get the config specs of the Runnable.
-
-        Returns:
-            The config specs of the Runnable.
-        """
-        from langchain_core.beta.runnables.context import (
-            CONTEXT_CONFIG_PREFIX,
-            _key_from_id,
-        )
-
-        # get all specs
-        all_specs = [
-            (spec, idx)
-            for idx, step in enumerate(self.steps)
-            for spec in step.config_specs
-        ]
-        # calculate context dependencies
-        specs_by_pos = groupby(
-            [tup for tup in all_specs if tup[0].id.startswith(CONTEXT_CONFIG_PREFIX)],
-            itemgetter(1),
-        )
-        next_deps: set[str] = set()
-        deps_by_pos: dict[int, set[str]] = {}
-        for pos, specs in specs_by_pos:
-            deps_by_pos[pos] = next_deps
-            next_deps = next_deps | {spec[0].id for spec in specs}
-        # assign context dependencies
-        for pos, (spec, idx) in enumerate(all_specs):
-            if spec.id.startswith(CONTEXT_CONFIG_PREFIX):
-                all_specs[pos] = (
-                    ConfigurableFieldSpec(
-                        id=spec.id,
-                        annotation=spec.annotation,
-                        name=spec.name,
-                        default=spec.default,
-                        description=spec.description,
-                        is_shared=spec.is_shared,
-                        dependencies=[
-                            d
-                            for d in deps_by_pos[idx]
-                            if _key_from_id(d) != _key_from_id(spec.id)
-                        ]
-                        + (spec.dependencies or []),
-                    ),
-                    idx,
-                )
-
-        return get_unique_config_specs(spec for spec, _ in all_specs)
-
-    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
-        """Get the graph representation of the Runnable.
-
-        Args:
-            config: The config to use. Defaults to None.
-
-        Returns:
-            The graph representation of the Runnable.
-
-        Raises:
-            ValueError: If a Runnable has no first or last node.
-        """
-        from langchain_core.runnables.graph import Graph
-
-        graph = Graph()
-        for step in self.steps:
-            current_last_node = graph.last_node()
-            step_graph = step.get_graph(config)
-            if step is not self.first:
-                step_graph.trim_first_node()
-            if step is not self.last:
-                step_graph.trim_last_node()
-            step_first_node, _ = graph.extend(step_graph)
-            if not step_first_node:
-                msg = f"Runnable {step} has no first node"
-                raise ValueError(msg)
-            if current_last_node:
-                graph.add_edge(current_last_node, step_first_node)
-
-        return graph
-
-    def __repr__(self) -> str:
-        return "\n| ".join(
-            repr(s) if i == 0 else indent_lines_after_first(repr(s), "| ")
-            for i, s in enumerate(self.steps)
-        )
-
-    def __or__(
-        self,
-        other: Union[
-            Runnable[Any, Other],
-            Callable[[Any], Other],
-            Callable[[Iterator[Any]], Iterator[Other]],
-            Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],
-        ],
-    ) -> RunnableSerializable[Input, Other]:
-        if isinstance(other, RunnableSequence):
-            return RunnableSequence(
-                self.first,
-                *self.middle,
-                self.last,
-                other.first,
-                *other.middle,
-                other.last,
-                name=self.name or other.name,
-            )
-        else:
-            return RunnableSequence(
-                self.first,
-                *self.middle,
-                self.last,
-                coerce_to_runnable(other),
-                name=self.name,
-            )
-
-    def __ror__(
-        self,
-        other: Union[
-            Runnable[Other, Any],
-            Callable[[Other], Any],
-            Callable[[Iterator[Other]], Iterator[Any]],
-            Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],
-        ],
-    ) -> RunnableSerializable[Other, Output]:
-        if isinstance(other, RunnableSequence):
-            return RunnableSequence(
-                other.first,
-                *other.middle,
-                other.last,
-                self.first,
-                *self.middle,
-                self.last,
-                name=other.name or self.name,
-            )
-        else:
-            return RunnableSequence(
-                coerce_to_runnable(other),
-                self.first,
-                *self.middle,
-                self.last,
-                name=self.name,
-            )
-
-    def invoke(
-        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Output:
-        from langchain_core.beta.runnables.context import config_with_context
-
-        # setup callbacks and context
-        config = config_with_context(ensure_config(config), self.steps)
-        callback_manager = get_callback_manager_for_config(config)
-        # start the root run
-        run_manager = callback_manager.on_chain_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-
-        # invoke all steps in sequence
-        try:
-            for i, step in enumerate(self.steps):
-                # mark each step as a child run
-                config = patch_config(
-                    config, callbacks=run_manager.get_child(f"seq:step:{i + 1}")
-                )
-                context = copy_context()
-                context.run(_set_config_context, config)
-                if i == 0:
-                    input = context.run(step.invoke, input, config, **kwargs)
-                else:
-                    input = context.run(step.invoke, input, config)
-        # finish the root run
-        except BaseException as e:
-            run_manager.on_chain_error(e)
-            raise
-        else:
-            run_manager.on_chain_end(input)
-            return cast(Output, input)
-
-    async def ainvoke(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Output:
-        from langchain_core.beta.runnables.context import aconfig_with_context
-
-        # setup callbacks and context
-        config = aconfig_with_context(ensure_config(config), self.steps)
-        callback_manager = get_async_callback_manager_for_config(config)
-        # start the root run
-        run_manager = await callback_manager.on_chain_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-
-        # invoke all steps in sequence
-        try:
-            for i, step in enumerate(self.steps):
-                # mark each step as a child run
-                config = patch_config(
-                    config, callbacks=run_manager.get_child(f"seq:step:{i + 1}")
-                )
-                context = copy_context()
-                context.run(_set_config_context, config)
-                if i == 0:
-                    part = functools.partial(step.ainvoke, input, config, **kwargs)
-                else:
-                    part = functools.partial(step.ainvoke, input, config)
-                if asyncio_accepts_context():
-                    input = await asyncio.create_task(part(), context=context)  # type: ignore
-                else:
-                    input = await asyncio.create_task(part())
-        # finish the root run
-        except BaseException as e:
-            await run_manager.on_chain_error(e)
-            raise
-        else:
-            await run_manager.on_chain_end(input)
-            return cast(Output, input)
-
-    def batch(
-        self,
-        inputs: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        from langchain_core.beta.runnables.context import config_with_context
-        from langchain_core.callbacks.manager import CallbackManager
-
-        if not inputs:
-            return []
-
-        # setup callbacks and context
-        configs = [
-            config_with_context(c, self.steps)
-            for c in get_config_list(config, len(inputs))
-        ]
-        callback_managers = [
-            CallbackManager.configure(
-                inheritable_callbacks=config.get("callbacks"),
-                local_callbacks=None,
-                verbose=False,
-                inheritable_tags=config.get("tags"),
-                local_tags=None,
-                inheritable_metadata=config.get("metadata"),
-                local_metadata=None,
-            )
-            for config in configs
-        ]
-        # start the root runs, one per input
-        run_managers = [
-            cm.on_chain_start(
-                None,
-                input,
-                name=config.get("run_name") or self.get_name(),
-                run_id=config.pop("run_id", None),
-            )
-            for cm, input, config in zip(callback_managers, inputs, configs)
-        ]
-
-        # invoke
-        try:
-            if return_exceptions:
-                # Track which inputs (by index) failed so far
-                # If an input has failed it will be present in this map,
-                # and the value will be the exception that was raised.
-                failed_inputs_map: dict[int, Exception] = {}
-                for stepidx, step in enumerate(self.steps):
-                    # Assemble the original indexes of the remaining inputs
-                    # (i.e. the ones that haven't failed yet)
-                    remaining_idxs = [
-                        i for i in range(len(configs)) if i not in failed_inputs_map
-                    ]
-                    # Invoke the step on the remaining inputs
-                    inputs = step.batch(
-                        [
-                            inp
-                            for i, inp in zip(remaining_idxs, inputs)
-                            if i not in failed_inputs_map
-                        ],
-                        [
-                            # each step a child run of the corresponding root run
-                            patch_config(
-                                config,
-                                callbacks=rm.get_child(f"seq:step:{stepidx + 1}"),
-                            )
-                            for i, (rm, config) in enumerate(zip(run_managers, configs))
-                            if i not in failed_inputs_map
-                        ],
-                        return_exceptions=return_exceptions,
-                        **(kwargs if stepidx == 0 else {}),
-                    )
-                    # If an input failed, add it to the map
-                    for i, inp in zip(remaining_idxs, inputs):
-                        if isinstance(inp, Exception):
-                            failed_inputs_map[i] = inp
-                    inputs = [inp for inp in inputs if not isinstance(inp, Exception)]
-                    # If all inputs have failed, stop processing
-                    if len(failed_inputs_map) == len(configs):
-                        break
-
-                # Reassemble the outputs, inserting Exceptions for failed inputs
-                inputs_copy = inputs.copy()
-                inputs = []
-                for i in range(len(configs)):
-                    if i in failed_inputs_map:
-                        inputs.append(cast(Input, failed_inputs_map[i]))
-                    else:
-                        inputs.append(inputs_copy.pop(0))
-            else:
-                for i, step in enumerate(self.steps):
-                    inputs = step.batch(
-                        inputs,
-                        [
-                            # each step a child run of the corresponding root run
-                            patch_config(
-                                config, callbacks=rm.get_child(f"seq:step:{i + 1}")
-                            )
-                            for rm, config in zip(run_managers, configs)
-                        ],
-                        return_exceptions=return_exceptions,
-                        **(kwargs if i == 0 else {}),
-                    )
-
-        # finish the root runs
-        except BaseException as e:
-            for rm in run_managers:
-                rm.on_chain_error(e)
-            if return_exceptions:
-                return cast(list[Output], [e for _ in inputs])
-            else:
-                raise
-        else:
-            first_exception: Optional[Exception] = None
-            for run_manager, out in zip(run_managers, inputs):
-                if isinstance(out, Exception):
-                    first_exception = first_exception or out
-                    run_manager.on_chain_error(out)
-                else:
-                    run_manager.on_chain_end(out)
-            if return_exceptions or first_exception is None:
-                return cast(list[Output], inputs)
-            else:
-                raise first_exception
-
-    async def abatch(
-        self,
-        inputs: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        from langchain_core.beta.runnables.context import aconfig_with_context
-        from langchain_core.callbacks.manager import AsyncCallbackManager
-
-        if not inputs:
-            return []
-
-        # setup callbacks and context
-        configs = [
-            aconfig_with_context(c, self.steps)
-            for c in get_config_list(config, len(inputs))
-        ]
-        callback_managers = [
-            AsyncCallbackManager.configure(
-                inheritable_callbacks=config.get("callbacks"),
-                local_callbacks=None,
-                verbose=False,
-                inheritable_tags=config.get("tags"),
-                local_tags=None,
-                inheritable_metadata=config.get("metadata"),
-                local_metadata=None,
-            )
-            for config in configs
-        ]
-        # start the root runs, one per input
-        run_managers: list[AsyncCallbackManagerForChainRun] = await asyncio.gather(
-            *(
-                cm.on_chain_start(
-                    None,
-                    input,
-                    name=config.get("run_name") or self.get_name(),
-                    run_id=config.pop("run_id", None),
-                )
-                for cm, input, config in zip(callback_managers, inputs, configs)
-            )
-        )
-
-        # invoke .batch() on each step
-        # this uses batching optimizations in Runnable subclasses, like LLM
-        try:
-            if return_exceptions:
-                # Track which inputs (by index) failed so far
-                # If an input has failed it will be present in this map,
-                # and the value will be the exception that was raised.
-                failed_inputs_map: dict[int, Exception] = {}
-                for stepidx, step in enumerate(self.steps):
-                    # Assemble the original indexes of the remaining inputs
-                    # (i.e. the ones that haven't failed yet)
-                    remaining_idxs = [
-                        i for i in range(len(configs)) if i not in failed_inputs_map
-                    ]
-                    # Invoke the step on the remaining inputs
-                    inputs = await step.abatch(
-                        [
-                            inp
-                            for i, inp in zip(remaining_idxs, inputs)
-                            if i not in failed_inputs_map
-                        ],
-                        [
-                            # each step a child run of the corresponding root run
-                            patch_config(
-                                config,
-                                callbacks=rm.get_child(f"seq:step:{stepidx + 1}"),
-                            )
-                            for i, (rm, config) in enumerate(zip(run_managers, configs))
-                            if i not in failed_inputs_map
-                        ],
-                        return_exceptions=return_exceptions,
-                        **(kwargs if stepidx == 0 else {}),
-                    )
-                    # If an input failed, add it to the map
-                    for i, inp in zip(remaining_idxs, inputs):
-                        if isinstance(inp, Exception):
-                            failed_inputs_map[i] = inp
-                    inputs = [inp for inp in inputs if not isinstance(inp, Exception)]
-                    # If all inputs have failed, stop processing
-                    if len(failed_inputs_map) == len(configs):
-                        break
-
-                # Reassemble the outputs, inserting Exceptions for failed inputs
-                inputs_copy = inputs.copy()
-                inputs = []
-                for i in range(len(configs)):
-                    if i in failed_inputs_map:
-                        inputs.append(cast(Input, failed_inputs_map[i]))
-                    else:
-                        inputs.append(inputs_copy.pop(0))
-            else:
-                for i, step in enumerate(self.steps):
-                    inputs = await step.abatch(
-                        inputs,
-                        [
-                            # each step a child run of the corresponding root run
-                            patch_config(
-                                config, callbacks=rm.get_child(f"seq:step:{i + 1}")
-                            )
-                            for rm, config in zip(run_managers, configs)
-                        ],
-                        return_exceptions=return_exceptions,
-                        **(kwargs if i == 0 else {}),
-                    )
-        # finish the root runs
-        except BaseException as e:
-            await asyncio.gather(*(rm.on_chain_error(e) for rm in run_managers))
-            if return_exceptions:
-                return cast(list[Output], [e for _ in inputs])
-            else:
-                raise
-        else:
-            first_exception: Optional[Exception] = None
-            coros: list[Awaitable[None]] = []
-            for run_manager, out in zip(run_managers, inputs):
-                if isinstance(out, Exception):
-                    first_exception = first_exception or out
-                    coros.append(run_manager.on_chain_error(out))
-                else:
-                    coros.append(run_manager.on_chain_end(out))
-            await asyncio.gather(*coros)
-            if return_exceptions or first_exception is None:
-                return cast(list[Output], inputs)
-            else:
-                raise first_exception
-
-    def _transform(
-        self,
-        input: Iterator[Input],
-        run_manager: CallbackManagerForChainRun,
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> Iterator[Output]:
-        from langchain_core.beta.runnables.context import config_with_context
-
-        steps = [self.first] + self.middle + [self.last]
-        config = config_with_context(config, self.steps)
-
-        # transform the input stream of each step with the next
-        # steps that don't natively support transforming an input stream will
-        # buffer input in memory until all available, and then start emitting output
-        final_pipeline = cast(Iterator[Output], input)
-        for idx, step in enumerate(steps):
-            config = patch_config(
-                config, callbacks=run_manager.get_child(f"seq:step:{idx + 1}")
-            )
-            if idx == 0:
-                final_pipeline = step.transform(final_pipeline, config, **kwargs)
-            else:
-                final_pipeline = step.transform(final_pipeline, config)
-
-        yield from final_pipeline
-
-    async def _atransform(
-        self,
-        input: AsyncIterator[Input],
-        run_manager: AsyncCallbackManagerForChainRun,
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> AsyncIterator[Output]:
-        from langchain_core.beta.runnables.context import aconfig_with_context
-
-        steps = [self.first] + self.middle + [self.last]
-        config = aconfig_with_context(config, self.steps)
-
-        # stream the last steps
-        # transform the input stream of each step with the next
-        # steps that don't natively support transforming an input stream will
-        # buffer input in memory until all available, and then start emitting output
-        final_pipeline = cast(AsyncIterator[Output], input)
-        for idx, step in enumerate(steps):
-            config = patch_config(
-                config,
-                callbacks=run_manager.get_child(f"seq:step:{idx + 1}"),
-            )
-            if idx == 0:
-                final_pipeline = step.atransform(final_pipeline, config, **kwargs)
-            else:
-                final_pipeline = step.atransform(final_pipeline, config)
-        async for output in final_pipeline:
-            yield output
-
-    def transform(
-        self,
-        input: Iterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        yield from self._transform_stream_with_config(
-            input,
-            self._transform,
-            patch_config(config, run_name=(config or {}).get("run_name") or self.name),
-            **kwargs,
-        )
-
-    def stream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        yield from self.transform(iter([input]), config, **kwargs)
-
-    async def atransform(
-        self,
-        input: AsyncIterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        async for chunk in self._atransform_stream_with_config(
-            input,
-            self._atransform,
-            patch_config(config, run_name=(config or {}).get("run_name") or self.name),
-            **kwargs,
-        ):
-            yield chunk
-
-    async def astream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        async def input_aiter() -> AsyncIterator[Input]:
-            yield input
-
-        async for chunk in self.atransform(input_aiter(), config, **kwargs):
-            yield chunk
-
-
-class RunnableParallel(RunnableSerializable[Input, dict[str, Any]]):
-    """Runnable that runs a mapping of Runnables in parallel, and returns a mapping
-    of their outputs.
-
-    RunnableParallel is one of the two main composition primitives for the LCEL,
-    alongside RunnableSequence. It invokes Runnables concurrently, providing the same
-    input to each.
-
-    A RunnableParallel can be instantiated directly or by using a dict literal within a
-    sequence.
-
-    Here is a simple example that uses functions to illustrate the use of
-    RunnableParallel:
-
-        .. code-block:: python
-
-            from langchain_core.runnables import RunnableLambda
-
-            def add_one(x: int) -> int:
-                return x + 1
-
-            def mul_two(x: int) -> int:
-                return x * 2
-
-            def mul_three(x: int) -> int:
-                return x * 3
-
-            runnable_1 = RunnableLambda(add_one)
-            runnable_2 = RunnableLambda(mul_two)
-            runnable_3 = RunnableLambda(mul_three)
-
-            sequence = runnable_1 | {  # this dict is coerced to a RunnableParallel
-                "mul_two": runnable_2,
-                "mul_three": runnable_3,
-            }
-            # Or equivalently:
-            # sequence = runnable_1 | RunnableParallel(
-            #     {"mul_two": runnable_2, "mul_three": runnable_3}
-            # )
-            # Also equivalently:
-            # sequence = runnable_1 | RunnableParallel(
-            #     mul_two=runnable_2,
-            #     mul_three=runnable_3,
-            # )
-
-            sequence.invoke(1)
-            await sequence.ainvoke(1)
-
-            sequence.batch([1, 2, 3])
-            await sequence.abatch([1, 2, 3])
-
-    RunnableParallel makes it easy to run Runnables in parallel. In the below example,
-    we simultaneously stream output from two different Runnables:
-
-        .. code-block:: python
-
-            from langchain_core.prompts import ChatPromptTemplate
-            from langchain_core.runnables import RunnableParallel
-            from langchain_openai import ChatOpenAI
-
-            model = ChatOpenAI()
-            joke_chain = (
-                ChatPromptTemplate.from_template("tell me a joke about {topic}")
-                | model
-            )
-            poem_chain = (
-                ChatPromptTemplate.from_template("write a 2-line poem about {topic}")
-                | model
-            )
-
-            runnable = RunnableParallel(joke=joke_chain, poem=poem_chain)
-
-            # Display stream
-            output = {key: "" for key, _ in runnable.output_schema()}
-            for chunk in runnable.stream({"topic": "bear"}):
-                for key in chunk:
-                    output[key] = output[key] + chunk[key].content
-                print(output)  # noqa: T201
-    """
-
-    steps__: Mapping[str, Runnable[Input, Any]]
-
-    def __init__(
-        self,
-        steps__: Optional[
-            Mapping[
-                str,
-                Union[
-                    Runnable[Input, Any],
-                    Callable[[Input], Any],
-                    Mapping[str, Union[Runnable[Input, Any], Callable[[Input], Any]]],
-                ],
-            ]
-        ] = None,
-        **kwargs: Union[
-            Runnable[Input, Any],
-            Callable[[Input], Any],
-            Mapping[str, Union[Runnable[Input, Any], Callable[[Input], Any]]],
-        ],
-    ) -> None:
-        merged = {**steps__} if steps__ is not None else {}
-        merged.update(kwargs)
-        super().__init__(  # type: ignore[call-arg]
-            steps__={key: coerce_to_runnable(r) for key, r in merged.items()}
-        )
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    def get_name(
-        self, suffix: Optional[str] = None, *, name: Optional[str] = None
-    ) -> str:
-        """Get the name of the Runnable.
-
-        Args:
-            suffix: The suffix to use. Defaults to None.
-            name: The name to use. Defaults to None.
-
-        Returns:
-            The name of the Runnable.
-        """
-        name = name or self.name or f"RunnableParallel<{','.join(self.steps__.keys())}>"
-        return super().get_name(suffix, name=name)
-
-    @property
-    @override
-    def InputType(self) -> Any:
-        """The type of the input to the Runnable."""
-        for step in self.steps__.values():
-            if step.InputType:
-                return step.InputType
-
-        return Any
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        """Get the input schema of the Runnable.
-
-        Args:
-            config: The config to use. Defaults to None.
-
-        Returns:
-            The input schema of the Runnable.
-        """
-        if all(
-            s.get_input_schema(config).model_json_schema().get("type", "object")
-            == "object"
-            for s in self.steps__.values()
-        ):
-            # This is correct, but pydantic typings/mypy don't think so.
-            return create_model_v2(  # type: ignore[call-overload]
-                self.get_name("Input"),
-                field_definitions={
-                    k: (v.annotation, v.default)
-                    for step in self.steps__.values()
-                    for k, v in step.get_input_schema(config).model_fields.items()
-                    if k != "__root__"
-                },
-            )
-
-        return super().get_input_schema(config)
-
-    def get_output_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        """Get the output schema of the Runnable.
-
-        Args:
-            config: The config to use. Defaults to None.
-
-        Returns:
-            The output schema of the Runnable.
-        """
-        fields = {k: (v.OutputType, ...) for k, v in self.steps__.items()}
-        return create_model_v2(self.get_name("Output"), field_definitions=fields)
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        """Get the config specs of the Runnable.
-
-        Returns:
-            The config specs of the Runnable.
-        """
-        return get_unique_config_specs(
-            spec for step in self.steps__.values() for spec in step.config_specs
-        )
-
-    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
-        """Get the graph representation of the Runnable.
-
-        Args:
-            config: The config to use. Defaults to None.
-
-        Returns:
-            The graph representation of the Runnable.
-
-        Raises:
-            ValueError: If a Runnable has no first or last node.
-        """
-        from langchain_core.runnables.graph import Graph
-
-        graph = Graph()
-        input_node = graph.add_node(self.get_input_schema(config))
-        output_node = graph.add_node(self.get_output_schema(config))
-        for step in self.steps__.values():
-            step_graph = step.get_graph()
-            step_graph.trim_first_node()
-            step_graph.trim_last_node()
-            if not step_graph:
-                graph.add_edge(input_node, output_node)
-            else:
-                step_first_node, step_last_node = graph.extend(step_graph)
-                if not step_first_node:
-                    msg = f"Runnable {step} has no first node"
-                    raise ValueError(msg)
-                if not step_last_node:
-                    msg = f"Runnable {step} has no last node"
-                    raise ValueError(msg)
-                graph.add_edge(input_node, step_first_node)
-                graph.add_edge(step_last_node, output_node)
-
-        return graph
-
-    def __repr__(self) -> str:
-        map_for_repr = ",\n  ".join(
-            f"{k}: {indent_lines_after_first(repr(v), '  ' + k + ': ')}"
-            for k, v in self.steps__.items()
-        )
-        return "{\n  " + map_for_repr + "\n}"
-
-    def invoke(
-        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> dict[str, Any]:
-        from langchain_core.callbacks.manager import CallbackManager
-
-        # setup callbacks
-        config = ensure_config(config)
-        callback_manager = CallbackManager.configure(
-            inheritable_callbacks=config.get("callbacks"),
-            local_callbacks=None,
-            verbose=False,
-            inheritable_tags=config.get("tags"),
-            local_tags=None,
-            inheritable_metadata=config.get("metadata"),
-            local_metadata=None,
-        )
-        # start the root run
-        run_manager = callback_manager.on_chain_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-
-        def _invoke_step(
-            step: Runnable[Input, Any], input: Input, config: RunnableConfig, key: str
-        ) -> Any:
-            child_config = patch_config(
-                config,
-                # mark each step as a child run
-                callbacks=run_manager.get_child(f"map:key:{key}"),
-            )
-            context = copy_context()
-            context.run(_set_config_context, child_config)
-            return context.run(
-                step.invoke,
-                input,
-                child_config,
-            )
-
-        # gather results from all steps
-        try:
-            # copy to avoid issues from the caller mutating the steps during invoke()
-            steps = dict(self.steps__)
-
-            with get_executor_for_config(config) as executor:
-                futures = [
-                    executor.submit(_invoke_step, step, input, config, key)
-                    for key, step in steps.items()
-                ]
-                output = {key: future.result() for key, future in zip(steps, futures)}
-        # finish the root run
-        except BaseException as e:
-            run_manager.on_chain_error(e)
-            raise
-        else:
-            run_manager.on_chain_end(output)
-            return output
-
-    async def ainvoke(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> dict[str, Any]:
-        # setup callbacks
-        config = ensure_config(config)
-        callback_manager = get_async_callback_manager_for_config(config)
-        # start the root run
-        run_manager = await callback_manager.on_chain_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-
-        async def _ainvoke_step(
-            step: Runnable[Input, Any], input: Input, config: RunnableConfig, key: str
-        ) -> Any:
-            child_config = patch_config(
-                config,
-                callbacks=run_manager.get_child(f"map:key:{key}"),
-            )
-            context = copy_context()
-            context.run(_set_config_context, child_config)
-            if asyncio_accepts_context():
-                return await asyncio.create_task(  # type: ignore
-                    step.ainvoke(input, child_config), context=context
-                )
-            else:
-                return await asyncio.create_task(step.ainvoke(input, child_config))
-
-        # gather results from all steps
-        try:
-            # copy to avoid issues from the caller mutating the steps during invoke()
-            steps = dict(self.steps__)
-            results = await asyncio.gather(
-                *(
-                    _ainvoke_step(
-                        step,
-                        input,
-                        # mark each step as a child run
-                        config,
-                        key,
-                    )
-                    for key, step in steps.items()
-                )
-            )
-            output = dict(zip(steps, results))
-        # finish the root run
-        except BaseException as e:
-            await run_manager.on_chain_error(e)
-            raise
-        else:
-            await run_manager.on_chain_end(output)
-            return output
-
-    def _transform(
-        self,
-        input: Iterator[Input],
-        run_manager: CallbackManagerForChainRun,
-        config: RunnableConfig,
-    ) -> Iterator[AddableDict]:
-        # Shallow copy steps to ignore mutations while in progress
-        steps = dict(self.steps__)
-        # Each step gets a copy of the input iterator,
-        # which is consumed in parallel in a separate thread.
-        input_copies = list(safetee(input, len(steps), lock=threading.Lock()))
-        with get_executor_for_config(config) as executor:
-            # Create the transform() generator for each step
-            named_generators = [
-                (
-                    name,
-                    step.transform(
-                        input_copies.pop(),
-                        patch_config(
-                            config, callbacks=run_manager.get_child(f"map:key:{name}")
-                        ),
-                    ),
-                )
-                for name, step in steps.items()
-            ]
-            # Start the first iteration of each generator
-            futures = {
-                executor.submit(next, generator): (step_name, generator)
-                for step_name, generator in named_generators
-            }
-            # Yield chunks from each as they become available,
-            # and start the next iteration of that generator that yielded it.
-            # When all generators are exhausted, stop.
-            while futures:
-                completed_futures, _ = wait(futures, return_when=FIRST_COMPLETED)
-                for future in completed_futures:
-                    (step_name, generator) = futures.pop(future)
-                    try:
-                        chunk = AddableDict({step_name: future.result()})
-                        yield chunk
-                        futures[executor.submit(next, generator)] = (
-                            step_name,
-                            generator,
-                        )
-                    except StopIteration:
-                        pass
-
-    def transform(
-        self,
-        input: Iterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Iterator[dict[str, Any]]:
-        yield from self._transform_stream_with_config(
-            input, self._transform, config, **kwargs
-        )
-
-    def stream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[dict[str, Any]]:
-        yield from self.transform(iter([input]), config)
-
-    async def _atransform(
-        self,
-        input: AsyncIterator[Input],
-        run_manager: AsyncCallbackManagerForChainRun,
-        config: RunnableConfig,
-    ) -> AsyncIterator[AddableDict]:
-        # Shallow copy steps to ignore mutations while in progress
-        steps = dict(self.steps__)
-        # Each step gets a copy of the input iterator,
-        # which is consumed in parallel in a separate thread.
-        input_copies = list(atee(input, len(steps), lock=asyncio.Lock()))
-        # Create the transform() generator for each step
-        named_generators = [
-            (
-                name,
-                step.atransform(
-                    input_copies.pop(),
-                    patch_config(
-                        config, callbacks=run_manager.get_child(f"map:key:{name}")
-                    ),
-                ),
-            )
-            for name, step in steps.items()
-        ]
-
-        # Wrap in a coroutine to satisfy linter
-        async def get_next_chunk(generator: AsyncIterator) -> Optional[Output]:
-            return await py_anext(generator)
-
-        # Start the first iteration of each generator
-        tasks = {
-            asyncio.create_task(get_next_chunk(generator)): (step_name, generator)
-            for step_name, generator in named_generators
-        }
-        # Yield chunks from each as they become available,
-        # and start the next iteration of the generator that yielded it.
-        # When all generators are exhausted, stop.
-        while tasks:
-            completed_tasks, _ = await asyncio.wait(
-                tasks, return_when=asyncio.FIRST_COMPLETED
-            )
-            for task in completed_tasks:
-                (step_name, generator) = tasks.pop(task)
-                try:
-                    chunk = AddableDict({step_name: task.result()})
-                    yield chunk
-                    new_task = asyncio.create_task(get_next_chunk(generator))
-                    tasks[new_task] = (step_name, generator)
-                except StopAsyncIteration:
-                    pass
-
-    async def atransform(
-        self,
-        input: AsyncIterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[dict[str, Any]]:
-        async for chunk in self._atransform_stream_with_config(
-            input, self._atransform, config, **kwargs
-        ):
-            yield chunk
-
-    async def astream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[dict[str, Any]]:
-        async def input_aiter() -> AsyncIterator[Input]:
-            yield input
-
-        async for chunk in self.atransform(input_aiter(), config):
-            yield chunk
-
-
-# We support both names
-RunnableMap = RunnableParallel
-
-
-class RunnableGenerator(Runnable[Input, Output]):
-    """Runnable that runs a generator function.
-
-    RunnableGenerators can be instantiated directly or by using a generator within
-    a sequence.
-
-    RunnableGenerators can be used to implement custom behavior, such as custom output
-    parsers, while preserving streaming capabilities. Given a generator function with
-    a signature Iterator[A] -> Iterator[B], wrapping it in a RunnableGenerator allows
-    it to emit output chunks as soon as they are streamed in from the previous step.
-
-    Note that if a generator function has a signature A -> Iterator[B], such that it
-    requires its input from the previous step to be completed before emitting chunks
-    (e.g., most LLMs need the entire prompt available to start generating), it can
-    instead be wrapped in a RunnableLambda.
-
-    Here is an example to show the basic mechanics of a RunnableGenerator:
-
-        .. code-block:: python
-
-            from typing import Any, AsyncIterator, Iterator
-
-            from langchain_core.runnables import RunnableGenerator
-
-
-            def gen(input: Iterator[Any]) -> Iterator[str]:
-                for token in ["Have", " a", " nice", " day"]:
-                    yield token
-
-
-            runnable = RunnableGenerator(gen)
-            runnable.invoke(None)  # "Have a nice day"
-            list(runnable.stream(None))  # ["Have", " a", " nice", " day"]
-            runnable.batch([None, None])  # ["Have a nice day", "Have a nice day"]
-
-
-            # Async version:
-            async def agen(input: AsyncIterator[Any]) -> AsyncIterator[str]:
-                for token in ["Have", " a", " nice", " day"]:
-                    yield token
-
-            runnable = RunnableGenerator(agen)
-            await runnable.ainvoke(None)  # "Have a nice day"
-            [p async for p in runnable.astream(None)] # ["Have", " a", " nice", " day"]
-
-    RunnableGenerator makes it easy to implement custom behavior within a streaming
-    context. Below we show an example:
-
-        .. code-block:: python
-
-            from langchain_core.prompts import ChatPromptTemplate
-            from langchain_core.runnables import RunnableGenerator, RunnableLambda
-            from langchain_openai import ChatOpenAI
-            from langchain_core.output_parsers import StrOutputParser
-
-
-            model = ChatOpenAI()
-            chant_chain = (
-                ChatPromptTemplate.from_template("Give me a 3 word chant about {topic}")
-                | model
-                | StrOutputParser()
-            )
-
-            def character_generator(input: Iterator[str]) -> Iterator[str]:
-                for token in input:
-                    if "," in token or "." in token:
-                        yield "👏" + token
-                    else:
-                        yield token
-
-
-            runnable = chant_chain | character_generator
-            assert type(runnable.last) is RunnableGenerator
-            "".join(runnable.stream({"topic": "waste"})) # Reduce👏, Reuse👏, Recycle👏.
-
-            # Note that RunnableLambda can be used to delay streaming of one step in a
-            # sequence until the previous step is finished:
-            def reverse_generator(input: str) -> Iterator[str]:
-                # Yield characters of input in reverse order.
-                for character in input[::-1]:
-                    yield character
-
-            runnable = chant_chain | RunnableLambda(reverse_generator)
-            "".join(runnable.stream({"topic": "waste"}))  # ".elcycer ,esuer ,ecudeR"
-    """
-
-    def __init__(
-        self,
-        transform: Union[
-            Callable[[Iterator[Input]], Iterator[Output]],
-            Callable[[AsyncIterator[Input]], AsyncIterator[Output]],
-        ],
-        atransform: Optional[
-            Callable[[AsyncIterator[Input]], AsyncIterator[Output]]
-        ] = None,
-        *,
-        name: Optional[str] = None,
-    ) -> None:
-        """Initialize a RunnableGenerator.
-
-        Args:
-            transform: The transform function.
-            atransform: The async transform function. Defaults to None.
-
-        Raises:
-            TypeError: If the transform is not a generator function.
-        """
-        if atransform is not None:
-            self._atransform = atransform
-            func_for_name: Callable = atransform
-
-        if is_async_generator(transform):
-            self._atransform = transform  # type: ignore[assignment]
-            func_for_name = transform
-        elif inspect.isgeneratorfunction(transform):
-            self._transform = transform
-            func_for_name = transform
-        else:
-            msg = (
-                "Expected a generator function type for `transform`."
-                f"Instead got an unsupported type: {type(transform)}"
-            )
-            raise TypeError(msg)
-
-        try:
-            self.name = name or func_for_name.__name__
-        except AttributeError:
-            self.name = "RunnableGenerator"
-
-    @property
-    @override
-    def InputType(self) -> Any:
-        func = getattr(self, "_transform", None) or self._atransform
-        try:
-            params = inspect.signature(func).parameters
-            first_param = next(iter(params.values()), None)
-            if first_param and first_param.annotation != inspect.Parameter.empty:
-                return getattr(first_param.annotation, "__args__", (Any,))[0]
-            else:
-                return Any
-        except ValueError:
-            return Any
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        # Override the default implementation.
-        # For a runnable generator, we need to bring to provide the
-        # module of the underlying function when creating the model.
-        root_type = self.InputType
-
-        func = getattr(self, "_transform", None) or self._atransform
-        module = getattr(func, "__module__", None)
-
-        if (
-            inspect.isclass(root_type)
-            and not isinstance(root_type, GenericAlias)
-            and issubclass(root_type, BaseModel)
-        ):
-            return root_type
-
-        return create_model_v2(
-            self.get_name("Input"),
-            root=root_type,
-            # To create the schema, we need to provide the module
-            # where the underlying function is defined.
-            # This allows pydantic to resolve type annotations appropriately.
-            module_name=module,
-        )
-
-    @property
-    @override
-    def OutputType(self) -> Any:
-        func = getattr(self, "_transform", None) or self._atransform
-        try:
-            sig = inspect.signature(func)
-            return (
-                getattr(sig.return_annotation, "__args__", (Any,))[0]
-                if sig.return_annotation != inspect.Signature.empty
-                else Any
-            )
-        except ValueError:
-            return Any
-
-    def get_output_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        # Override the default implementation.
-        # For a runnable generator, we need to bring to provide the
-        # module of the underlying function when creating the model.
-        root_type = self.OutputType
-        func = getattr(self, "_transform", None) or self._atransform
-        module = getattr(func, "__module__", None)
-
-        if (
-            inspect.isclass(root_type)
-            and not isinstance(root_type, GenericAlias)
-            and issubclass(root_type, BaseModel)
-        ):
-            return root_type
-
-        return create_model_v2(
-            self.get_name("Output"),
-            root=root_type,
-            # To create the schema, we need to provide the module
-            # where the underlying function is defined.
-            # This allows pydantic to resolve type annotations appropriately.
-            module_name=module,
-        )
-
-    def __eq__(self, other: Any) -> bool:
-        if isinstance(other, RunnableGenerator):
-            if hasattr(self, "_transform") and hasattr(other, "_transform"):
-                return self._transform == other._transform
-            elif hasattr(self, "_atransform") and hasattr(other, "_atransform"):
-                return self._atransform == other._atransform
-            else:
-                return False
-        else:
-            return False
-
-    def __repr__(self) -> str:
-        return f"RunnableGenerator({self.name})"
-
-    def transform(
-        self,
-        input: Iterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Iterator[Output]:
-        if not hasattr(self, "_transform"):
-            msg = f"{repr(self)} only supports async methods."
-            raise NotImplementedError(msg)
-        return self._transform_stream_with_config(
-            input,
-            self._transform,  # type: ignore[arg-type]
-            config,
-            **kwargs,  # type: ignore[arg-type]
-        )
-
-    def stream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Iterator[Output]:
-        return self.transform(iter([input]), config, **kwargs)
-
-    def invoke(
-        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Output:
-        final: Optional[Output] = None
-        for output in self.stream(input, config, **kwargs):
-            final = output if final is None else final + output  # type: ignore[operator]
-        return cast(Output, final)
-
-    def atransform(
-        self,
-        input: AsyncIterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[Output]:
-        if not hasattr(self, "_atransform"):
-            msg = f"{repr(self)} only supports sync methods."
-            raise NotImplementedError(msg)
-
-        return self._atransform_stream_with_config(
-            input, self._atransform, config, **kwargs
-        )
-
-    def astream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[Output]:
-        async def input_aiter() -> AsyncIterator[Input]:
-            yield input
-
-        return self.atransform(input_aiter(), config, **kwargs)
-
-    async def ainvoke(
-        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Output:
-        final: Optional[Output] = None
-        async for output in self.astream(input, config, **kwargs):
-            final = output if final is None else final + output  # type: ignore[operator]
-        return cast(Output, final)
-
-
-class RunnableLambda(Runnable[Input, Output]):
-    """RunnableLambda converts a python callable into a Runnable.
-
-    Wrapping a callable in a RunnableLambda makes the callable usable
-    within either a sync or async context.
-
-    RunnableLambda can be composed as any other Runnable and provides
-    seamless integration with LangChain tracing.
-
-    ``RunnableLambda`` is best suited for code that does not need to support
-    streaming. If you need to support streaming (i.e., be able to operate
-    on chunks of inputs and yield chunks of outputs), use ``RunnableGenerator``
-    instead.
-
-    Note that if a ``RunnableLambda`` returns an instance of ``Runnable``, that
-    instance is invoked (or streamed) during execution.
-
-    Examples:
-
-        .. code-block:: python
-
-            # This is a RunnableLambda
-            from langchain_core.runnables import RunnableLambda
-
-            def add_one(x: int) -> int:
-                return x + 1
-
-            runnable = RunnableLambda(add_one)
-
-            runnable.invoke(1) # returns 2
-            runnable.batch([1, 2, 3]) # returns [2, 3, 4]
-
-            # Async is supported by default by delegating to the sync implementation
-            await runnable.ainvoke(1) # returns 2
-            await runnable.abatch([1, 2, 3]) # returns [2, 3, 4]
-
-
-            # Alternatively, can provide both synd and sync implementations
-            async def add_one_async(x: int) -> int:
-                return x + 1
-
-            runnable = RunnableLambda(add_one, afunc=add_one_async)
-            runnable.invoke(1) # Uses add_one
-            await runnable.ainvoke(1) # Uses add_one_async
-    """
-
-    def __init__(
-        self,
-        func: Union[
-            Union[
-                Callable[[Input], Output],
-                Callable[[Input], Iterator[Output]],
-                Callable[[Input, RunnableConfig], Output],
-                Callable[[Input, CallbackManagerForChainRun], Output],
-                Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],
-            ],
-            Union[
-                Callable[[Input], Awaitable[Output]],
-                Callable[[Input], AsyncIterator[Output]],
-                Callable[[Input, RunnableConfig], Awaitable[Output]],
-                Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],
-                Callable[
-                    [Input, AsyncCallbackManagerForChainRun, RunnableConfig],
-                    Awaitable[Output],
-                ],
-            ],
-        ],
-        afunc: Optional[
-            Union[
-                Callable[[Input], Awaitable[Output]],
-                Callable[[Input], AsyncIterator[Output]],
-                Callable[[Input, RunnableConfig], Awaitable[Output]],
-                Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],
-                Callable[
-                    [Input, AsyncCallbackManagerForChainRun, RunnableConfig],
-                    Awaitable[Output],
-                ],
-            ]
-        ] = None,
-        name: Optional[str] = None,
-    ) -> None:
-        """Create a RunnableLambda from a callable, and async callable or both.
-
-        Accepts both sync and async variants to allow providing efficient
-        implementations for sync and async execution.
-
-        Args:
-            func: Either sync or async callable
-            afunc: An async callable that takes an input and returns an output.
-                Defaults to None.
-            name: The name of the Runnable. Defaults to None.
-
-        Raises:
-            TypeError: If the func is not a callable type.
-            TypeError: If both func and afunc are provided.
-        """
-        if afunc is not None:
-            self.afunc = afunc
-            func_for_name: Callable = afunc
-
-        if is_async_callable(func) or is_async_generator(func):
-            if afunc is not None:
-                msg = (
-                    "Func was provided as a coroutine function, but afunc was "
-                    "also provided. If providing both, func should be a regular "
-                    "function to avoid ambiguity."
-                )
-                raise TypeError(msg)
-            self.afunc = func
-            func_for_name = func
-        elif callable(func):
-            self.func = cast(Callable[[Input], Output], func)
-            func_for_name = func
-        else:
-            msg = (
-                "Expected a callable type for `func`."
-                f"Instead got an unsupported type: {type(func)}"
-            )
-            raise TypeError(msg)
-
-        try:
-            if name is not None:
-                self.name = name
-            elif func_for_name.__name__ != "<lambda>":
-                self.name = func_for_name.__name__
-        except AttributeError:
-            pass
-
-        self._repr: Optional[str] = None
-
-    @property
-    @override
-    def InputType(self) -> Any:
-        """The type of the input to this Runnable."""
-        func = getattr(self, "func", None) or self.afunc
-        try:
-            params = inspect.signature(func).parameters
-            first_param = next(iter(params.values()), None)
-            if first_param and first_param.annotation != inspect.Parameter.empty:
-                return first_param.annotation
-            else:
-                return Any
-        except ValueError:
-            return Any
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        """The pydantic schema for the input to this Runnable.
-
-        Args:
-            config: The config to use. Defaults to None.
-
-        Returns:
-            The input schema for this Runnable.
-        """
-        func = getattr(self, "func", None) or self.afunc
-
-        if isinstance(func, itemgetter):
-            # This is terrible, but afaict it's not possible to access _items
-            # on itemgetter objects, so we have to parse the repr
-            items = str(func).replace("operator.itemgetter(", "")[:-1].split(", ")
-            if all(
-                item[0] == "'" and item[-1] == "'" and len(item) > 2 for item in items
-            ):
-                fields = {item[1:-1]: (Any, ...) for item in items}
-                # It's a dict, lol
-                return create_model_v2(self.get_name("Input"), field_definitions=fields)
-            else:
-                module = getattr(func, "__module__", None)
-                return create_model_v2(
-                    self.get_name("Input"),
-                    root=list[Any],
-                    # To create the schema, we need to provide the module
-                    # where the underlying function is defined.
-                    # This allows pydantic to resolve type annotations appropriately.
-                    module_name=module,
-                )
-
-        if self.InputType != Any:
-            return super().get_input_schema(config)
-
-        if dict_keys := get_function_first_arg_dict_keys(func):
-            return create_model_v2(
-                self.get_name("Input"),
-                field_definitions=dict.fromkeys(dict_keys, (Any, ...)),
-            )
-
-        return super().get_input_schema(config)
-
-    @property
-    @override
-    def OutputType(self) -> Any:
-        """The type of the output of this Runnable as a type annotation.
-
-        Returns:
-            The type of the output of this Runnable.
-        """
-        func = getattr(self, "func", None) or self.afunc
-        try:
-            sig = inspect.signature(func)
-            if sig.return_annotation != inspect.Signature.empty:
-                # unwrap iterator types
-                if getattr(sig.return_annotation, "__origin__", None) in (
-                    collections.abc.Iterator,
-                    collections.abc.AsyncIterator,
-                ):
-                    return getattr(sig.return_annotation, "__args__", (Any,))[0]
-                return sig.return_annotation
-            else:
-                return Any
-        except ValueError:
-            return Any
-
-    def get_output_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        # Override the default implementation.
-        # For a runnable lambda, we need to bring to provide the
-        # module of the underlying function when creating the model.
-        root_type = self.OutputType
-        func = getattr(self, "func", None) or self.afunc
-        module = getattr(func, "__module__", None)
-
-        if (
-            inspect.isclass(root_type)
-            and not isinstance(root_type, GenericAlias)
-            and issubclass(root_type, BaseModel)
-        ):
-            return root_type
-
-        return create_model_v2(
-            self.get_name("Output"),
-            root=root_type,
-            # To create the schema, we need to provide the module
-            # where the underlying function is defined.
-            # This allows pydantic to resolve type annotations appropriately.
-            module_name=module,
-        )
-
-    @functools.cached_property
-    def deps(self) -> list[Runnable]:
-        """The dependencies of this Runnable.
-
-        Returns:
-            The dependencies of this Runnable. If the function has nonlocal
-            variables that are Runnables, they are considered dependencies.
-        """
-        if hasattr(self, "func"):
-            objects = get_function_nonlocals(self.func)
-        elif hasattr(self, "afunc"):
-            objects = get_function_nonlocals(self.afunc)
-        else:
-            objects = []
-
-        deps: list[Runnable] = []
-        for obj in objects:
-            if isinstance(obj, Runnable):
-                deps.append(obj)
-            elif isinstance(getattr(obj, "__self__", None), Runnable):
-                deps.append(obj.__self__)
-        return deps
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        return get_unique_config_specs(
-            spec for dep in self.deps for spec in dep.config_specs
-        )
-
-    def get_graph(self, config: RunnableConfig | None = None) -> Graph:
-        if deps := self.deps:
-            graph = Graph()
-            input_node = graph.add_node(self.get_input_schema(config))
-            output_node = graph.add_node(self.get_output_schema(config))
-            for dep in deps:
-                dep_graph = dep.get_graph()
-                dep_graph.trim_first_node()
-                dep_graph.trim_last_node()
-                if not dep_graph:
-                    graph.add_edge(input_node, output_node)
-                else:
-                    dep_first_node, dep_last_node = graph.extend(dep_graph)
-                    if not dep_first_node:
-                        msg = f"Runnable {dep} has no first node"
-                        raise ValueError(msg)
-                    if not dep_last_node:
-                        msg = f"Runnable {dep} has no last node"
-                        raise ValueError(msg)
-                    graph.add_edge(input_node, dep_first_node)
-                    graph.add_edge(dep_last_node, output_node)
-        else:
-            graph = super().get_graph(config)
-
-        return graph
-
-    def __eq__(self, other: Any) -> bool:
-        if isinstance(other, RunnableLambda):
-            if hasattr(self, "func") and hasattr(other, "func"):
-                return self.func == other.func
-            elif hasattr(self, "afunc") and hasattr(other, "afunc"):
-                return self.afunc == other.afunc
-            else:
-                return False
-        else:
-            return False
-
-    def __repr__(self) -> str:
-        """A string representation of this Runnable."""
-        if self._repr is None:
-            if hasattr(self, "func") and isinstance(self.func, itemgetter):
-                self._repr = f"RunnableLambda({str(self.func)[len('operator.') :]})"
-            elif hasattr(self, "func"):
-                self._repr = f"RunnableLambda({get_lambda_source(self.func) or '...'})"
-            elif hasattr(self, "afunc"):
-                self._repr = (
-                    f"RunnableLambda(afunc={get_lambda_source(self.afunc) or '...'})"
-                )
-            else:
-                self._repr = "RunnableLambda(...)"
-        return self._repr
-
-    def _invoke(
-        self,
-        input: Input,
-        run_manager: CallbackManagerForChainRun,
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> Output:
-        if inspect.isgeneratorfunction(self.func):
-            output: Optional[Output] = None
-            for chunk in call_func_with_variable_args(
-                cast(Callable[[Input], Iterator[Output]], self.func),
-                input,
-                config,
-                run_manager,
-                **kwargs,
-            ):
-                if output is None:
-                    output = chunk
-                else:
-                    try:
-                        output = output + chunk  # type: ignore[operator]
-                    except TypeError:
-                        output = chunk
-        else:
-            output = call_func_with_variable_args(
-                self.func, input, config, run_manager, **kwargs
-            )
-        # If the output is a Runnable, invoke it
-        if isinstance(output, Runnable):
-            recursion_limit = config["recursion_limit"]
-            if recursion_limit <= 0:
-                msg = (
-                    f"Recursion limit reached when invoking {self} with input {input}."
-                )
-                raise RecursionError(msg)
-            output = output.invoke(
-                input,
-                patch_config(
-                    config,
-                    callbacks=run_manager.get_child(),
-                    recursion_limit=recursion_limit - 1,
-                ),
-            )
-        return cast(Output, output)
-
-    async def _ainvoke(
-        self,
-        input: Input,
-        run_manager: AsyncCallbackManagerForChainRun,
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> Output:
-        if hasattr(self, "afunc"):
-            afunc = self.afunc
-        else:
-            if inspect.isgeneratorfunction(self.func):
-
-                def func(
-                    input: Input,
-                    run_manager: AsyncCallbackManagerForChainRun,
-                    config: RunnableConfig,
-                    **kwargs: Any,
-                ) -> Output:
-                    output: Optional[Output] = None
-                    for chunk in call_func_with_variable_args(
-                        cast(Callable[[Input], Iterator[Output]], self.func),
-                        input,
-                        config,
-                        run_manager.get_sync(),
-                        **kwargs,
-                    ):
-                        if output is None:
-                            output = chunk
-                        else:
-                            try:
-                                output = output + chunk  # type: ignore[operator]
-                            except TypeError:
-                                output = chunk
-                    return cast(Output, output)
-
-            else:
-
-                def func(
-                    input: Input,
-                    run_manager: AsyncCallbackManagerForChainRun,
-                    config: RunnableConfig,
-                    **kwargs: Any,
-                ) -> Output:
-                    return call_func_with_variable_args(
-                        self.func, input, config, run_manager.get_sync(), **kwargs
-                    )
-
-            @wraps(func)
-            async def f(*args, **kwargs):  # type: ignore[no-untyped-def]
-                return await run_in_executor(config, func, *args, **kwargs)
-
-            afunc = f
-
-        if is_async_generator(afunc):
-            output: Optional[Output] = None
-            async with aclosing(
-                cast(
-                    AsyncGenerator[Any, Any],
-                    acall_func_with_variable_args(
-                        cast(Callable, afunc),
-                        input,
-                        config,
-                        run_manager,
-                        **kwargs,
-                    ),
-                )
-            ) as stream:
-                async for chunk in cast(
-                    AsyncIterator[Output],
-                    stream,
-                ):
-                    if output is None:
-                        output = chunk
-                    else:
-                        try:
-                            output = output + chunk  # type: ignore[operator]
-                        except TypeError:
-                            output = chunk
-        else:
-            output = await acall_func_with_variable_args(
-                cast(Callable, afunc), input, config, run_manager, **kwargs
-            )
-        # If the output is a Runnable, invoke it
-        if isinstance(output, Runnable):
-            recursion_limit = config["recursion_limit"]
-            if recursion_limit <= 0:
-                msg = (
-                    f"Recursion limit reached when invoking {self} with input {input}."
-                )
-                raise RecursionError(msg)
-            output = await output.ainvoke(
-                input,
-                patch_config(
-                    config,
-                    callbacks=run_manager.get_child(),
-                    recursion_limit=recursion_limit - 1,
-                ),
-            )
-        return cast(Output, output)
-
-    def _config(
-        self, config: Optional[RunnableConfig], callable: Callable[..., Any]
-    ) -> RunnableConfig:
-        return ensure_config(config)
-
-    def invoke(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Output:
-        """Invoke this Runnable synchronously.
-
-        Args:
-            input: The input to this Runnable.
-            config: The config to use. Defaults to None.
-            kwargs: Additional keyword arguments.
-
-        Returns:
-            The output of this Runnable.
-
-        Raises:
-            TypeError: If the Runnable is a coroutine function.
-        """
-        if hasattr(self, "func"):
-            return self._call_with_config(
-                self._invoke,
-                input,
-                self._config(config, self.func),
-                **kwargs,
-            )
-        else:
-            msg = (
-                "Cannot invoke a coroutine function synchronously."
-                "Use `ainvoke` instead."
-            )
-            raise TypeError(msg)
-
-    async def ainvoke(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Output:
-        """Invoke this Runnable asynchronously.
-
-        Args:
-            input: The input to this Runnable.
-            config: The config to use. Defaults to None.
-            kwargs: Additional keyword arguments.
-
-        Returns:
-            The output of this Runnable.
-        """
-        the_func = self.afunc if hasattr(self, "afunc") else self.func
-        return await self._acall_with_config(
-            self._ainvoke,
-            input,
-            self._config(config, the_func),
-            **kwargs,
-        )
-
-    def _transform(
-        self,
-        input: Iterator[Input],
-        run_manager: CallbackManagerForChainRun,
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> Iterator[Output]:
-        final: Input
-        got_first_val = False
-        for ichunk in input:
-            # By definitions, RunnableLambdas consume all input before emitting output.
-            # If the input is not addable, then we'll assume that we can
-            # only operate on the last chunk.
-            # So we'll iterate until we get to the last chunk!
-            if not got_first_val:
-                final = ichunk
-                got_first_val = True
-            else:
-                try:
-                    final = final + ichunk  # type: ignore[operator]
-                except TypeError:
-                    final = ichunk
-
-        if inspect.isgeneratorfunction(self.func):
-            output: Optional[Output] = None
-            for chunk in call_func_with_variable_args(
-                self.func, cast(Input, final), config, run_manager, **kwargs
-            ):
-                yield chunk
-                if output is None:
-                    output = chunk
-                else:
-                    try:
-                        output = output + chunk
-                    except TypeError:
-                        output = chunk
-        else:
-            output = call_func_with_variable_args(
-                self.func, cast(Input, final), config, run_manager, **kwargs
-            )
-
-        # If the output is a Runnable, use its stream output
-        if isinstance(output, Runnable):
-            recursion_limit = config["recursion_limit"]
-            if recursion_limit <= 0:
-                msg = (
-                    f"Recursion limit reached when invoking {self} with input {final}."
-                )
-                raise RecursionError(msg)
-            for chunk in output.stream(
-                final,
-                patch_config(
-                    config,
-                    callbacks=run_manager.get_child(),
-                    recursion_limit=recursion_limit - 1,
-                ),
-            ):
-                yield chunk
-        elif not inspect.isgeneratorfunction(self.func):
-            # Otherwise, just yield it
-            yield cast(Output, output)
-
-    def transform(
-        self,
-        input: Iterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        if hasattr(self, "func"):
-            yield from self._transform_stream_with_config(
-                input,
-                self._transform,
-                self._config(config, self.func),
-                **kwargs,
-            )
-        else:
-            msg = (
-                "Cannot stream a coroutine function synchronously."
-                "Use `astream` instead."
-            )
-            raise TypeError(msg)
-
-    def stream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        return self.transform(iter([input]), config, **kwargs)
-
-    async def _atransform(
-        self,
-        input: AsyncIterator[Input],
-        run_manager: AsyncCallbackManagerForChainRun,
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> AsyncIterator[Output]:
-        final: Input
-        got_first_val = False
-        async for ichunk in input:
-            # By definitions, RunnableLambdas consume all input before emitting output.
-            # If the input is not addable, then we'll assume that we can
-            # only operate on the last chunk.
-            # So we'll iterate until we get to the last chunk!
-            if not got_first_val:
-                final = ichunk
-                got_first_val = True
-            else:
-                try:
-                    final = final + ichunk  # type: ignore[operator]
-                except TypeError:
-                    final = ichunk
-
-        if hasattr(self, "afunc"):
-            afunc = self.afunc
-        else:
-            if inspect.isgeneratorfunction(self.func):
-                msg = (
-                    "Cannot stream from a generator function asynchronously."
-                    "Use .stream() instead."
-                )
-                raise TypeError(msg)
-
-            def func(
-                input: Input,
-                run_manager: AsyncCallbackManagerForChainRun,
-                config: RunnableConfig,
-                **kwargs: Any,
-            ) -> Output:
-                return call_func_with_variable_args(
-                    self.func, input, config, run_manager.get_sync(), **kwargs
-                )
-
-            @wraps(func)
-            async def f(*args, **kwargs):  # type: ignore[no-untyped-def]
-                return await run_in_executor(config, func, *args, **kwargs)
-
-            afunc = f
-
-        if is_async_generator(afunc):
-            output: Optional[Output] = None
-            async for chunk in cast(
-                AsyncIterator[Output],
-                acall_func_with_variable_args(
-                    cast(Callable, afunc),
-                    cast(Input, final),
-                    config,
-                    run_manager,
-                    **kwargs,
-                ),
-            ):
-                yield chunk
-                if output is None:
-                    output = chunk
-                else:
-                    try:
-                        output = output + chunk  # type: ignore[operator]
-                    except TypeError:
-                        output = chunk
-        else:
-            output = await acall_func_with_variable_args(
-                cast(Callable, afunc), cast(Input, final), config, run_manager, **kwargs
-            )
-
-        # If the output is a Runnable, use its astream output
-        if isinstance(output, Runnable):
-            recursion_limit = config["recursion_limit"]
-            if recursion_limit <= 0:
-                msg = (
-                    f"Recursion limit reached when invoking {self} with input {final}."
-                )
-                raise RecursionError(msg)
-            async for chunk in output.astream(
-                final,
-                patch_config(
-                    config,
-                    callbacks=run_manager.get_child(),
-                    recursion_limit=recursion_limit - 1,
-                ),
-            ):
-                yield chunk
-        elif not is_async_generator(afunc):
-            # Otherwise, just yield it
-            yield cast(Output, output)
-
-    async def atransform(
-        self,
-        input: AsyncIterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        async for output in self._atransform_stream_with_config(
-            input,
-            self._atransform,
-            self._config(config, self.afunc if hasattr(self, "afunc") else self.func),
-            **kwargs,
-        ):
-            yield output
-
-    async def astream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        async def input_aiter() -> AsyncIterator[Input]:
-            yield input
-
-        async for chunk in self.atransform(input_aiter(), config, **kwargs):
-            yield chunk
-
-
-class RunnableEachBase(RunnableSerializable[list[Input], list[Output]]):
-    """Runnable that delegates calls to another Runnable
-    with each element of the input sequence.
-
-    Use only if creating a new RunnableEach subclass with different __init__ args.
-
-    See documentation for RunnableEach for more details.
-    """
-
-    bound: Runnable[Input, Output]
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    @property
-    @override
-    def InputType(self) -> Any:
-        return list[self.bound.InputType]  # type: ignore[name-defined]
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        return create_model_v2(
-            self.get_name("Input"),
-            root=(
-                list[self.bound.get_input_schema(config)],  # type: ignore
-                None,
-            ),
-            # create model needs access to appropriate type annotations to be
-            # able to construct the pydantic model.
-            # When we create the model, we pass information about the namespace
-            # where the model is being created, so the type annotations can
-            # be resolved correctly as well.
-            # self.__class__.__module__ handles the case when the Runnable is
-            # being sub-classed in a different module.
-            module_name=self.__class__.__module__,
-        )
-
-    @property
-    @override
-    def OutputType(self) -> type[list[Output]]:
-        return list[self.bound.OutputType]  # type: ignore[name-defined]
-
-    def get_output_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        schema = self.bound.get_output_schema(config)
-        return create_model_v2(
-            self.get_name("Output"),
-            root=list[schema],  # type: ignore[valid-type]
-            # create model needs access to appropriate type annotations to be
-            # able to construct the pydantic model.
-            # When we create the model, we pass information about the namespace
-            # where the model is being created, so the type annotations can
-            # be resolved correctly as well.
-            # self.__class__.__module__ handles the case when the Runnable is
-            # being sub-classed in a different module.
-            module_name=self.__class__.__module__,
-        )
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        return self.bound.config_specs
-
-    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
-        return self.bound.get_graph(config)
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    def _invoke(
-        self,
-        inputs: list[Input],
-        run_manager: CallbackManagerForChainRun,
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> list[Output]:
-        configs = [
-            patch_config(config, callbacks=run_manager.get_child()) for _ in inputs
-        ]
-        return self.bound.batch(inputs, configs, **kwargs)
-
-    def invoke(
-        self, input: list[Input], config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> list[Output]:
-        return self._call_with_config(self._invoke, input, config, **kwargs)
-
-    async def _ainvoke(
-        self,
-        inputs: list[Input],
-        run_manager: AsyncCallbackManagerForChainRun,
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> list[Output]:
-        configs = [
-            patch_config(config, callbacks=run_manager.get_child()) for _ in inputs
-        ]
-        return await self.bound.abatch(inputs, configs, **kwargs)
-
-    async def ainvoke(
-        self, input: list[Input], config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> list[Output]:
-        return await self._acall_with_config(self._ainvoke, input, config, **kwargs)
-
-    async def astream_events(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[StreamEvent]:
-        for _ in range(1):
-            msg = "RunnableEach does not support astream_events yet."
-            raise NotImplementedError(msg)
-            yield
-
-
-class RunnableEach(RunnableEachBase[Input, Output]):
-    """Runnable that delegates calls to another Runnable
-    with each element of the input sequence.
-
-    It allows you to call multiple inputs with the bounded Runnable.
-
-    RunnableEach makes it easy to run multiple inputs for the Runnable.
-    In the below example, we associate and run three inputs
-    with a Runnable:
-
-        .. code-block:: python
-
-            from langchain_core.runnables.base import RunnableEach
-            from langchain_openai import ChatOpenAI
-            from langchain_core.prompts import ChatPromptTemplate
-            from langchain_core.output_parsers import StrOutputParser
-            prompt = ChatPromptTemplate.from_template("Tell me a short joke about
-            {topic}")
-            model = ChatOpenAI()
-            output_parser = StrOutputParser()
-            runnable = prompt | model | output_parser
-            runnable_each = RunnableEach(bound=runnable)
-            output = runnable_each.invoke([{'topic':'Computer Science'},
-                                        {'topic':'Art'},
-                                        {'topic':'Biology'}])
-            print(output)  # noqa: T201
-    """
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    def get_name(
-        self, suffix: Optional[str] = None, *, name: Optional[str] = None
-    ) -> str:
-        name = name or self.name or f"RunnableEach<{self.bound.get_name()}>"
-        return super().get_name(suffix, name=name)
-
-    def bind(self, **kwargs: Any) -> RunnableEach[Input, Output]:
-        return RunnableEach(bound=self.bound.bind(**kwargs))
-
-    def with_config(
-        self, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> RunnableEach[Input, Output]:
-        return RunnableEach(bound=self.bound.with_config(config, **kwargs))
-
-    def with_listeners(
-        self,
-        *,
-        on_start: Optional[
-            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]
-        ] = None,
-        on_end: Optional[
-            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]
-        ] = None,
-        on_error: Optional[
-            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]
-        ] = None,
-    ) -> RunnableEach[Input, Output]:
-        """Bind lifecycle listeners to a Runnable, returning a new Runnable.
-
-        Args:
-            on_start: Called before the Runnable starts running, with the Run object.
-                Defaults to None.
-            on_end: Called after the Runnable finishes running, with the Run object.
-                Defaults to None.
-            on_error: Called if the Runnable throws an error, with the Run object.
-                Defaults to None.
-
-        Returns:
-            A new Runnable with the listeners bound.
-
-        The Run object contains information about the run, including its id,
-        type, input, output, error, start_time, end_time, and any tags or metadata
-        added to the run.
-        """
-        return RunnableEach(
-            bound=self.bound.with_listeners(
-                on_start=on_start, on_end=on_end, on_error=on_error
-            )
-        )
-
-    def with_alisteners(
-        self,
-        *,
-        on_start: Optional[AsyncListener] = None,
-        on_end: Optional[AsyncListener] = None,
-        on_error: Optional[AsyncListener] = None,
-    ) -> RunnableEach[Input, Output]:
-        """Bind async lifecycle listeners to a Runnable, returning a new Runnable.
-
-        Args:
-            on_start: Called asynchronously before the Runnable starts running,
-                      with the Run object. Defaults to None.
-            on_end: Called asynchronously after the Runnable finishes running,
-                    with the Run object. Defaults to None.
-            on_error: Called asynchronously if the Runnable throws an error,
-                    with the Run object. Defaults to None.
-
-        Returns:
-            A new Runnable with the listeners bound.
-
-        The Run object contains information about the run, including its id,
-        type, input, output, error, start_time, end_time, and any tags or metadata
-        added to the run.
-        """
-        return RunnableEach(
-            bound=self.bound.with_alisteners(
-                on_start=on_start, on_end=on_end, on_error=on_error
-            )
-        )
-
-
-class RunnableBindingBase(RunnableSerializable[Input, Output]):
-    """Runnable that delegates calls to another Runnable with a set of kwargs.
-
-    Use only if creating a new RunnableBinding subclass with different __init__ args.
-
-    See documentation for RunnableBinding for more details.
-    """
-
-    bound: Runnable[Input, Output]
-    """The underlying Runnable that this Runnable delegates to."""
-
-    kwargs: Mapping[str, Any] = Field(default_factory=dict)
-    """kwargs to pass to the underlying Runnable when running.
-
-    For example, when the Runnable binding is invoked the underlying
-    Runnable will be invoked with the same input but with these additional
-    kwargs.
-    """
-
-    config: RunnableConfig = Field(default_factory=RunnableConfig)  # type: ignore
-    """The config to bind to the underlying Runnable."""
-
-    config_factories: list[Callable[[RunnableConfig], RunnableConfig]] = Field(
-        default_factory=list
-    )
-    """The config factories to bind to the underlying Runnable."""
-
-    # Union[Type[Input], BaseModel] + things like List[str]
-    custom_input_type: Optional[Any] = None
-    """Override the input type of the underlying Runnable with a custom type.
-
-    The type can be a pydantic model, or a type annotation (e.g., `List[str]`).
-    """
-    # Union[Type[Output], BaseModel] + things like List[str]
-    custom_output_type: Optional[Any] = None
-    """Override the output type of the underlying Runnable with a custom type.
-
-    The type can be a pydantic model, or a type annotation (e.g., `List[str]`).
-    """
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    def __init__(
-        self,
-        *,
-        bound: Runnable[Input, Output],
-        kwargs: Optional[Mapping[str, Any]] = None,
-        config: Optional[RunnableConfig] = None,
-        config_factories: Optional[
-            list[Callable[[RunnableConfig], RunnableConfig]]
-        ] = None,
-        custom_input_type: Optional[Union[type[Input], BaseModel]] = None,
-        custom_output_type: Optional[Union[type[Output], BaseModel]] = None,
-        **other_kwargs: Any,
-    ) -> None:
-        """Create a RunnableBinding from a Runnable and kwargs.
-
-        Args:
-            bound: The underlying Runnable that this Runnable delegates calls to.
-            kwargs: optional kwargs to pass to the underlying Runnable, when running
-                    the underlying Runnable (e.g., via `invoke`, `batch`,
-                    `transform`, or `stream` or async variants)
-                    Defaults to None.
-            config: optional config to bind to the underlying Runnable.
-                    Defaults to None.
-            config_factories: optional list of config factories to apply to the
-                    config before binding to the underlying Runnable.
-                    Defaults to None.
-            custom_input_type: Specify to override the input type of the underlying
-                               Runnable with a custom type. Defaults to None.
-            custom_output_type: Specify to override the output type of the underlying
-                Runnable with a custom type. Defaults to None.
-            **other_kwargs: Unpacked into the base class.
-        """
-        super().__init__(  # type: ignore[call-arg]
-            bound=bound,
-            kwargs=kwargs or {},
-            config=config or {},
-            config_factories=config_factories or [],
-            custom_input_type=custom_input_type,
-            custom_output_type=custom_output_type,
-            **other_kwargs,
-        )
-        # if we don't explicitly set config to the TypedDict here,
-        # the pydantic init above will strip out any of the "extra"
-        # fields even though total=False on the typed dict.
-        self.config = config or {}
-
-    def get_name(
-        self, suffix: Optional[str] = None, *, name: Optional[str] = None
-    ) -> str:
-        return self.bound.get_name(suffix, name=name)
-
-    @property
-    @override
-    def InputType(self) -> type[Input]:
-        return (
-            cast(type[Input], self.custom_input_type)
-            if self.custom_input_type is not None
-            else self.bound.InputType
-        )
-
-    @property
-    @override
-    def OutputType(self) -> type[Output]:
-        return (
-            cast(type[Output], self.custom_output_type)
-            if self.custom_output_type is not None
-            else self.bound.OutputType
-        )
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        if self.custom_input_type is not None:
-            return super().get_input_schema(config)
-        return self.bound.get_input_schema(merge_configs(self.config, config))
-
-    def get_output_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        if self.custom_output_type is not None:
-            return super().get_output_schema(config)
-        return self.bound.get_output_schema(merge_configs(self.config, config))
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        return self.bound.config_specs
-
-    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
-        return self.bound.get_graph(self._merge_configs(config))
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    def _merge_configs(self, *configs: Optional[RunnableConfig]) -> RunnableConfig:
-        config = merge_configs(self.config, *configs)
-        return merge_configs(config, *(f(config) for f in self.config_factories))
-
-    def invoke(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Output:
-        return self.bound.invoke(
-            input,
-            self._merge_configs(config),
-            **{**self.kwargs, **kwargs},
-        )
-
-    async def ainvoke(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Output:
-        return await self.bound.ainvoke(
-            input,
-            self._merge_configs(config),
-            **{**self.kwargs, **kwargs},
-        )
-
-    def batch(
-        self,
-        inputs: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        if isinstance(config, list):
-            configs = cast(
-                list[RunnableConfig],
-                [self._merge_configs(conf) for conf in config],
-            )
-        else:
-            configs = [self._merge_configs(config) for _ in range(len(inputs))]
-        return self.bound.batch(
-            inputs,
-            configs,
-            return_exceptions=return_exceptions,
-            **{**self.kwargs, **kwargs},
-        )
-
-    async def abatch(
-        self,
-        inputs: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        if isinstance(config, list):
-            configs = cast(
-                list[RunnableConfig],
-                [self._merge_configs(conf) for conf in config],
-            )
-        else:
-            configs = [self._merge_configs(config) for _ in range(len(inputs))]
-        return await self.bound.abatch(
-            inputs,
-            configs,
-            return_exceptions=return_exceptions,
-            **{**self.kwargs, **kwargs},
-        )
-
-    @overload
-    def batch_as_completed(
-        self,
-        inputs: Sequence[Input],
-        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,
-        *,
-        return_exceptions: Literal[False] = False,
-        **kwargs: Any,
-    ) -> Iterator[tuple[int, Output]]: ...
-
-    @overload
-    def batch_as_completed(
-        self,
-        inputs: Sequence[Input],
-        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,
-        *,
-        return_exceptions: Literal[True],
-        **kwargs: Any,
-    ) -> Iterator[tuple[int, Union[Output, Exception]]]: ...
-
-    def batch_as_completed(
-        self,
-        inputs: Sequence[Input],
-        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> Iterator[tuple[int, Union[Output, Exception]]]:
-        if isinstance(config, Sequence):
-            configs = cast(
-                list[RunnableConfig],
-                [self._merge_configs(conf) for conf in config],
-            )
-        else:
-            configs = [self._merge_configs(config) for _ in range(len(inputs))]
-        # lol mypy
-        if return_exceptions:
-            yield from self.bound.batch_as_completed(
-                inputs,
-                configs,
-                return_exceptions=return_exceptions,
-                **{**self.kwargs, **kwargs},
-            )
-        else:
-            yield from self.bound.batch_as_completed(
-                inputs,
-                configs,
-                return_exceptions=return_exceptions,
-                **{**self.kwargs, **kwargs},
-            )
-
-    @overload
-    def abatch_as_completed(
-        self,
-        inputs: Sequence[Input],
-        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,
-        *,
-        return_exceptions: Literal[False] = False,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[tuple[int, Output]]: ...
-
-    @overload
-    def abatch_as_completed(
-        self,
-        inputs: Sequence[Input],
-        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,
-        *,
-        return_exceptions: Literal[True],
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]: ...
-
-    async def abatch_as_completed(
-        self,
-        inputs: Sequence[Input],
-        config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[tuple[int, Union[Output, Exception]]]:
-        if isinstance(config, Sequence):
-            configs = cast(
-                list[RunnableConfig],
-                [self._merge_configs(conf) for conf in config],
-            )
-        else:
-            configs = [self._merge_configs(config) for _ in range(len(inputs))]
-        if return_exceptions:
-            async for item in self.bound.abatch_as_completed(
-                inputs,
-                configs,
-                return_exceptions=return_exceptions,
-                **{**self.kwargs, **kwargs},
-            ):
-                yield item
-        else:
-            async for item in self.bound.abatch_as_completed(
-                inputs,
-                configs,
-                return_exceptions=return_exceptions,
-                **{**self.kwargs, **kwargs},
-            ):
-                yield item
-
-    def stream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        yield from self.bound.stream(
-            input,
-            self._merge_configs(config),
-            **{**self.kwargs, **kwargs},
-        )
-
-    async def astream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        async for item in self.bound.astream(
-            input,
-            self._merge_configs(config),
-            **{**self.kwargs, **kwargs},
-        ):
-            yield item
-
-    async def astream_events(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[StreamEvent]:
-        async for item in self.bound.astream_events(
-            input, self._merge_configs(config), **{**self.kwargs, **kwargs}
-        ):
-            yield item
-
-    def transform(
-        self,
-        input: Iterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Iterator[Output]:
-        yield from self.bound.transform(
-            input,
-            self._merge_configs(config),
-            **{**self.kwargs, **kwargs},
-        )
-
-    async def atransform(
-        self,
-        input: AsyncIterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[Output]:
-        async for item in self.bound.atransform(
-            input,
-            self._merge_configs(config),
-            **{**self.kwargs, **kwargs},
-        ):
-            yield item
-
-
-RunnableBindingBase.model_rebuild()
-
-
-class RunnableBinding(RunnableBindingBase[Input, Output]):
-    """Wrap a Runnable with additional functionality.
-
-    A RunnableBinding can be thought of as a "runnable decorator" that
-    preserves the essential features of Runnable; i.e., batching, streaming,
-    and async support, while adding additional functionality.
-
-    Any class that inherits from Runnable can be bound to a `RunnableBinding`.
-    Runnables expose a standard set of methods for creating `RunnableBindings`
-    or sub-classes of `RunnableBindings` (e.g., `RunnableRetry`,
-    `RunnableWithFallbacks`) that add additional functionality.
-
-    These methods include:
-
-    - ``bind``: Bind kwargs to pass to the underlying Runnable when running it.
-    - ``with_config``: Bind config to pass to the underlying Runnable when running it.
-    - ``with_listeners``:  Bind lifecycle listeners to the underlying Runnable.
-    - ``with_types``: Override the input and output types of the underlying Runnable.
-    - ``with_retry``: Bind a retry policy to the underlying Runnable.
-    - ``with_fallbacks``: Bind a fallback policy to the underlying Runnable.
-
-    Example:
-    `bind`: Bind kwargs to pass to the underlying Runnable when running it.
-
-        .. code-block:: python
-
-            # Create a Runnable binding that invokes the ChatModel with the
-            # additional kwarg `stop=['-']` when running it.
-            from langchain_community.chat_models import ChatOpenAI
-            model = ChatOpenAI()
-            model.invoke('Say "Parrot-MAGIC"', stop=['-']) # Should return `Parrot`
-            # Using it the easy way via `bind` method which returns a new
-            # RunnableBinding
-            runnable_binding = model.bind(stop=['-'])
-            runnable_binding.invoke('Say "Parrot-MAGIC"') # Should return `Parrot`
-
-        Can also be done by instantiating a RunnableBinding directly (not recommended):
-
-        .. code-block:: python
-
-            from langchain_core.runnables import RunnableBinding
-            runnable_binding = RunnableBinding(
-                bound=model,
-                kwargs={'stop': ['-']} # <-- Note the additional kwargs
-            )
-            runnable_binding.invoke('Say "Parrot-MAGIC"') # Should return `Parrot`
-    """
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    def bind(self, **kwargs: Any) -> Runnable[Input, Output]:
-        """Bind additional kwargs to a Runnable, returning a new Runnable.
-
-        Args:
-            **kwargs: The kwargs to bind to the Runnable.
-
-        Returns:
-            A new Runnable with the same type and config as the original,
-            but with the additional kwargs bound.
-        """
-        return self.__class__(
-            bound=self.bound,
-            config=self.config,
-            kwargs={**self.kwargs, **kwargs},
-            custom_input_type=self.custom_input_type,
-            custom_output_type=self.custom_output_type,
-        )
-
-    def with_config(
-        self,
-        config: Optional[RunnableConfig] = None,
-        # Sadly Unpack is not well supported by mypy so this will have to be untyped
-        **kwargs: Any,
-    ) -> Runnable[Input, Output]:
-        return self.__class__(
-            bound=self.bound,
-            kwargs=self.kwargs,
-            config=cast(RunnableConfig, {**self.config, **(config or {}), **kwargs}),
-            custom_input_type=self.custom_input_type,
-            custom_output_type=self.custom_output_type,
-        )
-
-    def with_listeners(
-        self,
-        *,
-        on_start: Optional[
-            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]
-        ] = None,
-        on_end: Optional[
-            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]
-        ] = None,
-        on_error: Optional[
-            Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]
-        ] = None,
-    ) -> Runnable[Input, Output]:
-        """Bind lifecycle listeners to a Runnable, returning a new Runnable.
-
-        Args:
-            on_start: Called before the Runnable starts running, with the Run object.
-                Defaults to None.
-            on_end: Called after the Runnable finishes running, with the Run object.
-                Defaults to None.
-            on_error: Called if the Runnable throws an error, with the Run object.
-                Defaults to None.
-
-        Returns:
-            The Runnable object contains information about the run, including its id,
-            type, input, output, error, start_time, end_time, and any tags or metadata
-            added to the run.
-        """
-        from langchain_core.tracers.root_listeners import RootListenersTracer
-
-        return self.__class__(
-            bound=self.bound,
-            kwargs=self.kwargs,
-            config=self.config,
-            config_factories=[
-                lambda config: {
-                    "callbacks": [
-                        RootListenersTracer(
-                            config=config,
-                            on_start=on_start,
-                            on_end=on_end,
-                            on_error=on_error,
-                        )
-                    ],
-                }
-            ],
-            custom_input_type=self.custom_input_type,
-            custom_output_type=self.custom_output_type,
-        )
-
-    def with_types(
-        self,
-        input_type: Optional[Union[type[Input], BaseModel]] = None,
-        output_type: Optional[Union[type[Output], BaseModel]] = None,
-    ) -> Runnable[Input, Output]:
-        return self.__class__(
-            bound=self.bound,
-            kwargs=self.kwargs,
-            config=self.config,
-            custom_input_type=(
-                input_type if input_type is not None else self.custom_input_type
-            ),
-            custom_output_type=(
-                output_type if output_type is not None else self.custom_output_type
-            ),
-        )
-
-    def with_retry(self, **kwargs: Any) -> Runnable[Input, Output]:
-        return self.__class__(
-            bound=self.bound.with_retry(**kwargs),
-            kwargs=self.kwargs,
-            config=self.config,
-        )
-
-    def __getattr__(self, name: str) -> Any:
-        attr = getattr(self.bound, name)
-
-        if callable(attr) and (
-            config_param := inspect.signature(attr).parameters.get("config")
-        ):
-            if config_param.kind == inspect.Parameter.KEYWORD_ONLY:
-
-                @wraps(attr)
-                def wrapper(*args: Any, **kwargs: Any) -> Any:
-                    return attr(
-                        *args,
-                        config=merge_configs(self.config, kwargs.pop("config", None)),
-                        **kwargs,
-                    )
-
-                return wrapper
-            elif config_param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:
-                idx = list(inspect.signature(attr).parameters).index("config")
-
-                @wraps(attr)
-                def wrapper(*args: Any, **kwargs: Any) -> Any:
-                    if len(args) >= idx + 1:
-                        argsl = list(args)
-                        argsl[idx] = merge_configs(self.config, argsl[idx])
-                        return attr(*argsl, **kwargs)
-                    else:
-                        return attr(
-                            *args,
-                            config=merge_configs(
-                                self.config, kwargs.pop("config", None)
-                            ),
-                            **kwargs,
-                        )
-
-                return wrapper
-
-        return attr
-
-
-class _RunnableCallableSync(Protocol[Input, Output]):
-    def __call__(self, __in: Input, *, config: RunnableConfig) -> Output: ...
-
-
-class _RunnableCallableAsync(Protocol[Input, Output]):
-    def __call__(self, __in: Input, *, config: RunnableConfig) -> Awaitable[Output]: ...
-
-
-class _RunnableCallableIterator(Protocol[Input, Output]):
-    def __call__(
-        self, __in: Iterator[Input], *, config: RunnableConfig
-    ) -> Iterator[Output]: ...
-
-
-class _RunnableCallableAsyncIterator(Protocol[Input, Output]):
-    def __call__(
-        self, __in: AsyncIterator[Input], *, config: RunnableConfig
-    ) -> AsyncIterator[Output]: ...
-
-
-RunnableLike = Union[
-    Runnable[Input, Output],
-    Callable[[Input], Output],
-    Callable[[Input], Awaitable[Output]],
-    Callable[[Iterator[Input]], Iterator[Output]],
-    Callable[[AsyncIterator[Input]], AsyncIterator[Output]],
-    _RunnableCallableSync[Input, Output],
-    _RunnableCallableAsync[Input, Output],
-    _RunnableCallableIterator[Input, Output],
-    _RunnableCallableAsyncIterator[Input, Output],
-    Mapping[str, Any],
-]
-
-
-def coerce_to_runnable(thing: RunnableLike) -> Runnable[Input, Output]:
-    """Coerce a Runnable-like object into a Runnable.
-
-    Args:
-        thing: A Runnable-like object.
-
-    Returns:
-        A Runnable.
-
-    Raises:
-        TypeError: If the object is not Runnable-like.
-    """
-    if isinstance(thing, Runnable):
-        return thing
-    elif is_async_generator(thing) or inspect.isgeneratorfunction(thing):
-        return RunnableGenerator(thing)
-    elif callable(thing):
-        return RunnableLambda(cast(Callable[[Input], Output], thing))
-    elif isinstance(thing, dict):
-        return cast(Runnable[Input, Output], RunnableParallel(thing))
-    else:
-        msg = (
-            f"Expected a Runnable, callable or dict."
-            f"Instead got an unsupported type: {type(thing)}"
-        )
-        raise TypeError(msg)
-
-
-@overload
-def chain(
-    func: Callable[[Input], Coroutine[Any, Any, Output]],
-) -> Runnable[Input, Output]: ...
-
-
-@overload
-def chain(
-    func: Callable[[Input], Iterator[Output]],
-) -> Runnable[Input, Output]: ...
-
-
-@overload
-def chain(
-    func: Callable[[Input], AsyncIterator[Output]],
-) -> Runnable[Input, Output]: ...
-
-
-@overload
-def chain(
-    func: Callable[[Input], Output],
-) -> Runnable[Input, Output]: ...
-
-
-def chain(
-    func: Union[
-        Callable[[Input], Output],
-        Callable[[Input], Iterator[Output]],
-        Callable[[Input], Coroutine[Any, Any, Output]],
-        Callable[[Input], AsyncIterator[Output]],
-    ],
-) -> Runnable[Input, Output]:
-    """Decorate a function to make it a Runnable.
-    Sets the name of the Runnable to the name of the function.
-    Any runnables called by the function will be traced as dependencies.
-
-    Args:
-        func: A callable.
-
-    Returns:
-        A Runnable.
-
-    Example:
-
-    .. code-block:: python
-
-        from langchain_core.runnables import chain
-        from langchain_core.prompts import PromptTemplate
-        from langchain_openai import OpenAI
-
-        @chain
-        def my_func(fields):
-            prompt = PromptTemplate("Hello, {name}!")
-            llm = OpenAI()
-            formatted = prompt.invoke(**fields)
-
-            for chunk in llm.stream(formatted):
-                yield chunk
-    """
-    return RunnableLambda(func)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/branch.py ./custom_langchain_core/runnables/branch.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/branch.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/branch.py	1970-01-01 09:00:00
@@ -1,475 +0,0 @@
-from collections.abc import AsyncIterator, Awaitable, Iterator, Mapping, Sequence
-from typing import (
-    Any,
-    Callable,
-    Optional,
-    Union,
-    cast,
-)
-
-from pydantic import BaseModel, ConfigDict
-
-from langchain_core.runnables.base import (
-    Runnable,
-    RunnableLike,
-    RunnableSerializable,
-    coerce_to_runnable,
-)
-from langchain_core.runnables.config import (
-    RunnableConfig,
-    ensure_config,
-    get_async_callback_manager_for_config,
-    get_callback_manager_for_config,
-    patch_config,
-)
-from langchain_core.runnables.utils import (
-    ConfigurableFieldSpec,
-    Input,
-    Output,
-    get_unique_config_specs,
-)
-
-
-class RunnableBranch(RunnableSerializable[Input, Output]):
-    """Runnable that selects which branch to run based on a condition.
-
-    The Runnable is initialized with a list of (condition, Runnable) pairs and
-    a default branch.
-
-    When operating on an input, the first condition that evaluates to True is
-    selected, and the corresponding Runnable is run on the input.
-
-    If no condition evaluates to True, the default branch is run on the input.
-
-    Parameters:
-        branches: A list of (condition, Runnable) pairs.
-        default: A Runnable to run if no condition is met.
-
-    Examples:
-
-        .. code-block:: python
-
-            from langchain_core.runnables import RunnableBranch
-
-            branch = RunnableBranch(
-                (lambda x: isinstance(x, str), lambda x: x.upper()),
-                (lambda x: isinstance(x, int), lambda x: x + 1),
-                (lambda x: isinstance(x, float), lambda x: x * 2),
-                lambda x: "goodbye",
-            )
-
-            branch.invoke("hello") # "HELLO"
-            branch.invoke(None) # "goodbye"
-    """
-
-    branches: Sequence[tuple[Runnable[Input, bool], Runnable[Input, Output]]]
-    default: Runnable[Input, Output]
-
-    def __init__(
-        self,
-        *branches: Union[
-            tuple[
-                Union[
-                    Runnable[Input, bool],
-                    Callable[[Input], bool],
-                    Callable[[Input], Awaitable[bool]],
-                ],
-                RunnableLike,
-            ],
-            RunnableLike,  # To accommodate the default branch
-        ],
-    ) -> None:
-        """A Runnable that runs one of two branches based on a condition.
-
-        Args:
-            *branches: A list of (condition, Runnable) pairs.
-                Defaults a Runnable to run if no condition is met.
-
-        Raises:
-            ValueError: If the number of branches is less than 2.
-            TypeError: If the default branch is not Runnable, Callable or Mapping.
-            TypeError: If a branch is not a tuple or list.
-            ValueError: If a branch is not of length 2.
-        """
-        if len(branches) < 2:
-            msg = "RunnableBranch requires at least two branches"
-            raise ValueError(msg)
-
-        default = branches[-1]
-
-        if not isinstance(
-            default,
-            (Runnable, Callable, Mapping),  # type: ignore[arg-type]
-        ):
-            msg = "RunnableBranch default must be Runnable, callable or mapping."
-            raise TypeError(msg)
-
-        default_ = cast(
-            Runnable[Input, Output], coerce_to_runnable(cast(RunnableLike, default))
-        )
-
-        _branches = []
-
-        for branch in branches[:-1]:
-            if not isinstance(branch, (tuple, list)):  # type: ignore[arg-type]
-                msg = (
-                    f"RunnableBranch branches must be "
-                    f"tuples or lists, not {type(branch)}"
-                )
-                raise TypeError(msg)
-
-            if len(branch) != 2:
-                msg = (
-                    f"RunnableBranch branches must be "
-                    f"tuples or lists of length 2, not {len(branch)}"
-                )
-                raise ValueError(msg)
-            condition, runnable = branch
-            condition = cast(Runnable[Input, bool], coerce_to_runnable(condition))
-            runnable = coerce_to_runnable(runnable)
-            _branches.append((condition, runnable))
-
-        super().__init__(
-            branches=_branches,
-            default=default_,
-        )  # type: ignore[call-arg]
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """RunnableBranch is serializable if all its branches are serializable."""
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        runnables = (
-            [self.default]
-            + [r for _, r in self.branches]
-            + [r for r, _ in self.branches]
-        )
-
-        for runnable in runnables:
-            if (
-                runnable.get_input_schema(config).model_json_schema().get("type")
-                is not None
-            ):
-                return runnable.get_input_schema(config)
-
-        return super().get_input_schema(config)
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        from langchain_core.beta.runnables.context import (
-            CONTEXT_CONFIG_PREFIX,
-            CONTEXT_CONFIG_SUFFIX_SET,
-        )
-
-        specs = get_unique_config_specs(
-            spec
-            for step in (
-                [self.default]
-                + [r for _, r in self.branches]
-                + [r for r, _ in self.branches]
-            )
-            for spec in step.config_specs
-        )
-        if any(
-            s.id.startswith(CONTEXT_CONFIG_PREFIX)
-            and s.id.endswith(CONTEXT_CONFIG_SUFFIX_SET)
-            for s in specs
-        ):
-            msg = "RunnableBranch cannot contain context setters."
-            raise ValueError(msg)
-        return specs
-
-    def invoke(
-        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Output:
-        """First evaluates the condition, then delegate to true or false branch.
-
-        Args:
-            input: The input to the Runnable.
-            config: The configuration for the Runnable. Defaults to None.
-            kwargs: Additional keyword arguments to pass to the Runnable.
-
-        Returns:
-            The output of the branch that was run.
-
-        Raises:
-
-        """
-        config = ensure_config(config)
-        callback_manager = get_callback_manager_for_config(config)
-        run_manager = callback_manager.on_chain_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-
-        try:
-            for idx, branch in enumerate(self.branches):
-                condition, runnable = branch
-
-                expression_value = condition.invoke(
-                    input,
-                    config=patch_config(
-                        config,
-                        callbacks=run_manager.get_child(tag=f"condition:{idx + 1}"),
-                    ),
-                )
-
-                if expression_value:
-                    output = runnable.invoke(
-                        input,
-                        config=patch_config(
-                            config,
-                            callbacks=run_manager.get_child(tag=f"branch:{idx + 1}"),
-                        ),
-                        **kwargs,
-                    )
-                    break
-            else:
-                output = self.default.invoke(
-                    input,
-                    config=patch_config(
-                        config, callbacks=run_manager.get_child(tag="branch:default")
-                    ),
-                    **kwargs,
-                )
-        except BaseException as e:
-            run_manager.on_chain_error(e)
-            raise
-        run_manager.on_chain_end(output)
-        return output
-
-    async def ainvoke(
-        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Output:
-        """Async version of invoke."""
-        config = ensure_config(config)
-        callback_manager = get_async_callback_manager_for_config(config)
-        run_manager = await callback_manager.on_chain_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-        try:
-            for idx, branch in enumerate(self.branches):
-                condition, runnable = branch
-
-                expression_value = await condition.ainvoke(
-                    input,
-                    config=patch_config(
-                        config,
-                        callbacks=run_manager.get_child(tag=f"condition:{idx + 1}"),
-                    ),
-                )
-
-                if expression_value:
-                    output = await runnable.ainvoke(
-                        input,
-                        config=patch_config(
-                            config,
-                            callbacks=run_manager.get_child(tag=f"branch:{idx + 1}"),
-                        ),
-                        **kwargs,
-                    )
-                    break
-            else:
-                output = await self.default.ainvoke(
-                    input,
-                    config=patch_config(
-                        config, callbacks=run_manager.get_child(tag="branch:default")
-                    ),
-                    **kwargs,
-                )
-        except BaseException as e:
-            await run_manager.on_chain_error(e)
-            raise
-        await run_manager.on_chain_end(output)
-        return output
-
-    def stream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        """First evaluates the condition,
-        then delegate to true or false branch.
-
-        Args:
-            input: The input to the Runnable.
-            config: The configuration for the Runnable. Defaults to None.
-            kwargs: Additional keyword arguments to pass to the Runnable.
-
-        Yields:
-            The output of the branch that was run.
-
-        Raises:
-            BaseException: If an error occurs during the execution of the Runnable.
-        """
-        config = ensure_config(config)
-        callback_manager = get_callback_manager_for_config(config)
-        run_manager = callback_manager.on_chain_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-        final_output: Optional[Output] = None
-        final_output_supported = True
-
-        try:
-            for idx, branch in enumerate(self.branches):
-                condition, runnable = branch
-
-                expression_value = condition.invoke(
-                    input,
-                    config=patch_config(
-                        config,
-                        callbacks=run_manager.get_child(tag=f"condition:{idx + 1}"),
-                    ),
-                )
-
-                if expression_value:
-                    for chunk in runnable.stream(
-                        input,
-                        config=patch_config(
-                            config,
-                            callbacks=run_manager.get_child(tag=f"branch:{idx + 1}"),
-                        ),
-                        **kwargs,
-                    ):
-                        yield chunk
-                        if final_output_supported:
-                            if final_output is None:
-                                final_output = chunk
-                            else:
-                                try:
-                                    final_output = final_output + chunk  # type: ignore
-                                except TypeError:
-                                    final_output = None
-                                    final_output_supported = False
-                    break
-            else:
-                for chunk in self.default.stream(
-                    input,
-                    config=patch_config(
-                        config,
-                        callbacks=run_manager.get_child(tag="branch:default"),
-                    ),
-                    **kwargs,
-                ):
-                    yield chunk
-                    if final_output_supported:
-                        if final_output is None:
-                            final_output = chunk
-                        else:
-                            try:
-                                final_output = final_output + chunk  # type: ignore
-                            except TypeError:
-                                final_output = None
-                                final_output_supported = False
-        except BaseException as e:
-            run_manager.on_chain_error(e)
-            raise
-        run_manager.on_chain_end(final_output)
-
-    async def astream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        """First evaluates the condition,
-        then delegate to true or false branch.
-
-        Args:
-            input: The input to the Runnable.
-            config: The configuration for the Runnable. Defaults to None.
-            kwargs: Additional keyword arguments to pass to the Runnable.
-
-        Yields:
-            The output of the branch that was run.
-
-        Raises:
-            BaseException: If an error occurs during the execution of the Runnable.
-        """
-        config = ensure_config(config)
-        callback_manager = get_async_callback_manager_for_config(config)
-        run_manager = await callback_manager.on_chain_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-        final_output: Optional[Output] = None
-        final_output_supported = True
-
-        try:
-            for idx, branch in enumerate(self.branches):
-                condition, runnable = branch
-
-                expression_value = await condition.ainvoke(
-                    input,
-                    config=patch_config(
-                        config,
-                        callbacks=run_manager.get_child(tag=f"condition:{idx + 1}"),
-                    ),
-                )
-
-                if expression_value:
-                    async for chunk in runnable.astream(
-                        input,
-                        config=patch_config(
-                            config,
-                            callbacks=run_manager.get_child(tag=f"branch:{idx + 1}"),
-                        ),
-                        **kwargs,
-                    ):
-                        yield chunk
-                        if final_output_supported:
-                            if final_output is None:
-                                final_output = chunk
-                            else:
-                                try:
-                                    final_output = final_output + chunk  # type: ignore
-                                except TypeError:
-                                    final_output = None
-                                    final_output_supported = False
-                    break
-            else:
-                async for chunk in self.default.astream(
-                    input,
-                    config=patch_config(
-                        config,
-                        callbacks=run_manager.get_child(tag="branch:default"),
-                    ),
-                    **kwargs,
-                ):
-                    yield chunk
-                    if final_output_supported:
-                        if final_output is None:
-                            final_output = chunk
-                        else:
-                            try:
-                                final_output = final_output + chunk  # type: ignore
-                            except TypeError:
-                                final_output = None
-                                final_output_supported = False
-        except BaseException as e:
-            await run_manager.on_chain_error(e)
-            raise
-        await run_manager.on_chain_end(final_output)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/config.py ./custom_langchain_core/runnables/config.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/config.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/config.py	1970-01-01 09:00:00
@@ -1,593 +0,0 @@
-from __future__ import annotations
-
-import asyncio
-import uuid
-import warnings
-from collections.abc import Awaitable, Generator, Iterable, Iterator, Sequence
-from concurrent.futures import Executor, Future, ThreadPoolExecutor
-from contextlib import contextmanager
-from contextvars import ContextVar, copy_context
-from functools import partial
-from typing import TYPE_CHECKING, Any, Callable, Optional, TypeVar, Union, cast
-
-from typing_extensions import ParamSpec, TypedDict
-
-from langchain_core.runnables.utils import (
-    Input,
-    Output,
-    accepts_config,
-    accepts_run_manager,
-)
-
-if TYPE_CHECKING:
-    from langchain_core.callbacks.base import BaseCallbackManager, Callbacks
-    from langchain_core.callbacks.manager import (
-        AsyncCallbackManager,
-        AsyncCallbackManagerForChainRun,
-        CallbackManager,
-        CallbackManagerForChainRun,
-    )
-else:
-    # Pydantic validates through typed dicts, but
-    # the callbacks need forward refs updated
-    Callbacks = Optional[Union[list, Any]]
-
-
-class EmptyDict(TypedDict, total=False):
-    """Empty dict type."""
-
-
-class RunnableConfig(TypedDict, total=False):
-    """Configuration for a Runnable."""
-
-    tags: list[str]
-    """
-    Tags for this call and any sub-calls (eg. a Chain calling an LLM).
-    You can use these to filter calls.
-    """
-
-    metadata: dict[str, Any]
-    """
-    Metadata for this call and any sub-calls (eg. a Chain calling an LLM).
-    Keys should be strings, values should be JSON-serializable.
-    """
-
-    callbacks: Callbacks
-    """
-    Callbacks for this call and any sub-calls (eg. a Chain calling an LLM).
-    Tags are passed to all callbacks, metadata is passed to handle*Start callbacks.
-    """
-
-    run_name: str
-    """
-    Name for the tracer run for this call. Defaults to the name of the class.
-    """
-
-    max_concurrency: Optional[int]
-    """
-    Maximum number of parallel calls to make. If not provided, defaults to
-    ThreadPoolExecutor's default.
-    """
-
-    recursion_limit: int
-    """
-    Maximum number of times a call can recurse. If not provided, defaults to 25.
-    """
-
-    configurable: dict[str, Any]
-    """
-    Runtime values for attributes previously made configurable on this Runnable,
-    or sub-Runnables, through .configurable_fields() or .configurable_alternatives().
-    Check .output_schema() for a description of the attributes that have been made
-    configurable.
-    """
-
-    run_id: Optional[uuid.UUID]
-    """
-    Unique identifier for the tracer run for this call. If not provided, a new UUID
-        will be generated.
-    """
-
-
-CONFIG_KEYS = [
-    "tags",
-    "metadata",
-    "callbacks",
-    "run_name",
-    "max_concurrency",
-    "recursion_limit",
-    "configurable",
-    "run_id",
-]
-
-COPIABLE_KEYS = [
-    "tags",
-    "metadata",
-    "callbacks",
-    "configurable",
-]
-
-DEFAULT_RECURSION_LIMIT = 25
-
-
-var_child_runnable_config: ContextVar[RunnableConfig | None] = ContextVar(
-    "child_runnable_config", default=None
-)
-
-
-def _set_config_context(config: RunnableConfig) -> None:
-    """Set the child Runnable config + tracing context.
-
-    Args:
-        config (RunnableConfig): The config to set.
-    """
-    from langchain_core.tracers.langchain import LangChainTracer
-
-    var_child_runnable_config.set(config)
-    if (
-        (callbacks := config.get("callbacks"))
-        and (
-            parent_run_id := getattr(callbacks, "parent_run_id", None)
-        )  # Is callback manager
-        and (
-            tracer := next(
-                (
-                    handler
-                    for handler in getattr(callbacks, "handlers", [])
-                    if isinstance(handler, LangChainTracer)
-                ),
-                None,
-            )
-        )
-        and (run := tracer.run_map.get(str(parent_run_id)))
-    ):
-        from langsmith.run_helpers import _set_tracing_context
-
-        _set_tracing_context({"parent": run})
-
-
-def ensure_config(config: Optional[RunnableConfig] = None) -> RunnableConfig:
-    """Ensure that a config is a dict with all keys present.
-
-    Args:
-        config (Optional[RunnableConfig], optional): The config to ensure.
-          Defaults to None.
-
-    Returns:
-        RunnableConfig: The ensured config.
-    """
-    empty = RunnableConfig(
-        tags=[],
-        metadata={},
-        callbacks=None,
-        recursion_limit=DEFAULT_RECURSION_LIMIT,
-        configurable={},
-    )
-    if var_config := var_child_runnable_config.get():
-        empty.update(
-            cast(
-                RunnableConfig,
-                {
-                    k: v.copy() if k in COPIABLE_KEYS else v  # type: ignore[attr-defined]
-                    for k, v in var_config.items()
-                    if v is not None
-                },
-            )
-        )
-    if config is not None:
-        empty.update(
-            cast(
-                RunnableConfig,
-                {
-                    k: v.copy() if k in COPIABLE_KEYS else v  # type: ignore[attr-defined]
-                    for k, v in config.items()
-                    if v is not None and k in CONFIG_KEYS
-                },
-            )
-        )
-    if config is not None:
-        for k, v in config.items():
-            if k not in CONFIG_KEYS and v is not None:
-                empty["configurable"][k] = v
-    for key, value in empty.get("configurable", {}).items():
-        if (
-            not key.startswith("__")
-            and isinstance(value, (str, int, float, bool))
-            and key not in empty["metadata"]
-        ):
-            empty["metadata"][key] = value
-    return empty
-
-
-def get_config_list(
-    config: Optional[Union[RunnableConfig, Sequence[RunnableConfig]]], length: int
-) -> list[RunnableConfig]:
-    """Get a list of configs from a single config or a list of configs.
-
-     It is useful for subclasses overriding batch() or abatch().
-
-    Args:
-        config (Optional[Union[RunnableConfig, List[RunnableConfig]]]):
-          The config or list of configs.
-        length (int): The length of the list.
-
-    Returns:
-        List[RunnableConfig]: The list of configs.
-
-    Raises:
-        ValueError: If the length of the list is not equal to the length of the inputs.
-
-    """
-    if length < 0:
-        msg = f"length must be >= 0, but got {length}"
-        raise ValueError(msg)
-    if isinstance(config, Sequence) and len(config) != length:
-        msg = (
-            f"config must be a list of the same length as inputs, "
-            f"but got {len(config)} configs for {length} inputs"
-        )
-        raise ValueError(msg)
-
-    if isinstance(config, Sequence):
-        return list(map(ensure_config, config))
-    if length > 1 and isinstance(config, dict) and config.get("run_id") is not None:
-        warnings.warn(
-            "Provided run_id be used only for the first element of the batch.",
-            category=RuntimeWarning,
-            stacklevel=3,
-        )
-        subsequent = cast(
-            RunnableConfig, {k: v for k, v in config.items() if k != "run_id"}
-        )
-        return [
-            ensure_config(subsequent) if i else ensure_config(config)
-            for i in range(length)
-        ]
-    return [ensure_config(config) for i in range(length)]
-
-
-def patch_config(
-    config: Optional[RunnableConfig],
-    *,
-    callbacks: Optional[BaseCallbackManager] = None,
-    recursion_limit: Optional[int] = None,
-    max_concurrency: Optional[int] = None,
-    run_name: Optional[str] = None,
-    configurable: Optional[dict[str, Any]] = None,
-) -> RunnableConfig:
-    """Patch a config with new values.
-
-    Args:
-        config (Optional[RunnableConfig]): The config to patch.
-        callbacks (Optional[BaseCallbackManager], optional): The callbacks to set.
-          Defaults to None.
-        recursion_limit (Optional[int], optional): The recursion limit to set.
-          Defaults to None.
-        max_concurrency (Optional[int], optional): The max concurrency to set.
-          Defaults to None.
-        run_name (Optional[str], optional): The run name to set. Defaults to None.
-        configurable (Optional[Dict[str, Any]], optional): The configurable to set.
-          Defaults to None.
-
-    Returns:
-        RunnableConfig: The patched config.
-    """
-    config = ensure_config(config)
-    if callbacks is not None:
-        # If we're replacing callbacks, we need to unset run_name
-        # As that should apply only to the same run as the original callbacks
-        config["callbacks"] = callbacks
-        if "run_name" in config:
-            del config["run_name"]
-        if "run_id" in config:
-            del config["run_id"]
-    if recursion_limit is not None:
-        config["recursion_limit"] = recursion_limit
-    if max_concurrency is not None:
-        config["max_concurrency"] = max_concurrency
-    if run_name is not None:
-        config["run_name"] = run_name
-    if configurable is not None:
-        config["configurable"] = {**config.get("configurable", {}), **configurable}
-    return config
-
-
-def merge_configs(*configs: Optional[RunnableConfig]) -> RunnableConfig:
-    """Merge multiple configs into one.
-
-    Args:
-        *configs (Optional[RunnableConfig]): The configs to merge.
-
-    Returns:
-        RunnableConfig: The merged config.
-    """
-    base: RunnableConfig = {}
-    # Even though the keys aren't literals, this is correct
-    # because both dicts are the same type
-    for config in (ensure_config(c) for c in configs if c is not None):
-        for key in config:
-            if key == "metadata":
-                base[key] = {  # type: ignore
-                    **base.get(key, {}),  # type: ignore
-                    **(config.get(key) or {}),  # type: ignore
-                }
-            elif key == "tags":
-                base[key] = sorted(  # type: ignore
-                    set(base.get(key, []) + (config.get(key) or [])),  # type: ignore
-                )
-            elif key == "configurable":
-                base[key] = {  # type: ignore
-                    **base.get(key, {}),  # type: ignore
-                    **(config.get(key) or {}),  # type: ignore
-                }
-            elif key == "callbacks":
-                base_callbacks = base.get("callbacks")
-                these_callbacks = config["callbacks"]
-                # callbacks can be either None, list[handler] or manager
-                # so merging two callbacks values has 6 cases
-                if isinstance(these_callbacks, list):
-                    if base_callbacks is None:
-                        base["callbacks"] = these_callbacks.copy()
-                    elif isinstance(base_callbacks, list):
-                        base["callbacks"] = base_callbacks + these_callbacks
-                    else:
-                        # base_callbacks is a manager
-                        mngr = base_callbacks.copy()
-                        for callback in these_callbacks:
-                            mngr.add_handler(callback, inherit=True)
-                        base["callbacks"] = mngr
-                elif these_callbacks is not None:
-                    # these_callbacks is a manager
-                    if base_callbacks is None:
-                        base["callbacks"] = these_callbacks.copy()
-                    elif isinstance(base_callbacks, list):
-                        mngr = these_callbacks.copy()
-                        for callback in base_callbacks:
-                            mngr.add_handler(callback, inherit=True)
-                        base["callbacks"] = mngr
-                    else:
-                        # base_callbacks is also a manager
-                        base["callbacks"] = base_callbacks.merge(these_callbacks)
-            elif key == "recursion_limit":
-                if config["recursion_limit"] != DEFAULT_RECURSION_LIMIT:
-                    base["recursion_limit"] = config["recursion_limit"]
-            elif key in COPIABLE_KEYS and config[key] is not None:  # type: ignore[literal-required]
-                base[key] = config[key].copy()  # type: ignore[literal-required]
-            else:
-                base[key] = config[key] or base.get(key)  # type: ignore
-    return base
-
-
-def call_func_with_variable_args(
-    func: Union[
-        Callable[[Input], Output],
-        Callable[[Input, RunnableConfig], Output],
-        Callable[[Input, CallbackManagerForChainRun], Output],
-        Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],
-    ],
-    input: Input,
-    config: RunnableConfig,
-    run_manager: Optional[CallbackManagerForChainRun] = None,
-    **kwargs: Any,
-) -> Output:
-    """Call function that may optionally accept a run_manager and/or config.
-
-    Args:
-        func (Union[Callable[[Input], Output],
-          Callable[[Input, CallbackManagerForChainRun], Output],
-          Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output]]):
-           The function to call.
-        input (Input): The input to the function.
-        config (RunnableConfig): The config to pass to the function.
-        run_manager (CallbackManagerForChainRun): The run manager to
-          pass to the function. Defaults to None.
-        **kwargs (Any): The keyword arguments to pass to the function.
-
-    Returns:
-        Output: The output of the function.
-    """
-    if accepts_config(func):
-        if run_manager is not None:
-            kwargs["config"] = patch_config(config, callbacks=run_manager.get_child())
-        else:
-            kwargs["config"] = config
-    if run_manager is not None and accepts_run_manager(func):
-        kwargs["run_manager"] = run_manager
-    return func(input, **kwargs)  # type: ignore[call-arg]
-
-
-def acall_func_with_variable_args(
-    func: Union[
-        Callable[[Input], Awaitable[Output]],
-        Callable[[Input, RunnableConfig], Awaitable[Output]],
-        Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],
-        Callable[
-            [Input, AsyncCallbackManagerForChainRun, RunnableConfig],
-            Awaitable[Output],
-        ],
-    ],
-    input: Input,
-    config: RunnableConfig,
-    run_manager: Optional[AsyncCallbackManagerForChainRun] = None,
-    **kwargs: Any,
-) -> Awaitable[Output]:
-    """Async call function that may optionally accept a run_manager and/or config.
-
-    Args:
-        func (Union[Callable[[Input], Awaitable[Output]], Callable[[Input,
-            AsyncCallbackManagerForChainRun], Awaitable[Output]], Callable[[Input,
-            AsyncCallbackManagerForChainRun, RunnableConfig], Awaitable[Output]]]):
-            The function to call.
-        input (Input): The input to the function.
-        config (RunnableConfig): The config to pass to the function.
-        run_manager (AsyncCallbackManagerForChainRun): The run manager
-          to pass to the function. Defaults to None.
-        **kwargs (Any): The keyword arguments to pass to the function.
-
-    Returns:
-        Output: The output of the function.
-    """
-    if accepts_config(func):
-        if run_manager is not None:
-            kwargs["config"] = patch_config(config, callbacks=run_manager.get_child())
-        else:
-            kwargs["config"] = config
-    if run_manager is not None and accepts_run_manager(func):
-        kwargs["run_manager"] = run_manager
-    return func(input, **kwargs)  # type: ignore[call-arg]
-
-
-def get_callback_manager_for_config(config: RunnableConfig) -> CallbackManager:
-    """Get a callback manager for a config.
-
-    Args:
-        config (RunnableConfig): The config.
-
-    Returns:
-        CallbackManager: The callback manager.
-    """
-    from langchain_core.callbacks.manager import CallbackManager
-
-    return CallbackManager.configure(
-        inheritable_callbacks=config.get("callbacks"),
-        inheritable_tags=config.get("tags"),
-        inheritable_metadata=config.get("metadata"),
-    )
-
-
-def get_async_callback_manager_for_config(
-    config: RunnableConfig,
-) -> AsyncCallbackManager:
-    """Get an async callback manager for a config.
-
-    Args:
-        config (RunnableConfig): The config.
-
-    Returns:
-        AsyncCallbackManager: The async callback manager.
-    """
-    from langchain_core.callbacks.manager import AsyncCallbackManager
-
-    return AsyncCallbackManager.configure(
-        inheritable_callbacks=config.get("callbacks"),
-        inheritable_tags=config.get("tags"),
-        inheritable_metadata=config.get("metadata"),
-    )
-
-
-P = ParamSpec("P")
-T = TypeVar("T")
-
-
-class ContextThreadPoolExecutor(ThreadPoolExecutor):
-    """ThreadPoolExecutor that copies the context to the child thread."""
-
-    def submit(  # type: ignore[override]
-        self,
-        func: Callable[P, T],
-        *args: P.args,
-        **kwargs: P.kwargs,
-    ) -> Future[T]:
-        """Submit a function to the executor.
-
-        Args:
-            func (Callable[..., T]): The function to submit.
-            *args (Any): The positional arguments to the function.
-            **kwargs (Any): The keyword arguments to the function.
-
-        Returns:
-            Future[T]: The future for the function.
-        """
-        return super().submit(
-            cast(Callable[..., T], partial(copy_context().run, func, *args, **kwargs))
-        )
-
-    def map(
-        self,
-        fn: Callable[..., T],
-        *iterables: Iterable[Any],
-        timeout: float | None = None,
-        chunksize: int = 1,
-    ) -> Iterator[T]:
-        """Map a function to multiple iterables.
-
-        Args:
-            fn (Callable[..., T]): The function to map.
-            *iterables (Iterable[Any]): The iterables to map over.
-            timeout (float | None, optional): The timeout for the map.
-                Defaults to None.
-            chunksize (int, optional): The chunksize for the map. Defaults to 1.
-
-        Returns:
-            Iterator[T]: The iterator for the mapped function.
-        """
-        contexts = [copy_context() for _ in range(len(iterables[0]))]  # type: ignore[arg-type]
-
-        def _wrapped_fn(*args: Any) -> T:
-            return contexts.pop().run(fn, *args)
-
-        return super().map(
-            _wrapped_fn,
-            *iterables,
-            timeout=timeout,
-            chunksize=chunksize,
-        )
-
-
-@contextmanager
-def get_executor_for_config(
-    config: Optional[RunnableConfig],
-) -> Generator[Executor, None, None]:
-    """Get an executor for a config.
-
-    Args:
-        config (RunnableConfig): The config.
-
-    Yields:
-        Generator[Executor, None, None]: The executor.
-    """
-    config = config or {}
-    with ContextThreadPoolExecutor(
-        max_workers=config.get("max_concurrency")
-    ) as executor:
-        yield executor
-
-
-async def run_in_executor(
-    executor_or_config: Optional[Union[Executor, RunnableConfig]],
-    func: Callable[P, T],
-    *args: P.args,
-    **kwargs: P.kwargs,
-) -> T:
-    """Run a function in an executor.
-
-    Args:
-        executor_or_config: The executor or config to run in.
-        func (Callable[P, Output]): The function.
-        *args (Any): The positional arguments to the function.
-        **kwargs (Any): The keyword arguments to the function.
-
-    Returns:
-        Output: The output of the function.
-
-    Raises:
-        RuntimeError: If the function raises a StopIteration.
-    """
-
-    def wrapper() -> T:
-        try:
-            return func(*args, **kwargs)
-        except StopIteration as exc:
-            # StopIteration can't be set on an asyncio.Future
-            # it raises a TypeError and leaves the Future pending forever
-            # so we need to convert it to a RuntimeError
-            raise RuntimeError from exc
-
-    if executor_or_config is None or isinstance(executor_or_config, dict):
-        # Use default executor with context copied from current context
-        return await asyncio.get_running_loop().run_in_executor(
-            None,
-            cast(Callable[..., T], partial(copy_context().run, wrapper)),
-        )
-
-    return await asyncio.get_running_loop().run_in_executor(executor_or_config, wrapper)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/configurable.py ./custom_langchain_core/runnables/configurable.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/configurable.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/configurable.py	1970-01-01 09:00:00
@@ -1,714 +0,0 @@
-from __future__ import annotations
-
-import enum
-import threading
-from abc import abstractmethod
-from collections.abc import AsyncIterator, Iterator, Sequence
-from collections.abc import Mapping as Mapping
-from functools import wraps
-from typing import (
-    Any,
-    Callable,
-    Optional,
-    Union,
-    cast,
-)
-from weakref import WeakValueDictionary
-
-from pydantic import BaseModel, ConfigDict
-from typing_extensions import override
-
-from langchain_core.runnables.base import Runnable, RunnableSerializable
-from langchain_core.runnables.config import (
-    RunnableConfig,
-    ensure_config,
-    get_config_list,
-    get_executor_for_config,
-    merge_configs,
-)
-from langchain_core.runnables.graph import Graph
-from langchain_core.runnables.utils import (
-    AnyConfigurableField,
-    ConfigurableField,
-    ConfigurableFieldMultiOption,
-    ConfigurableFieldSingleOption,
-    ConfigurableFieldSpec,
-    Input,
-    Output,
-    gather_with_concurrency,
-    get_unique_config_specs,
-)
-
-
-class DynamicRunnable(RunnableSerializable[Input, Output]):
-    """Serializable Runnable that can be dynamically configured.
-
-    A DynamicRunnable should be initiated using the `configurable_fields` or
-    `configurable_alternatives` method of a Runnable.
-
-    Parameters:
-        default: The default Runnable to use.
-        config: The configuration to use.
-    """
-
-    default: RunnableSerializable[Input, Output]
-
-    config: Optional[RunnableConfig] = None
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    @property
-    @override
-    def InputType(self) -> type[Input]:
-        return self.default.InputType
-
-    @property
-    @override
-    def OutputType(self) -> type[Output]:
-        return self.default.OutputType
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        runnable, config = self.prepare(config)
-        return runnable.get_input_schema(config)
-
-    def get_output_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        runnable, config = self.prepare(config)
-        return runnable.get_output_schema(config)
-
-    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
-        runnable, config = self.prepare(config)
-        return runnable.get_graph(config)
-
-    def with_config(
-        self,
-        config: Optional[RunnableConfig] = None,
-        # Sadly Unpack is not well supported by mypy so this will have to be untyped
-        **kwargs: Any,
-    ) -> Runnable[Input, Output]:
-        return self.__class__(
-            **{**self.__dict__, "config": ensure_config(merge_configs(config, kwargs))}  # type: ignore[arg-type]
-        )
-
-    def prepare(
-        self, config: Optional[RunnableConfig] = None
-    ) -> tuple[Runnable[Input, Output], RunnableConfig]:
-        """Prepare the Runnable for invocation.
-
-        Args:
-            config: The configuration to use. Defaults to None.
-
-        Returns:
-            Tuple[Runnable[Input, Output], RunnableConfig]: The prepared Runnable and
-            configuration.
-        """
-        runnable: Runnable[Input, Output] = self
-        while isinstance(runnable, DynamicRunnable):
-            runnable, config = runnable._prepare(merge_configs(runnable.config, config))
-        return runnable, cast(RunnableConfig, config)
-
-    @abstractmethod
-    def _prepare(
-        self, config: Optional[RunnableConfig] = None
-    ) -> tuple[Runnable[Input, Output], RunnableConfig]: ...
-
-    def invoke(
-        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Output:
-        runnable, config = self.prepare(config)
-        return runnable.invoke(input, config, **kwargs)
-
-    async def ainvoke(
-        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Output:
-        runnable, config = self.prepare(config)
-        return await runnable.ainvoke(input, config, **kwargs)
-
-    def batch(
-        self,
-        inputs: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        configs = get_config_list(config, len(inputs))
-        prepared = [self.prepare(c) for c in configs]
-
-        if all(p is self.default for p, _ in prepared):
-            return self.default.batch(
-                inputs,
-                [c for _, c in prepared],
-                return_exceptions=return_exceptions,
-                **kwargs,
-            )
-
-        if not inputs:
-            return []
-
-        def invoke(
-            prepared: tuple[Runnable[Input, Output], RunnableConfig],
-            input: Input,
-        ) -> Union[Output, Exception]:
-            bound, config = prepared
-            if return_exceptions:
-                try:
-                    return bound.invoke(input, config, **kwargs)
-                except Exception as e:
-                    return e
-            else:
-                return bound.invoke(input, config, **kwargs)
-
-        # If there's only one input, don't bother with the executor
-        if len(inputs) == 1:
-            return cast(list[Output], [invoke(prepared[0], inputs[0])])
-
-        with get_executor_for_config(configs[0]) as executor:
-            return cast(list[Output], list(executor.map(invoke, prepared, inputs)))
-
-    async def abatch(
-        self,
-        inputs: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        configs = get_config_list(config, len(inputs))
-        prepared = [self.prepare(c) for c in configs]
-
-        if all(p is self.default for p, _ in prepared):
-            return await self.default.abatch(
-                inputs,
-                [c for _, c in prepared],
-                return_exceptions=return_exceptions,
-                **kwargs,
-            )
-
-        if not inputs:
-            return []
-
-        async def ainvoke(
-            prepared: tuple[Runnable[Input, Output], RunnableConfig],
-            input: Input,
-        ) -> Union[Output, Exception]:
-            bound, config = prepared
-            if return_exceptions:
-                try:
-                    return await bound.ainvoke(input, config, **kwargs)
-                except Exception as e:
-                    return e
-            else:
-                return await bound.ainvoke(input, config, **kwargs)
-
-        coros = map(ainvoke, prepared, inputs)
-        return await gather_with_concurrency(configs[0].get("max_concurrency"), *coros)
-
-    def stream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        runnable, config = self.prepare(config)
-        return runnable.stream(input, config, **kwargs)
-
-    async def astream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        runnable, config = self.prepare(config)
-        async for chunk in runnable.astream(input, config, **kwargs):
-            yield chunk
-
-    def transform(
-        self,
-        input: Iterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        runnable, config = self.prepare(config)
-        return runnable.transform(input, config, **kwargs)
-
-    async def atransform(
-        self,
-        input: AsyncIterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        runnable, config = self.prepare(config)
-        async for chunk in runnable.atransform(input, config, **kwargs):
-            yield chunk
-
-    def __getattr__(self, name: str) -> Any:
-        attr = getattr(self.default, name)
-        if callable(attr):
-
-            @wraps(attr)
-            def wrapper(*args: Any, **kwargs: Any) -> Any:
-                for key, arg in kwargs.items():
-                    if key == "config" and (
-                        isinstance(arg, dict)
-                        and "configurable" in arg
-                        and isinstance(arg["configurable"], dict)
-                    ):
-                        runnable, config = self.prepare(cast(RunnableConfig, arg))
-                        kwargs = {**kwargs, "config": config}
-                        return getattr(runnable, name)(*args, **kwargs)
-
-                for idx, arg in enumerate(args):
-                    if (
-                        isinstance(arg, dict)
-                        and "configurable" in arg
-                        and isinstance(arg["configurable"], dict)
-                    ):
-                        runnable, config = self.prepare(cast(RunnableConfig, arg))
-                        argsl = list(args)
-                        argsl[idx] = config
-                        return getattr(runnable, name)(*argsl, **kwargs)
-
-                if self.config:
-                    runnable, config = self.prepare()
-                    return getattr(runnable, name)(*args, **kwargs)
-
-                return attr(*args, **kwargs)
-
-            return wrapper
-
-        else:
-            return attr
-
-
-class RunnableConfigurableFields(DynamicRunnable[Input, Output]):
-    """Runnable that can be dynamically configured.
-
-    A RunnableConfigurableFields should be initiated using the
-    `configurable_fields` method of a Runnable.
-
-    Parameters:
-        fields: The configurable fields to use.
-
-    Here is an example of using a RunnableConfigurableFields with LLMs:
-
-        .. code-block:: python
-
-            from langchain_core.prompts import PromptTemplate
-            from langchain_core.runnables import ConfigurableField
-            from langchain_openai import ChatOpenAI
-
-            model = ChatOpenAI(temperature=0).configurable_fields(
-                temperature=ConfigurableField(
-                    id="temperature",
-                    name="LLM Temperature",
-                    description="The temperature of the LLM",
-                )
-            )
-            # This creates a RunnableConfigurableFields for a chat model.
-
-            # When invoking the created RunnableSequence, you can pass in the
-            # value for your ConfigurableField's id which in this case
-            # will be change in temperature
-
-            prompt = PromptTemplate.from_template("Pick a random number above {x}")
-            chain = prompt | model
-
-            chain.invoke({"x": 0})
-            chain.invoke({"x": 0}, config={"configurable": {"temperature": 0.9}})
-
-
-    Here is an example of using a RunnableConfigurableFields with HubRunnables:
-
-        .. code-block:: python
-
-            from langchain_core.prompts import PromptTemplate
-            from langchain_core.runnables import ConfigurableField
-            from langchain_openai import ChatOpenAI
-            from langchain.runnables.hub import HubRunnable
-
-            prompt = HubRunnable("rlm/rag-prompt").configurable_fields(
-                owner_repo_commit=ConfigurableField(
-                    id="hub_commit",
-                    name="Hub Commit",
-                    description="The Hub commit to pull from",
-                )
-            )
-
-            prompt.invoke({"question": "foo", "context": "bar"})
-
-            # Invoking prompt with `with_config` method
-
-            prompt.invoke(
-                {"question": "foo", "context": "bar"},
-                config={"configurable": {"hub_commit": "rlm/rag-prompt-llama"}},
-            )
-    """
-
-    fields: dict[str, AnyConfigurableField]
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        """Get the configuration specs for the RunnableConfigurableFields.
-
-        Returns:
-            List[ConfigurableFieldSpec]: The configuration specs.
-        """
-        config_specs = []
-
-        for field_name, spec in self.fields.items():
-            if isinstance(spec, ConfigurableField):
-                config_specs.append(
-                    ConfigurableFieldSpec(
-                        id=spec.id,
-                        name=spec.name,
-                        description=spec.description
-                        or self.default.model_fields[field_name].description,
-                        annotation=spec.annotation
-                        or self.default.model_fields[field_name].annotation,
-                        default=getattr(self.default, field_name),
-                        is_shared=spec.is_shared,
-                    )
-                )
-            else:
-                config_specs.append(
-                    make_options_spec(
-                        spec, self.default.model_fields[field_name].description
-                    )
-                )
-
-        config_specs.extend(self.default.config_specs)
-
-        return get_unique_config_specs(config_specs)
-
-    def configurable_fields(
-        self, **kwargs: AnyConfigurableField
-    ) -> RunnableSerializable[Input, Output]:
-        """Get a new RunnableConfigurableFields with the specified
-        configurable fields.
-        """
-        return self.default.configurable_fields(**{**self.fields, **kwargs})
-
-    def _prepare(
-        self, config: Optional[RunnableConfig] = None
-    ) -> tuple[Runnable[Input, Output], RunnableConfig]:
-        config = ensure_config(config)
-        specs_by_id = {spec.id: (key, spec) for key, spec in self.fields.items()}
-        configurable_fields = {
-            specs_by_id[k][0]: v
-            for k, v in config.get("configurable", {}).items()
-            if k in specs_by_id and isinstance(specs_by_id[k][1], ConfigurableField)
-        }
-        configurable_single_options = {
-            k: v.options[(config.get("configurable", {}).get(v.id) or v.default)]
-            for k, v in self.fields.items()
-            if isinstance(v, ConfigurableFieldSingleOption)
-        }
-        configurable_multi_options = {
-            k: [
-                v.options[o]
-                for o in config.get("configurable", {}).get(v.id, v.default)
-            ]
-            for k, v in self.fields.items()
-            if isinstance(v, ConfigurableFieldMultiOption)
-        }
-        configurable = {
-            **configurable_fields,
-            **configurable_single_options,
-            **configurable_multi_options,
-        }
-
-        if configurable:
-            init_params = {
-                k: v
-                for k, v in self.default.__dict__.items()
-                if k in self.default.model_fields
-            }
-            return (
-                self.default.__class__(**{**init_params, **configurable}),
-                config,
-            )
-        else:
-            return (self.default, config)
-
-
-RunnableConfigurableFields.model_rebuild()
-
-
-# Before Python 3.11 native StrEnum is not available
-class StrEnum(str, enum.Enum):
-    """String enum."""
-
-
-_enums_for_spec: WeakValueDictionary[
-    Union[
-        ConfigurableFieldSingleOption, ConfigurableFieldMultiOption, ConfigurableField
-    ],
-    type[StrEnum],
-] = WeakValueDictionary()
-
-_enums_for_spec_lock = threading.Lock()
-
-
-class RunnableConfigurableAlternatives(DynamicRunnable[Input, Output]):
-    """Runnable that can be dynamically configured.
-
-    A RunnableConfigurableAlternatives should be initiated using the
-    `configurable_alternatives` method of a Runnable or can be
-    initiated directly as well.
-
-    Here is an example of using a RunnableConfigurableAlternatives that uses
-    alternative prompts to illustrate its functionality:
-
-        .. code-block:: python
-
-            from langchain_core.runnables import ConfigurableField
-            from langchain_openai import ChatOpenAI
-
-            # This creates a RunnableConfigurableAlternatives for Prompt Runnable
-            # with two alternatives.
-            prompt = PromptTemplate.from_template(
-                "Tell me a joke about {topic}"
-            ).configurable_alternatives(
-                ConfigurableField(id="prompt"),
-                default_key="joke",
-                poem=PromptTemplate.from_template("Write a short poem about {topic}")
-            )
-
-            # When invoking the created RunnableSequence, you can pass in the
-            # value for your ConfigurableField's id which in this case will either be
-            # `joke` or `poem`.
-            chain = prompt | ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
-
-            # The `with_config` method brings in the desired Prompt Runnable in your
-            # Runnable Sequence.
-            chain.with_config(configurable={"prompt": "poem"}).invoke({"topic": "bears"})
-
-
-    Equivalently, you can initialize RunnableConfigurableAlternatives directly
-    and use in LCEL in the same way:
-
-        .. code-block:: python
-
-            from langchain_core.runnables import ConfigurableField
-            from langchain_core.runnables.configurable import RunnableConfigurableAlternatives
-            from langchain_openai import ChatOpenAI
-
-            prompt = RunnableConfigurableAlternatives(
-                which=ConfigurableField(id='prompt'),
-                default=PromptTemplate.from_template("Tell me a joke about {topic}"),
-                default_key='joke',
-                prefix_keys=False,
-                alternatives={"poem":PromptTemplate.from_template("Write a short poem about {topic}")}
-            )
-            chain = prompt | ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
-            chain.with_config(configurable={"prompt": "poem"}).invoke({"topic": "bears"})
-
-    """  # noqa: E501
-
-    which: ConfigurableField
-    """The ConfigurableField to use to choose between alternatives."""
-
-    alternatives: dict[
-        str,
-        Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]],
-    ]
-    """The alternatives to choose from."""
-
-    default_key: str = "default"
-    """The enum value to use for the default option. Defaults to "default"."""
-
-    prefix_keys: bool
-    """Whether to prefix configurable fields of each alternative with a namespace
-    of the form <which.id>==<alternative_key>, eg. a key named "temperature" used by
-    the alternative named "gpt3" becomes "model==gpt3/temperature"."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        with _enums_for_spec_lock:
-            if which_enum := _enums_for_spec.get(self.which):
-                pass
-            else:
-                which_enum = StrEnum(  # type: ignore[call-overload]
-                    self.which.name or self.which.id,
-                    (
-                        (v, v)
-                        for v in list(self.alternatives.keys()) + [self.default_key]
-                    ),
-                )
-                _enums_for_spec[self.which] = cast(type[StrEnum], which_enum)
-        return get_unique_config_specs(
-            # which alternative
-            [
-                ConfigurableFieldSpec(
-                    id=self.which.id,
-                    name=self.which.name,
-                    description=self.which.description,
-                    annotation=which_enum,
-                    default=self.default_key,
-                    is_shared=self.which.is_shared,
-                ),
-            ]
-            # config specs of the default option
-            + (
-                [
-                    prefix_config_spec(s, f"{self.which.id}=={self.default_key}")
-                    for s in self.default.config_specs
-                ]
-                if self.prefix_keys
-                else self.default.config_specs
-            )
-            # config specs of the alternatives
-            + [
-                (
-                    prefix_config_spec(s, f"{self.which.id}=={alt_key}")
-                    if self.prefix_keys
-                    else s
-                )
-                for alt_key, alt in self.alternatives.items()
-                if isinstance(alt, RunnableSerializable)
-                for s in alt.config_specs
-            ]
-        )
-
-    def configurable_fields(
-        self, **kwargs: AnyConfigurableField
-    ) -> RunnableSerializable[Input, Output]:
-        return self.__class__(
-            which=self.which,
-            default=self.default.configurable_fields(**kwargs),
-            alternatives=self.alternatives,
-            default_key=self.default_key,
-            prefix_keys=self.prefix_keys,
-        )
-
-    def _prepare(
-        self, config: Optional[RunnableConfig] = None
-    ) -> tuple[Runnable[Input, Output], RunnableConfig]:
-        config = ensure_config(config)
-        which = config.get("configurable", {}).get(self.which.id, self.default_key)
-        # remap configurable keys for the chosen alternative
-        if self.prefix_keys:
-            config = cast(
-                RunnableConfig,
-                {
-                    **config,
-                    "configurable": {
-                        _strremoveprefix(k, f"{self.which.id}=={which}/"): v
-                        for k, v in config.get("configurable", {}).items()
-                    },
-                },
-            )
-        # return the chosen alternative
-        if which == self.default_key:
-            return (self.default, config)
-        elif which in self.alternatives:
-            alt = self.alternatives[which]
-            if isinstance(alt, Runnable):
-                return (alt, config)
-            else:
-                return (alt(), config)
-        else:
-            msg = f"Unknown alternative: {which}"
-            raise ValueError(msg)
-
-
-def _strremoveprefix(s: str, prefix: str) -> str:
-    """str.removeprefix() is only available in Python 3.9+."""
-    return s.replace(prefix, "", 1) if s.startswith(prefix) else s
-
-
-def prefix_config_spec(
-    spec: ConfigurableFieldSpec, prefix: str
-) -> ConfigurableFieldSpec:
-    """Prefix the id of a ConfigurableFieldSpec.
-
-    This is useful when a RunnableConfigurableAlternatives is used as a
-    ConfigurableField of another RunnableConfigurableAlternatives.
-
-    Args:
-        spec: The ConfigurableFieldSpec to prefix.
-        prefix: The prefix to add.
-
-    Returns:
-        ConfigurableFieldSpec: The prefixed ConfigurableFieldSpec.
-    """
-    return (
-        ConfigurableFieldSpec(
-            id=f"{prefix}/{spec.id}",
-            name=spec.name,
-            description=spec.description,
-            annotation=spec.annotation,
-            default=spec.default,
-            is_shared=spec.is_shared,
-        )
-        if not spec.is_shared
-        else spec
-    )
-
-
-def make_options_spec(
-    spec: Union[ConfigurableFieldSingleOption, ConfigurableFieldMultiOption],
-    description: Optional[str],
-) -> ConfigurableFieldSpec:
-    """Make a ConfigurableFieldSpec for a ConfigurableFieldSingleOption or
-    ConfigurableFieldMultiOption.
-
-    Args:
-        spec: The ConfigurableFieldSingleOption or ConfigurableFieldMultiOption.
-        description: The description to use if the spec does not have one.
-
-    Returns:
-        The ConfigurableFieldSpec.
-    """
-    with _enums_for_spec_lock:
-        if enum := _enums_for_spec.get(spec):
-            pass
-        else:
-            enum = StrEnum(  # type: ignore[call-overload]
-                spec.name or spec.id,
-                ((v, v) for v in list(spec.options.keys())),
-            )
-            _enums_for_spec[spec] = cast(type[StrEnum], enum)
-    if isinstance(spec, ConfigurableFieldSingleOption):
-        return ConfigurableFieldSpec(
-            id=spec.id,
-            name=spec.name,
-            description=spec.description or description,
-            annotation=enum,
-            default=spec.default,
-            is_shared=spec.is_shared,
-        )
-    else:
-        return ConfigurableFieldSpec(
-            id=spec.id,
-            name=spec.name,
-            description=spec.description or description,
-            annotation=Sequence[enum],  # type: ignore[valid-type]
-            default=spec.default,
-            is_shared=spec.is_shared,
-        )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/fallbacks.py ./custom_langchain_core/runnables/fallbacks.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/fallbacks.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/fallbacks.py	1970-01-01 09:00:00
@@ -1,656 +0,0 @@
-import asyncio
-import inspect
-import typing
-from collections.abc import AsyncIterator, Iterator, Sequence
-from contextvars import copy_context
-from functools import wraps
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Optional,
-    Union,
-    cast,
-)
-
-from pydantic import BaseModel, ConfigDict
-from typing_extensions import override
-
-from langchain_core.runnables.base import Runnable, RunnableSerializable
-from langchain_core.runnables.config import (
-    RunnableConfig,
-    _set_config_context,
-    ensure_config,
-    get_async_callback_manager_for_config,
-    get_callback_manager_for_config,
-    get_config_list,
-    patch_config,
-)
-from langchain_core.runnables.utils import (
-    ConfigurableFieldSpec,
-    Input,
-    Output,
-    asyncio_accepts_context,
-    get_unique_config_specs,
-)
-from langchain_core.utils.aiter import py_anext
-
-if TYPE_CHECKING:
-    from langchain_core.callbacks.manager import AsyncCallbackManagerForChainRun
-
-
-class RunnableWithFallbacks(RunnableSerializable[Input, Output]):
-    """Runnable that can fallback to other Runnables if it fails.
-
-    External APIs (e.g., APIs for a language model) may at times experience
-    degraded performance or even downtime.
-
-    In these cases, it can be useful to have a fallback Runnable that can be
-    used in place of the original Runnable (e.g., fallback to another LLM provider).
-
-    Fallbacks can be defined at the level of a single Runnable, or at the level
-    of a chain of Runnables. Fallbacks are tried in order until one succeeds or
-    all fail.
-
-    While you can instantiate a ``RunnableWithFallbacks`` directly, it is usually
-    more convenient to use the ``with_fallbacks`` method on a Runnable.
-
-    Example:
-
-        .. code-block:: python
-
-            from langchain_core.chat_models.openai import ChatOpenAI
-            from langchain_core.chat_models.anthropic import ChatAnthropic
-
-            model = ChatAnthropic(
-                model="claude-3-haiku-20240307"
-            ).with_fallbacks([ChatOpenAI(model="gpt-3.5-turbo-0125")])
-            # Will usually use ChatAnthropic, but fallback to ChatOpenAI
-            # if ChatAnthropic fails.
-            model.invoke('hello')
-
-            # And you can also use fallbacks at the level of a chain.
-            # Here if both LLM providers fail, we'll fallback to a good hardcoded
-            # response.
-
-            from langchain_core.prompts import PromptTemplate
-            from langchain_core.output_parser import StrOutputParser
-            from langchain_core.runnables import RunnableLambda
-
-            def when_all_is_lost(inputs):
-                return ("Looks like our LLM providers are down. "
-                        "Here's a nice 🦜️ emoji for you instead.")
-
-            chain_with_fallback = (
-                PromptTemplate.from_template('Tell me a joke about {topic}')
-                | model
-                | StrOutputParser()
-            ).with_fallbacks([RunnableLambda(when_all_is_lost)])
-    """
-
-    runnable: Runnable[Input, Output]
-    """The Runnable to run first."""
-    fallbacks: Sequence[Runnable[Input, Output]]
-    """A sequence of fallbacks to try."""
-    exceptions_to_handle: tuple[type[BaseException], ...] = (Exception,)
-    """The exceptions on which fallbacks should be tried.
-
-    Any exception that is not a subclass of these exceptions will be raised immediately.
-    """
-    exception_key: Optional[str] = None
-    """If string is specified then handled exceptions will be passed to fallbacks as
-        part of the input under the specified key. If None, exceptions
-        will not be passed to fallbacks. If used, the base Runnable and its fallbacks
-        must accept a dictionary as input."""
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    @property
-    @override
-    def InputType(self) -> type[Input]:
-        return self.runnable.InputType
-
-    @property
-    @override
-    def OutputType(self) -> type[Output]:
-        return self.runnable.OutputType
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        return self.runnable.get_input_schema(config)
-
-    def get_output_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        return self.runnable.get_output_schema(config)
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        return get_unique_config_specs(
-            spec
-            for step in [self.runnable, *self.fallbacks]
-            for spec in step.config_specs
-        )
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    @property
-    def runnables(self) -> Iterator[Runnable[Input, Output]]:
-        yield self.runnable
-        yield from self.fallbacks
-
-    def invoke(
-        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Output:
-        if self.exception_key is not None and not isinstance(input, dict):
-            msg = (
-                "If 'exception_key' is specified then input must be a dictionary."
-                f"However found a type of {type(input)} for input"
-            )
-            raise ValueError(msg)
-        # setup callbacks
-        config = ensure_config(config)
-        callback_manager = get_callback_manager_for_config(config)
-        # start the root run
-        run_manager = callback_manager.on_chain_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-        first_error = None
-        last_error = None
-        for runnable in self.runnables:
-            try:
-                if self.exception_key and last_error is not None:
-                    input[self.exception_key] = last_error
-                child_config = patch_config(config, callbacks=run_manager.get_child())
-                context = copy_context()
-                context.run(_set_config_context, child_config)
-                output = context.run(
-                    runnable.invoke,
-                    input,
-                    config,
-                    **kwargs,
-                )
-            except self.exceptions_to_handle as e:
-                if first_error is None:
-                    first_error = e
-                last_error = e
-            except BaseException as e:
-                run_manager.on_chain_error(e)
-                raise
-            else:
-                run_manager.on_chain_end(output)
-                return output
-        if first_error is None:
-            msg = "No error stored at end of fallbacks."
-            raise ValueError(msg)
-        run_manager.on_chain_error(first_error)
-        raise first_error
-
-    async def ainvoke(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Output:
-        if self.exception_key is not None and not isinstance(input, dict):
-            msg = (
-                "If 'exception_key' is specified then input must be a dictionary."
-                f"However found a type of {type(input)} for input"
-            )
-            raise ValueError(msg)
-        # setup callbacks
-        config = ensure_config(config)
-        callback_manager = get_async_callback_manager_for_config(config)
-        # start the root run
-        run_manager = await callback_manager.on_chain_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-
-        first_error = None
-        last_error = None
-        for runnable in self.runnables:
-            try:
-                if self.exception_key and last_error is not None:
-                    input[self.exception_key] = last_error
-                child_config = patch_config(config, callbacks=run_manager.get_child())
-                context = copy_context()
-                context.run(_set_config_context, child_config)
-                coro = runnable.ainvoke(input, child_config, **kwargs)
-                if asyncio_accepts_context():
-                    output = await asyncio.create_task(coro, context=context)  # type: ignore
-                else:
-                    output = await coro
-            except self.exceptions_to_handle as e:
-                if first_error is None:
-                    first_error = e
-                last_error = e
-            except BaseException as e:
-                await run_manager.on_chain_error(e)
-                raise
-            else:
-                await run_manager.on_chain_end(output)
-                return output
-        if first_error is None:
-            msg = "No error stored at end of fallbacks."
-            raise ValueError(msg)
-        await run_manager.on_chain_error(first_error)
-        raise first_error
-
-    def batch(
-        self,
-        inputs: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        from langchain_core.callbacks.manager import CallbackManager
-
-        if self.exception_key is not None and not all(
-            isinstance(input, dict) for input in inputs
-        ):
-            msg = (
-                "If 'exception_key' is specified then inputs must be dictionaries."
-                f"However found a type of {type(inputs[0])} for input"
-            )
-            raise ValueError(msg)
-
-        if not inputs:
-            return []
-
-        # setup callbacks
-        configs = get_config_list(config, len(inputs))
-        callback_managers = [
-            CallbackManager.configure(
-                inheritable_callbacks=config.get("callbacks"),
-                local_callbacks=None,
-                verbose=False,
-                inheritable_tags=config.get("tags"),
-                local_tags=None,
-                inheritable_metadata=config.get("metadata"),
-                local_metadata=None,
-            )
-            for config in configs
-        ]
-        # start the root runs, one per input
-        run_managers = [
-            cm.on_chain_start(
-                None,
-                input if isinstance(input, dict) else {"input": input},
-                name=config.get("run_name") or self.get_name(),
-                run_id=config.pop("run_id", None),
-            )
-            for cm, input, config in zip(callback_managers, inputs, configs)
-        ]
-
-        to_return: dict[int, Any] = {}
-        run_again = dict(enumerate(inputs))
-        handled_exceptions: dict[int, BaseException] = {}
-        first_to_raise = None
-        for runnable in self.runnables:
-            outputs = runnable.batch(
-                [input for _, input in sorted(run_again.items())],
-                [
-                    # each step a child run of the corresponding root run
-                    patch_config(configs[i], callbacks=run_managers[i].get_child())
-                    for i in sorted(run_again)
-                ],
-                return_exceptions=True,
-                **kwargs,
-            )
-            for (i, input), output in zip(sorted(run_again.copy().items()), outputs):
-                if isinstance(output, BaseException) and not isinstance(
-                    output, self.exceptions_to_handle
-                ):
-                    if not return_exceptions:
-                        first_to_raise = first_to_raise or output
-                    else:
-                        handled_exceptions[i] = cast(BaseException, output)
-                    run_again.pop(i)
-                elif isinstance(output, self.exceptions_to_handle):
-                    if self.exception_key:
-                        input[self.exception_key] = output  # type: ignore
-                    handled_exceptions[i] = cast(BaseException, output)
-                else:
-                    run_managers[i].on_chain_end(output)
-                    to_return[i] = output
-                    run_again.pop(i)
-                    handled_exceptions.pop(i, None)
-            if first_to_raise:
-                raise first_to_raise
-            if not run_again:
-                break
-
-        sorted_handled_exceptions = sorted(handled_exceptions.items())
-        for i, error in sorted_handled_exceptions:
-            run_managers[i].on_chain_error(error)
-        if not return_exceptions and sorted_handled_exceptions:
-            raise sorted_handled_exceptions[0][1]
-        to_return.update(handled_exceptions)
-        return [output for _, output in sorted(to_return.items())]
-
-    async def abatch(
-        self,
-        inputs: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        from langchain_core.callbacks.manager import AsyncCallbackManager
-
-        if self.exception_key is not None and not all(
-            isinstance(input, dict) for input in inputs
-        ):
-            msg = (
-                "If 'exception_key' is specified then inputs must be dictionaries."
-                f"However found a type of {type(inputs[0])} for input"
-            )
-            raise ValueError(msg)
-
-        if not inputs:
-            return []
-
-        # setup callbacks
-        configs = get_config_list(config, len(inputs))
-        callback_managers = [
-            AsyncCallbackManager.configure(
-                inheritable_callbacks=config.get("callbacks"),
-                local_callbacks=None,
-                verbose=False,
-                inheritable_tags=config.get("tags"),
-                local_tags=None,
-                inheritable_metadata=config.get("metadata"),
-                local_metadata=None,
-            )
-            for config in configs
-        ]
-        # start the root runs, one per input
-        run_managers: list[AsyncCallbackManagerForChainRun] = await asyncio.gather(
-            *(
-                cm.on_chain_start(
-                    None,
-                    input,
-                    name=config.get("run_name") or self.get_name(),
-                    run_id=config.pop("run_id", None),
-                )
-                for cm, input, config in zip(callback_managers, inputs, configs)
-            )
-        )
-
-        to_return = {}
-        run_again = dict(enumerate(inputs))
-        handled_exceptions: dict[int, BaseException] = {}
-        first_to_raise = None
-        for runnable in self.runnables:
-            outputs = await runnable.abatch(
-                [input for _, input in sorted(run_again.items())],
-                [
-                    # each step a child run of the corresponding root run
-                    patch_config(configs[i], callbacks=run_managers[i].get_child())
-                    for i in sorted(run_again)
-                ],
-                return_exceptions=True,
-                **kwargs,
-            )
-
-            for (i, input), output in zip(sorted(run_again.copy().items()), outputs):
-                if isinstance(output, BaseException) and not isinstance(
-                    output, self.exceptions_to_handle
-                ):
-                    if not return_exceptions:
-                        first_to_raise = first_to_raise or output
-                    else:
-                        handled_exceptions[i] = cast(BaseException, output)
-                    run_again.pop(i)
-                elif isinstance(output, self.exceptions_to_handle):
-                    if self.exception_key:
-                        input[self.exception_key] = output  # type: ignore
-                    handled_exceptions[i] = cast(BaseException, output)
-                else:
-                    to_return[i] = output
-                    await run_managers[i].on_chain_end(output)
-                    run_again.pop(i)
-                    handled_exceptions.pop(i, None)
-
-            if first_to_raise:
-                raise first_to_raise
-            if not run_again:
-                break
-
-        sorted_handled_exceptions = sorted(handled_exceptions.items())
-        await asyncio.gather(
-            *(
-                run_managers[i].on_chain_error(error)
-                for i, error in sorted_handled_exceptions
-            )
-        )
-        if not return_exceptions and sorted_handled_exceptions:
-            raise sorted_handled_exceptions[0][1]
-        to_return.update(handled_exceptions)
-        return [output for _, output in sorted(to_return.items())]  # type: ignore
-
-    def stream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        """"""
-        if self.exception_key is not None and not isinstance(input, dict):
-            msg = (
-                "If 'exception_key' is specified then input must be a dictionary."
-                f"However found a type of {type(input)} for input"
-            )
-            raise ValueError(msg)
-        # setup callbacks
-        config = ensure_config(config)
-        callback_manager = get_callback_manager_for_config(config)
-        # start the root run
-        run_manager = callback_manager.on_chain_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-        first_error = None
-        last_error = None
-        for runnable in self.runnables:
-            try:
-                if self.exception_key and last_error is not None:
-                    input[self.exception_key] = last_error
-                child_config = patch_config(config, callbacks=run_manager.get_child())
-                context = copy_context()
-                context.run(_set_config_context, child_config)
-                stream = context.run(
-                    runnable.stream,
-                    input,
-                    **kwargs,
-                )
-                chunk: Output = context.run(next, stream)  # type: ignore
-            except self.exceptions_to_handle as e:
-                first_error = e if first_error is None else first_error
-                last_error = e
-            except BaseException as e:
-                run_manager.on_chain_error(e)
-                raise
-            else:
-                first_error = None
-                break
-        if first_error:
-            run_manager.on_chain_error(first_error)
-            raise first_error
-
-        yield chunk
-        output: Optional[Output] = chunk
-        try:
-            for chunk in stream:
-                yield chunk
-                try:
-                    output = output + chunk  # type: ignore
-                except TypeError:
-                    output = None
-        except BaseException as e:
-            run_manager.on_chain_error(e)
-            raise
-        run_manager.on_chain_end(output)
-
-    async def astream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        if self.exception_key is not None and not isinstance(input, dict):
-            msg = (
-                "If 'exception_key' is specified then input must be a dictionary."
-                f"However found a type of {type(input)} for input"
-            )
-            raise ValueError(msg)
-        # setup callbacks
-        config = ensure_config(config)
-        callback_manager = get_async_callback_manager_for_config(config)
-        # start the root run
-        run_manager = await callback_manager.on_chain_start(
-            None,
-            input,
-            name=config.get("run_name") or self.get_name(),
-            run_id=config.pop("run_id", None),
-        )
-        first_error = None
-        last_error = None
-        for runnable in self.runnables:
-            try:
-                if self.exception_key and last_error is not None:
-                    input[self.exception_key] = last_error
-                child_config = patch_config(config, callbacks=run_manager.get_child())
-                context = copy_context()
-                context.run(_set_config_context, child_config)
-                stream = runnable.astream(
-                    input,
-                    child_config,
-                    **kwargs,
-                )
-                if asyncio_accepts_context():
-                    chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]
-                        py_anext(stream),  # type: ignore[arg-type]
-                        context=context,
-                    )
-                else:
-                    chunk = cast(Output, await py_anext(stream))
-            except self.exceptions_to_handle as e:
-                first_error = e if first_error is None else first_error
-                last_error = e
-            except BaseException as e:
-                await run_manager.on_chain_error(e)
-                raise
-            else:
-                first_error = None
-                break
-        if first_error:
-            await run_manager.on_chain_error(first_error)
-            raise first_error
-
-        yield chunk
-        output: Optional[Output] = chunk
-        try:
-            async for chunk in stream:
-                yield chunk
-                try:
-                    output = output + chunk  # type: ignore
-                except TypeError:
-                    output = None
-        except BaseException as e:
-            await run_manager.on_chain_error(e)
-            raise
-        await run_manager.on_chain_end(output)
-
-    def __getattr__(self, name: str) -> Any:
-        """Get an attribute from the wrapped Runnable and its fallbacks.
-
-        Returns:
-            If the attribute is anything other than a method that outputs a Runnable,
-            returns getattr(self.runnable, name). If the attribute is a method that
-            does return a new Runnable (e.g. llm.bind_tools([...]) outputs a new
-            RunnableBinding) then self.runnable and each of the runnables in
-            self.fallbacks is replaced with getattr(x, name).
-
-        Example:
-            .. code-block:: python
-
-                from langchain_openai import ChatOpenAI
-                from langchain_anthropic import ChatAnthropic
-
-                gpt_4o = ChatOpenAI(model="gpt-4o")
-                claude_3_sonnet = ChatAnthropic(model="claude-3-sonnet-20240229")
-                llm = gpt_4o.with_fallbacks([claude_3_sonnet])
-
-                llm.model_name
-                # -> "gpt-4o"
-
-                # .bind_tools() is called on both ChatOpenAI and ChatAnthropic
-                # Equivalent to:
-                # gpt_4o.bind_tools([...]).with_fallbacks([claude_3_sonnet.bind_tools([...])])
-                llm.bind_tools([...])
-                # -> RunnableWithFallbacks(
-                    runnable=RunnableBinding(bound=ChatOpenAI(...), kwargs={"tools": [...]}),
-                    fallbacks=[RunnableBinding(bound=ChatAnthropic(...), kwargs={"tools": [...]})],
-                )
-
-        """  # noqa: E501
-        attr = getattr(self.runnable, name)
-        if _returns_runnable(attr):
-
-            @wraps(attr)
-            def wrapped(*args: Any, **kwargs: Any) -> Any:
-                new_runnable = attr(*args, **kwargs)
-                new_fallbacks = []
-                for fallback in self.fallbacks:
-                    fallback_attr = getattr(fallback, name)
-                    new_fallbacks.append(fallback_attr(*args, **kwargs))
-
-                return self.__class__(
-                    **{
-                        **self.model_dump(),
-                        "runnable": new_runnable,
-                        "fallbacks": new_fallbacks,
-                    }
-                )
-
-            return wrapped
-
-        return attr
-
-
-def _returns_runnable(attr: Any) -> bool:
-    if not callable(attr):
-        return False
-    return_type = typing.get_type_hints(attr).get("return")
-    return bool(return_type and _is_runnable_type(return_type))
-
-
-def _is_runnable_type(type_: Any) -> bool:
-    if inspect.isclass(type_):
-        return issubclass(type_, Runnable)
-    origin = getattr(type_, "__origin__", None)
-    if inspect.isclass(origin):
-        return issubclass(origin, Runnable)
-    elif origin is typing.Union:
-        return all(_is_runnable_type(t) for t in type_.__args__)
-    else:
-        return False
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/graph.py ./custom_langchain_core/runnables/graph.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/graph.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/graph.py	1970-01-01 09:00:00
@@ -1,664 +0,0 @@
-from __future__ import annotations
-
-import inspect
-from collections import defaultdict
-from collections.abc import Sequence
-from dataclasses import dataclass, field
-from enum import Enum
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    NamedTuple,
-    Optional,
-    Protocol,
-    TypedDict,
-    Union,
-    overload,
-)
-from uuid import UUID, uuid4
-
-from pydantic import BaseModel
-
-from langchain_core.utils.pydantic import _IgnoreUnserializable, is_basemodel_subclass
-
-if TYPE_CHECKING:
-    from langchain_core.runnables.base import Runnable as RunnableType
-
-
-class Stringifiable(Protocol):
-    def __str__(self) -> str: ...
-
-
-class LabelsDict(TypedDict):
-    """Dictionary of labels for nodes and edges in a graph."""
-
-    nodes: dict[str, str]
-    """Labels for nodes."""
-    edges: dict[str, str]
-    """Labels for edges."""
-
-
-def is_uuid(value: str) -> bool:
-    """Check if a string is a valid UUID.
-
-    Args:
-        value: The string to check.
-
-    Returns:
-        True if the string is a valid UUID, False otherwise.
-    """
-    try:
-        UUID(value)
-    except ValueError:
-        return False
-    return True
-
-
-class Edge(NamedTuple):
-    """Edge in a graph.
-
-    Parameters:
-        source: The source node id.
-        target: The target node id.
-        data: Optional data associated with the edge. Defaults to None.
-        conditional: Whether the edge is conditional. Defaults to False.
-    """
-
-    source: str
-    target: str
-    data: Optional[Stringifiable] = None
-    conditional: bool = False
-
-    def copy(
-        self, *, source: Optional[str] = None, target: Optional[str] = None
-    ) -> Edge:
-        """Return a copy of the edge with optional new source and target nodes.
-
-        Args:
-            source: The new source node id. Defaults to None.
-            target: The new target node id. Defaults to None.
-
-        Returns:
-            A copy of the edge with the new source and target nodes.
-        """
-        return Edge(
-            source=source or self.source,
-            target=target or self.target,
-            data=self.data,
-            conditional=self.conditional,
-        )
-
-
-class Node(NamedTuple):
-    """Node in a graph.
-
-    Parameters:
-        id: The unique identifier of the node.
-        name: The name of the node.
-        data: The data of the node.
-        metadata: Optional metadata for the node. Defaults to None.
-    """
-
-    id: str
-    name: str
-    data: Union[type[BaseModel], RunnableType]
-    metadata: Optional[dict[str, Any]]
-
-    def copy(self, *, id: Optional[str] = None, name: Optional[str] = None) -> Node:
-        """Return a copy of the node with optional new id and name.
-
-        Args:
-            id: The new node id. Defaults to None.
-            name: The new node name. Defaults to None.
-
-        Returns:
-            A copy of the node with the new id and name.
-        """
-        return Node(
-            id=id or self.id,
-            name=name or self.name,
-            data=self.data,
-            metadata=self.metadata,
-        )
-
-
-class Branch(NamedTuple):
-    """Branch in a graph.
-
-    Parameters:
-        condition: A callable that returns a string representation of the condition.
-        ends: Optional dictionary of end node ids for the branches. Defaults
-            to None.
-    """
-
-    condition: Callable[..., str]
-    ends: Optional[dict[str, str]]
-
-
-class CurveStyle(Enum):
-    """Enum for different curve styles supported by Mermaid."""
-
-    BASIS = "basis"
-    BUMP_X = "bumpX"
-    BUMP_Y = "bumpY"
-    CARDINAL = "cardinal"
-    CATMULL_ROM = "catmullRom"
-    LINEAR = "linear"
-    MONOTONE_X = "monotoneX"
-    MONOTONE_Y = "monotoneY"
-    NATURAL = "natural"
-    STEP = "step"
-    STEP_AFTER = "stepAfter"
-    STEP_BEFORE = "stepBefore"
-
-
-@dataclass
-class NodeStyles:
-    """Schema for Hexadecimal color codes for different node types.
-
-    Parameters:
-        default: The default color code. Defaults to "fill:#f2f0ff,line-height:1.2".
-        first: The color code for the first node. Defaults to "fill-opacity:0".
-        last: The color code for the last node. Defaults to "fill:#bfb6fc".
-    """
-
-    default: str = "fill:#f2f0ff,line-height:1.2"
-    first: str = "fill-opacity:0"
-    last: str = "fill:#bfb6fc"
-
-
-class MermaidDrawMethod(Enum):
-    """Enum for different draw methods supported by Mermaid."""
-
-    PYPPETEER = "pyppeteer"  # Uses Pyppeteer to render the graph
-    API = "api"  # Uses Mermaid.INK API to render the graph
-
-
-def node_data_str(id: str, data: Union[type[BaseModel], RunnableType]) -> str:
-    """Convert the data of a node to a string.
-
-    Args:
-        id: The node id.
-        data: The node data.
-
-    Returns:
-        A string representation of the data.
-    """
-    from langchain_core.runnables.base import Runnable
-
-    if not is_uuid(id):
-        return id
-    elif isinstance(data, Runnable):
-        data_str = data.get_name()
-    else:
-        data_str = data.__name__
-    return data_str if not data_str.startswith("Runnable") else data_str[8:]
-
-
-def node_data_json(
-    node: Node, *, with_schemas: bool = False
-) -> dict[str, Union[str, dict[str, Any]]]:
-    """Convert the data of a node to a JSON-serializable format.
-
-    Args:
-        node: The node to convert.
-        with_schemas: Whether to include the schema of the data if
-            it is a Pydantic model. Defaults to False.
-
-    Returns:
-        A dictionary with the type of the data and the data itself.
-    """
-    from langchain_core.load.serializable import to_json_not_implemented
-    from langchain_core.runnables.base import Runnable, RunnableSerializable
-
-    if isinstance(node.data, RunnableSerializable):
-        json: dict[str, Any] = {
-            "type": "runnable",
-            "data": {
-                "id": node.data.lc_id(),
-                "name": node_data_str(node.id, node.data),
-            },
-        }
-    elif isinstance(node.data, Runnable):
-        json = {
-            "type": "runnable",
-            "data": {
-                "id": to_json_not_implemented(node.data)["id"],
-                "name": node_data_str(node.id, node.data),
-            },
-        }
-    elif inspect.isclass(node.data) and is_basemodel_subclass(node.data):
-        json = (
-            {
-                "type": "schema",
-                "data": node.data.model_json_schema(
-                    schema_generator=_IgnoreUnserializable
-                ),
-            }
-            if with_schemas
-            else {
-                "type": "schema",
-                "data": node_data_str(node.id, node.data),
-            }
-        )
-    else:
-        json = {
-            "type": "unknown",
-            "data": node_data_str(node.id, node.data),
-        }
-    if node.metadata is not None:
-        json["metadata"] = node.metadata
-    return json
-
-
-@dataclass
-class Graph:
-    """Graph of nodes and edges.
-
-    Parameters:
-        nodes: Dictionary of nodes in the graph. Defaults to an empty dictionary.
-        edges: List of edges in the graph. Defaults to an empty list.
-    """
-
-    nodes: dict[str, Node] = field(default_factory=dict)
-    edges: list[Edge] = field(default_factory=list)
-
-    def to_json(self, *, with_schemas: bool = False) -> dict[str, list[dict[str, Any]]]:
-        """Convert the graph to a JSON-serializable format.
-
-        Args:
-            with_schemas: Whether to include the schemas of the nodes if they are
-                Pydantic models. Defaults to False.
-
-        Returns:
-            A dictionary with the nodes and edges of the graph.
-        """
-        stable_node_ids = {
-            node.id: i if is_uuid(node.id) else node.id
-            for i, node in enumerate(self.nodes.values())
-        }
-        edges: list[dict[str, Any]] = []
-        for edge in self.edges:
-            edge_dict = {
-                "source": stable_node_ids[edge.source],
-                "target": stable_node_ids[edge.target],
-            }
-            if edge.data is not None:
-                edge_dict["data"] = edge.data
-            if edge.conditional:
-                edge_dict["conditional"] = True
-            edges.append(edge_dict)
-
-        return {
-            "nodes": [
-                {
-                    "id": stable_node_ids[node.id],
-                    **node_data_json(node, with_schemas=with_schemas),
-                }
-                for node in self.nodes.values()
-            ],
-            "edges": edges,
-        }
-
-    def __bool__(self) -> bool:
-        return bool(self.nodes)
-
-    def next_id(self) -> str:
-        """Return a new unique node
-        identifier that can be used to add a node to the graph.
-        """
-        return uuid4().hex
-
-    def add_node(
-        self,
-        data: Union[type[BaseModel], RunnableType],
-        id: Optional[str] = None,
-        *,
-        metadata: Optional[dict[str, Any]] = None,
-    ) -> Node:
-        """Add a node to the graph and return it.
-
-        Args:
-            data: The data of the node.
-            id: The id of the node. Defaults to None.
-            metadata: Optional metadata for the node. Defaults to None.
-
-        Returns:
-            The node that was added to the graph.
-
-        Raises:
-            ValueError: If a node with the same id already exists.
-        """
-        if id is not None and id in self.nodes:
-            msg = f"Node with id {id} already exists"
-            raise ValueError(msg)
-        id = id or self.next_id()
-        node = Node(id=id, data=data, metadata=metadata, name=node_data_str(id, data))
-        self.nodes[node.id] = node
-        return node
-
-    def remove_node(self, node: Node) -> None:
-        """Remove a node from the graph and all edges connected to it.
-
-        Args:
-            node: The node to remove.
-        """
-        self.nodes.pop(node.id)
-        self.edges = [
-            edge
-            for edge in self.edges
-            if edge.source != node.id and edge.target != node.id
-        ]
-
-    def add_edge(
-        self,
-        source: Node,
-        target: Node,
-        data: Optional[Stringifiable] = None,
-        conditional: bool = False,
-    ) -> Edge:
-        """Add an edge to the graph and return it.
-
-        Args:
-            source: The source node of the edge.
-            target: The target node of the edge.
-            data: Optional data associated with the edge. Defaults to None.
-            conditional: Whether the edge is conditional. Defaults to False.
-
-        Returns:
-            The edge that was added to the graph.
-
-        Raises:
-            ValueError: If the source or target node is not in the graph.
-        """
-        if source.id not in self.nodes:
-            msg = f"Source node {source.id} not in graph"
-            raise ValueError(msg)
-        if target.id not in self.nodes:
-            msg = f"Target node {target.id} not in graph"
-            raise ValueError(msg)
-        edge = Edge(
-            source=source.id, target=target.id, data=data, conditional=conditional
-        )
-        self.edges.append(edge)
-        return edge
-
-    def extend(
-        self, graph: Graph, *, prefix: str = ""
-    ) -> tuple[Optional[Node], Optional[Node]]:
-        """Add all nodes and edges from another graph.
-        Note this doesn't check for duplicates, nor does it connect the graphs.
-
-        Args:
-            graph: The graph to add.
-            prefix: The prefix to add to the node ids. Defaults to "".
-
-        Returns:
-            A tuple of the first and last nodes of the subgraph.
-        """
-        if all(is_uuid(node.id) for node in graph.nodes.values()):
-            prefix = ""
-
-        def prefixed(id: str) -> str:
-            return f"{prefix}:{id}" if prefix else id
-
-        # prefix each node
-        self.nodes.update(
-            {prefixed(k): v.copy(id=prefixed(k)) for k, v in graph.nodes.items()}
-        )
-        # prefix each edge's source and target
-        self.edges.extend(
-            [
-                edge.copy(source=prefixed(edge.source), target=prefixed(edge.target))
-                for edge in graph.edges
-            ]
-        )
-        # return (prefixed) first and last nodes of the subgraph
-        first, last = graph.first_node(), graph.last_node()
-        return (
-            first.copy(id=prefixed(first.id)) if first else None,
-            last.copy(id=prefixed(last.id)) if last else None,
-        )
-
-    def reid(self) -> Graph:
-        """Return a new graph with all nodes re-identified,
-        using their unique, readable names where possible.
-        """
-        node_name_to_ids = defaultdict(list)
-        for node in self.nodes.values():
-            node_name_to_ids[node.name].append(node.id)
-
-        unique_labels = {
-            node_id: node_name if len(node_ids) == 1 else f"{node_name}_{i + 1}"
-            for node_name, node_ids in node_name_to_ids.items()
-            for i, node_id in enumerate(node_ids)
-        }
-
-        def _get_node_id(node_id: str) -> str:
-            label = unique_labels[node_id]
-            if is_uuid(node_id):
-                return label
-            else:
-                return node_id
-
-        return Graph(
-            nodes={
-                _get_node_id(id): node.copy(id=_get_node_id(id))
-                for id, node in self.nodes.items()
-            },
-            edges=[
-                edge.copy(
-                    source=_get_node_id(edge.source),
-                    target=_get_node_id(edge.target),
-                )
-                for edge in self.edges
-            ],
-        )
-
-    def first_node(self) -> Optional[Node]:
-        """Find the single node that is not a target of any edge.
-        If there is no such node, or there are multiple, return None.
-        When drawing the graph, this node would be the origin.
-        """
-        return _first_node(self)
-
-    def last_node(self) -> Optional[Node]:
-        """Find the single node that is not a source of any edge.
-        If there is no such node, or there are multiple, return None.
-        When drawing the graph, this node would be the destination.
-        """
-        return _last_node(self)
-
-    def trim_first_node(self) -> None:
-        """Remove the first node if it exists and has a single outgoing edge,
-        i.e., if removing it would not leave the graph without a "first" node.
-        """
-        first_node = self.first_node()
-        if (
-            first_node
-            and _first_node(self, exclude=[first_node.id])
-            and len({e for e in self.edges if e.source == first_node.id}) == 1
-        ):
-            self.remove_node(first_node)
-
-    def trim_last_node(self) -> None:
-        """Remove the last node if it exists and has a single incoming edge,
-        i.e., if removing it would not leave the graph without a "last" node.
-        """
-        last_node = self.last_node()
-        if (
-            last_node
-            and _last_node(self, exclude=[last_node.id])
-            and len({e for e in self.edges if e.target == last_node.id}) == 1
-        ):
-            self.remove_node(last_node)
-
-    def draw_ascii(self) -> str:
-        """Draw the graph as an ASCII art string."""
-        from langchain_core.runnables.graph_ascii import draw_ascii
-
-        return draw_ascii(
-            {node.id: node.name for node in self.nodes.values()},
-            self.edges,
-        )
-
-    def print_ascii(self) -> None:
-        """Print the graph as an ASCII art string."""
-        print(self.draw_ascii())  # noqa: T201
-
-    @overload
-    def draw_png(
-        self,
-        output_file_path: str,
-        fontname: Optional[str] = None,
-        labels: Optional[LabelsDict] = None,
-    ) -> None: ...
-
-    @overload
-    def draw_png(
-        self,
-        output_file_path: None,
-        fontname: Optional[str] = None,
-        labels: Optional[LabelsDict] = None,
-    ) -> bytes: ...
-
-    def draw_png(
-        self,
-        output_file_path: Optional[str] = None,
-        fontname: Optional[str] = None,
-        labels: Optional[LabelsDict] = None,
-    ) -> Union[bytes, None]:
-        """Draw the graph as a PNG image.
-
-        Args:
-            output_file_path: The path to save the image to. If None, the image
-                is not saved. Defaults to None.
-            fontname: The name of the font to use. Defaults to None.
-            labels: Optional labels for nodes and edges in the graph. Defaults to None.
-
-        Returns:
-            The PNG image as bytes if output_file_path is None, None otherwise.
-        """
-        from langchain_core.runnables.graph_png import PngDrawer
-
-        default_node_labels = {node.id: node.name for node in self.nodes.values()}
-
-        return PngDrawer(
-            fontname,
-            LabelsDict(
-                nodes={
-                    **default_node_labels,
-                    **(labels["nodes"] if labels is not None else {}),
-                },
-                edges=labels["edges"] if labels is not None else {},
-            ),
-        ).draw(self, output_file_path)
-
-    def draw_mermaid(
-        self,
-        *,
-        with_styles: bool = True,
-        curve_style: CurveStyle = CurveStyle.LINEAR,
-        node_colors: Optional[NodeStyles] = None,
-        wrap_label_n_words: int = 9,
-    ) -> str:
-        """Draw the graph as a Mermaid syntax string.
-
-        Args:
-            with_styles: Whether to include styles in the syntax. Defaults to True.
-            curve_style: The style of the edges. Defaults to CurveStyle.LINEAR.
-            node_colors: The colors of the nodes. Defaults to NodeStyles().
-            wrap_label_n_words: The number of words to wrap the node labels at.
-                Defaults to 9.
-
-        Returns:
-            The Mermaid syntax string.
-        """
-        from langchain_core.runnables.graph_mermaid import draw_mermaid
-
-        graph = self.reid()
-        first_node = graph.first_node()
-        last_node = graph.last_node()
-
-        return draw_mermaid(
-            nodes=graph.nodes,
-            edges=graph.edges,
-            first_node=first_node.id if first_node else None,
-            last_node=last_node.id if last_node else None,
-            with_styles=with_styles,
-            curve_style=curve_style,
-            node_styles=node_colors,
-            wrap_label_n_words=wrap_label_n_words,
-        )
-
-    def draw_mermaid_png(
-        self,
-        *,
-        curve_style: CurveStyle = CurveStyle.LINEAR,
-        node_colors: Optional[NodeStyles] = None,
-        wrap_label_n_words: int = 9,
-        output_file_path: Optional[str] = None,
-        draw_method: MermaidDrawMethod = MermaidDrawMethod.API,
-        background_color: str = "white",
-        padding: int = 10,
-    ) -> bytes:
-        """Draw the graph as a PNG image using Mermaid.
-
-        Args:
-            curve_style: The style of the edges. Defaults to CurveStyle.LINEAR.
-            node_colors: The colors of the nodes. Defaults to NodeStyles().
-            wrap_label_n_words: The number of words to wrap the node labels at.
-                Defaults to 9.
-            output_file_path: The path to save the image to. If None, the image
-                is not saved. Defaults to None.
-            draw_method: The method to use to draw the graph.
-                Defaults to MermaidDrawMethod.API.
-            background_color: The color of the background. Defaults to "white".
-            padding: The padding around the graph. Defaults to 10.
-
-        Returns:
-            The PNG image as bytes.
-        """
-        from langchain_core.runnables.graph_mermaid import draw_mermaid_png
-
-        mermaid_syntax = self.draw_mermaid(
-            curve_style=curve_style,
-            node_colors=node_colors,
-            wrap_label_n_words=wrap_label_n_words,
-        )
-        return draw_mermaid_png(
-            mermaid_syntax=mermaid_syntax,
-            output_file_path=output_file_path,
-            draw_method=draw_method,
-            background_color=background_color,
-            padding=padding,
-        )
-
-
-def _first_node(graph: Graph, exclude: Sequence[str] = ()) -> Optional[Node]:
-    """Find the single node that is not a target of any edge.
-    Exclude nodes/sources with ids in the exclude list.
-    If there is no such node, or there are multiple, return None.
-    When drawing the graph, this node would be the origin.
-    """
-    targets = {edge.target for edge in graph.edges if edge.source not in exclude}
-    found: list[Node] = []
-    for node in graph.nodes.values():
-        if node.id not in exclude and node.id not in targets:
-            found.append(node)
-    return found[0] if len(found) == 1 else None
-
-
-def _last_node(graph: Graph, exclude: Sequence[str] = ()) -> Optional[Node]:
-    """Find the single node that is not a source of any edge.
-    Exclude nodes/targets with ids in the exclude list.
-    If there is no such node, or there are multiple, return None.
-    When drawing the graph, this node would be the destination.
-    """
-    sources = {edge.source for edge in graph.edges if edge.target not in exclude}
-    found: list[Node] = []
-    for node in graph.nodes.values():
-        if node.id not in exclude and node.id not in sources:
-            found.append(node)
-    return found[0] if len(found) == 1 else None
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/graph_ascii.py ./custom_langchain_core/runnables/graph_ascii.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/graph_ascii.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/graph_ascii.py	1970-01-01 09:00:00
@@ -1,328 +0,0 @@
-"""Draws DAG in ASCII.
-Adapted from https://github.com/iterative/dvc/blob/main/dvc/dagascii.py.
-"""
-
-import math
-import os
-from collections.abc import Mapping, Sequence
-from typing import Any
-
-from langchain_core.runnables.graph import Edge as LangEdge
-
-
-class VertexViewer:
-    """Class to define vertex box boundaries that will be accounted for during
-    graph building by grandalf.
-
-    Args:
-        name (str): name of the vertex.
-    """
-
-    HEIGHT = 3  # top and bottom box edges + text
-    """Height of the box."""
-
-    def __init__(self, name: str) -> None:
-        self._h = self.HEIGHT  # top and bottom box edges + text
-        self._w = len(name) + 2  # right and left bottom edges + text
-
-    @property
-    def h(self) -> int:
-        """Height of the box."""
-        return self._h
-
-    @property
-    def w(self) -> int:
-        """Width of the box."""
-        return self._w
-
-
-class AsciiCanvas:
-    """Class for drawing in ASCII.
-
-    Args:
-        cols (int): number of columns in the canvas. Should be > 1.
-        lines (int): number of lines in the canvas. Should be > 1.
-    """
-
-    TIMEOUT = 10
-
-    def __init__(self, cols: int, lines: int) -> None:
-        if cols <= 1 or lines <= 1:
-            msg = "Canvas dimensions should be > 1"
-            raise ValueError(msg)
-
-        self.cols = cols
-        self.lines = lines
-
-        self.canvas = [[" "] * cols for line in range(lines)]
-
-    def draw(self) -> str:
-        """Draws ASCII canvas on the screen."""
-        lines = map("".join, self.canvas)
-        return os.linesep.join(lines)
-
-    def point(self, x: int, y: int, char: str) -> None:
-        """Create a point on ASCII canvas.
-
-        Args:
-            x (int): x coordinate. Should be >= 0 and < number of columns in
-                the canvas.
-            y (int): y coordinate. Should be >= 0 an < number of lines in the
-                canvas.
-            char (str): character to place in the specified point on the
-                canvas.
-        """
-        if len(char) != 1:
-            msg = "char should be a single character"
-            raise ValueError(msg)
-        if x >= self.cols or x < 0:
-            msg = "x should be >= 0 and < number of columns"
-            raise ValueError(msg)
-        if y >= self.lines or y < 0:
-            msg = "y should be >= 0 and < number of lines"
-            raise ValueError(msg)
-
-        self.canvas[y][x] = char
-
-    def line(self, x0: int, y0: int, x1: int, y1: int, char: str) -> None:
-        """Create a line on ASCII canvas.
-
-        Args:
-            x0 (int): x coordinate where the line should start.
-            y0 (int): y coordinate where the line should start.
-            x1 (int): x coordinate where the line should end.
-            y1 (int): y coordinate where the line should end.
-            char (str): character to draw the line with.
-        """
-        if x0 > x1:
-            x1, x0 = x0, x1
-            y1, y0 = y0, y1
-
-        dx = x1 - x0
-        dy = y1 - y0
-
-        if dx == 0 and dy == 0:
-            self.point(x0, y0, char)
-        elif abs(dx) >= abs(dy):
-            for x in range(x0, x1 + 1):
-                y = y0 if dx == 0 else y0 + int(round((x - x0) * dy / float(dx)))
-                self.point(x, y, char)
-        elif y0 < y1:
-            for y in range(y0, y1 + 1):
-                x = x0 if dy == 0 else x0 + int(round((y - y0) * dx / float(dy)))
-                self.point(x, y, char)
-        else:
-            for y in range(y1, y0 + 1):
-                x = x0 if dy == 0 else x1 + int(round((y - y1) * dx / float(dy)))
-                self.point(x, y, char)
-
-    def text(self, x: int, y: int, text: str) -> None:
-        """Print a text on ASCII canvas.
-
-        Args:
-            x (int): x coordinate where the text should start.
-            y (int): y coordinate where the text should start.
-            text (str): string that should be printed.
-        """
-        for i, char in enumerate(text):
-            self.point(x + i, y, char)
-
-    def box(self, x0: int, y0: int, width: int, height: int) -> None:
-        """Create a box on ASCII canvas.
-
-        Args:
-            x0 (int): x coordinate of the box corner.
-            y0 (int): y coordinate of the box corner.
-            width (int): box width.
-            height (int): box height.
-        """
-        if width <= 1 or height <= 1:
-            msg = "Box dimensions should be > 1"
-            raise ValueError(msg)
-
-        width -= 1
-        height -= 1
-
-        for x in range(x0, x0 + width):
-            self.point(x, y0, "-")
-            self.point(x, y0 + height, "-")
-
-        for y in range(y0, y0 + height):
-            self.point(x0, y, "|")
-            self.point(x0 + width, y, "|")
-
-        self.point(x0, y0, "+")
-        self.point(x0 + width, y0, "+")
-        self.point(x0, y0 + height, "+")
-        self.point(x0 + width, y0 + height, "+")
-
-
-def _build_sugiyama_layout(
-    vertices: Mapping[str, str], edges: Sequence[LangEdge]
-) -> Any:
-    try:
-        from grandalf.graphs import Edge, Graph, Vertex  # type: ignore[import]
-        from grandalf.layouts import SugiyamaLayout  # type: ignore[import]
-        from grandalf.routing import (  # type: ignore[import]
-            EdgeViewer,
-            route_with_lines,
-        )
-    except ImportError as exc:
-        msg = "Install grandalf to draw graphs: `pip install grandalf`."
-        raise ImportError(msg) from exc
-
-    #
-    # Just a reminder about naming conventions:
-    # +------------X
-    # |
-    # |
-    # |
-    # |
-    # Y
-    #
-
-    vertices_ = {id: Vertex(f" {data} ") for id, data in vertices.items()}
-    edges_ = [Edge(vertices_[s], vertices_[e], data=cond) for s, e, _, cond in edges]
-    vertices_list = vertices_.values()
-    graph = Graph(vertices_list, edges_)
-
-    for vertex in vertices_list:
-        vertex.view = VertexViewer(vertex.data)
-
-    # NOTE: determine min box length to create the best layout
-    minw = min(v.view.w for v in vertices_list)
-
-    for edge in edges_:
-        edge.view = EdgeViewer()
-
-    sug = SugiyamaLayout(graph.C[0])
-    graph = graph.C[0]
-    roots = list(filter(lambda x: len(x.e_in()) == 0, graph.sV))
-
-    sug.init_all(roots=roots, optimize=True)
-
-    sug.yspace = VertexViewer.HEIGHT
-    sug.xspace = minw
-    sug.route_edge = route_with_lines
-
-    sug.draw()
-
-    return sug
-
-
-def draw_ascii(vertices: Mapping[str, str], edges: Sequence[LangEdge]) -> str:
-    """Build a DAG and draw it in ASCII.
-
-    Args:
-        vertices (list): list of graph vertices.
-        edges (list): list of graph edges.
-
-    Returns:
-        str: ASCII representation
-
-    Example:
-
-        .. code-block:: python
-
-            from langchain_core.runnables.graph_ascii import draw_ascii
-
-            vertices = {1: "1", 2: "2", 3: "3", 4: "4"}
-            edges = [
-                (source, target, None, None)
-                for source, target in [(1, 2), (2, 3), (2, 4), (1, 4)]
-            ]
-
-
-            print(draw_ascii(vertices, edges))
-
-        .. code-block:: none
-
-                 +---+
-                 | 1 |
-                 +---+
-                 *    *
-                *     *
-               *       *
-            +---+       *
-            | 2 |       *
-            +---+**     *
-              *    **   *
-              *      ** *
-              *        **
-            +---+     +---+
-            | 3 |     | 4 |
-            +---+     +---+
-    """
-    # NOTE: coordinates might me negative, so we need to shift
-    # everything to the positive plane before we actually draw it.
-    xlist: list[float] = []
-    ylist: list[float] = []
-
-    sug = _build_sugiyama_layout(vertices, edges)
-
-    for vertex in sug.g.sV:
-        # NOTE: moving boxes w/2 to the left
-        xlist.extend(
-            (
-                vertex.view.xy[0] - vertex.view.w / 2.0,
-                vertex.view.xy[0] + vertex.view.w / 2.0,
-            )
-        )
-        ylist.extend((vertex.view.xy[1], vertex.view.xy[1] + vertex.view.h))
-
-    for edge in sug.g.sE:
-        for x, y in edge.view._pts:
-            xlist.append(x)
-            ylist.append(y)
-
-    minx = min(xlist)
-    miny = min(ylist)
-    maxx = max(xlist)
-    maxy = max(ylist)
-
-    canvas_cols = int(math.ceil(math.ceil(maxx) - math.floor(minx))) + 1
-    canvas_lines = int(round(maxy - miny))
-
-    canvas = AsciiCanvas(canvas_cols, canvas_lines)
-
-    # NOTE: first draw edges so that node boxes could overwrite them
-    for edge in sug.g.sE:
-        if len(edge.view._pts) <= 1:
-            msg = "Not enough points to draw an edge"
-            raise ValueError(msg)
-        for index in range(1, len(edge.view._pts)):
-            start = edge.view._pts[index - 1]
-            end = edge.view._pts[index]
-
-            start_x = int(round(start[0] - minx))
-            start_y = int(round(start[1] - miny))
-            end_x = int(round(end[0] - minx))
-            end_y = int(round(end[1] - miny))
-
-            if start_x < 0 or start_y < 0 or end_x < 0 or end_y < 0:
-                msg = (
-                    "Invalid edge coordinates: "
-                    f"start_x={start_x}, "
-                    f"start_y={start_y}, "
-                    f"end_x={end_x}, "
-                    f"end_y={end_y}"
-                )
-                raise ValueError(msg)
-
-            canvas.line(start_x, start_y, end_x, end_y, "." if edge.data else "*")
-
-    for vertex in sug.g.sV:
-        # NOTE: moving boxes w/2 to the left
-        x = vertex.view.xy[0] - vertex.view.w / 2.0
-        y = vertex.view.xy[1]
-
-        canvas.box(
-            int(round(x - minx)),
-            int(round(y - miny)),
-            vertex.view.w,
-            vertex.view.h,
-        )
-
-        canvas.text(int(round(x - minx)) + 1, int(round(y - miny)) + 1, vertex.data)
-
-    return canvas.draw()
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/graph_mermaid.py ./custom_langchain_core/runnables/graph_mermaid.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/graph_mermaid.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/graph_mermaid.py	1970-01-01 09:00:00
@@ -1,349 +0,0 @@
-import asyncio
-import base64
-import re
-from dataclasses import asdict
-from typing import Literal, Optional
-
-from langchain_core.runnables.graph import (
-    CurveStyle,
-    Edge,
-    MermaidDrawMethod,
-    Node,
-    NodeStyles,
-)
-
-MARKDOWN_SPECIAL_CHARS = "*_`"
-
-
-def draw_mermaid(
-    nodes: dict[str, Node],
-    edges: list[Edge],
-    *,
-    first_node: Optional[str] = None,
-    last_node: Optional[str] = None,
-    with_styles: bool = True,
-    curve_style: CurveStyle = CurveStyle.LINEAR,
-    node_styles: Optional[NodeStyles] = None,
-    wrap_label_n_words: int = 9,
-) -> str:
-    """Draws a Mermaid graph using the provided graph data.
-
-    Args:
-        nodes (dict[str, str]): List of node ids.
-        edges (List[Edge]): List of edges, object with a source,
-            target and data.
-        first_node (str, optional): Id of the first node. Defaults to None.
-        last_node (str, optional): Id of the last node. Defaults to None.
-        with_styles (bool, optional): Whether to include styles in the graph.
-            Defaults to True.
-        curve_style (CurveStyle, optional): Curve style for the edges.
-            Defaults to CurveStyle.LINEAR.
-        node_styles (NodeStyles, optional): Node colors for different types.
-            Defaults to NodeStyles().
-        wrap_label_n_words (int, optional): Words to wrap the edge labels.
-            Defaults to 9.
-
-    Returns:
-        str: Mermaid graph syntax.
-    """
-    # Initialize Mermaid graph configuration
-    mermaid_graph = (
-        (
-            f"%%{{init: {{'flowchart': {{'curve': '{curve_style.value}'"
-            f"}}}}}}%%\ngraph TD;\n"
-        )
-        if with_styles
-        else "graph TD;\n"
-    )
-
-    if with_styles:
-        # Node formatting templates
-        default_class_label = "default"
-        format_dict = {default_class_label: "{0}({1})"}
-        if first_node is not None:
-            format_dict[first_node] = "{0}([{1}]):::first"
-        if last_node is not None:
-            format_dict[last_node] = "{0}([{1}]):::last"
-
-        # Add nodes to the graph
-        for key, node in nodes.items():
-            node_name = node.name.split(":")[-1]
-            label = (
-                f"<p>{node_name}</p>"
-                if node_name.startswith(tuple(MARKDOWN_SPECIAL_CHARS))
-                and node_name.endswith(tuple(MARKDOWN_SPECIAL_CHARS))
-                else node_name
-            )
-            if node.metadata:
-                label = (
-                    f"{label}<hr/><small><em>"
-                    + "\n".join(
-                        f"{key} = {value}" for key, value in node.metadata.items()
-                    )
-                    + "</em></small>"
-                )
-            node_label = format_dict.get(key, format_dict[default_class_label]).format(
-                _escape_node_label(key), label
-            )
-            mermaid_graph += f"\t{node_label}\n"
-
-    # Group edges by their common prefixes
-    edge_groups: dict[str, list[Edge]] = {}
-    for edge in edges:
-        src_parts = edge.source.split(":")
-        tgt_parts = edge.target.split(":")
-        common_prefix = ":".join(
-            src for src, tgt in zip(src_parts, tgt_parts) if src == tgt
-        )
-        edge_groups.setdefault(common_prefix, []).append(edge)
-
-    seen_subgraphs = set()
-
-    def add_subgraph(edges: list[Edge], prefix: str) -> None:
-        nonlocal mermaid_graph
-        self_loop = len(edges) == 1 and edges[0].source == edges[0].target
-        if prefix and not self_loop:
-            subgraph = prefix.split(":")[-1]
-            if subgraph in seen_subgraphs:
-                msg = (
-                    f"Found duplicate subgraph '{subgraph}' -- this likely means that "
-                    "you're reusing a subgraph node with the same name. "
-                    "Please adjust your graph to have subgraph nodes with unique names."
-                )
-                raise ValueError(msg)
-
-            seen_subgraphs.add(subgraph)
-            mermaid_graph += f"\tsubgraph {subgraph}\n"
-
-        for edge in edges:
-            source, target = edge.source, edge.target
-
-            # Add BR every wrap_label_n_words words
-            if edge.data is not None:
-                edge_data = edge.data
-                words = str(edge_data).split()  # Split the string into words
-                # Group words into chunks of wrap_label_n_words size
-                if len(words) > wrap_label_n_words:
-                    edge_data = "&nbsp<br>&nbsp".join(
-                        " ".join(words[i : i + wrap_label_n_words])
-                        for i in range(0, len(words), wrap_label_n_words)
-                    )
-                if edge.conditional:
-                    edge_label = f" -. &nbsp;{edge_data}&nbsp; .-> "
-                else:
-                    edge_label = f" -- &nbsp;{edge_data}&nbsp; --> "
-            else:
-                edge_label = " -.-> " if edge.conditional else " --> "
-
-            mermaid_graph += (
-                f"\t{_escape_node_label(source)}{edge_label}"
-                f"{_escape_node_label(target)};\n"
-            )
-
-        # Recursively add nested subgraphs
-        for nested_prefix in edge_groups:
-            if not nested_prefix.startswith(prefix + ":") or nested_prefix == prefix:
-                continue
-            add_subgraph(edge_groups[nested_prefix], nested_prefix)
-
-        if prefix and not self_loop:
-            mermaid_graph += "\tend\n"
-
-    # Start with the top-level edges (no common prefix)
-    add_subgraph(edge_groups.get("", []), "")
-
-    # Add remaining subgraphs
-    for prefix in edge_groups:
-        if ":" in prefix or prefix == "":
-            continue
-        add_subgraph(edge_groups[prefix], prefix)
-
-    # Add custom styles for nodes
-    if with_styles:
-        mermaid_graph += _generate_mermaid_graph_styles(node_styles or NodeStyles())
-    return mermaid_graph
-
-
-def _escape_node_label(node_label: str) -> str:
-    """Escapes the node label for Mermaid syntax."""
-    return re.sub(r"[^a-zA-Z-_0-9]", "_", node_label)
-
-
-def _generate_mermaid_graph_styles(node_colors: NodeStyles) -> str:
-    """Generates Mermaid graph styles for different node types."""
-    styles = ""
-    for class_name, style in asdict(node_colors).items():
-        styles += f"\tclassDef {class_name} {style}\n"
-    return styles
-
-
-def draw_mermaid_png(
-    mermaid_syntax: str,
-    output_file_path: Optional[str] = None,
-    draw_method: MermaidDrawMethod = MermaidDrawMethod.API,
-    background_color: Optional[str] = "white",
-    padding: int = 10,
-) -> bytes:
-    """Draws a Mermaid graph as PNG using provided syntax.
-
-    Args:
-        mermaid_syntax (str): Mermaid graph syntax.
-        output_file_path (str, optional): Path to save the PNG image.
-            Defaults to None.
-        draw_method (MermaidDrawMethod, optional): Method to draw the graph.
-            Defaults to MermaidDrawMethod.API.
-        background_color (str, optional): Background color of the image.
-            Defaults to "white".
-        padding (int, optional): Padding around the image. Defaults to 10.
-
-    Returns:
-        bytes: PNG image bytes.
-
-    Raises:
-        ValueError: If an invalid draw method is provided.
-    """
-    if draw_method == MermaidDrawMethod.PYPPETEER:
-        import asyncio
-
-        img_bytes = asyncio.run(
-            _render_mermaid_using_pyppeteer(
-                mermaid_syntax, output_file_path, background_color, padding
-            )
-        )
-    elif draw_method == MermaidDrawMethod.API:
-        img_bytes = _render_mermaid_using_api(
-            mermaid_syntax, output_file_path, background_color
-        )
-    else:
-        supported_methods = ", ".join([m.value for m in MermaidDrawMethod])
-        msg = (
-            f"Invalid draw method: {draw_method}. "
-            f"Supported draw methods are: {supported_methods}"
-        )
-        raise ValueError(msg)
-
-    return img_bytes
-
-
-async def _render_mermaid_using_pyppeteer(
-    mermaid_syntax: str,
-    output_file_path: Optional[str] = None,
-    background_color: Optional[str] = "white",
-    padding: int = 10,
-    device_scale_factor: int = 3,
-) -> bytes:
-    """Renders Mermaid graph using Pyppeteer."""
-    try:
-        from pyppeteer import launch  # type: ignore[import]
-    except ImportError as e:
-        msg = "Install Pyppeteer to use the Pyppeteer method: `pip install pyppeteer`."
-        raise ImportError(msg) from e
-
-    browser = await launch()
-    page = await browser.newPage()
-
-    # Setup Mermaid JS
-    await page.goto("about:blank")
-    await page.addScriptTag(
-        {"url": "https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"}
-    )
-    await page.evaluate(
-        """() => {
-                mermaid.initialize({startOnLoad:true});
-            }"""
-    )
-
-    # Render SVG
-    svg_code = await page.evaluate(
-        """(mermaidGraph) => {
-                return mermaid.mermaidAPI.render('mermaid', mermaidGraph);
-            }""",
-        mermaid_syntax,
-    )
-
-    # Set the page background to white
-    await page.evaluate(
-        """(svg, background_color) => {
-            document.body.innerHTML = svg;
-            document.body.style.background = background_color;
-        }""",
-        svg_code["svg"],
-        background_color,
-    )
-
-    # Take a screenshot
-    dimensions = await page.evaluate(
-        """() => {
-            const svgElement = document.querySelector('svg');
-            const rect = svgElement.getBoundingClientRect();
-            return { width: rect.width, height: rect.height };
-        }"""
-    )
-    await page.setViewport(
-        {
-            "width": int(dimensions["width"] + padding),
-            "height": int(dimensions["height"] + padding),
-            "deviceScaleFactor": device_scale_factor,
-        }
-    )
-
-    img_bytes = await page.screenshot({"fullPage": False})
-    await browser.close()
-
-    def write_to_file(path: str, bytes: bytes) -> None:
-        with open(path, "wb") as file:
-            file.write(bytes)
-
-    if output_file_path is not None:
-        await asyncio.get_event_loop().run_in_executor(
-            None, write_to_file, output_file_path, img_bytes
-        )
-
-    return img_bytes
-
-
-def _render_mermaid_using_api(
-    mermaid_syntax: str,
-    output_file_path: Optional[str] = None,
-    background_color: Optional[str] = "white",
-    file_type: Optional[Literal["jpeg", "png", "webp"]] = "png",
-) -> bytes:
-    """Renders Mermaid graph using the Mermaid.INK API."""
-    try:
-        import requests  # type: ignore[import]
-    except ImportError as e:
-        msg = (
-            "Install the `requests` module to use the Mermaid.INK API: "
-            "`pip install requests`."
-        )
-        raise ImportError(msg) from e
-
-    # Use Mermaid API to render the image
-    mermaid_syntax_encoded = base64.b64encode(mermaid_syntax.encode("utf8")).decode(
-        "ascii"
-    )
-
-    # Check if the background color is a hexadecimal color code using regex
-    if background_color is not None:
-        hex_color_pattern = re.compile(r"^#(?:[0-9a-fA-F]{3}){1,2}$")
-        if not hex_color_pattern.match(background_color):
-            background_color = f"!{background_color}"
-
-    image_url = (
-        f"https://mermaid.ink/img/{mermaid_syntax_encoded}"
-        f"?type={file_type}&bgColor={background_color}"
-    )
-    response = requests.get(image_url, timeout=10)
-    if response.status_code == 200:
-        img_bytes = response.content
-        if output_file_path is not None:
-            with open(output_file_path, "wb") as file:
-                file.write(response.content)
-
-        return img_bytes
-    else:
-        msg = (
-            f"Failed to render the graph using the Mermaid.INK API. "
-            f"Status code: {response.status_code}."
-        )
-        raise ValueError(msg)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/graph_png.py ./custom_langchain_core/runnables/graph_png.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/graph_png.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/graph_png.py	1970-01-01 09:00:00
@@ -1,189 +0,0 @@
-from typing import Any, Optional
-
-from langchain_core.runnables.graph import Graph, LabelsDict
-
-
-class PngDrawer:
-    """Helper class to draw a state graph into a PNG file.
-
-    It requires `graphviz` and `pygraphviz` to be installed.
-    :param fontname: The font to use for the labels
-    :param labels: A dictionary of label overrides. The dictionary
-        should have the following format:
-        {
-            "nodes": {
-                "node1": "CustomLabel1",
-                "node2": "CustomLabel2",
-                "__end__": "End Node"
-            },
-            "edges": {
-                "continue": "ContinueLabel",
-                "end": "EndLabel"
-            }
-        }
-        The keys are the original labels, and the values are the new labels.
-    Usage:
-        drawer = PngDrawer()
-        drawer.draw(state_graph, 'graph.png')
-    """
-
-    def __init__(
-        self, fontname: Optional[str] = None, labels: Optional[LabelsDict] = None
-    ) -> None:
-        """Initializes the PNG drawer.
-
-        Args:
-            fontname: The font to use for the labels. Defaults to "arial".
-            labels: A dictionary of label overrides. The dictionary
-                should have the following format:
-                {
-                    "nodes": {
-                        "node1": "CustomLabel1",
-                        "node2": "CustomLabel2",
-                        "__end__": "End Node"
-                    },
-                    "edges": {
-                        "continue": "ContinueLabel",
-                        "end": "EndLabel"
-                    }
-                }
-                The keys are the original labels, and the values are the new labels.
-                Defaults to None.
-        """
-        self.fontname = fontname or "arial"
-        self.labels = labels or LabelsDict(nodes={}, edges={})
-
-    def get_node_label(self, label: str) -> str:
-        """Returns the label to use for a node.
-
-        Args:
-            label: The original label.
-
-        Returns:
-            The new label.
-        """
-        label = self.labels.get("nodes", {}).get(label, label)
-        return f"<<B>{label}</B>>"
-
-    def get_edge_label(self, label: str) -> str:
-        """Returns the label to use for an edge.
-
-        Args:
-            label: The original label.
-
-        Returns:
-            The new label.
-        """
-        label = self.labels.get("edges", {}).get(label, label)
-        return f"<<U>{label}</U>>"
-
-    def add_node(self, viz: Any, node: str) -> None:
-        """Adds a node to the graph.
-
-        Args:
-            viz: The graphviz object.
-            node: The node to add.
-
-        Returns:
-            None
-        """
-        viz.add_node(
-            node,
-            label=self.get_node_label(node),
-            style="filled",
-            fillcolor="yellow",
-            fontsize=15,
-            fontname=self.fontname,
-        )
-
-    def add_edge(
-        self,
-        viz: Any,
-        source: str,
-        target: str,
-        label: Optional[str] = None,
-        conditional: bool = False,
-    ) -> None:
-        """Adds an edge to the graph.
-
-        Args:
-            viz: The graphviz object.
-            source: The source node.
-            target: The target node.
-            label: The label for the edge. Defaults to None.
-            conditional: Whether the edge is conditional. Defaults to False.
-
-        Returns:
-            None
-        """
-        viz.add_edge(
-            source,
-            target,
-            label=self.get_edge_label(label) if label else "",
-            fontsize=12,
-            fontname=self.fontname,
-            style="dotted" if conditional else "solid",
-        )
-
-    def draw(self, graph: Graph, output_path: Optional[str] = None) -> Optional[bytes]:
-        """Draw the given state graph into a PNG file.
-
-        Requires `graphviz` and `pygraphviz` to be installed.
-        :param graph: The graph to draw
-        :param output_path: The path to save the PNG. If None, PNG bytes are returned.
-        """
-        try:
-            import pygraphviz as pgv  # type: ignore[import]
-        except ImportError as exc:
-            msg = "Install pygraphviz to draw graphs: `pip install pygraphviz`."
-            raise ImportError(msg) from exc
-
-        # Create a directed graph
-        viz = pgv.AGraph(directed=True, nodesep=0.9, ranksep=1.0)
-
-        # Add nodes, conditional edges, and edges to the graph
-        self.add_nodes(viz, graph)
-        self.add_edges(viz, graph)
-
-        # Update entrypoint and END styles
-        self.update_styles(viz, graph)
-
-        # Save the graph as PNG
-        try:
-            return viz.draw(output_path, format="png", prog="dot")
-        finally:
-            viz.close()
-
-    def add_nodes(self, viz: Any, graph: Graph) -> None:
-        """Add nodes to the graph.
-
-        Args:
-            viz: The graphviz object.
-            graph: The graph to draw.
-        """
-        for node in graph.nodes:
-            self.add_node(viz, node)
-
-    def add_edges(self, viz: Any, graph: Graph) -> None:
-        """Add edges to the graph.
-
-        Args:
-            viz: The graphviz object.
-            graph: The graph to draw.
-        """
-        for start, end, data, cond in graph.edges:
-            self.add_edge(
-                viz, start, end, str(data) if data is not None else None, cond
-            )
-
-    def update_styles(self, viz: Any, graph: Graph) -> None:
-        """Update the styles of the entrypoint and END nodes.
-
-        Args:
-            viz: The graphviz object.
-            graph: The graph to draw.
-        """
-        if first := graph.first_node():
-            viz.get_node(first.id).attr.update(fillcolor="lightblue")
-        if last := graph.last_node():
-            viz.get_node(last.id).attr.update(fillcolor="orange")
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/history.py ./custom_langchain_core/runnables/history.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/history.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/history.py	1970-01-01 09:00:00
@@ -1,632 +0,0 @@
-from __future__ import annotations
-
-import inspect
-from collections.abc import Sequence
-from types import GenericAlias
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    Optional,
-    Union,
-)
-
-from pydantic import BaseModel
-from typing_extensions import override
-
-from langchain_core.chat_history import BaseChatMessageHistory
-from langchain_core.load.load import load
-from langchain_core.runnables.base import Runnable, RunnableBindingBase, RunnableLambda
-from langchain_core.runnables.passthrough import RunnablePassthrough
-from langchain_core.runnables.utils import (
-    ConfigurableFieldSpec,
-    Output,
-    get_unique_config_specs,
-)
-from langchain_core.utils.pydantic import create_model_v2
-
-if TYPE_CHECKING:
-    from langchain_core.language_models.base import LanguageModelLike
-    from langchain_core.messages.base import BaseMessage
-    from langchain_core.runnables.config import RunnableConfig
-    from langchain_core.tracers.schemas import Run
-
-
-MessagesOrDictWithMessages = Union[Sequence["BaseMessage"], dict[str, Any]]
-GetSessionHistoryCallable = Callable[..., BaseChatMessageHistory]
-
-
-class RunnableWithMessageHistory(RunnableBindingBase):
-    """Runnable that manages chat message history for another Runnable.
-
-    A chat message history is a sequence of messages that represent a conversation.
-
-    RunnableWithMessageHistory wraps another Runnable and manages the chat message
-    history for it; it is responsible for reading and updating the chat message
-    history.
-
-    The formats supported for the inputs and outputs of the wrapped Runnable
-    are described below.
-
-    RunnableWithMessageHistory must always be called with a config that contains
-    the appropriate parameters for the chat message history factory.
-
-    By default, the Runnable is expected to take a single configuration parameter
-    called `session_id` which is a string. This parameter is used to create a new
-    or look up an existing chat message history that matches the given session_id.
-
-    In this case, the invocation would look like this:
-
-    `with_history.invoke(..., config={"configurable": {"session_id": "bar"}})`
-    ; e.g., ``{"configurable": {"session_id": "<SESSION_ID>"}}``.
-
-    The configuration can be customized by passing in a list of
-    ``ConfigurableFieldSpec`` objects to the ``history_factory_config`` parameter (see
-    example below).
-
-    In the examples, we will use a chat message history with an in-memory
-    implementation to make it easy to experiment and see the results.
-
-    For production use cases, you will want to use a persistent implementation
-    of chat message history, such as ``RedisChatMessageHistory``.
-
-    Parameters:
-        get_session_history: Function that returns a new BaseChatMessageHistory.
-            This function should either take a single positional argument
-            `session_id` of type string and return a corresponding
-            chat message history instance.
-        input_messages_key: Must be specified if the base runnable accepts a dict
-            as input. The key in the input dict that contains the messages.
-        output_messages_key: Must be specified if the base Runnable returns a dict
-            as output. The key in the output dict that contains the messages.
-        history_messages_key: Must be specified if the base runnable accepts a dict
-            as input and expects a separate key for historical messages.
-        history_factory_config: Configure fields that should be passed to the
-            chat history factory. See ``ConfigurableFieldSpec`` for more details.
-
-    Example: Chat message history with an in-memory implementation for testing.
-
-    .. code-block:: python
-
-        from operator import itemgetter
-        from typing import List
-
-        from langchain_openai.chat_models import ChatOpenAI
-
-        from langchain_core.chat_history import BaseChatMessageHistory
-        from langchain_core.documents import Document
-        from langchain_core.messages import BaseMessage, AIMessage
-        from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
-        from pydantic import BaseModel, Field
-        from langchain_core.runnables import (
-            RunnableLambda,
-            ConfigurableFieldSpec,
-            RunnablePassthrough,
-        )
-        from langchain_core.runnables.history import RunnableWithMessageHistory
-
-
-        class InMemoryHistory(BaseChatMessageHistory, BaseModel):
-            \"\"\"In memory implementation of chat message history.\"\"\"
-
-            messages: List[BaseMessage] = Field(default_factory=list)
-
-            def add_messages(self, messages: List[BaseMessage]) -> None:
-                \"\"\"Add a list of messages to the store\"\"\"
-                self.messages.extend(messages)
-
-            def clear(self) -> None:
-                self.messages = []
-
-        # Here we use a global variable to store the chat message history.
-        # This will make it easier to inspect it to see the underlying results.
-        store = {}
-
-        def get_by_session_id(session_id: str) -> BaseChatMessageHistory:
-            if session_id not in store:
-                store[session_id] = InMemoryHistory()
-            return store[session_id]
-
-
-        history = get_by_session_id("1")
-        history.add_message(AIMessage(content="hello"))
-        print(store)  # noqa: T201
-
-
-    Example where the wrapped Runnable takes a dictionary input:
-
-        .. code-block:: python
-
-            from typing import Optional
-
-            from langchain_community.chat_models import ChatAnthropic
-            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
-            from langchain_core.runnables.history import RunnableWithMessageHistory
-
-
-            prompt = ChatPromptTemplate.from_messages([
-                ("system", "You're an assistant who's good at {ability}"),
-                MessagesPlaceholder(variable_name="history"),
-                ("human", "{question}"),
-            ])
-
-            chain = prompt | ChatAnthropic(model="claude-2")
-
-            chain_with_history = RunnableWithMessageHistory(
-                chain,
-                # Uses the get_by_session_id function defined in the example
-                # above.
-                get_by_session_id,
-                input_messages_key="question",
-                history_messages_key="history",
-            )
-
-            print(chain_with_history.invoke(  # noqa: T201
-                {"ability": "math", "question": "What does cosine mean?"},
-                config={"configurable": {"session_id": "foo"}}
-            ))
-
-            # Uses the store defined in the example above.
-            print(store)  # noqa: T201
-
-            print(chain_with_history.invoke(  # noqa: T201
-                {"ability": "math", "question": "What's its inverse"},
-                config={"configurable": {"session_id": "foo"}}
-            ))
-
-            print(store)  # noqa: T201
-
-
-    Example where the session factory takes two keys, user_id and conversation id):
-
-        .. code-block:: python
-
-            store = {}
-
-            def get_session_history(
-                user_id: str, conversation_id: str
-            ) -> BaseChatMessageHistory:
-                if (user_id, conversation_id) not in store:
-                    store[(user_id, conversation_id)] = InMemoryHistory()
-                return store[(user_id, conversation_id)]
-
-            prompt = ChatPromptTemplate.from_messages([
-                ("system", "You're an assistant who's good at {ability}"),
-                MessagesPlaceholder(variable_name="history"),
-                ("human", "{question}"),
-            ])
-
-            chain = prompt | ChatAnthropic(model="claude-2")
-
-            with_message_history = RunnableWithMessageHistory(
-                chain,
-                get_session_history=get_session_history,
-                input_messages_key="question",
-                history_messages_key="history",
-                history_factory_config=[
-                    ConfigurableFieldSpec(
-                        id="user_id",
-                        annotation=str,
-                        name="User ID",
-                        description="Unique identifier for the user.",
-                        default="",
-                        is_shared=True,
-                    ),
-                    ConfigurableFieldSpec(
-                        id="conversation_id",
-                        annotation=str,
-                        name="Conversation ID",
-                        description="Unique identifier for the conversation.",
-                        default="",
-                        is_shared=True,
-                    ),
-                ],
-            )
-
-            with_message_history.invoke(
-                {"ability": "math", "question": "What does cosine mean?"},
-                config={"configurable": {"user_id": "123", "conversation_id": "1"}}
-            )
-
-    """
-
-    get_session_history: GetSessionHistoryCallable
-    input_messages_key: Optional[str] = None
-    output_messages_key: Optional[str] = None
-    history_messages_key: Optional[str] = None
-    history_factory_config: Sequence[ConfigurableFieldSpec]
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    def __init__(
-        self,
-        runnable: Union[
-            Runnable[
-                Union[MessagesOrDictWithMessages],
-                Union[str, BaseMessage, MessagesOrDictWithMessages],
-            ],
-            LanguageModelLike,
-        ],
-        get_session_history: GetSessionHistoryCallable,
-        *,
-        input_messages_key: Optional[str] = None,
-        output_messages_key: Optional[str] = None,
-        history_messages_key: Optional[str] = None,
-        history_factory_config: Optional[Sequence[ConfigurableFieldSpec]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Initialize RunnableWithMessageHistory.
-
-        Args:
-            runnable: The base Runnable to be wrapped. Must take as input one of:
-                1. A sequence of BaseMessages
-                2. A dict with one key for all messages
-                3. A dict with one key for the current input string/message(s) and
-                    a separate key for historical messages. If the input key points
-                    to a string, it will be treated as a HumanMessage in history.
-
-                Must return as output one of:
-                1. A string which can be treated as an AIMessage
-                2. A BaseMessage or sequence of BaseMessages
-                3. A dict with a key for a BaseMessage or sequence of BaseMessages
-
-            get_session_history: Function that returns a new BaseChatMessageHistory.
-                This function should either take a single positional argument
-                `session_id` of type string and return a corresponding
-                chat message history instance.
-                .. code-block:: python
-
-                    def get_session_history(
-                        session_id: str,
-                        *,
-                        user_id: Optional[str]=None
-                    ) -> BaseChatMessageHistory:
-                      ...
-
-                Or it should take keyword arguments that match the keys of
-                `session_history_config_specs` and return a corresponding
-                chat message history instance.
-
-                .. code-block:: python
-
-                    def get_session_history(
-                        *,
-                        user_id: str,
-                        thread_id: str,
-                    ) -> BaseChatMessageHistory:
-                        ...
-
-            input_messages_key: Must be specified if the base runnable accepts a dict
-                as input. Default is None.
-            output_messages_key: Must be specified if the base runnable returns a dict
-                as output. Default is None.
-            history_messages_key: Must be specified if the base runnable accepts a dict
-                as input and expects a separate key for historical messages.
-            history_factory_config: Configure fields that should be passed to the
-                chat history factory. See ``ConfigurableFieldSpec`` for more details.
-                Specifying these allows you to pass multiple config keys
-                into the get_session_history factory.
-            **kwargs: Arbitrary additional kwargs to pass to parent class
-                ``RunnableBindingBase`` init.
-        """
-        history_chain: Runnable = RunnableLambda(
-            self._enter_history, self._aenter_history
-        ).with_config(run_name="load_history")
-        messages_key = history_messages_key or input_messages_key
-        if messages_key:
-            history_chain = RunnablePassthrough.assign(
-                **{messages_key: history_chain}
-            ).with_config(run_name="insert_history")
-
-        runnable_sync: Runnable = runnable.with_listeners(on_end=self._exit_history)
-        runnable_async: Runnable = runnable.with_alisteners(on_end=self._aexit_history)
-
-        def _call_runnable_sync(_input: Any) -> Runnable:
-            return runnable_sync
-
-        async def _call_runnable_async(_input: Any) -> Runnable:
-            return runnable_async
-
-        bound: Runnable = (
-            history_chain
-            | RunnableLambda(
-                _call_runnable_sync,
-                _call_runnable_async,
-            ).with_config(run_name="check_sync_or_async")
-        ).with_config(run_name="RunnableWithMessageHistory")
-
-        if history_factory_config:
-            _config_specs = history_factory_config
-        else:
-            # If not provided, then we'll use the default session_id field
-            _config_specs = [
-                ConfigurableFieldSpec(
-                    id="session_id",
-                    annotation=str,
-                    name="Session ID",
-                    description="Unique identifier for a session.",
-                    default="",
-                    is_shared=True,
-                ),
-            ]
-
-        super().__init__(
-            get_session_history=get_session_history,
-            input_messages_key=input_messages_key,
-            output_messages_key=output_messages_key,
-            bound=bound,
-            history_messages_key=history_messages_key,
-            history_factory_config=_config_specs,
-            **kwargs,
-        )
-        self._history_chain = history_chain
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        """Get the configuration specs for the RunnableWithMessageHistory."""
-        return get_unique_config_specs(
-            super().config_specs + list(self.history_factory_config)
-        )
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        from langchain_core.messages import BaseMessage
-
-        fields: dict = {}
-        if self.input_messages_key and self.history_messages_key:
-            fields[self.input_messages_key] = (
-                Union[str, BaseMessage, Sequence[BaseMessage]],
-                ...,
-            )
-        elif self.input_messages_key:
-            fields[self.input_messages_key] = (Sequence[BaseMessage], ...)
-        else:
-            return create_model_v2(
-                "RunnableWithChatHistoryInput",
-                module_name=self.__class__.__module__,
-                root=(Sequence[BaseMessage], ...),
-            )
-        return create_model_v2(  # type: ignore[call-overload]
-            "RunnableWithChatHistoryInput",
-            field_definitions=fields,
-            module_name=self.__class__.__module__,
-        )
-
-    @property
-    @override
-    def OutputType(self) -> type[Output]:
-        output_type = self._history_chain.OutputType
-        return output_type
-
-    def get_output_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        """Get a pydantic model that can be used to validate output to the Runnable.
-
-        Runnables that leverage the configurable_fields and configurable_alternatives
-        methods will have a dynamic output schema that depends on which
-        configuration the Runnable is invoked with.
-
-        This method allows to get an output schema for a specific configuration.
-
-        Args:
-            config: A config to use when generating the schema.
-
-        Returns:
-            A pydantic model that can be used to validate output.
-        """
-        root_type = self.OutputType
-
-        if (
-            inspect.isclass(root_type)
-            and not isinstance(root_type, GenericAlias)
-            and issubclass(root_type, BaseModel)
-        ):
-            return root_type
-
-        return create_model_v2(
-            "RunnableWithChatHistoryOutput",
-            root=root_type,
-            module_name=self.__class__.__module__,
-        )
-
-    def _is_not_async(self, *args: Sequence[Any], **kwargs: dict[str, Any]) -> bool:
-        return False
-
-    async def _is_async(self, *args: Sequence[Any], **kwargs: dict[str, Any]) -> bool:
-        return True
-
-    def _get_input_messages(
-        self, input_val: Union[str, BaseMessage, Sequence[BaseMessage], dict]
-    ) -> list[BaseMessage]:
-        from langchain_core.messages import BaseMessage
-
-        # If dictionary, try to pluck the single key representing messages
-        if isinstance(input_val, dict):
-            if self.input_messages_key:
-                key = self.input_messages_key
-            elif len(input_val) == 1:
-                key = list(input_val.keys())[0]
-            else:
-                key = "input"
-            input_val = input_val[key]
-
-        # If value is a string, convert to a human message
-        if isinstance(input_val, str):
-            from langchain_core.messages import HumanMessage
-
-            return [HumanMessage(content=input_val)]
-        # If value is a single message, convert to a list
-        elif isinstance(input_val, BaseMessage):
-            return [input_val]
-        # If value is a list or tuple...
-        elif isinstance(input_val, (list, tuple)):
-            # Handle empty case
-            if len(input_val) == 0:
-                return list(input_val)
-            # If is a list of list, then return the first value
-            # This occurs for chat models - since we batch inputs
-            if isinstance(input_val[0], list):
-                if len(input_val) != 1:
-                    msg = f"Expected a single list of messages. Got {input_val}."
-                    raise ValueError(msg)
-                return input_val[0]
-            return list(input_val)
-        else:
-            msg = (
-                f"Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. "
-                f"Got {input_val}."
-            )
-            raise ValueError(msg)  # noqa: TRY004
-
-    def _get_output_messages(
-        self, output_val: Union[str, BaseMessage, Sequence[BaseMessage], dict]
-    ) -> list[BaseMessage]:
-        from langchain_core.messages import BaseMessage
-
-        # If dictionary, try to pluck the single key representing messages
-        if isinstance(output_val, dict):
-            if self.output_messages_key:
-                key = self.output_messages_key
-            elif len(output_val) == 1:
-                key = list(output_val.keys())[0]
-            else:
-                key = "output"
-            # If you are wrapping a chat model directly
-            # The output is actually this weird generations object
-            if key not in output_val and "generations" in output_val:
-                output_val = output_val["generations"][0][0]["message"]
-            else:
-                output_val = output_val[key]
-
-        if isinstance(output_val, str):
-            from langchain_core.messages import AIMessage
-
-            return [AIMessage(content=output_val)]
-        # If value is a single message, convert to a list
-        elif isinstance(output_val, BaseMessage):
-            return [output_val]
-        elif isinstance(output_val, (list, tuple)):
-            return list(output_val)
-        else:
-            msg = (
-                f"Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. "
-                f"Got {output_val}."
-            )
-            raise ValueError(msg)  # noqa: TRY004
-
-    def _enter_history(self, input: Any, config: RunnableConfig) -> list[BaseMessage]:
-        hist: BaseChatMessageHistory = config["configurable"]["message_history"]
-        messages = hist.messages.copy()
-
-        if not self.history_messages_key:
-            # return all messages
-            input_val = (
-                input if not self.input_messages_key else input[self.input_messages_key]
-            )
-            messages += self._get_input_messages(input_val)
-        return messages
-
-    async def _aenter_history(
-        self, input: dict[str, Any], config: RunnableConfig
-    ) -> list[BaseMessage]:
-        hist: BaseChatMessageHistory = config["configurable"]["message_history"]
-        messages = (await hist.aget_messages()).copy()
-
-        if not self.history_messages_key:
-            # return all messages
-            input_val = (
-                input if not self.input_messages_key else input[self.input_messages_key]
-            )
-            messages += self._get_input_messages(input_val)
-        return messages
-
-    def _exit_history(self, run: Run, config: RunnableConfig) -> None:
-        hist: BaseChatMessageHistory = config["configurable"]["message_history"]
-
-        # Get the input messages
-        inputs = load(run.inputs)
-        input_messages = self._get_input_messages(inputs)
-        # If historic messages were prepended to the input messages, remove them to
-        # avoid adding duplicate messages to history.
-        if not self.history_messages_key:
-            historic_messages = config["configurable"]["message_history"].messages
-            input_messages = input_messages[len(historic_messages) :]
-
-        # Get the output messages
-        output_val = load(run.outputs)
-        output_messages = self._get_output_messages(output_val)
-        hist.add_messages(input_messages + output_messages)
-
-    async def _aexit_history(self, run: Run, config: RunnableConfig) -> None:
-        hist: BaseChatMessageHistory = config["configurable"]["message_history"]
-
-        # Get the input messages
-        inputs = load(run.inputs)
-        input_messages = self._get_input_messages(inputs)
-        # If historic messages were prepended to the input messages, remove them to
-        # avoid adding duplicate messages to history.
-        if not self.history_messages_key:
-            historic_messages = await hist.aget_messages()
-            input_messages = input_messages[len(historic_messages) :]
-
-        # Get the output messages
-        output_val = load(run.outputs)
-        output_messages = self._get_output_messages(output_val)
-        await hist.aadd_messages(input_messages + output_messages)
-
-    def _merge_configs(self, *configs: Optional[RunnableConfig]) -> RunnableConfig:
-        config = super()._merge_configs(*configs)
-        expected_keys = [field_spec.id for field_spec in self.history_factory_config]
-
-        configurable = config.get("configurable", {})
-
-        missing_keys = set(expected_keys) - set(configurable.keys())
-        parameter_names = _get_parameter_names(self.get_session_history)
-
-        if missing_keys and parameter_names:
-            example_input = {self.input_messages_key: "foo"}
-            example_configurable = dict.fromkeys(missing_keys, "[your-value-here]")
-            example_config = {"configurable": example_configurable}
-            msg = (
-                f"Missing keys {sorted(missing_keys)} in config['configurable'] "
-                f"Expected keys are {sorted(expected_keys)}."
-                f"When using via .invoke() or .stream(), pass in a config; "
-                f"e.g., chain.invoke({example_input}, {example_config})"
-            )
-            raise ValueError(msg)
-
-        if len(expected_keys) == 1:
-            if parameter_names:
-                # If arity = 1, then invoke function by positional arguments
-                message_history = self.get_session_history(
-                    configurable[expected_keys[0]]
-                )
-            else:
-                if not config:
-                    config["configurable"] = {}
-                message_history = self.get_session_history()
-        else:
-            # otherwise verify that names of keys patch and invoke by named arguments
-            if set(expected_keys) != set(parameter_names):
-                msg = (
-                    f"Expected keys {sorted(expected_keys)} do not match parameter "
-                    f"names {sorted(parameter_names)} of get_session_history."
-                )
-                raise ValueError(msg)
-
-            message_history = self.get_session_history(
-                **{key: configurable[key] for key in expected_keys}
-            )
-        config["configurable"]["message_history"] = message_history
-        return config
-
-
-def _get_parameter_names(callable_: GetSessionHistoryCallable) -> list[str]:
-    """Get the parameter names of the callable."""
-    sig = inspect.signature(callable_)
-    return list(sig.parameters.keys())
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/learnable.py ./custom_langchain_core/runnables/learnable.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/learnable.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/learnable.py	1970-01-01 09:00:00
@@ -1,15 +0,0 @@
-# from langchain_core.runnables.base import RunnableBinding
-
-
-# class RunnableLearnable(RunnableBinding):
-#     def __init__(self, *args, **kwargs):
-#         super().__init__(*args, **kwargs)
-#         self.parameters = []
-
-#     def backward(self):
-#         for param in self.parameters:
-#             param.backward()
-
-#     def update(self, optimizer):
-#         for param in self.parameters:
-#             optimizer.update(param)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/passthrough.py ./custom_langchain_core/runnables/passthrough.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/passthrough.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/passthrough.py	1970-01-01 09:00:00
@@ -1,807 +0,0 @@
-"""Implementation of the RunnablePassthrough."""
-
-from __future__ import annotations
-
-import asyncio
-import inspect
-import threading
-from collections.abc import AsyncIterator, Awaitable, Iterator, Mapping
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    Optional,
-    Union,
-    cast,
-)
-
-from pydantic import BaseModel, RootModel
-from typing_extensions import override
-
-from langchain_core.runnables.base import (
-    Other,
-    Runnable,
-    RunnableParallel,
-    RunnableSerializable,
-)
-from langchain_core.runnables.config import (
-    RunnableConfig,
-    acall_func_with_variable_args,
-    call_func_with_variable_args,
-    ensure_config,
-    get_executor_for_config,
-    patch_config,
-)
-from langchain_core.runnables.graph import Graph
-from langchain_core.runnables.utils import (
-    AddableDict,
-    ConfigurableFieldSpec,
-)
-from langchain_core.utils.aiter import atee, py_anext
-from langchain_core.utils.iter import safetee
-from langchain_core.utils.pydantic import create_model_v2
-
-if TYPE_CHECKING:
-    from langchain_core.callbacks.manager import (
-        AsyncCallbackManagerForChainRun,
-        CallbackManagerForChainRun,
-    )
-
-
-def identity(x: Other) -> Other:
-    """Identity function.
-
-    Args:
-        x (Other): input.
-
-    Returns:
-        Other: output.
-    """
-    return x
-
-
-async def aidentity(x: Other) -> Other:
-    """Async identity function.
-
-    Args:
-        x (Other): input.
-
-    Returns:
-        Other: output.
-    """
-    return x
-
-
-class RunnablePassthrough(RunnableSerializable[Other, Other]):
-    """Runnable to passthrough inputs unchanged or with additional keys.
-
-    This Runnable behaves almost like the identity function, except that it
-    can be configured to add additional keys to the output, if the input is a
-    dict.
-
-    The examples below demonstrate this Runnable works using a few simple
-    chains. The chains rely on simple lambdas to make the examples easy to execute
-    and experiment with.
-
-    Parameters:
-        func (Callable[[Other], None], optional): Function to be called with the input.
-        afunc (Callable[[Other], Awaitable[None]], optional): Async function to
-            be called with the input.
-        input_type (Optional[Type[Other]], optional): Type of the input.
-        **kwargs (Any): Additional keyword arguments.
-
-    Examples:
-
-        .. code-block:: python
-
-            from langchain_core.runnables import (
-                RunnableLambda,
-                RunnableParallel,
-                RunnablePassthrough,
-            )
-
-            runnable = RunnableParallel(
-                origin=RunnablePassthrough(),
-                modified=lambda x: x+1
-            )
-
-            runnable.invoke(1) # {'origin': 1, 'modified': 2}
-
-
-            def fake_llm(prompt: str) -> str: # Fake LLM for the example
-                return "completion"
-
-            chain = RunnableLambda(fake_llm) | {
-                'original': RunnablePassthrough(), # Original LLM output
-                'parsed': lambda text: text[::-1] # Parsing logic
-            }
-
-            chain.invoke('hello') # {'original': 'completion', 'parsed': 'noitelpmoc'}
-
-    In some cases, it may be useful to pass the input through while adding some
-    keys to the output. In this case, you can use the `assign` method:
-
-        .. code-block:: python
-
-            from langchain_core.runnables import RunnablePassthrough
-
-            def fake_llm(prompt: str) -> str: # Fake LLM for the example
-                return "completion"
-
-            runnable = {
-                'llm1':  fake_llm,
-                'llm2':  fake_llm,
-            } | RunnablePassthrough.assign(
-                total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])
-            )
-
-            runnable.invoke('hello')
-            # {'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}
-    """
-
-    input_type: Optional[type[Other]] = None
-
-    func: Optional[
-        Union[Callable[[Other], None], Callable[[Other, RunnableConfig], None]]
-    ] = None
-
-    afunc: Optional[
-        Union[
-            Callable[[Other], Awaitable[None]],
-            Callable[[Other, RunnableConfig], Awaitable[None]],
-        ]
-    ] = None
-
-    def __repr_args__(self) -> Any:
-        # Without this repr(self) raises a RecursionError
-        # See https://github.com/pydantic/pydantic/issues/7327
-        return []
-
-    def __init__(
-        self,
-        func: Optional[
-            Union[
-                Union[Callable[[Other], None], Callable[[Other, RunnableConfig], None]],
-                Union[
-                    Callable[[Other], Awaitable[None]],
-                    Callable[[Other, RunnableConfig], Awaitable[None]],
-                ],
-            ]
-        ] = None,
-        afunc: Optional[
-            Union[
-                Callable[[Other], Awaitable[None]],
-                Callable[[Other, RunnableConfig], Awaitable[None]],
-            ]
-        ] = None,
-        *,
-        input_type: Optional[type[Other]] = None,
-        **kwargs: Any,
-    ) -> None:
-        if inspect.iscoroutinefunction(func):
-            afunc = func
-            func = None
-
-        super().__init__(func=func, afunc=afunc, input_type=input_type, **kwargs)  # type: ignore[call-arg]
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    @property
-    @override
-    def InputType(self) -> Any:
-        return self.input_type or Any
-
-    @property
-    @override
-    def OutputType(self) -> Any:
-        return self.input_type or Any
-
-    @classmethod
-    def assign(
-        cls,
-        **kwargs: Union[
-            Runnable[dict[str, Any], Any],
-            Callable[[dict[str, Any]], Any],
-            Mapping[
-                str,
-                Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]],
-            ],
-        ],
-    ) -> RunnableAssign:
-        """Merge the Dict input with the output produced by the mapping argument.
-
-        Args:
-            **kwargs: Runnable, Callable or a Mapping from keys to Runnables
-                or Callables.
-
-        Returns:
-            A Runnable that merges the Dict input with the output produced by the
-            mapping argument.
-        """
-        return RunnableAssign(RunnableParallel[dict[str, Any]](kwargs))
-
-    def invoke(
-        self, input: Other, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Other:
-        if self.func is not None:
-            call_func_with_variable_args(
-                self.func, input, ensure_config(config), **kwargs
-            )
-        return self._call_with_config(identity, input, config)
-
-    async def ainvoke(
-        self,
-        input: Other,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Other:
-        if self.afunc is not None:
-            await acall_func_with_variable_args(
-                self.afunc, input, ensure_config(config), **kwargs
-            )
-        elif self.func is not None:
-            call_func_with_variable_args(
-                self.func, input, ensure_config(config), **kwargs
-            )
-        return await self._acall_with_config(aidentity, input, config)
-
-    def transform(
-        self,
-        input: Iterator[Other],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Iterator[Other]:
-        if self.func is None:
-            for chunk in self._transform_stream_with_config(input, identity, config):
-                yield chunk
-        else:
-            final: Other
-            got_first_chunk = False
-
-            for chunk in self._transform_stream_with_config(input, identity, config):
-                yield chunk
-
-                if not got_first_chunk:
-                    final = chunk
-                    got_first_chunk = True
-                else:
-                    try:
-                        final = final + chunk  # type: ignore[operator]
-                    except TypeError:
-                        final = chunk
-
-            if got_first_chunk:
-                call_func_with_variable_args(
-                    self.func, final, ensure_config(config), **kwargs
-                )
-
-    async def atransform(
-        self,
-        input: AsyncIterator[Other],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[Other]:
-        if self.afunc is None and self.func is None:
-            async for chunk in self._atransform_stream_with_config(
-                input, identity, config
-            ):
-                yield chunk
-        else:
-            got_first_chunk = False
-
-            async for chunk in self._atransform_stream_with_config(
-                input, identity, config
-            ):
-                yield chunk
-
-                # By definitions, a function will operate on the aggregated
-                # input. So we'll aggregate the input until we get to the last
-                # chunk.
-                # If the input is not addable, then we'll assume that we can
-                # only operate on the last chunk.
-                if not got_first_chunk:
-                    final = chunk
-                    got_first_chunk = True
-                else:
-                    try:
-                        final = final + chunk  # type: ignore[operator]
-                    except TypeError:
-                        final = chunk
-
-            if got_first_chunk:
-                config = ensure_config(config)
-                if self.afunc is not None:
-                    await acall_func_with_variable_args(
-                        self.afunc, final, config, **kwargs
-                    )
-                elif self.func is not None:
-                    call_func_with_variable_args(self.func, final, config, **kwargs)
-
-    def stream(
-        self,
-        input: Other,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Iterator[Other]:
-        return self.transform(iter([input]), config, **kwargs)
-
-    async def astream(
-        self,
-        input: Other,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[Other]:
-        async def input_aiter() -> AsyncIterator[Other]:
-            yield input
-
-        async for chunk in self.atransform(input_aiter(), config, **kwargs):
-            yield chunk
-
-
-_graph_passthrough: RunnablePassthrough = RunnablePassthrough()
-
-
-class RunnableAssign(RunnableSerializable[dict[str, Any], dict[str, Any]]):
-    """Runnable that assigns key-value pairs to Dict[str, Any] inputs.
-
-    The `RunnableAssign` class takes input dictionaries and, through a
-    `RunnableParallel` instance, applies transformations, then combines
-    these with the original data, introducing new key-value pairs based
-    on the mapper's logic.
-
-    Parameters:
-        mapper (RunnableParallel[Dict[str, Any]]): A `RunnableParallel` instance
-            that will be used to transform the input dictionary.
-
-    Examples:
-        .. code-block:: python
-
-            # This is a RunnableAssign
-            from typing import Dict
-            from langchain_core.runnables.passthrough import (
-                RunnableAssign,
-                RunnableParallel,
-            )
-            from langchain_core.runnables.base import RunnableLambda
-
-            def add_ten(x: Dict[str, int]) -> Dict[str, int]:
-                return {"added": x["input"] + 10}
-
-            mapper = RunnableParallel(
-                {"add_step": RunnableLambda(add_ten),}
-            )
-
-            runnable_assign = RunnableAssign(mapper)
-
-            # Synchronous example
-            runnable_assign.invoke({"input": 5})
-            # returns {'input': 5, 'add_step': {'added': 15}}
-
-            # Asynchronous example
-            await runnable_assign.ainvoke({"input": 5})
-            # returns {'input': 5, 'add_step': {'added': 15}}
-    """
-
-    mapper: RunnableParallel
-
-    def __init__(self, mapper: RunnableParallel[dict[str, Any]], **kwargs: Any) -> None:
-        super().__init__(mapper=mapper, **kwargs)  # type: ignore[call-arg]
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    def get_name(
-        self, suffix: Optional[str] = None, *, name: Optional[str] = None
-    ) -> str:
-        name = (
-            name
-            or self.name
-            or f"RunnableAssign<{','.join(self.mapper.steps__.keys())}>"
-        )
-        return super().get_name(suffix, name=name)
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        map_input_schema = self.mapper.get_input_schema(config)
-        if not issubclass(map_input_schema, RootModel):
-            # ie. it's a dict
-            return map_input_schema
-
-        return super().get_input_schema(config)
-
-    def get_output_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        map_input_schema = self.mapper.get_input_schema(config)
-        map_output_schema = self.mapper.get_output_schema(config)
-        if not issubclass(map_input_schema, RootModel) and not issubclass(
-            map_output_schema, RootModel
-        ):
-            fields = {}
-
-            for name, field_info in map_input_schema.model_fields.items():
-                fields[name] = (field_info.annotation, field_info.default)
-
-            for name, field_info in map_output_schema.model_fields.items():
-                fields[name] = (field_info.annotation, field_info.default)
-
-            return create_model_v2(  # type: ignore[call-overload]
-                "RunnableAssignOutput", field_definitions=fields
-            )
-        elif not issubclass(map_output_schema, RootModel):
-            # ie. only map output is a dict
-            # ie. input type is either unknown or inferred incorrectly
-            return map_output_schema
-
-        return super().get_output_schema(config)
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        return self.mapper.config_specs
-
-    def get_graph(self, config: RunnableConfig | None = None) -> Graph:
-        # get graph from mapper
-        graph = self.mapper.get_graph(config)
-        # add passthrough node and edges
-        input_node = graph.first_node()
-        output_node = graph.last_node()
-        if input_node is not None and output_node is not None:
-            passthrough_node = graph.add_node(_graph_passthrough)
-            graph.add_edge(input_node, passthrough_node)
-            graph.add_edge(passthrough_node, output_node)
-        return graph
-
-    def _invoke(
-        self,
-        input: dict[str, Any],
-        run_manager: CallbackManagerForChainRun,
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> dict[str, Any]:
-        if not isinstance(input, dict):
-            msg = "The input to RunnablePassthrough.assign() must be a dict."
-            raise ValueError(msg)  # noqa: TRY004
-
-        return {
-            **input,
-            **self.mapper.invoke(
-                input,
-                patch_config(config, callbacks=run_manager.get_child()),
-                **kwargs,
-            ),
-        }
-
-    def invoke(
-        self,
-        input: dict[str, Any],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> dict[str, Any]:
-        return self._call_with_config(self._invoke, input, config, **kwargs)
-
-    async def _ainvoke(
-        self,
-        input: dict[str, Any],
-        run_manager: AsyncCallbackManagerForChainRun,
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> dict[str, Any]:
-        if not isinstance(input, dict):
-            msg = "The input to RunnablePassthrough.assign() must be a dict."
-            raise ValueError(msg)  # noqa: TRY004
-
-        return {
-            **input,
-            **await self.mapper.ainvoke(
-                input,
-                patch_config(config, callbacks=run_manager.get_child()),
-                **kwargs,
-            ),
-        }
-
-    async def ainvoke(
-        self,
-        input: dict[str, Any],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> dict[str, Any]:
-        return await self._acall_with_config(self._ainvoke, input, config, **kwargs)
-
-    def _transform(
-        self,
-        input: Iterator[dict[str, Any]],
-        run_manager: CallbackManagerForChainRun,
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> Iterator[dict[str, Any]]:
-        # collect mapper keys
-        mapper_keys = set(self.mapper.steps__.keys())
-        # create two streams, one for the map and one for the passthrough
-        for_passthrough, for_map = safetee(input, 2, lock=threading.Lock())
-
-        # create map output stream
-        map_output = self.mapper.transform(
-            for_map,
-            patch_config(
-                config,
-                callbacks=run_manager.get_child(),
-            ),
-            **kwargs,
-        )
-
-        # get executor to start map output stream in background
-        with get_executor_for_config(config) as executor:
-            # start map output stream
-            first_map_chunk_future = executor.submit(
-                next,
-                map_output,  # type: ignore
-                None,
-            )
-            # consume passthrough stream
-            for chunk in for_passthrough:
-                if not isinstance(chunk, dict):
-                    msg = "The input to RunnablePassthrough.assign() must be a dict."
-                    raise ValueError(msg)  # noqa: TRY004
-                # remove mapper keys from passthrough chunk, to be overwritten by map
-                filtered = AddableDict(
-                    {k: v for k, v in chunk.items() if k not in mapper_keys}
-                )
-                if filtered:
-                    yield filtered
-            # yield map output
-            yield cast(dict[str, Any], first_map_chunk_future.result())
-            for chunk in map_output:
-                yield chunk
-
-    def transform(
-        self,
-        input: Iterator[dict[str, Any]],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any | None,
-    ) -> Iterator[dict[str, Any]]:
-        yield from self._transform_stream_with_config(
-            input, self._transform, config, **kwargs
-        )
-
-    async def _atransform(
-        self,
-        input: AsyncIterator[dict[str, Any]],
-        run_manager: AsyncCallbackManagerForChainRun,
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> AsyncIterator[dict[str, Any]]:
-        # collect mapper keys
-        mapper_keys = set(self.mapper.steps__.keys())
-        # create two streams, one for the map and one for the passthrough
-        for_passthrough, for_map = atee(input, 2, lock=asyncio.Lock())
-        # create map output stream
-        map_output = self.mapper.atransform(
-            for_map,
-            patch_config(
-                config,
-                callbacks=run_manager.get_child(),
-            ),
-            **kwargs,
-        )
-        # start map output stream
-        first_map_chunk_task: asyncio.Task = asyncio.create_task(
-            py_anext(map_output, None),  # type: ignore[arg-type]
-        )
-        # consume passthrough stream
-        async for chunk in for_passthrough:
-            if not isinstance(chunk, dict):
-                msg = "The input to RunnablePassthrough.assign() must be a dict."
-                raise ValueError(msg)  # noqa: TRY004
-
-            # remove mapper keys from passthrough chunk, to be overwritten by map output
-            filtered = AddableDict(
-                {k: v for k, v in chunk.items() if k not in mapper_keys}
-            )
-            if filtered:
-                yield filtered
-        # yield map output
-        yield await first_map_chunk_task
-        async for chunk in map_output:
-            yield chunk
-
-    async def atransform(
-        self,
-        input: AsyncIterator[dict[str, Any]],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[dict[str, Any]]:
-        async for chunk in self._atransform_stream_with_config(
-            input, self._atransform, config, **kwargs
-        ):
-            yield chunk
-
-    def stream(
-        self,
-        input: dict[str, Any],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Iterator[dict[str, Any]]:
-        return self.transform(iter([input]), config, **kwargs)
-
-    async def astream(
-        self,
-        input: dict[str, Any],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[dict[str, Any]]:
-        async def input_aiter() -> AsyncIterator[dict[str, Any]]:
-            yield input
-
-        async for chunk in self.atransform(input_aiter(), config, **kwargs):
-            yield chunk
-
-
-class RunnablePick(RunnableSerializable[dict[str, Any], dict[str, Any]]):
-    """Runnable that picks keys from Dict[str, Any] inputs.
-
-    RunnablePick class represents a Runnable that selectively picks keys from a
-    dictionary input. It allows you to specify one or more keys to extract
-    from the input dictionary. It returns a new dictionary containing only
-    the selected keys.
-
-    Parameters:
-        keys (Union[str, List[str]]): A single key or a list of keys to pick from
-            the input dictionary.
-
-    Example :
-        .. code-block:: python
-
-            from langchain_core.runnables.passthrough import RunnablePick
-
-            input_data = {
-                'name': 'John',
-                'age': 30,
-                'city': 'New York',
-                'country': 'USA'
-            }
-
-            runnable = RunnablePick(keys=['name', 'age'])
-
-            output_data = runnable.invoke(input_data)
-
-            print(output_data)  # Output: {'name': 'John', 'age': 30}
-    """
-
-    keys: Union[str, list[str]]
-
-    def __init__(self, keys: Union[str, list[str]], **kwargs: Any) -> None:
-        super().__init__(keys=keys, **kwargs)  # type: ignore[call-arg]
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    def get_name(
-        self, suffix: Optional[str] = None, *, name: Optional[str] = None
-    ) -> str:
-        name = (
-            name
-            or self.name
-            or f"RunnablePick<{','.join([self.keys] if isinstance(self.keys, str) else self.keys)}>"  # noqa: E501
-        )
-        return super().get_name(suffix, name=name)
-
-    def _pick(self, input: dict[str, Any]) -> Any:
-        if not isinstance(input, dict):
-            msg = "The input to RunnablePassthrough.assign() must be a dict."
-            raise ValueError(msg)  # noqa: TRY004
-
-        if isinstance(self.keys, str):
-            return input.get(self.keys)
-        else:
-            picked = {k: input.get(k) for k in self.keys if k in input}
-            if picked:
-                return AddableDict(picked)
-            else:
-                return None
-
-    def _invoke(
-        self,
-        input: dict[str, Any],
-    ) -> dict[str, Any]:
-        return self._pick(input)
-
-    def invoke(
-        self,
-        input: dict[str, Any],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> dict[str, Any]:
-        return self._call_with_config(self._invoke, input, config, **kwargs)
-
-    async def _ainvoke(
-        self,
-        input: dict[str, Any],
-    ) -> dict[str, Any]:
-        return self._pick(input)
-
-    async def ainvoke(
-        self,
-        input: dict[str, Any],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> dict[str, Any]:
-        return await self._acall_with_config(self._ainvoke, input, config, **kwargs)
-
-    def _transform(
-        self,
-        input: Iterator[dict[str, Any]],
-    ) -> Iterator[dict[str, Any]]:
-        for chunk in input:
-            picked = self._pick(chunk)
-            if picked is not None:
-                yield picked
-
-    def transform(
-        self,
-        input: Iterator[dict[str, Any]],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Iterator[dict[str, Any]]:
-        yield from self._transform_stream_with_config(
-            input, self._transform, config, **kwargs
-        )
-
-    async def _atransform(
-        self,
-        input: AsyncIterator[dict[str, Any]],
-    ) -> AsyncIterator[dict[str, Any]]:
-        async for chunk in input:
-            picked = self._pick(chunk)
-            if picked is not None:
-                yield picked
-
-    async def atransform(
-        self,
-        input: AsyncIterator[dict[str, Any]],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[dict[str, Any]]:
-        async for chunk in self._atransform_stream_with_config(
-            input, self._atransform, config, **kwargs
-        ):
-            yield chunk
-
-    def stream(
-        self,
-        input: dict[str, Any],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Iterator[dict[str, Any]]:
-        return self.transform(iter([input]), config, **kwargs)
-
-    async def astream(
-        self,
-        input: dict[str, Any],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[dict[str, Any]]:
-        async def input_aiter() -> AsyncIterator[dict[str, Any]]:
-            yield input
-
-        async for chunk in self.atransform(input_aiter(), config, **kwargs):
-            yield chunk
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/retry.py ./custom_langchain_core/runnables/retry.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/retry.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/retry.py	1970-01-01 09:00:00
@@ -1,337 +0,0 @@
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Optional,
-    TypeVar,
-    Union,
-    cast,
-)
-
-from tenacity import (
-    AsyncRetrying,
-    RetryCallState,
-    RetryError,
-    Retrying,
-    retry_if_exception_type,
-    stop_after_attempt,
-    wait_exponential_jitter,
-)
-
-from langchain_core.runnables.base import Input, Output, RunnableBindingBase
-from langchain_core.runnables.config import RunnableConfig, patch_config
-
-if TYPE_CHECKING:
-    from langchain_core.callbacks.manager import (
-        AsyncCallbackManagerForChainRun,
-        CallbackManagerForChainRun,
-    )
-
-    T = TypeVar("T", CallbackManagerForChainRun, AsyncCallbackManagerForChainRun)
-U = TypeVar("U")
-
-
-class RunnableRetry(RunnableBindingBase[Input, Output]):
-    """Retry a Runnable if it fails.
-
-    RunnableRetry can be used to add retry logic to any object
-    that subclasses the base Runnable.
-
-    Such retries are especially useful for network calls that may fail
-    due to transient errors.
-
-    The RunnableRetry is implemented as a RunnableBinding. The easiest
-    way to use it is through the `.with_retry()` method on all Runnables.
-
-    Example:
-    Here's an example that uses a RunnableLambda to raise an exception
-
-        .. code-block:: python
-
-            import time
-
-            def foo(input) -> None:
-                '''Fake function that raises an exception.'''
-                raise ValueError(f"Invoking foo failed. At time {time.time()}")
-
-            runnable = RunnableLambda(foo)
-
-            runnable_with_retries = runnable.with_retry(
-                retry_if_exception_type=(ValueError,), # Retry only on ValueError
-                wait_exponential_jitter=True, # Add jitter to the exponential backoff
-                stop_after_attempt=2, # Try twice
-            )
-
-            # The method invocation above is equivalent to the longer form below:
-
-            runnable_with_retries = RunnableRetry(
-                bound=runnable,
-                retry_exception_types=(ValueError,),
-                max_attempt_number=2,
-                wait_exponential_jitter=True
-            )
-
-    This logic can be used to retry any Runnable, including a chain of Runnables,
-    but in general it's best practice to keep the scope of the retry as small as
-    possible. For example, if you have a chain of Runnables, you should only retry
-    the Runnable that is likely to fail, not the entire chain.
-
-    Example:
-
-        .. code-block:: python
-
-            from langchain_core.chat_models import ChatOpenAI
-            from langchain_core.prompts import PromptTemplate
-
-            template = PromptTemplate.from_template("tell me a joke about {topic}.")
-            model = ChatOpenAI(temperature=0.5)
-
-            # Good
-            chain = template | model.with_retry()
-
-            # Bad
-            chain = template | model
-            retryable_chain = chain.with_retry()
-    """
-
-    retry_exception_types: tuple[type[BaseException], ...] = (Exception,)
-    """The exception types to retry on. By default all exceptions are retried.
-
-    In general you should only retry on exceptions that are likely to be
-    transient, such as network errors.
-
-    Good exceptions to retry are all server errors (5xx) and selected client
-    errors (4xx) such as 429 Too Many Requests.
-    """
-
-    wait_exponential_jitter: bool = True
-    """Whether to add jitter to the exponential backoff."""
-
-    max_attempt_number: int = 3
-    """The maximum number of attempts to retry the Runnable."""
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    @property
-    def _kwargs_retrying(self) -> dict[str, Any]:
-        kwargs: dict[str, Any] = {}
-
-        if self.max_attempt_number:
-            kwargs["stop"] = stop_after_attempt(self.max_attempt_number)
-
-        if self.wait_exponential_jitter:
-            kwargs["wait"] = wait_exponential_jitter()
-
-        if self.retry_exception_types:
-            kwargs["retry"] = retry_if_exception_type(self.retry_exception_types)
-
-        return kwargs
-
-    def _sync_retrying(self, **kwargs: Any) -> Retrying:
-        return Retrying(**self._kwargs_retrying, **kwargs)
-
-    def _async_retrying(self, **kwargs: Any) -> AsyncRetrying:
-        return AsyncRetrying(**self._kwargs_retrying, **kwargs)
-
-    def _patch_config(
-        self,
-        config: RunnableConfig,
-        run_manager: "T",
-        retry_state: RetryCallState,
-    ) -> RunnableConfig:
-        attempt = retry_state.attempt_number
-        tag = f"retry:attempt:{attempt}" if attempt > 1 else None
-        return patch_config(config, callbacks=run_manager.get_child(tag))
-
-    def _patch_config_list(
-        self,
-        config: list[RunnableConfig],
-        run_manager: list["T"],
-        retry_state: RetryCallState,
-    ) -> list[RunnableConfig]:
-        return [
-            self._patch_config(c, rm, retry_state) for c, rm in zip(config, run_manager)
-        ]
-
-    def _invoke(
-        self,
-        input: Input,
-        run_manager: "CallbackManagerForChainRun",
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> Output:
-        for attempt in self._sync_retrying(reraise=True):
-            with attempt:
-                result = super().invoke(
-                    input,
-                    self._patch_config(config, run_manager, attempt.retry_state),
-                    **kwargs,
-                )
-            if attempt.retry_state.outcome and not attempt.retry_state.outcome.failed:
-                attempt.retry_state.set_result(result)
-        return result
-
-    def invoke(
-        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Output:
-        return self._call_with_config(self._invoke, input, config, **kwargs)
-
-    async def _ainvoke(
-        self,
-        input: Input,
-        run_manager: "AsyncCallbackManagerForChainRun",
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> Output:
-        async for attempt in self._async_retrying(reraise=True):
-            with attempt:
-                result = await super().ainvoke(
-                    input,
-                    self._patch_config(config, run_manager, attempt.retry_state),
-                    **kwargs,
-                )
-            if attempt.retry_state.outcome and not attempt.retry_state.outcome.failed:
-                attempt.retry_state.set_result(result)
-        return result
-
-    async def ainvoke(
-        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Output:
-        return await self._acall_with_config(self._ainvoke, input, config, **kwargs)
-
-    def _batch(
-        self,
-        inputs: list[Input],
-        run_manager: list["CallbackManagerForChainRun"],
-        config: list[RunnableConfig],
-        **kwargs: Any,
-    ) -> list[Union[Output, Exception]]:
-        results_map: dict[int, Output] = {}
-
-        def pending(iterable: list[U]) -> list[U]:
-            return [item for idx, item in enumerate(iterable) if idx not in results_map]
-
-        not_set: list[Output] = []
-        result = not_set
-        try:
-            for attempt in self._sync_retrying():
-                with attempt:
-                    # Get the results of the inputs that have not succeeded yet.
-                    result = super().batch(
-                        pending(inputs),
-                        self._patch_config_list(
-                            pending(config), pending(run_manager), attempt.retry_state
-                        ),
-                        return_exceptions=True,
-                        **kwargs,
-                    )
-                    # Register the results of the inputs that have succeeded.
-                    first_exception = None
-                    for i, r in enumerate(result):
-                        if isinstance(r, Exception):
-                            if not first_exception:
-                                first_exception = r
-                            continue
-                        results_map[i] = r
-                    # If any exception occurred, raise it, to retry the failed ones
-                    if first_exception:
-                        raise first_exception
-                if (
-                    attempt.retry_state.outcome
-                    and not attempt.retry_state.outcome.failed
-                ):
-                    attempt.retry_state.set_result(result)
-        except RetryError as e:
-            if result is not_set:
-                result = cast(list[Output], [e] * len(inputs))
-
-        outputs: list[Union[Output, Exception]] = []
-        for idx in range(len(inputs)):
-            if idx in results_map:
-                outputs.append(results_map[idx])
-            else:
-                outputs.append(result.pop(0))
-        return outputs
-
-    def batch(
-        self,
-        inputs: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Any,
-    ) -> list[Output]:
-        return self._batch_with_config(
-            self._batch, inputs, config, return_exceptions=return_exceptions, **kwargs
-        )
-
-    async def _abatch(
-        self,
-        inputs: list[Input],
-        run_manager: list["AsyncCallbackManagerForChainRun"],
-        config: list[RunnableConfig],
-        **kwargs: Any,
-    ) -> list[Union[Output, Exception]]:
-        results_map: dict[int, Output] = {}
-
-        def pending(iterable: list[U]) -> list[U]:
-            return [item for idx, item in enumerate(iterable) if idx not in results_map]
-
-        not_set: list[Output] = []
-        result = not_set
-        try:
-            async for attempt in self._async_retrying():
-                with attempt:
-                    # Get the results of the inputs that have not succeeded yet.
-                    result = await super().abatch(
-                        pending(inputs),
-                        self._patch_config_list(
-                            pending(config), pending(run_manager), attempt.retry_state
-                        ),
-                        return_exceptions=True,
-                        **kwargs,
-                    )
-                    # Register the results of the inputs that have succeeded.
-                    first_exception = None
-                    for i, r in enumerate(result):
-                        if isinstance(r, Exception):
-                            if not first_exception:
-                                first_exception = r
-                            continue
-                        results_map[i] = r
-                    # If any exception occurred, raise it, to retry the failed ones
-                    if first_exception:
-                        raise first_exception
-                if (
-                    attempt.retry_state.outcome
-                    and not attempt.retry_state.outcome.failed
-                ):
-                    attempt.retry_state.set_result(result)
-        except RetryError as e:
-            if result is not_set:
-                result = cast(list[Output], [e] * len(inputs))
-
-        outputs: list[Union[Output, Exception]] = []
-        for idx in range(len(inputs)):
-            if idx in results_map:
-                outputs.append(results_map[idx])
-            else:
-                outputs.append(result.pop(0))
-        return outputs
-
-    async def abatch(
-        self,
-        inputs: list[Input],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Any,
-    ) -> list[Output]:
-        return await self._abatch_with_config(
-            self._abatch, inputs, config, return_exceptions=return_exceptions, **kwargs
-        )
-
-    # stream() and transform() are not retried because retrying a stream
-    # is not very intuitive.
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/router.py ./custom_langchain_core/runnables/router.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/router.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/router.py	1970-01-01 09:00:00
@@ -1,225 +0,0 @@
-from __future__ import annotations
-
-from collections.abc import AsyncIterator, Iterator, Mapping
-from itertools import starmap
-from typing import (
-    Any,
-    Callable,
-    Optional,
-    Union,
-    cast,
-)
-
-from pydantic import ConfigDict
-from typing_extensions import TypedDict
-
-from langchain_core.runnables.base import (
-    Input,
-    Output,
-    Runnable,
-    RunnableSerializable,
-    coerce_to_runnable,
-)
-from langchain_core.runnables.config import (
-    RunnableConfig,
-    get_config_list,
-    get_executor_for_config,
-)
-from langchain_core.runnables.utils import (
-    ConfigurableFieldSpec,
-    gather_with_concurrency,
-    get_unique_config_specs,
-)
-
-
-class RouterInput(TypedDict):
-    """Router input.
-
-    Attributes:
-        key: The key to route on.
-        input: The input to pass to the selected Runnable.
-    """
-
-    key: str
-    input: Any
-
-
-class RouterRunnable(RunnableSerializable[RouterInput, Output]):
-    """Runnable that routes to a set of Runnables based on Input['key'].
-    Returns the output of the selected Runnable.
-
-    Parameters:
-        runnables: A mapping of keys to Runnables.
-
-    For example,
-
-    .. code-block:: python
-
-        from langchain_core.runnables.router import RouterRunnable
-        from langchain_core.runnables import RunnableLambda
-
-        add = RunnableLambda(func=lambda x: x + 1)
-        square = RunnableLambda(func=lambda x: x**2)
-
-        router = RouterRunnable(runnables={"add": add, "square": square})
-        router.invoke({"key": "square", "input": 3})
-    """
-
-    runnables: Mapping[str, Runnable[Any, Output]]
-
-    @property
-    def config_specs(self) -> list[ConfigurableFieldSpec]:
-        return get_unique_config_specs(
-            spec for step in self.runnables.values() for spec in step.config_specs
-        )
-
-    def __init__(
-        self,
-        runnables: Mapping[str, Union[Runnable[Any, Output], Callable[[Any], Output]]],
-    ) -> None:
-        super().__init__(  # type: ignore[call-arg]
-            runnables={key: coerce_to_runnable(r) for key, r in runnables.items()}
-        )
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Return whether this class is serializable."""
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> list[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-
-    def invoke(
-        self, input: RouterInput, config: Optional[RunnableConfig] = None, **kwargs: Any
-    ) -> Output:
-        key = input["key"]
-        actual_input = input["input"]
-        if key not in self.runnables:
-            msg = f"No runnable associated with key '{key}'"
-            raise ValueError(msg)
-
-        runnable = self.runnables[key]
-        return runnable.invoke(actual_input, config)
-
-    async def ainvoke(
-        self,
-        input: RouterInput,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Output:
-        key = input["key"]
-        actual_input = input["input"]
-        if key not in self.runnables:
-            msg = f"No runnable associated with key '{key}'"
-            raise ValueError(msg)
-
-        runnable = self.runnables[key]
-        return await runnable.ainvoke(actual_input, config)
-
-    def batch(
-        self,
-        inputs: list[RouterInput],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        if not inputs:
-            return []
-
-        keys = [input["key"] for input in inputs]
-        actual_inputs = [input["input"] for input in inputs]
-        if any(key not in self.runnables for key in keys):
-            msg = "One or more keys do not have a corresponding runnable"
-            raise ValueError(msg)
-
-        def invoke(
-            runnable: Runnable, input: Input, config: RunnableConfig
-        ) -> Union[Output, Exception]:
-            if return_exceptions:
-                try:
-                    return runnable.invoke(input, config, **kwargs)
-                except Exception as e:
-                    return e
-            else:
-                return runnable.invoke(input, config, **kwargs)
-
-        runnables = [self.runnables[key] for key in keys]
-        configs = get_config_list(config, len(inputs))
-        with get_executor_for_config(configs[0]) as executor:
-            return cast(
-                list[Output],
-                list(executor.map(invoke, runnables, actual_inputs, configs)),
-            )
-
-    async def abatch(
-        self,
-        inputs: list[RouterInput],
-        config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
-        *,
-        return_exceptions: bool = False,
-        **kwargs: Optional[Any],
-    ) -> list[Output]:
-        if not inputs:
-            return []
-
-        keys = [input["key"] for input in inputs]
-        actual_inputs = [input["input"] for input in inputs]
-        if any(key not in self.runnables for key in keys):
-            msg = "One or more keys do not have a corresponding runnable"
-            raise ValueError(msg)
-
-        async def ainvoke(
-            runnable: Runnable, input: Input, config: RunnableConfig
-        ) -> Union[Output, Exception]:
-            if return_exceptions:
-                try:
-                    return await runnable.ainvoke(input, config, **kwargs)
-                except Exception as e:
-                    return e
-            else:
-                return await runnable.ainvoke(input, config, **kwargs)
-
-        runnables = [self.runnables[key] for key in keys]
-        configs = get_config_list(config, len(inputs))
-        return await gather_with_concurrency(
-            configs[0].get("max_concurrency"),
-            *starmap(ainvoke, zip(runnables, actual_inputs, configs)),
-        )
-
-    def stream(
-        self,
-        input: RouterInput,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        key = input["key"]
-        actual_input = input["input"]
-        if key not in self.runnables:
-            msg = f"No runnable associated with key '{key}'"
-            raise ValueError(msg)
-
-        runnable = self.runnables[key]
-        yield from runnable.stream(actual_input, config)
-
-    async def astream(
-        self,
-        input: RouterInput,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        key = input["key"]
-        actual_input = input["input"]
-        if key not in self.runnables:
-            msg = f"No runnable associated with key '{key}'"
-            raise ValueError(msg)
-
-        runnable = self.runnables[key]
-        async for output in runnable.astream(actual_input, config):
-            yield output
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/schema.py ./custom_langchain_core/runnables/schema.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/schema.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/schema.py	1970-01-01 09:00:00
@@ -1,175 +0,0 @@
-"""Module contains typedefs that are used with Runnables."""
-
-from __future__ import annotations
-
-from collections.abc import Sequence
-from typing import Any, Literal, Union
-
-from typing_extensions import NotRequired, TypedDict
-
-
-class EventData(TypedDict, total=False):
-    """Data associated with a streaming event."""
-
-    input: Any
-    """The input passed to the Runnable that generated the event.
-
-    Inputs will sometimes be available at the *START* of the Runnable, and
-    sometimes at the *END* of the Runnable.
-
-    If a Runnable is able to stream its inputs, then its input by definition
-    won't be known until the *END* of the Runnable when it has finished streaming
-    its inputs.
-    """
-    output: Any
-    """The output of the Runnable that generated the event.
-
-    Outputs will only be available at the *END* of the Runnable.
-
-    For most Runnables, this field can be inferred from the `chunk` field,
-    though there might be some exceptions for special cased Runnables (e.g., like
-    chat models), which may return more information.
-    """
-    chunk: Any
-    """A streaming chunk from the output that generated the event.
-
-    chunks support addition in general, and adding them up should result
-    in the output of the Runnable that generated the event.
-    """
-
-
-class BaseStreamEvent(TypedDict):
-    """Streaming event.
-
-    Schema of a streaming event which is produced from the astream_events method.
-
-    Example:
-
-        .. code-block:: python
-
-            from langchain_core.runnables import RunnableLambda
-
-            async def reverse(s: str) -> str:
-                return s[::-1]
-
-            chain = RunnableLambda(func=reverse)
-
-            events = [event async for event in chain.astream_events("hello")]
-
-            # will produce the following events
-            # (where some fields have been omitted for brevity):
-            [
-                {
-                    "data": {"input": "hello"},
-                    "event": "on_chain_start",
-                    "metadata": {},
-                    "name": "reverse",
-                    "tags": [],
-                },
-                {
-                    "data": {"chunk": "olleh"},
-                    "event": "on_chain_stream",
-                    "metadata": {},
-                    "name": "reverse",
-                    "tags": [],
-                },
-                {
-                    "data": {"output": "olleh"},
-                    "event": "on_chain_end",
-                    "metadata": {},
-                    "name": "reverse",
-                    "tags": [],
-                },
-            ]
-    """
-
-    event: str
-    """Event names are of the format: on_[runnable_type]_(start|stream|end).
-
-    Runnable types are one of:
-
-    - **llm** - used by non chat models
-    - **chat_model** - used by chat models
-    - **prompt** --  e.g., ChatPromptTemplate
-    - **tool** -- from tools defined via @tool decorator or inheriting
-        from Tool/BaseTool
-    - **chain** - most Runnables are of this type
-
-    Further, the events are categorized as one of:
-
-    - **start** - when the Runnable starts
-    - **stream** - when the Runnable is streaming
-    - **end* - when the Runnable ends
-
-    start, stream and end are associated with slightly different `data` payload.
-
-    Please see the documentation for `EventData` for more details.
-    """
-    run_id: str
-    """An randomly generated ID to keep track of the execution of the given Runnable.
-
-    Each child Runnable that gets invoked as part of the execution of a parent Runnable
-    is assigned its own unique ID.
-    """
-    tags: NotRequired[list[str]]
-    """Tags associated with the Runnable that generated this event.
-
-    Tags are always inherited from parent Runnables.
-
-    Tags can either be bound to a Runnable using `.with_config({"tags":  ["hello"]})`
-    or passed at run time using `.astream_events(..., {"tags": ["hello"]})`.
-    """
-    metadata: NotRequired[dict[str, Any]]
-    """Metadata associated with the Runnable that generated this event.
-
-    Metadata can either be bound to a Runnable using
-
-        `.with_config({"metadata": { "foo": "bar" }})`
-
-    or passed at run time using
-
-        `.astream_events(..., {"metadata": {"foo": "bar"}})`.
-    """
-
-    parent_ids: Sequence[str]
-    """A list of the parent IDs associated with this event.
-
-    Root Events will have an empty list.
-
-    For example, if a Runnable A calls Runnable B, then the event generated by Runnable
-    B will have Runnable A's ID in the parent_ids field.
-
-    The order of the parent IDs is from the root parent to the immediate parent.
-
-    Only supported as of v2 of the astream events API. v1 will return an empty list.
-    """
-
-
-class StandardStreamEvent(BaseStreamEvent):
-    """A standard stream event that follows LangChain convention for event data."""
-
-    data: EventData
-    """Event data.
-
-    The contents of the event data depend on the event type.
-    """
-    name: str
-    """The name of the Runnable that generated the event."""
-
-
-class CustomStreamEvent(BaseStreamEvent):
-    """Custom stream event created by the user.
-
-    .. versionadded:: 0.2.15
-    """
-
-    # Overwrite the event field to be more specific.
-    event: Literal["on_custom_event"]  # type: ignore[misc]
-    """The event type."""
-    name: str
-    """User defined name for the event."""
-    data: Any
-    """The data associated with the event. Free form and can be anything."""
-
-
-StreamEvent = Union[StandardStreamEvent, CustomStreamEvent]
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/runnables/utils.py ./custom_langchain_core/runnables/utils.py
--- .venv/lib/python3.12/site-packages/langchain_core/runnables/utils.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/runnables/utils.py	1970-01-01 09:00:00
@@ -1,758 +0,0 @@
-"""Utility code for runnables."""
-
-from __future__ import annotations
-
-import ast
-import asyncio
-import inspect
-import textwrap
-from collections.abc import (
-    AsyncIterable,
-    AsyncIterator,
-    Awaitable,
-    Coroutine,
-    Iterable,
-    Mapping,
-    Sequence,
-)
-from functools import lru_cache
-from inspect import signature
-from itertools import groupby
-from typing import (
-    Any,
-    Callable,
-    NamedTuple,
-    Optional,
-    Protocol,
-    TypeVar,
-    Union,
-)
-
-from typing_extensions import TypeGuard, override
-
-from langchain_core.runnables.schema import StreamEvent
-
-# Re-export create-model for backwards compatibility
-from langchain_core.utils.pydantic import create_model as create_model
-
-Input = TypeVar("Input", contravariant=True)
-# Output type should implement __concat__, as eg str, list, dict do
-Output = TypeVar("Output", covariant=True)
-
-
-async def gated_coro(semaphore: asyncio.Semaphore, coro: Coroutine) -> Any:
-    """Run a coroutine with a semaphore.
-
-    Args:
-        semaphore: The semaphore to use.
-        coro: The coroutine to run.
-
-    Returns:
-        The result of the coroutine.
-    """
-    async with semaphore:
-        return await coro
-
-
-async def gather_with_concurrency(n: Union[int, None], *coros: Coroutine) -> list:
-    """Gather coroutines with a limit on the number of concurrent coroutines.
-
-    Args:
-        n: The number of coroutines to run concurrently.
-        *coros: The coroutines to run.
-
-    Returns:
-        The results of the coroutines.
-    """
-    if n is None:
-        return await asyncio.gather(*coros)
-
-    semaphore = asyncio.Semaphore(n)
-
-    return await asyncio.gather(*(gated_coro(semaphore, c) for c in coros))
-
-
-def accepts_run_manager(callable: Callable[..., Any]) -> bool:
-    """Check if a callable accepts a run_manager argument.
-
-    Args:
-        callable: The callable to check.
-
-    Returns:
-        bool: True if the callable accepts a run_manager argument, False otherwise.
-    """
-    try:
-        return signature(callable).parameters.get("run_manager") is not None
-    except ValueError:
-        return False
-
-
-def accepts_config(callable: Callable[..., Any]) -> bool:
-    """Check if a callable accepts a config argument.
-
-    Args:
-        callable: The callable to check.
-
-    Returns:
-        bool: True if the callable accepts a config argument, False otherwise.
-    """
-    try:
-        return signature(callable).parameters.get("config") is not None
-    except ValueError:
-        return False
-
-
-def accepts_context(callable: Callable[..., Any]) -> bool:
-    """Check if a callable accepts a context argument.
-
-    Args:
-        callable: The callable to check.
-
-    Returns:
-        bool: True if the callable accepts a context argument, False otherwise.
-    """
-    try:
-        return signature(callable).parameters.get("context") is not None
-    except ValueError:
-        return False
-
-
-@lru_cache(maxsize=1)
-def asyncio_accepts_context() -> bool:
-    return accepts_context(asyncio.create_task)
-
-
-class IsLocalDict(ast.NodeVisitor):
-    """Check if a name is a local dict."""
-
-    def __init__(self, name: str, keys: set[str]) -> None:
-        """Initialize the visitor.
-
-        Args:
-            name: The name to check.
-            keys: The keys to populate.
-        """
-        self.name = name
-        self.keys = keys
-
-    @override
-    def visit_Subscript(self, node: ast.Subscript) -> Any:
-        """Visit a subscript node.
-
-        Args:
-            node: The node to visit.
-
-        Returns:
-            Any: The result of the visit.
-        """
-        if (
-            isinstance(node.ctx, ast.Load)
-            and isinstance(node.value, ast.Name)
-            and node.value.id == self.name
-            and isinstance(node.slice, ast.Constant)
-            and isinstance(node.slice.value, str)
-        ):
-            # we've found a subscript access on the name we're looking for
-            self.keys.add(node.slice.value)
-
-    @override
-    def visit_Call(self, node: ast.Call) -> Any:
-        """Visit a call node.
-
-        Args:
-            node: The node to visit.
-
-        Returns:
-            Any: The result of the visit.
-        """
-        if (
-            isinstance(node.func, ast.Attribute)
-            and isinstance(node.func.value, ast.Name)
-            and node.func.value.id == self.name
-            and node.func.attr == "get"
-            and len(node.args) in (1, 2)
-            and isinstance(node.args[0], ast.Constant)
-            and isinstance(node.args[0].value, str)
-        ):
-            # we've found a .get() call on the name we're looking for
-            self.keys.add(node.args[0].value)
-
-
-class IsFunctionArgDict(ast.NodeVisitor):
-    """Check if the first argument of a function is a dict."""
-
-    def __init__(self) -> None:
-        self.keys: set[str] = set()
-
-    @override
-    def visit_Lambda(self, node: ast.Lambda) -> Any:
-        """Visit a lambda function.
-
-        Args:
-            node: The node to visit.
-
-        Returns:
-            Any: The result of the visit.
-        """
-        if not node.args.args:
-            return
-        input_arg_name = node.args.args[0].arg
-        IsLocalDict(input_arg_name, self.keys).visit(node.body)
-
-    @override
-    def visit_FunctionDef(self, node: ast.FunctionDef) -> Any:
-        """Visit a function definition.
-
-        Args:
-            node: The node to visit.
-
-        Returns:
-            Any: The result of the visit.
-        """
-        if not node.args.args:
-            return
-        input_arg_name = node.args.args[0].arg
-        IsLocalDict(input_arg_name, self.keys).visit(node)
-
-    @override
-    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> Any:
-        """Visit an async function definition.
-
-        Args:
-            node: The node to visit.
-
-        Returns:
-            Any: The result of the visit.
-        """
-        if not node.args.args:
-            return
-        input_arg_name = node.args.args[0].arg
-        IsLocalDict(input_arg_name, self.keys).visit(node)
-
-
-class NonLocals(ast.NodeVisitor):
-    """Get nonlocal variables accessed."""
-
-    def __init__(self) -> None:
-        self.loads: set[str] = set()
-        self.stores: set[str] = set()
-
-    @override
-    def visit_Name(self, node: ast.Name) -> Any:
-        """Visit a name node.
-
-        Args:
-            node: The node to visit.
-
-        Returns:
-            Any: The result of the visit.
-        """
-        if isinstance(node.ctx, ast.Load):
-            self.loads.add(node.id)
-        elif isinstance(node.ctx, ast.Store):
-            self.stores.add(node.id)
-
-    @override
-    def visit_Attribute(self, node: ast.Attribute) -> Any:
-        """Visit an attribute node.
-
-        Args:
-            node: The node to visit.
-
-        Returns:
-            Any: The result of the visit.
-        """
-        if isinstance(node.ctx, ast.Load):
-            parent = node.value
-            attr_expr = node.attr
-            while isinstance(parent, ast.Attribute):
-                attr_expr = parent.attr + "." + attr_expr
-                parent = parent.value
-            if isinstance(parent, ast.Name):
-                self.loads.add(parent.id + "." + attr_expr)
-                self.loads.discard(parent.id)
-            elif isinstance(parent, ast.Call):
-                if isinstance(parent.func, ast.Name):
-                    self.loads.add(parent.func.id)
-                else:
-                    parent = parent.func
-                    attr_expr = ""
-                    while isinstance(parent, ast.Attribute):
-                        if attr_expr:
-                            attr_expr = parent.attr + "." + attr_expr
-                        else:
-                            attr_expr = parent.attr
-                        parent = parent.value
-                    if isinstance(parent, ast.Name):
-                        self.loads.add(parent.id + "." + attr_expr)
-
-
-class FunctionNonLocals(ast.NodeVisitor):
-    """Get the nonlocal variables accessed of a function."""
-
-    def __init__(self) -> None:
-        self.nonlocals: set[str] = set()
-
-    @override
-    def visit_FunctionDef(self, node: ast.FunctionDef) -> Any:
-        """Visit a function definition.
-
-        Args:
-            node: The node to visit.
-
-        Returns:
-            Any: The result of the visit.
-        """
-        visitor = NonLocals()
-        visitor.visit(node)
-        self.nonlocals.update(visitor.loads - visitor.stores)
-
-    @override
-    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> Any:
-        """Visit an async function definition.
-
-        Args:
-            node: The node to visit.
-
-        Returns:
-            Any: The result of the visit.
-        """
-        visitor = NonLocals()
-        visitor.visit(node)
-        self.nonlocals.update(visitor.loads - visitor.stores)
-
-    @override
-    def visit_Lambda(self, node: ast.Lambda) -> Any:
-        """Visit a lambda function.
-
-        Args:
-            node: The node to visit.
-
-        Returns:
-            Any: The result of the visit.
-        """
-        visitor = NonLocals()
-        visitor.visit(node)
-        self.nonlocals.update(visitor.loads - visitor.stores)
-
-
-class GetLambdaSource(ast.NodeVisitor):
-    """Get the source code of a lambda function."""
-
-    def __init__(self) -> None:
-        """Initialize the visitor."""
-        self.source: Optional[str] = None
-        self.count = 0
-
-    @override
-    def visit_Lambda(self, node: ast.Lambda) -> Any:
-        """Visit a lambda function.
-
-        Args:
-            node: The node to visit.
-
-        Returns:
-            Any: The result of the visit.
-        """
-        self.count += 1
-        if hasattr(ast, "unparse"):
-            self.source = ast.unparse(node)
-
-
-def get_function_first_arg_dict_keys(func: Callable) -> Optional[list[str]]:
-    """Get the keys of the first argument of a function if it is a dict.
-
-    Args:
-        func: The function to check.
-
-    Returns:
-        Optional[List[str]]: The keys of the first argument if it is a dict,
-            None otherwise.
-    """
-    try:
-        code = inspect.getsource(func)
-        tree = ast.parse(textwrap.dedent(code))
-        visitor = IsFunctionArgDict()
-        visitor.visit(tree)
-        return sorted(visitor.keys) if visitor.keys else None
-    except (SyntaxError, TypeError, OSError, SystemError):
-        return None
-
-
-def get_lambda_source(func: Callable) -> Optional[str]:
-    """Get the source code of a lambda function.
-
-    Args:
-        func: a Callable that can be a lambda function.
-
-    Returns:
-        str: the source code of the lambda function.
-    """
-    try:
-        name = func.__name__ if func.__name__ != "<lambda>" else None
-    except AttributeError:
-        name = None
-    try:
-        code = inspect.getsource(func)
-        tree = ast.parse(textwrap.dedent(code))
-        visitor = GetLambdaSource()
-        visitor.visit(tree)
-    except (SyntaxError, TypeError, OSError, SystemError):
-        return name
-    return visitor.source if visitor.count == 1 else name
-
-
-@lru_cache(maxsize=256)
-def get_function_nonlocals(func: Callable) -> list[Any]:
-    """Get the nonlocal variables accessed by a function.
-
-    Args:
-        func: The function to check.
-
-    Returns:
-        List[Any]: The nonlocal variables accessed by the function.
-    """
-    try:
-        code = inspect.getsource(func)
-        tree = ast.parse(textwrap.dedent(code))
-        visitor = FunctionNonLocals()
-        visitor.visit(tree)
-        values: list[Any] = []
-        closure = (
-            inspect.getclosurevars(func.__wrapped__)
-            if hasattr(func, "__wrapped__") and callable(func.__wrapped__)
-            else inspect.getclosurevars(func)
-        )
-        candidates = {**closure.globals, **closure.nonlocals}
-        for k, v in candidates.items():
-            if k in visitor.nonlocals:
-                values.append(v)
-            for kk in visitor.nonlocals:
-                if "." in kk and kk.startswith(k):
-                    vv = v
-                    for part in kk.split(".")[1:]:
-                        if vv is None:
-                            break
-                        else:
-                            try:
-                                vv = getattr(vv, part)
-                            except AttributeError:
-                                break
-                    else:
-                        values.append(vv)
-    except (SyntaxError, TypeError, OSError, SystemError):
-        return []
-
-    return values
-
-
-def indent_lines_after_first(text: str, prefix: str) -> str:
-    """Indent all lines of text after the first line.
-
-    Args:
-        text: The text to indent.
-        prefix: Used to determine the number of spaces to indent.
-
-    Returns:
-        str: The indented text.
-    """
-    n_spaces = len(prefix)
-    spaces = " " * n_spaces
-    lines = text.splitlines()
-    return "\n".join([lines[0]] + [spaces + line for line in lines[1:]])
-
-
-class AddableDict(dict[str, Any]):
-    """Dictionary that can be added to another dictionary."""
-
-    def __add__(self, other: AddableDict) -> AddableDict:
-        chunk = AddableDict(self)
-        for key in other:
-            if key not in chunk or chunk[key] is None:
-                chunk[key] = other[key]
-            elif other[key] is not None:
-                try:
-                    added = chunk[key] + other[key]
-                except TypeError:
-                    added = other[key]
-                chunk[key] = added
-        return chunk
-
-    def __radd__(self, other: AddableDict) -> AddableDict:
-        chunk = AddableDict(other)
-        for key in self:
-            if key not in chunk or chunk[key] is None:
-                chunk[key] = self[key]
-            elif self[key] is not None:
-                try:
-                    added = chunk[key] + self[key]
-                except TypeError:
-                    added = self[key]
-                chunk[key] = added
-        return chunk
-
-
-_T_co = TypeVar("_T_co", covariant=True)
-_T_contra = TypeVar("_T_contra", contravariant=True)
-
-
-class SupportsAdd(Protocol[_T_contra, _T_co]):
-    """Protocol for objects that support addition."""
-
-    def __add__(self, __x: _T_contra) -> _T_co: ...
-
-
-Addable = TypeVar("Addable", bound=SupportsAdd[Any, Any])
-
-
-def add(addables: Iterable[Addable]) -> Optional[Addable]:
-    """Add a sequence of addable objects together.
-
-    Args:
-        addables: The addable objects to add.
-
-    Returns:
-        Optional[Addable]: The result of adding the addable objects.
-    """
-    final: Optional[Addable] = None
-    for chunk in addables:
-        final = chunk if final is None else final + chunk
-    return final
-
-
-async def aadd(addables: AsyncIterable[Addable]) -> Optional[Addable]:
-    """Asynchronously add a sequence of addable objects together.
-
-    Args:
-        addables: The addable objects to add.
-
-    Returns:
-        Optional[Addable]: The result of adding the addable objects.
-    """
-    final: Optional[Addable] = None
-    async for chunk in addables:
-        final = chunk if final is None else final + chunk
-    return final
-
-
-class ConfigurableField(NamedTuple):
-    """Field that can be configured by the user.
-
-    Parameters:
-        id: The unique identifier of the field.
-        name: The name of the field. Defaults to None.
-        description: The description of the field. Defaults to None.
-        annotation: The annotation of the field. Defaults to None.
-        is_shared: Whether the field is shared. Defaults to False.
-    """
-
-    id: str
-
-    name: Optional[str] = None
-    description: Optional[str] = None
-    annotation: Optional[Any] = None
-    is_shared: bool = False
-
-    def __hash__(self) -> int:
-        return hash((self.id, self.annotation))
-
-
-class ConfigurableFieldSingleOption(NamedTuple):
-    """Field that can be configured by the user with a default value.
-
-    Parameters:
-        id: The unique identifier of the field.
-        options: The options for the field.
-        default: The default value for the field.
-        name: The name of the field. Defaults to None.
-        description: The description of the field. Defaults to None.
-        is_shared: Whether the field is shared. Defaults to False.
-    """
-
-    id: str
-    options: Mapping[str, Any]
-    default: str
-
-    name: Optional[str] = None
-    description: Optional[str] = None
-    is_shared: bool = False
-
-    def __hash__(self) -> int:
-        return hash((self.id, tuple(self.options.keys()), self.default))
-
-
-class ConfigurableFieldMultiOption(NamedTuple):
-    """Field that can be configured by the user with multiple default values.
-
-    Parameters:
-        id: The unique identifier of the field.
-        options: The options for the field.
-        default: The default values for the field.
-        name: The name of the field. Defaults to None.
-        description: The description of the field. Defaults to None.
-        is_shared: Whether the field is shared. Defaults to False.
-    """
-
-    id: str
-    options: Mapping[str, Any]
-    default: Sequence[str]
-
-    name: Optional[str] = None
-    description: Optional[str] = None
-    is_shared: bool = False
-
-    def __hash__(self) -> int:
-        return hash((self.id, tuple(self.options.keys()), tuple(self.default)))
-
-
-AnyConfigurableField = Union[
-    ConfigurableField, ConfigurableFieldSingleOption, ConfigurableFieldMultiOption
-]
-
-
-class ConfigurableFieldSpec(NamedTuple):
-    """Field that can be configured by the user. It is a specification of a field.
-
-    Parameters:
-        id: The unique identifier of the field.
-        annotation: The annotation of the field.
-        name: The name of the field. Defaults to None.
-        description: The description of the field. Defaults to None.
-        default: The default value for the field. Defaults to None.
-        is_shared: Whether the field is shared. Defaults to False.
-        dependencies: The dependencies of the field. Defaults to None.
-    """
-
-    id: str
-    annotation: Any
-
-    name: Optional[str] = None
-    description: Optional[str] = None
-    default: Any = None
-    is_shared: bool = False
-    dependencies: Optional[list[str]] = None
-
-
-def get_unique_config_specs(
-    specs: Iterable[ConfigurableFieldSpec],
-) -> list[ConfigurableFieldSpec]:
-    """Get the unique config specs from a sequence of config specs.
-
-    Args:
-        specs: The config specs.
-
-    Returns:
-        List[ConfigurableFieldSpec]: The unique config specs.
-
-    Raises:
-        ValueError: If the runnable sequence contains conflicting config specs.
-    """
-    grouped = groupby(
-        sorted(specs, key=lambda s: (s.id, *(s.dependencies or []))), lambda s: s.id
-    )
-    unique: list[ConfigurableFieldSpec] = []
-    for id, dupes in grouped:
-        first = next(dupes)
-        others = list(dupes)
-        if len(others) == 0 or all(o == first for o in others):
-            unique.append(first)
-        else:
-            msg = (
-                "RunnableSequence contains conflicting config specs"
-                f"for {id}: {[first] + others}"
-            )
-            raise ValueError(msg)
-    return unique
-
-
-class _RootEventFilter:
-    def __init__(
-        self,
-        *,
-        include_names: Optional[Sequence[str]] = None,
-        include_types: Optional[Sequence[str]] = None,
-        include_tags: Optional[Sequence[str]] = None,
-        exclude_names: Optional[Sequence[str]] = None,
-        exclude_types: Optional[Sequence[str]] = None,
-        exclude_tags: Optional[Sequence[str]] = None,
-    ) -> None:
-        """Utility to filter the root event in the astream_events implementation.
-
-        This is simply binding the arguments to the namespace to make save on
-        a bit of typing in the astream_events implementation.
-        """
-        self.include_names = include_names
-        self.include_types = include_types
-        self.include_tags = include_tags
-        self.exclude_names = exclude_names
-        self.exclude_types = exclude_types
-        self.exclude_tags = exclude_tags
-
-    def include_event(self, event: StreamEvent, root_type: str) -> bool:
-        """Determine whether to include an event."""
-        if (
-            self.include_names is None
-            and self.include_types is None
-            and self.include_tags is None
-        ):
-            include = True
-        else:
-            include = False
-
-        event_tags = event.get("tags") or []
-
-        if self.include_names is not None:
-            include = include or event["name"] in self.include_names
-        if self.include_types is not None:
-            include = include or root_type in self.include_types
-        if self.include_tags is not None:
-            include = include or any(tag in self.include_tags for tag in event_tags)
-
-        if self.exclude_names is not None:
-            include = include and event["name"] not in self.exclude_names
-        if self.exclude_types is not None:
-            include = include and root_type not in self.exclude_types
-        if self.exclude_tags is not None:
-            include = include and all(
-                tag not in self.exclude_tags for tag in event_tags
-            )
-
-        return include
-
-
-def is_async_generator(
-    func: Any,
-) -> TypeGuard[Callable[..., AsyncIterator]]:
-    """Check if a function is an async generator.
-
-    Args:
-        func: The function to check.
-
-    Returns:
-        TypeGuard[Callable[..., AsyncIterator]: True if the function is
-            an async generator, False otherwise.
-    """
-    return (
-        inspect.isasyncgenfunction(func)
-        or hasattr(func, "__call__")  # noqa: B004
-        and inspect.isasyncgenfunction(func.__call__)
-    )
-
-
-def is_async_callable(
-    func: Any,
-) -> TypeGuard[Callable[..., Awaitable]]:
-    """Check if a function is async.
-
-    Args:
-        func: The function to check.
-
-    Returns:
-        TypeGuard[Callable[..., Awaitable]: True if the function is async,
-            False otherwise.
-    """
-    return (
-        asyncio.iscoroutinefunction(func)
-        or hasattr(func, "__call__")  # noqa: B004
-        and asyncio.iscoroutinefunction(func.__call__)
-    )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/stores.py ./custom_langchain_core/stores.py
--- .venv/lib/python3.12/site-packages/langchain_core/stores.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/stores.py	1970-01-01 09:00:00
@@ -1,334 +0,0 @@
-"""**Store** implements the key-value stores and storage helpers.
-
-Module provides implementations of various key-value stores that conform
-to a simple key-value interface.
-
-The primary goal of these storages is to support implementation of caching.
-"""
-
-from abc import ABC, abstractmethod
-from collections.abc import AsyncIterator, Iterator, Sequence
-from typing import (
-    Any,
-    Generic,
-    Optional,
-    TypeVar,
-    Union,
-)
-
-from langchain_core.exceptions import LangChainException
-from langchain_core.runnables import run_in_executor
-
-K = TypeVar("K")
-V = TypeVar("V")
-
-
-class BaseStore(Generic[K, V], ABC):
-    """Abstract interface for a key-value store.
-
-    This is an interface that's meant to abstract away the details of
-    different key-value stores. It provides a simple interface for
-    getting, setting, and deleting key-value pairs.
-
-    The basic methods are `mget`, `mset`, and `mdelete` for getting,
-    setting, and deleting multiple key-value pairs at once. The `yield_keys`
-    method is used to iterate over keys that match a given prefix.
-
-    The async versions of these methods are also provided, which are
-    meant to be used in async contexts. The async methods are named with
-    an `a` prefix, e.g., `amget`, `amset`, `amdelete`, and `ayield_keys`.
-
-    By default, the `amget`, `amset`, `amdelete`, and `ayield_keys` methods
-    are implemented using the synchronous methods. If the store can natively
-    support async  operations, it should override these methods.
-
-    By design the methods only accept batches of keys and values, and not
-    single keys or values. This is done to force user code to work with batches
-    which will usually be more efficient by saving on round trips to the store.
-
-    Examples:
-
-        .. code-block:: python
-
-            from langchain.storage import BaseStore
-
-            class MyInMemoryStore(BaseStore[str, int]):
-
-                def __init__(self):
-                    self.store = {}
-
-                def mget(self, keys):
-                    return [self.store.get(key) for key in keys]
-
-                def mset(self, key_value_pairs):
-                    for key, value in key_value_pairs:
-                        self.store[key] = value
-
-                def mdelete(self, keys):
-                    for key in keys:
-                        if key in self.store:
-                            del self.store[key]
-
-                def yield_keys(self, prefix=None):
-                    if prefix is None:
-                        yield from self.store.keys()
-                    else:
-                        for key in self.store.keys():
-                            if key.startswith(prefix):
-                                yield key
-    """
-
-    @abstractmethod
-    def mget(self, keys: Sequence[K]) -> list[Optional[V]]:
-        """Get the values associated with the given keys.
-
-        Args:
-            keys (Sequence[K]): A sequence of keys.
-
-        Returns:
-            A sequence of optional values associated with the keys.
-            If a key is not found, the corresponding value will be None.
-        """
-
-    async def amget(self, keys: Sequence[K]) -> list[Optional[V]]:
-        """Async get the values associated with the given keys.
-
-        Args:
-            keys (Sequence[K]): A sequence of keys.
-
-        Returns:
-            A sequence of optional values associated with the keys.
-            If a key is not found, the corresponding value will be None.
-        """
-        return await run_in_executor(None, self.mget, keys)
-
-    @abstractmethod
-    def mset(self, key_value_pairs: Sequence[tuple[K, V]]) -> None:
-        """Set the values for the given keys.
-
-        Args:
-            key_value_pairs (Sequence[Tuple[K, V]]): A sequence of key-value pairs.
-        """
-
-    async def amset(self, key_value_pairs: Sequence[tuple[K, V]]) -> None:
-        """Async set the values for the given keys.
-
-        Args:
-            key_value_pairs (Sequence[Tuple[K, V]]): A sequence of key-value pairs.
-        """
-        return await run_in_executor(None, self.mset, key_value_pairs)
-
-    @abstractmethod
-    def mdelete(self, keys: Sequence[K]) -> None:
-        """Delete the given keys and their associated values.
-
-        Args:
-            keys (Sequence[K]): A sequence of keys to delete.
-        """
-
-    async def amdelete(self, keys: Sequence[K]) -> None:
-        """Async delete the given keys and their associated values.
-
-        Args:
-            keys (Sequence[K]): A sequence of keys to delete.
-        """
-        return await run_in_executor(None, self.mdelete, keys)
-
-    @abstractmethod
-    def yield_keys(
-        self, *, prefix: Optional[str] = None
-    ) -> Union[Iterator[K], Iterator[str]]:
-        """Get an iterator over keys that match the given prefix.
-
-        Args:
-            prefix (str): The prefix to match.
-
-        Yields:
-            Iterator[K | str]: An iterator over keys that match the given prefix.
-            This method is allowed to return an iterator over either K or str
-            depending on what makes more sense for the given store.
-        """
-
-    async def ayield_keys(
-        self, *, prefix: Optional[str] = None
-    ) -> Union[AsyncIterator[K], AsyncIterator[str]]:
-        """Async get an iterator over keys that match the given prefix.
-
-        Args:
-            prefix (str): The prefix to match.
-
-        Yields:
-            Iterator[K | str]: An iterator over keys that match the given prefix.
-            This method is allowed to return an iterator over either K or str
-            depending on what makes more sense for the given store.
-        """
-        iterator = await run_in_executor(None, self.yield_keys, prefix=prefix)
-        done = object()
-        while True:
-            item = await run_in_executor(None, lambda it: next(it, done), iterator)
-            if item is done:
-                break
-            yield item  # type: ignore[misc]
-
-
-ByteStore = BaseStore[str, bytes]
-
-
-class InMemoryBaseStore(BaseStore[str, V], Generic[V]):
-    """In-memory implementation of the BaseStore using a dictionary."""
-
-    def __init__(self) -> None:
-        """Initialize an empty store."""
-        self.store: dict[str, V] = {}
-
-    def mget(self, keys: Sequence[str]) -> list[Optional[V]]:
-        """Get the values associated with the given keys.
-
-        Args:
-            keys (Sequence[str]): A sequence of keys.
-
-        Returns:
-            A sequence of optional values associated with the keys.
-            If a key is not found, the corresponding value will be None.
-        """
-        return [self.store.get(key) for key in keys]
-
-    async def amget(self, keys: Sequence[str]) -> list[Optional[V]]:
-        """Async get the values associated with the given keys.
-
-        Args:
-            keys (Sequence[str]): A sequence of keys.
-
-        Returns:
-            A sequence of optional values associated with the keys.
-            If a key is not found, the corresponding value will be None.
-        """
-        return self.mget(keys)
-
-    def mset(self, key_value_pairs: Sequence[tuple[str, V]]) -> None:
-        """Set the values for the given keys.
-
-        Args:
-            key_value_pairs (Sequence[Tuple[str, V]]): A sequence of key-value pairs.
-
-        Returns:
-            None
-        """
-        for key, value in key_value_pairs:
-            self.store[key] = value
-
-    async def amset(self, key_value_pairs: Sequence[tuple[str, V]]) -> None:
-        """Async set the values for the given keys.
-
-        Args:
-            key_value_pairs (Sequence[Tuple[str, V]]): A sequence of key-value pairs.
-
-        Returns:
-            None
-        """
-        return self.mset(key_value_pairs)
-
-    def mdelete(self, keys: Sequence[str]) -> None:
-        """Delete the given keys and their associated values.
-
-        Args:
-            keys (Sequence[str]): A sequence of keys to delete.
-        """
-        for key in keys:
-            if key in self.store:
-                del self.store[key]
-
-    async def amdelete(self, keys: Sequence[str]) -> None:
-        """Async delete the given keys and their associated values.
-
-        Args:
-            keys (Sequence[str]): A sequence of keys to delete.
-        """
-        self.mdelete(keys)
-
-    def yield_keys(self, prefix: Optional[str] = None) -> Iterator[str]:
-        """Get an iterator over keys that match the given prefix.
-
-        Args:
-            prefix (str, optional): The prefix to match. Defaults to None.
-
-        Yields:
-            Iterator[str]: An iterator over keys that match the given prefix.
-        """
-        if prefix is None:
-            yield from self.store.keys()
-        else:
-            for key in self.store:
-                if key.startswith(prefix):
-                    yield key
-
-    async def ayield_keys(self, prefix: Optional[str] = None) -> AsyncIterator[str]:
-        """Async get an async iterator over keys that match the given prefix.
-
-        Args:
-            prefix (str, optional): The prefix to match. Defaults to None.
-
-        Yields:
-            AsyncIterator[str]: An async iterator over keys that match the given prefix.
-        """
-        if prefix is None:
-            for key in self.store:
-                yield key
-        else:
-            for key in self.store:
-                if key.startswith(prefix):
-                    yield key
-
-
-class InMemoryStore(InMemoryBaseStore[Any]):
-    """In-memory store for any type of data.
-
-    Attributes:
-        store (Dict[str, Any]): The underlying dictionary that stores
-            the key-value pairs.
-
-    Examples:
-
-        .. code-block:: python
-
-            from langchain.storage import InMemoryStore
-
-            store = InMemoryStore()
-            store.mset([('key1', 'value1'), ('key2', 'value2')])
-            store.mget(['key1', 'key2'])
-            # ['value1', 'value2']
-            store.mdelete(['key1'])
-            list(store.yield_keys())
-            # ['key2']
-            list(store.yield_keys(prefix='k'))
-            # ['key2']
-    """
-
-
-class InMemoryByteStore(InMemoryBaseStore[bytes]):
-    """In-memory store for bytes.
-
-    Attributes:
-        store (Dict[str, bytes]): The underlying dictionary that stores
-            the key-value pairs.
-
-    Examples:
-
-        .. code-block:: python
-
-            from langchain.storage import InMemoryByteStore
-
-            store = InMemoryByteStore()
-            store.mset([('key1', b'value1'), ('key2', b'value2')])
-            store.mget(['key1', 'key2'])
-            # [b'value1', b'value2']
-            store.mdelete(['key1'])
-            list(store.yield_keys())
-            # ['key2']
-            list(store.yield_keys(prefix='k'))
-            # ['key2']
-    """
-
-
-class InvalidKeyException(LangChainException):
-    """Raised when a key is invalid; e.g., uses incorrect characters."""
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/structured_query.py ./custom_langchain_core/structured_query.py
--- .venv/lib/python3.12/site-packages/langchain_core/structured_query.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/structured_query.py	1970-01-01 09:00:00
@@ -1,185 +0,0 @@
-"""Internal representation of a structured query language."""
-
-from __future__ import annotations
-
-from abc import ABC, abstractmethod
-from collections.abc import Sequence
-from enum import Enum
-from typing import Any, Optional, Union
-
-from pydantic import BaseModel
-
-
-class Visitor(ABC):
-    """Defines interface for IR translation using a visitor pattern."""
-
-    allowed_comparators: Optional[Sequence[Comparator]] = None
-    """Allowed comparators for the visitor."""
-    allowed_operators: Optional[Sequence[Operator]] = None
-    """Allowed operators for the visitor."""
-
-    def _validate_func(self, func: Union[Operator, Comparator]) -> None:
-        if (
-            isinstance(func, Operator)
-            and self.allowed_operators is not None
-            and func not in self.allowed_operators
-        ):
-            msg = (
-                f"Received disallowed operator {func}. Allowed "
-                f"comparators are {self.allowed_operators}"
-            )
-            raise ValueError(msg)
-        if (
-            isinstance(func, Comparator)
-            and self.allowed_comparators is not None
-            and func not in self.allowed_comparators
-        ):
-            msg = (
-                f"Received disallowed comparator {func}. Allowed "
-                f"comparators are {self.allowed_comparators}"
-            )
-            raise ValueError(msg)
-
-    @abstractmethod
-    def visit_operation(self, operation: Operation) -> Any:
-        """Translate an Operation.
-
-        Args:
-            operation: Operation to translate.
-        """
-
-    @abstractmethod
-    def visit_comparison(self, comparison: Comparison) -> Any:
-        """Translate a Comparison.
-
-        Args:
-            comparison: Comparison to translate.
-        """
-
-    @abstractmethod
-    def visit_structured_query(self, structured_query: StructuredQuery) -> Any:
-        """Translate a StructuredQuery.
-
-        Args:
-            structured_query: StructuredQuery to translate.
-        """
-
-
-def _to_snake_case(name: str) -> str:
-    """Convert a name into snake_case."""
-    snake_case = ""
-    for i, char in enumerate(name):
-        if char.isupper() and i != 0:
-            snake_case += "_" + char.lower()
-        else:
-            snake_case += char.lower()
-    return snake_case
-
-
-class Expr(BaseModel):
-    """Base class for all expressions."""
-
-    def accept(self, visitor: Visitor) -> Any:
-        """Accept a visitor.
-
-        Args:
-            visitor: visitor to accept.
-
-        Returns:
-            result of visiting.
-        """
-        return getattr(visitor, f"visit_{_to_snake_case(self.__class__.__name__)}")(
-            self
-        )
-
-
-class Operator(str, Enum):
-    """Enumerator of the operations."""
-
-    AND = "and"
-    OR = "or"
-    NOT = "not"
-
-
-class Comparator(str, Enum):
-    """Enumerator of the comparison operators."""
-
-    EQ = "eq"
-    NE = "ne"
-    GT = "gt"
-    GTE = "gte"
-    LT = "lt"
-    LTE = "lte"
-    CONTAIN = "contain"
-    LIKE = "like"
-    IN = "in"
-    NIN = "nin"
-
-
-class FilterDirective(Expr, ABC):
-    """Filtering expression."""
-
-
-class Comparison(FilterDirective):
-    """Comparison to a value.
-
-    Parameters:
-        comparator: The comparator to use.
-        attribute: The attribute to compare.
-        value: The value to compare to.
-    """
-
-    comparator: Comparator
-    attribute: str
-    value: Any
-
-    def __init__(
-        self, comparator: Comparator, attribute: str, value: Any, **kwargs: Any
-    ) -> None:
-        # super exists from BaseModel
-        super().__init__(  # type: ignore[call-arg]
-            comparator=comparator, attribute=attribute, value=value, **kwargs
-        )
-
-
-class Operation(FilterDirective):
-    """Logical operation over other directives.
-
-    Parameters:
-        operator: The operator to use.
-        arguments: The arguments to the operator.
-    """
-
-    operator: Operator
-    arguments: list[FilterDirective]
-
-    def __init__(
-        self, operator: Operator, arguments: list[FilterDirective], **kwargs: Any
-    ) -> None:
-        # super exists from BaseModel
-        super().__init__(  # type: ignore[call-arg]
-            operator=operator, arguments=arguments, **kwargs
-        )
-
-
-class StructuredQuery(Expr):
-    """Structured query."""
-
-    query: str
-    """Query string."""
-    filter: Optional[FilterDirective]
-    """Filtering expression."""
-    limit: Optional[int]
-    """Limit on the number of results."""
-
-    def __init__(
-        self,
-        query: str,
-        filter: Optional[FilterDirective],
-        limit: Optional[int] = None,
-        **kwargs: Any,
-    ) -> None:
-        # super exists from BaseModel
-        super().__init__(  # type: ignore[call-arg]
-            query=query, filter=filter, limit=limit, **kwargs
-        )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/sys_info.py ./custom_langchain_core/sys_info.py
--- .venv/lib/python3.12/site-packages/langchain_core/sys_info.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/sys_info.py	1970-01-01 09:00:00
@@ -1,141 +0,0 @@
-"""**sys_info** prints information about the system and langchain packages
-for debugging purposes.
-"""
-
-from collections.abc import Sequence
-
-
-def _get_sub_deps(packages: Sequence[str]) -> list[str]:
-    """Get any specified sub-dependencies."""
-    from importlib import metadata
-
-    sub_deps = set()
-    _underscored_packages = {pkg.replace("-", "_") for pkg in packages}
-
-    for pkg in packages:
-        try:
-            required = metadata.requires(pkg)
-        except metadata.PackageNotFoundError:
-            continue
-
-        if not required:
-            continue
-
-        for req in required:
-            try:
-                cleaned_req = req.split(" ")[0]
-            except Exception:  # In case parsing of requirement spec fails
-                continue
-
-            if cleaned_req.replace("-", "_") not in _underscored_packages:
-                sub_deps.add(cleaned_req)
-
-    return sorted(sub_deps, key=lambda x: x.lower())
-
-
-def print_sys_info(*, additional_pkgs: Sequence[str] = ()) -> None:
-    """Print information about the environment for debugging purposes.
-
-    Args:
-        additional_pkgs: Additional packages to include in the output.
-    """
-    import pkgutil
-    import platform
-    import sys
-    from importlib import metadata, util
-
-    # Packages that do not start with "langchain" prefix.
-    other_langchain_packages = [
-        "langserve",
-        "langsmith",
-    ]
-
-    langchain_pkgs = [
-        name for _, name, _ in pkgutil.iter_modules() if name.startswith("langchain")
-    ]
-
-    langgraph_pkgs = [
-        name for _, name, _ in pkgutil.iter_modules() if name.startswith("langgraph")
-    ]
-
-    all_packages = sorted(
-        set(
-            langchain_pkgs
-            + langgraph_pkgs
-            + other_langchain_packages
-            + list(additional_pkgs)
-        )
-    )
-
-    # Always surface these packages to the top
-    order_by = ["langchain_core", "langchain", "langchain_community", "langsmith"]
-
-    for pkg in reversed(order_by):
-        if pkg in all_packages:
-            all_packages.remove(pkg)
-            all_packages = [pkg] + list(all_packages)
-
-    system_info = {
-        "OS": platform.system(),
-        "OS Version": platform.version(),
-        "Python Version": sys.version,
-    }
-    print()  # noqa: T201
-    print("System Information")  # noqa: T201
-    print("------------------")  # noqa: T201
-    print("> OS: ", system_info["OS"])  # noqa: T201
-    print("> OS Version: ", system_info["OS Version"])  # noqa: T201
-    print("> Python Version: ", system_info["Python Version"])  # noqa: T201
-
-    # Print out only langchain packages
-    print()  # noqa: T201
-    print("Package Information")  # noqa: T201
-    print("-------------------")  # noqa: T201
-
-    not_installed = []
-
-    for pkg in all_packages:
-        try:
-            found_package = util.find_spec(pkg)
-        except Exception:
-            found_package = None
-        if found_package is None:
-            not_installed.append(pkg)
-            continue
-
-        # Package version
-        try:
-            package_version = metadata.version(pkg)
-        except Exception:
-            package_version = None
-
-        # Print package with version
-        if package_version is not None:
-            print(f"> {pkg}: {package_version}")  # noqa: T201
-        else:
-            print(f"> {pkg}: Installed. No version info available.")  # noqa: T201
-
-    if not_installed:
-        print()  # noqa: T201
-        print("Optional packages not installed")  # noqa: T201
-        print("-------------------------------")  # noqa: T201
-        for pkg in not_installed:
-            print(f"> {pkg}")  # noqa: T201
-
-    sub_dependencies = _get_sub_deps(all_packages)
-
-    if sub_dependencies:
-        print()  # noqa: T201
-        print("Other Dependencies")  # noqa: T201
-        print("------------------")  # noqa: T201
-
-        for dep in sub_dependencies:
-            try:
-                dep_version = metadata.version(dep)
-                print(f"> {dep}: {dep_version}")  # noqa: T201
-            except Exception:
-                print(f"> {dep}: Installed. No version info available.")  # noqa: T201
-
-
-if __name__ == "__main__":
-    print_sys_info()
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tools/__init__.py ./custom_langchain_core/tools/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/tools/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tools/__init__.py	1970-01-01 09:00:00
@@ -1,61 +0,0 @@
-"""**Tools** are classes that an Agent uses to interact with the world.
-
-Each tool has a **description**. Agent uses the description to choose the right
-tool for the job.
-
-**Class hierarchy:**
-
-.. code-block::
-
-    RunnableSerializable --> BaseTool --> <name>Tool  # Examples: AIPluginTool, BaseGraphQLTool
-                                          <name>      # Examples: BraveSearch, HumanInputRun
-
-**Main helpers:**
-
-.. code-block::
-
-    CallbackManagerForToolRun, AsyncCallbackManagerForToolRun
-"""  # noqa: E501
-
-from __future__ import annotations
-
-from langchain_core.tools.base import (
-    FILTERED_ARGS as FILTERED_ARGS,
-)
-from langchain_core.tools.base import (
-    BaseTool as BaseTool,
-)
-from langchain_core.tools.base import (
-    BaseToolkit as BaseToolkit,
-)
-from langchain_core.tools.base import (
-    InjectedToolArg as InjectedToolArg,
-)
-from langchain_core.tools.base import InjectedToolCallId as InjectedToolCallId
-from langchain_core.tools.base import SchemaAnnotationError as SchemaAnnotationError
-from langchain_core.tools.base import (
-    ToolException as ToolException,
-)
-from langchain_core.tools.base import (
-    _get_runnable_config_param as _get_runnable_config_param,
-)
-from langchain_core.tools.base import (
-    create_schema_from_function as create_schema_from_function,
-)
-from langchain_core.tools.convert import (
-    convert_runnable_to_tool as convert_runnable_to_tool,
-)
-from langchain_core.tools.convert import tool as tool
-from langchain_core.tools.render import ToolsRenderer as ToolsRenderer
-from langchain_core.tools.render import (
-    render_text_description as render_text_description,
-)
-from langchain_core.tools.render import (
-    render_text_description_and_args as render_text_description_and_args,
-)
-from langchain_core.tools.retriever import RetrieverInput as RetrieverInput
-from langchain_core.tools.retriever import (
-    create_retriever_tool as create_retriever_tool,
-)
-from langchain_core.tools.simple import Tool as Tool
-from langchain_core.tools.structured import StructuredTool as StructuredTool
Binary files .venv/lib/python3.12/site-packages/langchain_core/tools/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/tools/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tools/__pycache__/base.cpython-312.pyc and ./custom_langchain_core/tools/__pycache__/base.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tools/__pycache__/convert.cpython-312.pyc and ./custom_langchain_core/tools/__pycache__/convert.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tools/__pycache__/render.cpython-312.pyc and ./custom_langchain_core/tools/__pycache__/render.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tools/__pycache__/retriever.cpython-312.pyc and ./custom_langchain_core/tools/__pycache__/retriever.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tools/__pycache__/simple.cpython-312.pyc and ./custom_langchain_core/tools/__pycache__/simple.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tools/__pycache__/structured.cpython-312.pyc and ./custom_langchain_core/tools/__pycache__/structured.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tools/base.py ./custom_langchain_core/tools/base.py
--- .venv/lib/python3.12/site-packages/langchain_core/tools/base.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tools/base.py	1970-01-01 09:00:00
@@ -1,1141 +0,0 @@
-from __future__ import annotations
-
-import asyncio
-import functools
-import inspect
-import json
-import uuid
-import warnings
-from abc import ABC, abstractmethod
-from collections.abc import Sequence
-from contextvars import copy_context
-from inspect import signature
-from typing import (
-    Annotated,
-    Any,
-    Callable,
-    Literal,
-    Optional,
-    TypeVar,
-    Union,
-    cast,
-    get_args,
-    get_origin,
-    get_type_hints,
-)
-
-from pydantic import (
-    BaseModel,
-    ConfigDict,
-    Field,
-    PydanticDeprecationWarning,
-    SkipValidation,
-    ValidationError,
-    model_validator,
-    validate_arguments,
-)
-from pydantic.v1 import BaseModel as BaseModelV1
-from pydantic.v1 import ValidationError as ValidationErrorV1
-from pydantic.v1 import validate_arguments as validate_arguments_v1
-
-from langchain_core._api import deprecated
-from langchain_core.callbacks import (
-    AsyncCallbackManager,
-    BaseCallbackManager,
-    CallbackManager,
-    Callbacks,
-)
-from langchain_core.messages.tool import ToolCall, ToolMessage, ToolOutputMixin
-from langchain_core.runnables import (
-    RunnableConfig,
-    RunnableSerializable,
-    ensure_config,
-    patch_config,
-    run_in_executor,
-)
-from langchain_core.runnables.config import _set_config_context
-from langchain_core.runnables.utils import asyncio_accepts_context
-from langchain_core.utils.function_calling import (
-    _parse_google_docstring,
-    _py_38_safe_origin,
-)
-from langchain_core.utils.pydantic import (
-    TypeBaseModel,
-    _create_subset_model,
-    get_fields,
-    is_basemodel_subclass,
-    is_pydantic_v1_subclass,
-    is_pydantic_v2_subclass,
-)
-
-FILTERED_ARGS = ("run_manager", "callbacks")
-
-
-class SchemaAnnotationError(TypeError):
-    """Raised when 'args_schema' is missing or has an incorrect type annotation."""
-
-
-def _is_annotated_type(typ: type[Any]) -> bool:
-    return get_origin(typ) is Annotated
-
-
-def _get_annotation_description(arg_type: type) -> str | None:
-    if _is_annotated_type(arg_type):
-        annotated_args = get_args(arg_type)
-        for annotation in annotated_args[1:]:
-            if isinstance(annotation, str):
-                return annotation
-    return None
-
-
-def _get_filtered_args(
-    inferred_model: type[BaseModel],
-    func: Callable,
-    *,
-    filter_args: Sequence[str],
-    include_injected: bool = True,
-) -> dict:
-    """Get the arguments from a function's signature."""
-    schema = inferred_model.model_json_schema()["properties"]
-    valid_keys = signature(func).parameters
-    return {
-        k: schema[k]
-        for i, (k, param) in enumerate(valid_keys.items())
-        if k not in filter_args
-        and (i > 0 or param.name not in ("self", "cls"))
-        and (include_injected or not _is_injected_arg_type(param.annotation))
-    }
-
-
-def _parse_python_function_docstring(
-    function: Callable, annotations: dict, error_on_invalid_docstring: bool = False
-) -> tuple[str, dict]:
-    """Parse the function and argument descriptions from the docstring of a function.
-
-    Assumes the function docstring follows Google Python style guide.
-    """
-    docstring = inspect.getdoc(function)
-    return _parse_google_docstring(
-        docstring,
-        list(annotations),
-        error_on_invalid_docstring=error_on_invalid_docstring,
-    )
-
-
-def _validate_docstring_args_against_annotations(
-    arg_descriptions: dict, annotations: dict
-) -> None:
-    """Raise error if docstring arg is not in type annotations."""
-    for docstring_arg in arg_descriptions:
-        if docstring_arg not in annotations:
-            msg = f"Arg {docstring_arg} in docstring not found in function signature."
-            raise ValueError(msg)
-
-
-def _infer_arg_descriptions(
-    fn: Callable,
-    *,
-    parse_docstring: bool = False,
-    error_on_invalid_docstring: bool = False,
-) -> tuple[str, dict]:
-    """Infer argument descriptions from a function's docstring."""
-    if hasattr(inspect, "get_annotations"):
-        # This is for python < 3.10
-        annotations = inspect.get_annotations(fn)  # type: ignore
-    else:
-        annotations = getattr(fn, "__annotations__", {})
-    if parse_docstring:
-        description, arg_descriptions = _parse_python_function_docstring(
-            fn, annotations, error_on_invalid_docstring=error_on_invalid_docstring
-        )
-    else:
-        description = inspect.getdoc(fn) or ""
-        arg_descriptions = {}
-    if parse_docstring:
-        _validate_docstring_args_against_annotations(arg_descriptions, annotations)
-    for arg, arg_type in annotations.items():
-        if arg in arg_descriptions:
-            continue
-        if desc := _get_annotation_description(arg_type):
-            arg_descriptions[arg] = desc
-    return description, arg_descriptions
-
-
-def _is_pydantic_annotation(annotation: Any, pydantic_version: str = "v2") -> bool:
-    """Determine if a type annotation is a Pydantic model."""
-    base_model_class = BaseModelV1 if pydantic_version == "v1" else BaseModel
-    try:
-        return issubclass(annotation, base_model_class)
-    except TypeError:
-        return False
-
-
-def _function_annotations_are_pydantic_v1(
-    signature: inspect.Signature, func: Callable
-) -> bool:
-    """Determine if all Pydantic annotations in a function signature are from V1."""
-    any_v1_annotations = any(
-        _is_pydantic_annotation(parameter.annotation, pydantic_version="v1")
-        for parameter in signature.parameters.values()
-    )
-    any_v2_annotations = any(
-        _is_pydantic_annotation(parameter.annotation, pydantic_version="v2")
-        for parameter in signature.parameters.values()
-    )
-    if any_v1_annotations and any_v2_annotations:
-        msg = (
-            f"Function {func} contains a mix of Pydantic v1 and v2 annotations. "
-            "Only one version of Pydantic annotations per function is supported."
-        )
-        raise NotImplementedError(msg)
-    return any_v1_annotations and not any_v2_annotations
-
-
-class _SchemaConfig:
-    """Configuration for the pydantic model.
-
-    This is used to configure the pydantic model created from
-    a function's signature.
-
-    Parameters:
-        extra: Whether to allow extra fields in the model.
-        arbitrary_types_allowed: Whether to allow arbitrary types in the model.
-            Defaults to True.
-    """
-
-    extra: str = "forbid"
-    arbitrary_types_allowed: bool = True
-
-
-def create_schema_from_function(
-    model_name: str,
-    func: Callable,
-    *,
-    filter_args: Optional[Sequence[str]] = None,
-    parse_docstring: bool = False,
-    error_on_invalid_docstring: bool = False,
-    include_injected: bool = True,
-) -> type[BaseModel]:
-    """Create a pydantic schema from a function's signature.
-
-    Args:
-        model_name: Name to assign to the generated pydantic schema.
-        func: Function to generate the schema from.
-        filter_args: Optional list of arguments to exclude from the schema.
-            Defaults to FILTERED_ARGS.
-        parse_docstring: Whether to parse the function's docstring for descriptions
-            for each argument. Defaults to False.
-        error_on_invalid_docstring: if ``parse_docstring`` is provided, configure
-            whether to raise ValueError on invalid Google Style docstrings.
-            Defaults to False.
-        include_injected: Whether to include injected arguments in the schema.
-            Defaults to True, since we want to include them in the schema
-            when *validating* tool inputs.
-
-    Returns:
-        A pydantic model with the same arguments as the function.
-    """
-    sig = inspect.signature(func)
-
-    if _function_annotations_are_pydantic_v1(sig, func):
-        validated = validate_arguments_v1(func, config=_SchemaConfig)  # type: ignore
-    else:
-        # https://docs.pydantic.dev/latest/usage/validation_decorator/
-        with warnings.catch_warnings():
-            # We are using deprecated functionality here.
-            # This code should be re-written to simply construct a pydantic model
-            # using inspect.signature and create_model.
-            warnings.simplefilter("ignore", category=PydanticDeprecationWarning)
-            validated = validate_arguments(func, config=_SchemaConfig)  # type: ignore
-
-    # Let's ignore `self` and `cls` arguments for class and instance methods
-    # If qualified name has a ".", then it likely belongs in a class namespace
-    in_class = bool(func.__qualname__ and "." in func.__qualname__)
-
-    has_args = False
-    has_kwargs = False
-
-    for param in sig.parameters.values():
-        if param.kind == param.VAR_POSITIONAL:
-            has_args = True
-        elif param.kind == param.VAR_KEYWORD:
-            has_kwargs = True
-
-    inferred_model = validated.model  # type: ignore
-
-    if filter_args:
-        filter_args_ = filter_args
-    else:
-        # Handle classmethods and instance methods
-        existing_params: list[str] = list(sig.parameters.keys())
-        if existing_params and existing_params[0] in ("self", "cls") and in_class:
-            filter_args_ = [existing_params[0]] + list(FILTERED_ARGS)
-        else:
-            filter_args_ = list(FILTERED_ARGS)
-
-        for existing_param in existing_params:
-            if not include_injected and _is_injected_arg_type(
-                sig.parameters[existing_param].annotation
-            ):
-                filter_args_.append(existing_param)
-
-    description, arg_descriptions = _infer_arg_descriptions(
-        func,
-        parse_docstring=parse_docstring,
-        error_on_invalid_docstring=error_on_invalid_docstring,
-    )
-    # Pydantic adds placeholder virtual fields we need to strip
-    valid_properties = []
-    for field in get_fields(inferred_model):
-        if not has_args and field == "args":
-            continue
-        if not has_kwargs and field == "kwargs":
-            continue
-
-        if field == "v__duplicate_kwargs":  # Internal pydantic field
-            continue
-
-        if field not in filter_args_:
-            valid_properties.append(field)
-
-    return _create_subset_model(
-        model_name,
-        inferred_model,
-        list(valid_properties),
-        descriptions=arg_descriptions,
-        fn_description=description,
-    )
-
-
-class ToolException(Exception):  # noqa: N818
-    """Optional exception that tool throws when execution error occurs.
-
-    When this exception is thrown, the agent will not stop working,
-    but it will handle the exception according to the handle_tool_error
-    variable of the tool, and the processing result will be returned
-    to the agent as observation, and printed in red on the console.
-    """
-
-
-ArgsSchema = Union[TypeBaseModel, dict[str, Any]]
-
-
-class BaseTool(RunnableSerializable[Union[str, dict, ToolCall], Any]):
-    """Interface LangChain tools must implement."""
-
-    def __init_subclass__(cls, **kwargs: Any) -> None:
-        """Create the definition of the new tool class."""
-        super().__init_subclass__(**kwargs)
-
-        args_schema_type = cls.__annotations__.get("args_schema", None)
-
-        if args_schema_type is not None and args_schema_type == BaseModel:
-            # Throw errors for common mis-annotations.
-            # TODO: Use get_args / get_origin and fully
-            # specify valid annotations.
-            typehint_mandate = """
-class ChildTool(BaseTool):
-    ...
-    args_schema: Type[BaseModel] = SchemaClass
-    ..."""
-            name = cls.__name__
-            msg = (
-                f"Tool definition for {name} must include valid type annotations"
-                f" for argument 'args_schema' to behave as expected.\n"
-                f"Expected annotation of 'Type[BaseModel]'"
-                f" but got '{args_schema_type}'.\n"
-                f"Expected class looks like:\n"
-                f"{typehint_mandate}"
-            )
-            raise SchemaAnnotationError(msg)
-
-    name: str
-    """The unique name of the tool that clearly communicates its purpose."""
-    description: str
-    """Used to tell the model how/when/why to use the tool.
-
-    You can provide few-shot examples as a part of the description.
-    """
-
-    args_schema: Annotated[Optional[ArgsSchema], SkipValidation()] = Field(
-        default=None, description="The tool schema."
-    )
-    """Pydantic model class to validate and parse the tool's input arguments.
-
-    Args schema should be either:
-
-    - A subclass of pydantic.BaseModel.
-    or
-    - A subclass of pydantic.v1.BaseModel if accessing v1 namespace in pydantic 2
-    or
-    - a JSON schema dict
-    """
-    return_direct: bool = False
-    """Whether to return the tool's output directly.
-
-    Setting this to True means
-    that after the tool is called, the AgentExecutor will stop looping.
-    """
-    verbose: bool = False
-    """Whether to log the tool's progress."""
-
-    callbacks: Callbacks = Field(default=None, exclude=True)
-    """Callbacks to be called during tool execution."""
-
-    callback_manager: Optional[BaseCallbackManager] = deprecated(
-        name="callback_manager", since="0.1.7", removal="1.0", alternative="callbacks"
-    )(
-        Field(
-            default=None,
-            exclude=True,
-            description="Callback manager to add to the run trace.",
-        )
-    )
-    tags: Optional[list[str]] = None
-    """Optional list of tags associated with the tool. Defaults to None.
-    These tags will be associated with each call to this tool,
-    and passed as arguments to the handlers defined in `callbacks`.
-    You can use these to eg identify a specific instance of a tool with its use case.
-    """
-    metadata: Optional[dict[str, Any]] = None
-    """Optional metadata associated with the tool. Defaults to None.
-    This metadata will be associated with each call to this tool,
-    and passed as arguments to the handlers defined in `callbacks`.
-    You can use these to eg identify a specific instance of a tool with its use case.
-    """
-
-    handle_tool_error: Optional[Union[bool, str, Callable[[ToolException], str]]] = (
-        False
-    )
-    """Handle the content of the ToolException thrown."""
-
-    handle_validation_error: Optional[
-        Union[bool, str, Callable[[Union[ValidationError, ValidationErrorV1]], str]]
-    ] = False
-    """Handle the content of the ValidationError thrown."""
-
-    response_format: Literal["content", "content_and_artifact"] = "content"
-    """The tool response format. Defaults to 'content'.
-
-    If "content" then the output of the tool is interpreted as the contents of a
-    ToolMessage. If "content_and_artifact" then the output is expected to be a
-    two-tuple corresponding to the (content, artifact) of a ToolMessage.
-    """
-
-    def __init__(self, **kwargs: Any) -> None:
-        """Initialize the tool."""
-        if (
-            "args_schema" in kwargs
-            and kwargs["args_schema"] is not None
-            and not is_basemodel_subclass(kwargs["args_schema"])
-            and not isinstance(kwargs["args_schema"], dict)
-        ):
-            msg = (
-                "args_schema must be a subclass of pydantic BaseModel or "
-                f"a JSON schema dict. Got: {kwargs['args_schema']}."
-            )
-            raise TypeError(msg)
-        super().__init__(**kwargs)
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    @property
-    def is_single_input(self) -> bool:
-        """Whether the tool only accepts a single input."""
-        keys = {k for k in self.args if k != "kwargs"}
-        return len(keys) == 1
-
-    @property
-    def args(self) -> dict:
-        if isinstance(self.args_schema, dict):
-            json_schema = self.args_schema
-        else:
-            input_schema = self.get_input_schema()
-            json_schema = input_schema.model_json_schema()
-        return json_schema["properties"]
-
-    @property
-    def tool_call_schema(self) -> ArgsSchema:
-        if isinstance(self.args_schema, dict):
-            return self.args_schema
-
-        full_schema = self.get_input_schema()
-        fields = []
-        for name, type_ in get_all_basemodel_annotations(full_schema).items():
-            if not _is_injected_arg_type(type_):
-                fields.append(name)
-        return _create_subset_model(
-            self.name, full_schema, fields, fn_description=self.description
-        )
-
-    # --- Runnable ---
-
-    def get_input_schema(
-        self, config: Optional[RunnableConfig] = None
-    ) -> type[BaseModel]:
-        """The tool's input schema.
-
-        Args:
-            config: The configuration for the tool.
-
-        Returns:
-            The input schema for the tool.
-        """
-        if self.args_schema is not None:
-            if isinstance(self.args_schema, dict):
-                return super().get_input_schema(config)
-            return self.args_schema
-        else:
-            return create_schema_from_function(self.name, self._run)
-
-    def invoke(
-        self,
-        input: Union[str, dict, ToolCall],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Any:
-        tool_input, kwargs = _prep_run_args(input, config, **kwargs)
-        return self.run(tool_input, **kwargs)
-
-    async def ainvoke(
-        self,
-        input: Union[str, dict, ToolCall],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Any:
-        tool_input, kwargs = _prep_run_args(input, config, **kwargs)
-        return await self.arun(tool_input, **kwargs)
-
-    # --- Tool ---
-
-    def _parse_input(
-        self, tool_input: Union[str, dict], tool_call_id: Optional[str]
-    ) -> Union[str, dict[str, Any]]:
-        """Convert tool input to a pydantic model.
-
-        Args:
-            tool_input: The input to the tool.
-        """
-        input_args = self.args_schema
-        if isinstance(tool_input, str):
-            if input_args is not None:
-                if isinstance(input_args, dict):
-                    msg = (
-                        "String tool inputs are not allowed when "
-                        "using tools with JSON schema args_schema."
-                    )
-                    raise ValueError(msg)
-                key_ = next(iter(get_fields(input_args).keys()))
-                if hasattr(input_args, "model_validate"):
-                    input_args.model_validate({key_: tool_input})
-                else:
-                    input_args.parse_obj({key_: tool_input})
-            return tool_input
-        else:
-            if input_args is not None:
-                if isinstance(input_args, dict):
-                    return tool_input
-                elif issubclass(input_args, BaseModel):
-                    for k, v in get_all_basemodel_annotations(input_args).items():
-                        if (
-                            _is_injected_arg_type(v, injected_type=InjectedToolCallId)
-                            and k not in tool_input
-                        ):
-                            if tool_call_id is None:
-                                msg = (
-                                    "When tool includes an InjectedToolCallId "
-                                    "argument, tool must always be invoked with a full "
-                                    "model ToolCall of the form: {'args': {...}, "
-                                    "'name': '...', 'type': 'tool_call', "
-                                    "'tool_call_id': '...'}"
-                                )
-                                raise ValueError(msg)
-                            tool_input[k] = tool_call_id
-                    result = input_args.model_validate(tool_input)
-                    result_dict = result.model_dump()
-                elif issubclass(input_args, BaseModelV1):
-                    for k, v in get_all_basemodel_annotations(input_args).items():
-                        if (
-                            _is_injected_arg_type(v, injected_type=InjectedToolCallId)
-                            and k not in tool_input
-                        ):
-                            if tool_call_id is None:
-                                msg = (
-                                    "When tool includes an InjectedToolCallId "
-                                    "argument, tool must always be invoked with a full "
-                                    "model ToolCall of the form: {'args': {...}, "
-                                    "'name': '...', 'type': 'tool_call', "
-                                    "'tool_call_id': '...'}"
-                                )
-                                raise ValueError(msg)
-                            tool_input[k] = tool_call_id
-                    result = input_args.parse_obj(tool_input)
-                    result_dict = result.dict()
-                else:
-                    msg = (
-                        "args_schema must be a Pydantic BaseModel, "
-                        f"got {self.args_schema}"
-                    )
-                    raise NotImplementedError(msg)
-                return {
-                    k: getattr(result, k)
-                    for k, v in result_dict.items()
-                    if k in tool_input
-                }
-            return tool_input
-
-    @model_validator(mode="before")
-    @classmethod
-    def raise_deprecation(cls, values: dict) -> Any:
-        """Raise deprecation warning if callback_manager is used.
-
-        Args:
-            values: The values to validate.
-
-        Returns:
-            The validated values.
-        """
-        if values.get("callback_manager") is not None:
-            warnings.warn(
-                "callback_manager is deprecated. Please use callbacks instead.",
-                DeprecationWarning,
-                stacklevel=6,
-            )
-            values["callbacks"] = values.pop("callback_manager", None)
-        return values
-
-    @abstractmethod
-    def _run(self, *args: Any, **kwargs: Any) -> Any:
-        """Use the tool.
-
-        Add run_manager: Optional[CallbackManagerForToolRun] = None
-        to child implementations to enable tracing.
-        """
-
-    async def _arun(self, *args: Any, **kwargs: Any) -> Any:
-        """Use the tool asynchronously.
-
-        Add run_manager: Optional[AsyncCallbackManagerForToolRun] = None
-        to child implementations to enable tracing.
-        """
-        if kwargs.get("run_manager") and signature(self._run).parameters.get(
-            "run_manager"
-        ):
-            kwargs["run_manager"] = kwargs["run_manager"].get_sync()
-        return await run_in_executor(None, self._run, *args, **kwargs)
-
-    def _to_args_and_kwargs(
-        self, tool_input: Union[str, dict], tool_call_id: Optional[str]
-    ) -> tuple[tuple, dict]:
-        if (
-            self.args_schema is not None
-            and isinstance(self.args_schema, type)
-            and is_basemodel_subclass(self.args_schema)
-            and not get_fields(self.args_schema)
-        ):
-            # StructuredTool with no args
-            return (), {}
-        tool_input = self._parse_input(tool_input, tool_call_id)
-        # For backwards compatibility, if run_input is a string,
-        # pass as a positional argument.
-        if isinstance(tool_input, str):
-            return (tool_input,), {}
-        else:
-            return (), tool_input
-
-    def run(
-        self,
-        tool_input: Union[str, dict[str, Any]],
-        verbose: Optional[bool] = None,
-        start_color: Optional[str] = "green",
-        color: Optional[str] = "green",
-        callbacks: Callbacks = None,
-        *,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        run_name: Optional[str] = None,
-        run_id: Optional[uuid.UUID] = None,
-        config: Optional[RunnableConfig] = None,
-        tool_call_id: Optional[str] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run the tool.
-
-        Args:
-            tool_input: The input to the tool.
-            verbose: Whether to log the tool's progress. Defaults to None.
-            start_color: The color to use when starting the tool. Defaults to 'green'.
-            color: The color to use when ending the tool. Defaults to 'green'.
-            callbacks: Callbacks to be called during tool execution. Defaults to None.
-            tags: Optional list of tags associated with the tool. Defaults to None.
-            metadata: Optional metadata associated with the tool. Defaults to None.
-            run_name: The name of the run. Defaults to None.
-            run_id: The id of the run. Defaults to None.
-            config: The configuration for the tool. Defaults to None.
-            tool_call_id: The id of the tool call. Defaults to None.
-            kwargs: Keyword arguments to be passed to tool callbacks
-
-        Returns:
-            The output of the tool.
-
-        Raises:
-            ToolException: If an error occurs during tool execution.
-        """
-        callback_manager = CallbackManager.configure(
-            callbacks,
-            self.callbacks,
-            self.verbose or bool(verbose),
-            tags,
-            self.tags,
-            metadata,
-            self.metadata,
-        )
-
-        run_manager = callback_manager.on_tool_start(
-            {"name": self.name, "description": self.description},
-            tool_input if isinstance(tool_input, str) else str(tool_input),
-            color=start_color,
-            name=run_name,
-            run_id=run_id,
-            # Inputs by definition should always be dicts.
-            # For now, it's unclear whether this assumption is ever violated,
-            # but if it is we will send a `None` value to the callback instead
-            # TODO: will need to address issue via a patch.
-            inputs=tool_input if isinstance(tool_input, dict) else None,
-            **kwargs,
-        )
-
-        content = None
-        artifact = None
-        status = "success"
-        error_to_raise: Union[Exception, KeyboardInterrupt, None] = None
-        try:
-            child_config = patch_config(config, callbacks=run_manager.get_child())
-            context = copy_context()
-            context.run(_set_config_context, child_config)
-            tool_args, tool_kwargs = self._to_args_and_kwargs(tool_input, tool_call_id)
-            if signature(self._run).parameters.get("run_manager"):
-                tool_kwargs = tool_kwargs | {"run_manager": run_manager}
-            if config_param := _get_runnable_config_param(self._run):
-                tool_kwargs = tool_kwargs | {config_param: config}
-            response = context.run(self._run, *tool_args, **tool_kwargs)
-            if self.response_format == "content_and_artifact":
-                if not isinstance(response, tuple) or len(response) != 2:
-                    msg = (
-                        "Since response_format='content_and_artifact' "
-                        "a two-tuple of the message content and raw tool output is "
-                        f"expected. Instead generated response of type: "
-                        f"{type(response)}."
-                    )
-                    error_to_raise = ValueError(msg)
-                else:
-                    content, artifact = response
-            else:
-                content = response
-        except (ValidationError, ValidationErrorV1) as e:
-            if not self.handle_validation_error:
-                error_to_raise = e
-            else:
-                content = _handle_validation_error(e, flag=self.handle_validation_error)
-                status = "error"
-        except ToolException as e:
-            if not self.handle_tool_error:
-                error_to_raise = e
-            else:
-                content = _handle_tool_error(e, flag=self.handle_tool_error)
-                status = "error"
-        except (Exception, KeyboardInterrupt) as e:
-            error_to_raise = e
-
-        if error_to_raise:
-            run_manager.on_tool_error(error_to_raise)
-            raise error_to_raise
-        output = _format_output(content, artifact, tool_call_id, self.name, status)
-        run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)
-        return output
-
-    async def arun(
-        self,
-        tool_input: Union[str, dict],
-        verbose: Optional[bool] = None,
-        start_color: Optional[str] = "green",
-        color: Optional[str] = "green",
-        callbacks: Callbacks = None,
-        *,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        run_name: Optional[str] = None,
-        run_id: Optional[uuid.UUID] = None,
-        config: Optional[RunnableConfig] = None,
-        tool_call_id: Optional[str] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Run the tool asynchronously.
-
-        Args:
-            tool_input: The input to the tool.
-            verbose: Whether to log the tool's progress. Defaults to None.
-            start_color: The color to use when starting the tool. Defaults to 'green'.
-            color: The color to use when ending the tool. Defaults to 'green'.
-            callbacks: Callbacks to be called during tool execution. Defaults to None.
-            tags: Optional list of tags associated with the tool. Defaults to None.
-            metadata: Optional metadata associated with the tool. Defaults to None.
-            run_name: The name of the run. Defaults to None.
-            run_id: The id of the run. Defaults to None.
-            config: The configuration for the tool. Defaults to None.
-            tool_call_id: The id of the tool call. Defaults to None.
-            kwargs: Keyword arguments to be passed to tool callbacks
-
-        Returns:
-            The output of the tool.
-
-        Raises:
-            ToolException: If an error occurs during tool execution.
-        """
-        callback_manager = AsyncCallbackManager.configure(
-            callbacks,
-            self.callbacks,
-            self.verbose or bool(verbose),
-            tags,
-            self.tags,
-            metadata,
-            self.metadata,
-        )
-        run_manager = await callback_manager.on_tool_start(
-            {"name": self.name, "description": self.description},
-            tool_input if isinstance(tool_input, str) else str(tool_input),
-            color=start_color,
-            name=run_name,
-            run_id=run_id,
-            # Inputs by definition should always be dicts.
-            # For now, it's unclear whether this assumption is ever violated,
-            # but if it is we will send a `None` value to the callback instead
-            # TODO: will need to address issue via a patch.
-            inputs=tool_input if isinstance(tool_input, dict) else None,
-            **kwargs,
-        )
-        content = None
-        artifact = None
-        status = "success"
-        error_to_raise: Optional[Union[Exception, KeyboardInterrupt]] = None
-        try:
-            tool_args, tool_kwargs = self._to_args_and_kwargs(tool_input, tool_call_id)
-            child_config = patch_config(config, callbacks=run_manager.get_child())
-            context = copy_context()
-            context.run(_set_config_context, child_config)
-            func_to_check = (
-                self._run if self.__class__._arun is BaseTool._arun else self._arun
-            )
-            if signature(func_to_check).parameters.get("run_manager"):
-                tool_kwargs["run_manager"] = run_manager
-            if config_param := _get_runnable_config_param(func_to_check):
-                tool_kwargs[config_param] = config
-
-            coro = context.run(self._arun, *tool_args, **tool_kwargs)
-            if asyncio_accepts_context():
-                response = await asyncio.create_task(coro, context=context)  # type: ignore
-            else:
-                response = await coro
-            if self.response_format == "content_and_artifact":
-                if not isinstance(response, tuple) or len(response) != 2:
-                    msg = (
-                        "Since response_format='content_and_artifact' "
-                        "a two-tuple of the message content and raw tool output is "
-                        f"expected. Instead generated response of type: "
-                        f"{type(response)}."
-                    )
-                    error_to_raise = ValueError(msg)
-                else:
-                    content, artifact = response
-            else:
-                content = response
-        except ValidationError as e:
-            if not self.handle_validation_error:
-                error_to_raise = e
-            else:
-                content = _handle_validation_error(e, flag=self.handle_validation_error)
-                status = "error"
-        except ToolException as e:
-            if not self.handle_tool_error:
-                error_to_raise = e
-            else:
-                content = _handle_tool_error(e, flag=self.handle_tool_error)
-                status = "error"
-        except (Exception, KeyboardInterrupt) as e:
-            error_to_raise = e
-
-        if error_to_raise:
-            await run_manager.on_tool_error(error_to_raise)
-            raise error_to_raise
-
-        output = _format_output(content, artifact, tool_call_id, self.name, status)
-        await run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)
-        return output
-
-    @deprecated("0.1.47", alternative="invoke", removal="1.0")
-    def __call__(self, tool_input: str, callbacks: Callbacks = None) -> str:
-        """Make tool callable."""
-        return self.run(tool_input, callbacks=callbacks)
-
-
-def _is_tool_call(x: Any) -> bool:
-    return isinstance(x, dict) and x.get("type") == "tool_call"
-
-
-def _handle_validation_error(
-    e: Union[ValidationError, ValidationErrorV1],
-    *,
-    flag: Union[
-        Literal[True], str, Callable[[Union[ValidationError, ValidationErrorV1]], str]
-    ],
-) -> str:
-    if isinstance(flag, bool):
-        content = "Tool input validation error"
-    elif isinstance(flag, str):
-        content = flag
-    elif callable(flag):
-        content = flag(e)
-    else:
-        msg = (
-            f"Got unexpected type of `handle_validation_error`. Expected bool, "
-            f"str or callable. Received: {flag}"
-        )
-        raise ValueError(msg)  # noqa: TRY004
-    return content
-
-
-def _handle_tool_error(
-    e: ToolException,
-    *,
-    flag: Optional[Union[Literal[True], str, Callable[[ToolException], str]]],
-) -> str:
-    if isinstance(flag, bool):
-        content = e.args[0] if e.args else "Tool execution error"
-    elif isinstance(flag, str):
-        content = flag
-    elif callable(flag):
-        content = flag(e)
-    else:
-        msg = (
-            f"Got unexpected type of `handle_tool_error`. Expected bool, str "
-            f"or callable. Received: {flag}"
-        )
-        raise ValueError(msg)  # noqa: TRY004
-    return content
-
-
-def _prep_run_args(
-    input: Union[str, dict, ToolCall],
-    config: Optional[RunnableConfig],
-    **kwargs: Any,
-) -> tuple[Union[str, dict], dict]:
-    config = ensure_config(config)
-    if _is_tool_call(input):
-        tool_call_id: Optional[str] = cast(ToolCall, input)["id"]
-        tool_input: Union[str, dict] = cast(ToolCall, input)["args"].copy()
-    else:
-        tool_call_id = None
-        tool_input = cast(Union[str, dict], input)
-    return (
-        tool_input,
-        dict(
-            callbacks=config.get("callbacks"),
-            tags=config.get("tags"),
-            metadata=config.get("metadata"),
-            run_name=config.get("run_name"),
-            run_id=config.pop("run_id", None),
-            config=config,
-            tool_call_id=tool_call_id,
-            **kwargs,
-        ),
-    )
-
-
-def _format_output(
-    content: Any,
-    artifact: Any,
-    tool_call_id: Optional[str],
-    name: str,
-    status: str,
-) -> Union[ToolOutputMixin, Any]:
-    if isinstance(content, ToolOutputMixin) or not tool_call_id:
-        return content
-    if not _is_message_content_type(content):
-        content = _stringify(content)
-    return ToolMessage(
-        content,
-        artifact=artifact,
-        tool_call_id=tool_call_id,
-        name=name,
-        status=status,
-    )
-
-
-def _is_message_content_type(obj: Any) -> bool:
-    """Check for OpenAI or Anthropic format tool message content."""
-    return (
-        isinstance(obj, str)
-        or isinstance(obj, list)
-        and all(_is_message_content_block(e) for e in obj)
-    )
-
-
-def _is_message_content_block(obj: Any) -> bool:
-    """Check for OpenAI or Anthropic format tool message content blocks."""
-    if isinstance(obj, str):
-        return True
-    elif isinstance(obj, dict):
-        return obj.get("type", None) in ("text", "image_url", "image", "json")
-    else:
-        return False
-
-
-def _stringify(content: Any) -> str:
-    try:
-        return json.dumps(content, ensure_ascii=False)
-    except Exception:
-        return str(content)
-
-
-def _get_type_hints(func: Callable) -> Optional[dict[str, type]]:
-    if isinstance(func, functools.partial):
-        func = func.func
-    try:
-        return get_type_hints(func)
-    except Exception:
-        return None
-
-
-def _get_runnable_config_param(func: Callable) -> Optional[str]:
-    type_hints = _get_type_hints(func)
-    if not type_hints:
-        return None
-    for name, type_ in type_hints.items():
-        if type_ is RunnableConfig:
-            return name
-    return None
-
-
-class InjectedToolArg:
-    """Annotation for a Tool arg that is **not** meant to be generated by a model."""
-
-
-class InjectedToolCallId(InjectedToolArg):
-    r'''Annotation for injecting the tool_call_id.
-
-    Example:
-        ..code-block:: python
-
-            from typing_extensions import Annotated
-
-            from langchain_core.messages import ToolMessage
-            from langchain_core.tools import tool, InjectedToolCallID
-
-            @tool
-            def foo(x: int, tool_call_id: Annotated[str, InjectedToolCallID]) -> ToolMessage:
-                """Return x."""
-                return ToolMessage(str(x), artifact=x, name="foo", tool_call_id=tool_call_id)
-    '''  # noqa: E501
-
-
-def _is_injected_arg_type(
-    type_: type, injected_type: Optional[type[InjectedToolArg]] = None
-) -> bool:
-    injected_type = injected_type or InjectedToolArg
-    return any(
-        isinstance(arg, injected_type)
-        or (isinstance(arg, type) and issubclass(arg, injected_type))
-        for arg in get_args(type_)[1:]
-    )
-
-
-def get_all_basemodel_annotations(
-    cls: Union[TypeBaseModel, Any], *, default_to_bound: bool = True
-) -> dict[str, type]:
-    # cls has no subscript: cls = FooBar
-    if isinstance(cls, type):
-        annotations: dict[str, type] = {}
-        for name, param in inspect.signature(cls).parameters.items():
-            # Exclude hidden init args added by pydantic Config. For example if
-            # BaseModel(extra="allow") then "extra_data" will part of init sig.
-            if (
-                fields := getattr(cls, "model_fields", {})  # pydantic v2+
-                or getattr(cls, "__fields__", {})  # pydantic v1
-            ) and name not in fields:
-                continue
-            annotations[name] = param.annotation
-        orig_bases: tuple = getattr(cls, "__orig_bases__", ())
-    # cls has subscript: cls = FooBar[int]
-    else:
-        annotations = get_all_basemodel_annotations(
-            get_origin(cls), default_to_bound=False
-        )
-        orig_bases = (cls,)
-
-    # Pydantic v2 automatically resolves inherited generics, Pydantic v1 does not.
-    if not (isinstance(cls, type) and is_pydantic_v2_subclass(cls)):
-        # if cls = FooBar inherits from Baz[str], orig_bases will contain Baz[str]
-        # if cls = FooBar inherits from Baz, orig_bases will contain Baz
-        # if cls = FooBar[int], orig_bases will contain FooBar[int]
-        for parent in orig_bases:
-            # if class = FooBar inherits from Baz, parent = Baz
-            if isinstance(parent, type) and is_pydantic_v1_subclass(parent):
-                annotations.update(
-                    get_all_basemodel_annotations(parent, default_to_bound=False)
-                )
-                continue
-
-            parent_origin = get_origin(parent)
-
-            # if class = FooBar inherits from non-pydantic class
-            if not parent_origin:
-                continue
-
-            # if class = FooBar inherits from Baz[str]:
-            # parent = Baz[str],
-            # parent_origin = Baz,
-            # generic_type_vars = (type vars in Baz)
-            # generic_map = {type var in Baz: str}
-            generic_type_vars: tuple = getattr(parent_origin, "__parameters__", ())
-            generic_map = dict(zip(generic_type_vars, get_args(parent)))
-            for field in getattr(parent_origin, "__annotations__", {}):
-                annotations[field] = _replace_type_vars(
-                    annotations[field], generic_map, default_to_bound
-                )
-
-    return {
-        k: _replace_type_vars(v, default_to_bound=default_to_bound)
-        for k, v in annotations.items()
-    }
-
-
-def _replace_type_vars(
-    type_: type,
-    generic_map: Optional[dict[TypeVar, type]] = None,
-    default_to_bound: bool = True,
-) -> type:
-    generic_map = generic_map or {}
-    if isinstance(type_, TypeVar):
-        if type_ in generic_map:
-            return generic_map[type_]
-        elif default_to_bound:
-            return type_.__bound__ or Any
-        else:
-            return type_
-    elif (origin := get_origin(type_)) and (args := get_args(type_)):
-        new_args = tuple(
-            _replace_type_vars(arg, generic_map, default_to_bound) for arg in args
-        )
-        return _py_38_safe_origin(origin)[new_args]  # type: ignore[index]
-    else:
-        return type_
-
-
-class BaseToolkit(BaseModel, ABC):
-    """Base Toolkit representing a collection of related tools."""
-
-    @abstractmethod
-    def get_tools(self) -> list[BaseTool]:
-        """Get the tools in the toolkit."""
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tools/convert.py ./custom_langchain_core/tools/convert.py
--- .venv/lib/python3.12/site-packages/langchain_core/tools/convert.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tools/convert.py	1970-01-01 09:00:00
@@ -1,423 +0,0 @@
-import inspect
-from typing import Any, Callable, Literal, Optional, Union, get_type_hints, overload
-
-from pydantic import BaseModel, Field, create_model
-
-from langchain_core.callbacks import Callbacks
-from langchain_core.runnables import Runnable
-from langchain_core.tools.base import BaseTool
-from langchain_core.tools.simple import Tool
-from langchain_core.tools.structured import StructuredTool
-
-
-@overload
-def tool(
-    *,
-    return_direct: bool = False,
-    args_schema: Optional[type] = None,
-    infer_schema: bool = True,
-    response_format: Literal["content", "content_and_artifact"] = "content",
-    parse_docstring: bool = False,
-    error_on_invalid_docstring: bool = True,
-) -> Callable[[Union[Callable, Runnable]], BaseTool]: ...
-
-
-@overload
-def tool(
-    name_or_callable: str,
-    runnable: Runnable,
-    *,
-    return_direct: bool = False,
-    args_schema: Optional[type] = None,
-    infer_schema: bool = True,
-    response_format: Literal["content", "content_and_artifact"] = "content",
-    parse_docstring: bool = False,
-    error_on_invalid_docstring: bool = True,
-) -> BaseTool: ...
-
-
-@overload
-def tool(
-    name_or_callable: Callable,
-    *,
-    return_direct: bool = False,
-    args_schema: Optional[type] = None,
-    infer_schema: bool = True,
-    response_format: Literal["content", "content_and_artifact"] = "content",
-    parse_docstring: bool = False,
-    error_on_invalid_docstring: bool = True,
-) -> BaseTool: ...
-
-
-@overload
-def tool(
-    name_or_callable: str,
-    *,
-    return_direct: bool = False,
-    args_schema: Optional[type] = None,
-    infer_schema: bool = True,
-    response_format: Literal["content", "content_and_artifact"] = "content",
-    parse_docstring: bool = False,
-    error_on_invalid_docstring: bool = True,
-) -> Callable[[Union[Callable, Runnable]], BaseTool]: ...
-
-
-def tool(
-    name_or_callable: Optional[Union[str, Callable]] = None,
-    runnable: Optional[Runnable] = None,
-    *args: Any,
-    return_direct: bool = False,
-    args_schema: Optional[type] = None,
-    infer_schema: bool = True,
-    response_format: Literal["content", "content_and_artifact"] = "content",
-    parse_docstring: bool = False,
-    error_on_invalid_docstring: bool = True,
-) -> Union[
-    BaseTool,
-    Callable[[Union[Callable, Runnable]], BaseTool],
-]:
-    """Make tools out of functions, can be used with or without arguments.
-
-    Args:
-        name_or_callable: Optional name of the tool or the callable to be
-            converted to a tool. Must be provided as a positional argument.
-        runnable: Optional runnable to convert to a tool. Must be provided as a
-            positional argument.
-        return_direct: Whether to return directly from the tool rather
-            than continuing the agent loop. Defaults to False.
-        args_schema: optional argument schema for user to specify.
-            Defaults to None.
-        infer_schema: Whether to infer the schema of the arguments from
-            the function's signature. This also makes the resultant tool
-            accept a dictionary input to its `run()` function.
-            Defaults to True.
-        response_format: The tool response format. If "content" then the output of
-            the tool is interpreted as the contents of a ToolMessage. If
-            "content_and_artifact" then the output is expected to be a two-tuple
-            corresponding to the (content, artifact) of a ToolMessage.
-            Defaults to "content".
-        parse_docstring: if ``infer_schema`` and ``parse_docstring``, will attempt to
-            parse parameter descriptions from Google Style function docstrings.
-            Defaults to False.
-        error_on_invalid_docstring: if ``parse_docstring`` is provided, configure
-            whether to raise ValueError on invalid Google Style docstrings.
-            Defaults to True.
-
-    Returns:
-        The tool.
-
-    Requires:
-        - Function must be of type (str) -> str
-        - Function must have a docstring
-
-    Examples:
-        .. code-block:: python
-
-            @tool
-            def search_api(query: str) -> str:
-                # Searches the API for the query.
-                return
-
-            @tool("search", return_direct=True)
-            def search_api(query: str) -> str:
-                # Searches the API for the query.
-                return
-
-            @tool(response_format="content_and_artifact")
-            def search_api(query: str) -> Tuple[str, dict]:
-                return "partial json of results", {"full": "object of results"}
-
-    .. versionadded:: 0.2.14
-    Parse Google-style docstrings:
-
-        .. code-block:: python
-
-            @tool(parse_docstring=True)
-            def foo(bar: str, baz: int) -> str:
-                \"\"\"The foo.
-
-                Args:
-                    bar: The bar.
-                    baz: The baz.
-                \"\"\"
-                return bar
-
-            foo.args_schema.model_json_schema()
-
-        .. code-block:: python
-
-            {
-                "title": "foo",
-                "description": "The foo.",
-                "type": "object",
-                "properties": {
-                    "bar": {
-                        "title": "Bar",
-                        "description": "The bar.",
-                        "type": "string"
-                    },
-                    "baz": {
-                        "title": "Baz",
-                        "description": "The baz.",
-                        "type": "integer"
-                    }
-                },
-                "required": [
-                    "bar",
-                    "baz"
-                ]
-            }
-
-        Note that parsing by default will raise ``ValueError`` if the docstring
-        is considered invalid. A docstring is considered invalid if it contains
-        arguments not in the function signature, or is unable to be parsed into
-        a summary and "Args:" blocks. Examples below:
-
-        .. code-block:: python
-
-            # No args section
-            def invalid_docstring_1(bar: str, baz: int) -> str:
-                \"\"\"The foo.\"\"\"
-                return bar
-
-            # Improper whitespace between summary and args section
-            def invalid_docstring_2(bar: str, baz: int) -> str:
-                \"\"\"The foo.
-                Args:
-                    bar: The bar.
-                    baz: The baz.
-                \"\"\"
-                return bar
-
-            # Documented args absent from function signature
-            def invalid_docstring_3(bar: str, baz: int) -> str:
-                \"\"\"The foo.
-
-                Args:
-                    banana: The bar.
-                    monkey: The baz.
-                \"\"\"
-                return bar
-    """
-
-    def _create_tool_factory(
-        tool_name: str,
-    ) -> Callable[[Union[Callable, Runnable]], BaseTool]:
-        """Create a decorator that takes a callable and returns a tool.
-
-        Args:
-            tool_name: The name that will be assigned to the tool.
-
-        Returns:
-            A function that takes a callable or Runnable and returns a tool.
-        """
-
-        def _tool_factory(dec_func: Union[Callable, Runnable]) -> BaseTool:
-            if isinstance(dec_func, Runnable):
-                runnable = dec_func
-
-                if runnable.input_schema.model_json_schema().get("type") != "object":
-                    msg = "Runnable must have an object schema."
-                    raise ValueError(msg)
-
-                async def ainvoke_wrapper(
-                    callbacks: Optional[Callbacks] = None, **kwargs: Any
-                ) -> Any:
-                    return await runnable.ainvoke(kwargs, {"callbacks": callbacks})
-
-                def invoke_wrapper(
-                    callbacks: Optional[Callbacks] = None, **kwargs: Any
-                ) -> Any:
-                    return runnable.invoke(kwargs, {"callbacks": callbacks})
-
-                coroutine = ainvoke_wrapper
-                func = invoke_wrapper
-                schema: Optional[type[BaseModel]] = runnable.input_schema
-                description = repr(runnable)
-            elif inspect.iscoroutinefunction(dec_func):
-                coroutine = dec_func
-                func = None
-                schema = args_schema
-                description = None
-            else:
-                coroutine = None
-                func = dec_func
-                schema = args_schema
-                description = None
-
-            if infer_schema or args_schema is not None:
-                return StructuredTool.from_function(
-                    func,
-                    coroutine,
-                    name=tool_name,
-                    description=description,
-                    return_direct=return_direct,
-                    args_schema=schema,
-                    infer_schema=infer_schema,
-                    response_format=response_format,
-                    parse_docstring=parse_docstring,
-                    error_on_invalid_docstring=error_on_invalid_docstring,
-                )
-            # If someone doesn't want a schema applied, we must treat it as
-            # a simple string->string function
-            if dec_func.__doc__ is None:
-                msg = (
-                    "Function must have a docstring if "
-                    "description not provided and infer_schema is False."
-                )
-                raise ValueError(msg)
-            return Tool(
-                name=tool_name,
-                func=func,
-                description=f"{tool_name} tool",
-                return_direct=return_direct,
-                coroutine=coroutine,
-                response_format=response_format,
-            )
-
-        return _tool_factory
-
-    if len(args) != 0:
-        # Triggered if a user attempts to use positional arguments that
-        # do not exist in the function signature
-        # e.g., @tool("name", runnable, "extra_arg")
-        # Here, "extra_arg" is not a valid argument
-        msg = "Too many arguments for tool decorator. A decorator "
-        raise ValueError(msg)
-
-    if runnable is not None:
-        # tool is used as a function
-        # tool_from_runnable = tool("name", runnable)
-        if not name_or_callable:
-            msg = "Runnable without name for tool constructor"
-            raise ValueError(msg)
-        if not isinstance(name_or_callable, str):
-            msg = "Name must be a string for tool constructor"
-            raise ValueError(msg)
-        return _create_tool_factory(name_or_callable)(runnable)
-    elif name_or_callable is not None:
-        if callable(name_or_callable) and hasattr(name_or_callable, "__name__"):
-            # Used as a decorator without parameters
-            # @tool
-            # def my_tool():
-            #    pass
-            return _create_tool_factory(name_or_callable.__name__)(name_or_callable)
-        elif isinstance(name_or_callable, str):
-            # Used with a new name for the tool
-            # @tool("search")
-            # def my_tool():
-            #    pass
-            #
-            # or
-            #
-            # @tool("search", parse_docstring=True)
-            # def my_tool():
-            #    pass
-            return _create_tool_factory(name_or_callable)
-        else:
-            msg = (
-                f"The first argument must be a string or a callable with a __name__ "
-                f"for tool decorator. Got {type(name_or_callable)}"
-            )
-            raise ValueError(msg)
-    else:
-        # Tool is used as a decorator with parameters specified
-        # @tool(parse_docstring=True)
-        # def my_tool():
-        #    pass
-        def _partial(func: Union[Callable, Runnable]) -> BaseTool:
-            """Partial function that takes a callable and returns a tool."""
-            name_ = func.get_name() if isinstance(func, Runnable) else func.__name__
-            tool_factory = _create_tool_factory(name_)
-            return tool_factory(func)
-
-        return _partial
-
-
-def _get_description_from_runnable(runnable: Runnable) -> str:
-    """Generate a placeholder description of a runnable."""
-    input_schema = runnable.input_schema.model_json_schema()
-    return f"Takes {input_schema}."
-
-
-def _get_schema_from_runnable_and_arg_types(
-    runnable: Runnable,
-    name: str,
-    arg_types: Optional[dict[str, type]] = None,
-) -> type[BaseModel]:
-    """Infer args_schema for tool."""
-    if arg_types is None:
-        try:
-            arg_types = get_type_hints(runnable.InputType)
-        except TypeError as e:
-            msg = (
-                "Tool input must be str or dict. If dict, dict arguments must be "
-                "typed. Either annotate types (e.g., with TypedDict) or pass "
-                f"arg_types into `.as_tool` to specify. {str(e)}"
-            )
-            raise TypeError(msg) from e
-    fields = {key: (key_type, Field(...)) for key, key_type in arg_types.items()}
-    return create_model(name, **fields)  # type: ignore
-
-
-def convert_runnable_to_tool(
-    runnable: Runnable,
-    args_schema: Optional[type[BaseModel]] = None,
-    *,
-    name: Optional[str] = None,
-    description: Optional[str] = None,
-    arg_types: Optional[dict[str, type]] = None,
-) -> BaseTool:
-    """Convert a Runnable into a BaseTool.
-
-    Args:
-        runnable: The runnable to convert.
-        args_schema: The schema for the tool's input arguments. Defaults to None.
-        name: The name of the tool. Defaults to None.
-        description: The description of the tool. Defaults to None.
-        arg_types: The types of the arguments. Defaults to None.
-
-    Returns:
-        The tool.
-    """
-    if args_schema:
-        runnable = runnable.with_types(input_type=args_schema)
-    description = description or _get_description_from_runnable(runnable)
-    name = name or runnable.get_name()
-
-    schema = runnable.input_schema.model_json_schema()
-    if schema.get("type") == "string":
-        return Tool(
-            name=name,
-            func=runnable.invoke,
-            coroutine=runnable.ainvoke,
-            description=description,
-        )
-    else:
-
-        async def ainvoke_wrapper(
-            callbacks: Optional[Callbacks] = None, **kwargs: Any
-        ) -> Any:
-            return await runnable.ainvoke(kwargs, config={"callbacks": callbacks})
-
-        def invoke_wrapper(callbacks: Optional[Callbacks] = None, **kwargs: Any) -> Any:
-            return runnable.invoke(kwargs, config={"callbacks": callbacks})
-
-        if (
-            arg_types is None
-            and schema.get("type") == "object"
-            and schema.get("properties")
-        ):
-            args_schema = runnable.input_schema
-        else:
-            args_schema = _get_schema_from_runnable_and_arg_types(
-                runnable, name, arg_types=arg_types
-            )
-
-        return StructuredTool.from_function(
-            name=name,
-            func=invoke_wrapper,
-            coroutine=ainvoke_wrapper,
-            description=description,
-            args_schema=args_schema,
-        )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tools/render.py ./custom_langchain_core/tools/render.py
--- .venv/lib/python3.12/site-packages/langchain_core/tools/render.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tools/render.py	1970-01-01 09:00:00
@@ -1,65 +0,0 @@
-from __future__ import annotations
-
-from inspect import signature
-from typing import Callable
-
-from langchain_core.tools.base import BaseTool
-
-ToolsRenderer = Callable[[list[BaseTool]], str]
-
-
-def render_text_description(tools: list[BaseTool]) -> str:
-    """Render the tool name and description in plain text.
-
-    Args:
-        tools: The tools to render.
-
-    Returns:
-        The rendered text.
-
-    Output will be in the format of:
-
-    .. code-block:: markdown
-
-        search: This tool is used for search
-        calculator: This tool is used for math
-    """
-    descriptions = []
-    for tool in tools:
-        if hasattr(tool, "func") and tool.func:
-            sig = signature(tool.func)
-            description = f"{tool.name}{sig} - {tool.description}"
-        else:
-            description = f"{tool.name} - {tool.description}"
-
-        descriptions.append(description)
-    return "\n".join(descriptions)
-
-
-def render_text_description_and_args(tools: list[BaseTool]) -> str:
-    """Render the tool name, description, and args in plain text.
-
-    Args:
-        tools: The tools to render.
-
-    Returns:
-        The rendered text.
-
-    Output will be in the format of:
-
-    .. code-block:: markdown
-
-        search: This tool is used for search, args: {"query": {"type": "string"}}
-        calculator: This tool is used for math, \
-args: {"expression": {"type": "string"}}
-    """
-    tool_strings = []
-    for tool in tools:
-        args_schema = str(tool.args)
-        if hasattr(tool, "func") and tool.func:
-            sig = signature(tool.func)
-            description = f"{tool.name}{sig} - {tool.description}"
-        else:
-            description = f"{tool.name} - {tool.description}"
-        tool_strings.append(f"{description}, args: {args_schema}")
-    return "\n".join(tool_strings)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tools/retriever.py ./custom_langchain_core/tools/retriever.py
--- .venv/lib/python3.12/site-packages/langchain_core/tools/retriever.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tools/retriever.py	1970-01-01 09:00:00
@@ -1,113 +0,0 @@
-from __future__ import annotations
-
-from functools import partial
-from typing import Literal, Optional, Union
-
-from pydantic import BaseModel, Field
-
-from langchain_core.callbacks import Callbacks
-from langchain_core.documents import Document
-from langchain_core.prompts import (
-    BasePromptTemplate,
-    PromptTemplate,
-    aformat_document,
-    format_document,
-)
-from langchain_core.retrievers import BaseRetriever
-from langchain_core.tools.simple import Tool
-
-
-class RetrieverInput(BaseModel):
-    """Input to the retriever."""
-
-    query: str = Field(description="query to look up in retriever")
-
-
-def _get_relevant_documents(
-    query: str,
-    retriever: BaseRetriever,
-    document_prompt: BasePromptTemplate,
-    document_separator: str,
-    callbacks: Callbacks = None,
-    response_format: Literal["content", "content_and_artifact"] = "content",
-) -> Union[str, tuple[str, list[Document]]]:
-    docs = retriever.invoke(query, config={"callbacks": callbacks})
-    content = document_separator.join(
-        format_document(doc, document_prompt) for doc in docs
-    )
-    if response_format == "content_and_artifact":
-        return (content, docs)
-
-    return content
-
-
-async def _aget_relevant_documents(
-    query: str,
-    retriever: BaseRetriever,
-    document_prompt: BasePromptTemplate,
-    document_separator: str,
-    callbacks: Callbacks = None,
-    response_format: Literal["content", "content_and_artifact"] = "content",
-) -> Union[str, tuple[str, list[Document]]]:
-    docs = await retriever.ainvoke(query, config={"callbacks": callbacks})
-    content = document_separator.join(
-        [await aformat_document(doc, document_prompt) for doc in docs]
-    )
-
-    if response_format == "content_and_artifact":
-        return (content, docs)
-
-    return content
-
-
-def create_retriever_tool(
-    retriever: BaseRetriever,
-    name: str,
-    description: str,
-    *,
-    document_prompt: Optional[BasePromptTemplate] = None,
-    document_separator: str = "\n\n",
-    response_format: Literal["content", "content_and_artifact"] = "content",
-) -> Tool:
-    """Create a tool to do retrieval of documents.
-
-    Args:
-        retriever: The retriever to use for the retrieval
-        name: The name for the tool. This will be passed to the language model,
-            so should be unique and somewhat descriptive.
-        description: The description for the tool. This will be passed to the language
-            model, so should be descriptive.
-        document_prompt: The prompt to use for the document. Defaults to None.
-        document_separator: The separator to use between documents. Defaults to "\n\n".
-        response_format: The tool response format. If "content" then the output of
-                the tool is interpreted as the contents of a ToolMessage. If
-                "content_and_artifact" then the output is expected to be a two-tuple
-                corresponding to the (content, artifact) of a ToolMessage (artifact
-                being a list of documents in this case). Defaults to "content".
-
-    Returns:
-        Tool class to pass to an agent.
-    """
-    document_prompt = document_prompt or PromptTemplate.from_template("{page_content}")
-    func = partial(
-        _get_relevant_documents,
-        retriever=retriever,
-        document_prompt=document_prompt,
-        document_separator=document_separator,
-        response_format=response_format,
-    )
-    afunc = partial(
-        _aget_relevant_documents,
-        retriever=retriever,
-        document_prompt=document_prompt,
-        document_separator=document_separator,
-        response_format=response_format,
-    )
-    return Tool(
-        name=name,
-        description=description,
-        func=func,
-        coroutine=afunc,
-        args_schema=RetrieverInput,
-        response_format=response_format,
-    )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tools/simple.py ./custom_langchain_core/tools/simple.py
--- .venv/lib/python3.12/site-packages/langchain_core/tools/simple.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tools/simple.py	1970-01-01 09:00:00
@@ -1,175 +0,0 @@
-from __future__ import annotations
-
-from collections.abc import Awaitable
-from inspect import signature
-from typing import (
-    Any,
-    Callable,
-    Optional,
-    Union,
-)
-
-from langchain_core.callbacks import (
-    AsyncCallbackManagerForToolRun,
-    CallbackManagerForToolRun,
-)
-from langchain_core.messages import ToolCall
-from langchain_core.runnables import RunnableConfig, run_in_executor
-from langchain_core.tools.base import (
-    ArgsSchema,
-    BaseTool,
-    ToolException,
-    _get_runnable_config_param,
-)
-
-
-class Tool(BaseTool):
-    """Tool that takes in function or coroutine directly."""
-
-    description: str = ""
-    func: Optional[Callable[..., str]]
-    """The function to run when the tool is called."""
-    coroutine: Optional[Callable[..., Awaitable[str]]] = None
-    """The asynchronous version of the function."""
-
-    # --- Runnable ---
-
-    async def ainvoke(
-        self,
-        input: Union[str, dict, ToolCall],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Any:
-        if not self.coroutine:
-            # If the tool does not implement async, fall back to default implementation
-            return await run_in_executor(config, self.invoke, input, config, **kwargs)
-
-        return await super().ainvoke(input, config, **kwargs)
-
-    # --- Tool ---
-
-    @property
-    def args(self) -> dict:
-        """The tool's input arguments.
-
-        Returns:
-            The input arguments for the tool.
-        """
-        if self.args_schema is not None:
-            if isinstance(self.args_schema, dict):
-                json_schema = self.args_schema
-            else:
-                json_schema = self.args_schema.model_json_schema()
-            return json_schema["properties"]
-        # For backwards compatibility, if the function signature is ambiguous,
-        # assume it takes a single string input.
-        return {"tool_input": {"type": "string"}}
-
-    def _to_args_and_kwargs(
-        self, tool_input: Union[str, dict], tool_call_id: Optional[str]
-    ) -> tuple[tuple, dict]:
-        """Convert tool input to pydantic model."""
-        args, kwargs = super()._to_args_and_kwargs(tool_input, tool_call_id)
-        # For backwards compatibility. The tool must be run with a single input
-        all_args = list(args) + list(kwargs.values())
-        if len(all_args) != 1:
-            msg = (
-                f"""Too many arguments to single-input tool {self.name}.
-                Consider using StructuredTool instead."""
-                f" Args: {all_args}"
-            )
-            raise ToolException(msg)
-        return tuple(all_args), {}
-
-    def _run(
-        self,
-        *args: Any,
-        config: RunnableConfig,
-        run_manager: Optional[CallbackManagerForToolRun] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Use the tool."""
-        if self.func:
-            if run_manager and signature(self.func).parameters.get("callbacks"):
-                kwargs["callbacks"] = run_manager.get_child()
-            if config_param := _get_runnable_config_param(self.func):
-                kwargs[config_param] = config
-            return self.func(*args, **kwargs)
-        msg = "Tool does not support sync invocation."
-        raise NotImplementedError(msg)
-
-    async def _arun(
-        self,
-        *args: Any,
-        config: RunnableConfig,
-        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Use the tool asynchronously."""
-        if self.coroutine:
-            if run_manager and signature(self.coroutine).parameters.get("callbacks"):
-                kwargs["callbacks"] = run_manager.get_child()
-            if config_param := _get_runnable_config_param(self.coroutine):
-                kwargs[config_param] = config
-            return await self.coroutine(*args, **kwargs)
-
-        # NOTE: this code is unreachable since _arun is only called if coroutine is not
-        # None.
-        return await super()._arun(
-            *args, config=config, run_manager=run_manager, **kwargs
-        )
-
-    # TODO: this is for backwards compatibility, remove in future
-    def __init__(
-        self, name: str, func: Optional[Callable], description: str, **kwargs: Any
-    ) -> None:
-        """Initialize tool."""
-        super().__init__(  # type: ignore[call-arg]
-            name=name, func=func, description=description, **kwargs
-        )
-
-    @classmethod
-    def from_function(
-        cls,
-        func: Optional[Callable],
-        name: str,  # We keep these required to support backwards compatibility
-        description: str,
-        return_direct: bool = False,
-        args_schema: Optional[ArgsSchema] = None,
-        coroutine: Optional[
-            Callable[..., Awaitable[Any]]
-        ] = None,  # This is last for compatibility, but should be after func
-        **kwargs: Any,
-    ) -> Tool:
-        """Initialize tool from a function.
-
-        Args:
-            func: The function to create the tool from.
-            name: The name of the tool.
-            description: The description of the tool.
-            return_direct: Whether to return the output directly. Defaults to False.
-            args_schema: The schema of the tool's input arguments. Defaults to None.
-            coroutine: The asynchronous version of the function. Defaults to None.
-            kwargs: Additional arguments to pass to the tool.
-
-        Returns:
-            The tool.
-
-        Raises:
-            ValueError: If the function is not provided.
-        """
-        if func is None and coroutine is None:
-            msg = "Function and/or coroutine must be provided"
-            raise ValueError(msg)
-        return cls(
-            name=name,
-            func=func,
-            coroutine=coroutine,
-            description=description,
-            return_direct=return_direct,
-            args_schema=args_schema,
-            **kwargs,
-        )
-
-
-Tool.model_rebuild()
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tools/structured.py ./custom_langchain_core/tools/structured.py
--- .venv/lib/python3.12/site-packages/langchain_core/tools/structured.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tools/structured.py	1970-01-01 09:00:00
@@ -1,219 +0,0 @@
-from __future__ import annotations
-
-import textwrap
-from collections.abc import Awaitable
-from inspect import signature
-from typing import (
-    Annotated,
-    Any,
-    Callable,
-    Literal,
-    Optional,
-    Union,
-)
-
-from pydantic import Field, SkipValidation
-
-from langchain_core.callbacks import (
-    AsyncCallbackManagerForToolRun,
-    CallbackManagerForToolRun,
-)
-from langchain_core.messages import ToolCall
-from langchain_core.runnables import RunnableConfig, run_in_executor
-from langchain_core.tools.base import (
-    FILTERED_ARGS,
-    ArgsSchema,
-    BaseTool,
-    _get_runnable_config_param,
-    create_schema_from_function,
-)
-
-
-class StructuredTool(BaseTool):
-    """Tool that can operate on any number of inputs."""
-
-    description: str = ""
-    args_schema: Annotated[ArgsSchema, SkipValidation()] = Field(
-        ..., description="The tool schema."
-    )
-    """The input arguments' schema."""
-    func: Optional[Callable[..., Any]] = None
-    """The function to run when the tool is called."""
-    coroutine: Optional[Callable[..., Awaitable[Any]]] = None
-    """The asynchronous version of the function."""
-
-    # --- Runnable ---
-
-    # TODO: Is this needed?
-    async def ainvoke(
-        self,
-        input: Union[str, dict, ToolCall],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Any:
-        if not self.coroutine:
-            # If the tool does not implement async, fall back to default implementation
-            return await run_in_executor(config, self.invoke, input, config, **kwargs)
-
-        return await super().ainvoke(input, config, **kwargs)
-
-    # --- Tool ---
-
-    @property
-    def args(self) -> dict:
-        """The tool's input arguments."""
-        if isinstance(self.args_schema, dict):
-            json_schema = self.args_schema
-        else:
-            input_schema = self.get_input_schema()
-            json_schema = input_schema.model_json_schema()
-        return json_schema["properties"]
-
-    def _run(
-        self,
-        *args: Any,
-        config: RunnableConfig,
-        run_manager: Optional[CallbackManagerForToolRun] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Use the tool."""
-        if self.func:
-            if run_manager and signature(self.func).parameters.get("callbacks"):
-                kwargs["callbacks"] = run_manager.get_child()
-            if config_param := _get_runnable_config_param(self.func):
-                kwargs[config_param] = config
-            return self.func(*args, **kwargs)
-        msg = "StructuredTool does not support sync invocation."
-        raise NotImplementedError(msg)
-
-    async def _arun(
-        self,
-        *args: Any,
-        config: RunnableConfig,
-        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Use the tool asynchronously."""
-        if self.coroutine:
-            if run_manager and signature(self.coroutine).parameters.get("callbacks"):
-                kwargs["callbacks"] = run_manager.get_child()
-            if config_param := _get_runnable_config_param(self.coroutine):
-                kwargs[config_param] = config
-            return await self.coroutine(*args, **kwargs)
-
-        # If self.coroutine is None, then this will delegate to the default
-        # implementation which is expected to delegate to _run on a separate thread.
-        return await super()._arun(
-            *args, config=config, run_manager=run_manager, **kwargs
-        )
-
-    @classmethod
-    def from_function(
-        cls,
-        func: Optional[Callable] = None,
-        coroutine: Optional[Callable[..., Awaitable[Any]]] = None,
-        name: Optional[str] = None,
-        description: Optional[str] = None,
-        return_direct: bool = False,
-        args_schema: Optional[ArgsSchema] = None,
-        infer_schema: bool = True,
-        *,
-        response_format: Literal["content", "content_and_artifact"] = "content",
-        parse_docstring: bool = False,
-        error_on_invalid_docstring: bool = False,
-        **kwargs: Any,
-    ) -> StructuredTool:
-        """Create tool from a given function.
-
-        A classmethod that helps to create a tool from a function.
-
-        Args:
-            func: The function from which to create a tool.
-            coroutine: The async function from which to create a tool.
-            name: The name of the tool. Defaults to the function name.
-            description: The description of the tool.
-                Defaults to the function docstring.
-            return_direct: Whether to return the result directly or as a callback.
-                Defaults to False.
-            args_schema: The schema of the tool's input arguments. Defaults to None.
-            infer_schema: Whether to infer the schema from the function's signature.
-                Defaults to True.
-            response_format: The tool response format. If "content" then the output of
-                the tool is interpreted as the contents of a ToolMessage. If
-                "content_and_artifact" then the output is expected to be a two-tuple
-                corresponding to the (content, artifact) of a ToolMessage.
-                Defaults to "content".
-            parse_docstring: if ``infer_schema`` and ``parse_docstring``, will attempt
-                to parse parameter descriptions from Google Style function docstrings.
-                Defaults to False.
-            error_on_invalid_docstring: if ``parse_docstring`` is provided, configure
-                whether to raise ValueError on invalid Google Style docstrings.
-                Defaults to False.
-            kwargs: Additional arguments to pass to the tool
-
-        Returns:
-            The tool.
-
-        Raises:
-            ValueError: If the function is not provided.
-
-        Examples:
-
-            .. code-block:: python
-
-                def add(a: int, b: int) -> int:
-                    \"\"\"Add two numbers\"\"\"
-                    return a + b
-                tool = StructuredTool.from_function(add)
-                tool.run(1, 2) # 3
-        """
-        if func is not None:
-            source_function = func
-        elif coroutine is not None:
-            source_function = coroutine
-        else:
-            msg = "Function and/or coroutine must be provided"
-            raise ValueError(msg)
-        name = name or source_function.__name__
-        if args_schema is None and infer_schema:
-            # schema name is appended within function
-            args_schema = create_schema_from_function(
-                name,
-                source_function,
-                parse_docstring=parse_docstring,
-                error_on_invalid_docstring=error_on_invalid_docstring,
-                filter_args=_filter_schema_args(source_function),
-            )
-        description_ = description
-        if description is None and not parse_docstring:
-            description_ = source_function.__doc__ or None
-        if description_ is None and args_schema:
-            description_ = args_schema.__doc__ or None
-        if description_ is None:
-            msg = "Function must have a docstring if description not provided."
-            raise ValueError(msg)
-        if description is None:
-            # Only apply if using the function's docstring
-            description_ = textwrap.dedent(description_).strip()
-
-        # Description example:
-        # search_api(query: str) - Searches the API for the query.
-        description_ = f"{description_.strip()}"
-        return cls(
-            name=name,
-            func=func,
-            coroutine=coroutine,
-            args_schema=args_schema,  # type: ignore[arg-type]
-            description=description_,
-            return_direct=return_direct,
-            response_format=response_format,
-            **kwargs,
-        )
-
-
-def _filter_schema_args(func: Callable) -> list[str]:
-    filter_args = list(FILTERED_ARGS)
-    if config_param := _get_runnable_config_param(func):
-        filter_args.append(config_param)
-    # filter_args.extend(_get_non_model_params(type_hints))
-    return filter_args
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/__init__.py ./custom_langchain_core/tracers/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/__init__.py	1970-01-01 09:00:00
@@ -1,31 +0,0 @@
-"""**Tracers** are classes for tracing runs.
-
-**Class hierarchy:**
-
-.. code-block::
-
-    BaseCallbackHandler --> BaseTracer --> <name>Tracer  # Examples: LangChainTracer, RootListenersTracer
-                                       --> <name>  # Examples: LogStreamCallbackHandler
-"""  # noqa: E501
-
-__all__ = [
-    "BaseTracer",
-    "EvaluatorCallbackHandler",
-    "LangChainTracer",
-    "ConsoleCallbackHandler",
-    "Run",
-    "RunLog",
-    "RunLogPatch",
-    "LogStreamCallbackHandler",
-]
-
-from langchain_core.tracers.base import BaseTracer
-from langchain_core.tracers.evaluation import EvaluatorCallbackHandler
-from langchain_core.tracers.langchain import LangChainTracer
-from langchain_core.tracers.log_stream import (
-    LogStreamCallbackHandler,
-    RunLog,
-    RunLogPatch,
-)
-from langchain_core.tracers.schemas import Run
-from langchain_core.tracers.stdout import ConsoleCallbackHandler
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/_streaming.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/_streaming.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/base.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/base.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/context.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/context.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/core.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/core.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/evaluation.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/evaluation.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/event_stream.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/event_stream.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/langchain.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/langchain.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/langchain_v1.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/langchain_v1.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/log_stream.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/log_stream.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/memory_stream.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/memory_stream.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/root_listeners.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/root_listeners.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/run_collector.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/run_collector.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/schemas.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/schemas.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/tracers/__pycache__/stdout.cpython-312.pyc and ./custom_langchain_core/tracers/__pycache__/stdout.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/_streaming.py ./custom_langchain_core/tracers/_streaming.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/_streaming.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/_streaming.py	1970-01-01 09:00:00
@@ -1,34 +0,0 @@
-"""Internal tracers used for stream_log and astream events implementations."""
-
-import abc
-from collections.abc import AsyncIterator, Iterator
-from typing import TypeVar
-from uuid import UUID
-
-T = TypeVar("T")
-
-
-class _StreamingCallbackHandler(abc.ABC):
-    """For internal use.
-
-    This is a common mixin that the callback handlers
-    for both astream events and astream log inherit from.
-
-    The `tap_output_aiter` method is invoked in some contexts
-    to produce callbacks for intermediate results.
-    """
-
-    @abc.abstractmethod
-    def tap_output_aiter(
-        self, run_id: UUID, output: AsyncIterator[T]
-    ) -> AsyncIterator[T]:
-        """Used for internal astream_log and astream events implementations."""
-
-    @abc.abstractmethod
-    def tap_output_iter(self, run_id: UUID, output: Iterator[T]) -> Iterator[T]:
-        """Used for internal astream_log and astream events implementations."""
-
-
-__all__ = [
-    "_StreamingCallbackHandler",
-]
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/base.py ./custom_langchain_core/tracers/base.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/base.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/base.py	1970-01-01 09:00:00
@@ -1,900 +0,0 @@
-"""Base interfaces for tracing runs."""
-
-from __future__ import annotations
-
-import asyncio
-import logging
-from abc import ABC, abstractmethod
-from collections.abc import Sequence
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Optional,
-    Union,
-)
-from uuid import UUID
-
-from tenacity import RetryCallState
-
-from langchain_core.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler
-from langchain_core.exceptions import TracerException  # noqa
-from langchain_core.messages import BaseMessage
-from langchain_core.outputs import ChatGenerationChunk, GenerationChunk, LLMResult
-from langchain_core.tracers.core import _TracerCore
-from langchain_core.tracers.schemas import Run
-
-if TYPE_CHECKING:
-    from langchain_core.documents import Document
-
-logger = logging.getLogger(__name__)
-
-
-class BaseTracer(_TracerCore, BaseCallbackHandler, ABC):
-    """Base interface for tracers."""
-
-    @abstractmethod
-    def _persist_run(self, run: Run) -> None:
-        """Persist a run."""
-
-    def _start_trace(self, run: Run) -> None:
-        """Start a trace for a run."""
-        super()._start_trace(run)
-        self._on_run_create(run)
-
-    def _end_trace(self, run: Run) -> None:
-        """End a trace for a run."""
-        if not run.parent_run_id:
-            self._persist_run(run)
-        self.run_map.pop(str(run.id))
-        self._on_run_update(run)
-
-    def on_chat_model_start(
-        self,
-        serialized: dict[str, Any],
-        messages: list[list[BaseMessage]],
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Start a trace for an LLM run.
-
-        Args:
-            serialized: The serialized model.
-            messages: The messages to start the chat with.
-            run_id: The run ID.
-            tags: The tags for the run. Defaults to None.
-            parent_run_id: The parent run ID. Defaults to None.
-            metadata: The metadata for the run. Defaults to None.
-            name: The name of the run.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        chat_model_run = self._create_chat_model_run(
-            serialized=serialized,
-            messages=messages,
-            run_id=run_id,
-            parent_run_id=parent_run_id,
-            tags=tags,
-            metadata=metadata,
-            name=name,
-            **kwargs,
-        )
-        self._start_trace(chat_model_run)
-        self._on_chat_model_start(chat_model_run)
-        return chat_model_run
-
-    def on_llm_start(
-        self,
-        serialized: dict[str, Any],
-        prompts: list[str],
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Start a trace for an LLM run.
-
-        Args:
-            serialized: The serialized model.
-            prompts: The prompts to start the LLM with.
-            run_id: The run ID.
-            tags: The tags for the run. Defaults to None.
-            parent_run_id: The parent run ID. Defaults to None.
-            metadata: The metadata for the run. Defaults to None.
-            name: The name of the run.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        llm_run = self._create_llm_run(
-            serialized=serialized,
-            prompts=prompts,
-            run_id=run_id,
-            parent_run_id=parent_run_id,
-            tags=tags,
-            metadata=metadata,
-            name=name,
-            **kwargs,
-        )
-        self._start_trace(llm_run)
-        self._on_llm_start(llm_run)
-        return llm_run
-
-    def on_llm_new_token(
-        self,
-        token: str,
-        *,
-        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Run on new LLM token. Only available when streaming is enabled.
-
-        Args:
-            token: The token.
-            chunk: The chunk. Defaults to None.
-            run_id: The run ID.
-            parent_run_id: The parent run ID. Defaults to None.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        # "chat_model" is only used for the experimental new streaming_events format.
-        # This change should not affect any existing tracers.
-        llm_run = self._llm_run_with_token_event(
-            token=token,
-            run_id=run_id,
-            chunk=chunk,
-            parent_run_id=parent_run_id,
-            **kwargs,
-        )
-        self._on_llm_new_token(llm_run, token, chunk)
-        return llm_run
-
-    def on_retry(
-        self,
-        retry_state: RetryCallState,
-        *,
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> Run:
-        """Run on retry.
-
-        Args:
-            retry_state: The retry state.
-            run_id: The run ID.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        llm_run = self._llm_run_with_retry_event(
-            retry_state=retry_state,
-            run_id=run_id,
-        )
-        return llm_run
-
-    def on_llm_end(self, response: LLMResult, *, run_id: UUID, **kwargs: Any) -> Run:
-        """End a trace for an LLM run.
-
-        Args:
-            response: The response.
-            run_id: The run ID.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        # "chat_model" is only used for the experimental new streaming_events format.
-        # This change should not affect any existing tracers.
-        llm_run = self._complete_llm_run(
-            response=response,
-            run_id=run_id,
-        )
-        self._end_trace(llm_run)
-        self._on_llm_end(llm_run)
-        return llm_run
-
-    def on_llm_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> Run:
-        """Handle an error for an LLM run.
-
-        Args:
-            error: The error.
-            run_id: The run ID.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        # "chat_model" is only used for the experimental new streaming_events format.
-        # This change should not affect any existing tracers.
-        llm_run = self._errored_llm_run(
-            error=error,
-            run_id=run_id,
-        )
-        self._end_trace(llm_run)
-        self._on_llm_error(llm_run)
-        return llm_run
-
-    def on_chain_start(
-        self,
-        serialized: dict[str, Any],
-        inputs: dict[str, Any],
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        run_type: Optional[str] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Start a trace for a chain run.
-
-        Args:
-            serialized: The serialized chain.
-            inputs: The inputs for the chain.
-            run_id: The run ID.
-            tags: The tags for the run. Defaults to None.
-            parent_run_id: The parent run ID. Defaults to None.
-            metadata: The metadata for the run. Defaults to None.
-            run_type: The type of the run. Defaults to None.
-            name: The name of the run.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        chain_run = self._create_chain_run(
-            serialized=serialized,
-            inputs=inputs,
-            run_id=run_id,
-            tags=tags,
-            parent_run_id=parent_run_id,
-            metadata=metadata,
-            run_type=run_type,
-            name=name,
-            **kwargs,
-        )
-        self._start_trace(chain_run)
-        self._on_chain_start(chain_run)
-        return chain_run
-
-    def on_chain_end(
-        self,
-        outputs: dict[str, Any],
-        *,
-        run_id: UUID,
-        inputs: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """End a trace for a chain run.
-
-        Args:
-            outputs: The outputs for the chain.
-            run_id: The run ID.
-            inputs: The inputs for the chain. Defaults to None.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        chain_run = self._complete_chain_run(
-            outputs=outputs,
-            run_id=run_id,
-            inputs=inputs,
-            **kwargs,
-        )
-        self._end_trace(chain_run)
-        self._on_chain_end(chain_run)
-        return chain_run
-
-    def on_chain_error(
-        self,
-        error: BaseException,
-        *,
-        inputs: Optional[dict[str, Any]] = None,
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> Run:
-        """Handle an error for a chain run.
-
-        Args:
-            error: The error.
-            inputs: The inputs for the chain. Defaults to None.
-            run_id: The run ID.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        chain_run = self._errored_chain_run(
-            error=error,
-            run_id=run_id,
-            inputs=inputs,
-            **kwargs,
-        )
-        self._end_trace(chain_run)
-        self._on_chain_error(chain_run)
-        return chain_run
-
-    def on_tool_start(
-        self,
-        serialized: dict[str, Any],
-        input_str: str,
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        inputs: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Start a trace for a tool run.
-
-        Args:
-            serialized: The serialized tool.
-            input_str: The input string.
-            run_id: The run ID.
-            tags: The tags for the run. Defaults to None.
-            parent_run_id: The parent run ID. Defaults to None.
-            metadata: The metadata for the run. Defaults to None.
-            name: The name of the run.
-            inputs: The inputs for the tool.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        tool_run = self._create_tool_run(
-            serialized=serialized,
-            input_str=input_str,
-            run_id=run_id,
-            tags=tags,
-            parent_run_id=parent_run_id,
-            metadata=metadata,
-            name=name,
-            inputs=inputs,
-            **kwargs,
-        )
-        self._start_trace(tool_run)
-        self._on_tool_start(tool_run)
-        return tool_run
-
-    def on_tool_end(self, output: Any, *, run_id: UUID, **kwargs: Any) -> Run:
-        """End a trace for a tool run.
-
-        Args:
-            output: The output for the tool.
-            run_id: The run ID.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        tool_run = self._complete_tool_run(
-            output=output,
-            run_id=run_id,
-            **kwargs,
-        )
-        self._end_trace(tool_run)
-        self._on_tool_end(tool_run)
-        return tool_run
-
-    def on_tool_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> Run:
-        """Handle an error for a tool run.
-
-        Args:
-            error: The error.
-            run_id: The run ID.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        tool_run = self._errored_tool_run(
-            error=error,
-            run_id=run_id,
-        )
-        self._end_trace(tool_run)
-        self._on_tool_error(tool_run)
-        return tool_run
-
-    def on_retriever_start(
-        self,
-        serialized: dict[str, Any],
-        query: str,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Run when the Retriever starts running.
-
-        Args:
-            serialized: The serialized retriever.
-            query: The query.
-            run_id: The run ID.
-            parent_run_id: The parent run ID. Defaults to None.
-            tags: The tags for the run. Defaults to None.
-            metadata: The metadata for the run. Defaults to None.
-            name: The name of the run.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        retrieval_run = self._create_retrieval_run(
-            serialized=serialized,
-            query=query,
-            run_id=run_id,
-            parent_run_id=parent_run_id,
-            tags=tags,
-            metadata=metadata,
-            name=name,
-            **kwargs,
-        )
-        self._start_trace(retrieval_run)
-        self._on_retriever_start(retrieval_run)
-        return retrieval_run
-
-    def on_retriever_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> Run:
-        """Run when Retriever errors.
-
-        Args:
-            error: The error.
-            run_id: The run ID.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        retrieval_run = self._errored_retrieval_run(
-            error=error,
-            run_id=run_id,
-            **kwargs,
-        )
-        self._end_trace(retrieval_run)
-        self._on_retriever_error(retrieval_run)
-        return retrieval_run
-
-    def on_retriever_end(
-        self, documents: Sequence[Document], *, run_id: UUID, **kwargs: Any
-    ) -> Run:
-        """Run when the Retriever ends running.
-
-        Args:
-            documents: The documents.
-            run_id: The run ID.
-            kwargs: Additional arguments.
-
-        Returns:
-            The run.
-        """
-        retrieval_run = self._complete_retrieval_run(
-            documents=documents,
-            run_id=run_id,
-            **kwargs,
-        )
-        self._end_trace(retrieval_run)
-        self._on_retriever_end(retrieval_run)
-        return retrieval_run
-
-    def __deepcopy__(self, memo: dict) -> BaseTracer:
-        """Deepcopy the tracer."""
-        return self
-
-    def __copy__(self) -> BaseTracer:
-        """Copy the tracer."""
-        return self
-
-
-class AsyncBaseTracer(_TracerCore, AsyncCallbackHandler, ABC):
-    """Async Base interface for tracers."""
-
-    @abstractmethod
-    async def _persist_run(self, run: Run) -> None:
-        """Persist a run."""
-
-    async def _start_trace(self, run: Run) -> None:
-        """Start a trace for a run.
-
-        Starting a trace will run concurrently with each _on_[run_type]_start method.
-        No _on_[run_type]_start callback should depend on operations in _start_trace.
-        """
-        super()._start_trace(run)
-        await self._on_run_create(run)
-
-    async def _end_trace(self, run: Run) -> None:
-        """End a trace for a run.
-
-        Ending a trace will run concurrently with each _on_[run_type]_end method.
-        No _on_[run_type]_end callback should depend on operations in _end_trace.
-        """
-        if not run.parent_run_id:
-            await self._persist_run(run)
-        self.run_map.pop(str(run.id))
-        await self._on_run_update(run)
-
-    async def on_chat_model_start(
-        self,
-        serialized: dict[str, Any],
-        messages: list[list[BaseMessage]],
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> Any:
-        chat_model_run = self._create_chat_model_run(
-            serialized=serialized,
-            messages=messages,
-            run_id=run_id,
-            parent_run_id=parent_run_id,
-            tags=tags,
-            metadata=metadata,
-            name=name,
-            **kwargs,
-        )
-        tasks = [
-            self._start_trace(chat_model_run),
-            self._on_chat_model_start(chat_model_run),
-        ]
-        await asyncio.gather(*tasks)
-        return chat_model_run
-
-    async def on_llm_start(
-        self,
-        serialized: dict[str, Any],
-        prompts: list[str],
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> None:
-        llm_run = self._create_llm_run(
-            serialized=serialized,
-            prompts=prompts,
-            run_id=run_id,
-            parent_run_id=parent_run_id,
-            tags=tags,
-            metadata=metadata,
-            **kwargs,
-        )
-        tasks = [self._start_trace(llm_run), self._on_llm_start(llm_run)]
-        await asyncio.gather(*tasks)
-
-    async def on_llm_new_token(
-        self,
-        token: str,
-        *,
-        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> None:
-        llm_run = self._llm_run_with_token_event(
-            token=token,
-            run_id=run_id,
-            chunk=chunk,
-            parent_run_id=parent_run_id,
-            **kwargs,
-        )
-        await self._on_llm_new_token(llm_run, token, chunk)
-
-    async def on_retry(
-        self,
-        retry_state: RetryCallState,
-        *,
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> None:
-        self._llm_run_with_retry_event(
-            retry_state=retry_state,
-            run_id=run_id,
-        )
-
-    async def on_llm_end(
-        self,
-        response: LLMResult,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        llm_run = self._complete_llm_run(
-            response=response,
-            run_id=run_id,
-        )
-        tasks = [self._on_llm_end(llm_run), self._end_trace(llm_run)]
-        await asyncio.gather(*tasks)
-
-    async def on_llm_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        llm_run = self._errored_llm_run(
-            error=error,
-            run_id=run_id,
-        )
-        tasks = [self._on_llm_error(llm_run), self._end_trace(llm_run)]
-        await asyncio.gather(*tasks)
-
-    async def on_chain_start(
-        self,
-        serialized: dict[str, Any],
-        inputs: dict[str, Any],
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        run_type: Optional[str] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> None:
-        chain_run = self._create_chain_run(
-            serialized=serialized,
-            inputs=inputs,
-            run_id=run_id,
-            tags=tags,
-            parent_run_id=parent_run_id,
-            metadata=metadata,
-            run_type=run_type,
-            name=name,
-            **kwargs,
-        )
-        tasks = [self._start_trace(chain_run), self._on_chain_start(chain_run)]
-        await asyncio.gather(*tasks)
-
-    async def on_chain_end(
-        self,
-        outputs: dict[str, Any],
-        *,
-        run_id: UUID,
-        inputs: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> None:
-        chain_run = self._complete_chain_run(
-            outputs=outputs,
-            run_id=run_id,
-            inputs=inputs,
-            **kwargs,
-        )
-        tasks = [self._end_trace(chain_run), self._on_chain_end(chain_run)]
-        await asyncio.gather(*tasks)
-
-    async def on_chain_error(
-        self,
-        error: BaseException,
-        *,
-        inputs: Optional[dict[str, Any]] = None,
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> None:
-        chain_run = self._errored_chain_run(
-            error=error,
-            inputs=inputs,
-            run_id=run_id,
-            **kwargs,
-        )
-        tasks = [self._end_trace(chain_run), self._on_chain_error(chain_run)]
-        await asyncio.gather(*tasks)
-
-    async def on_tool_start(
-        self,
-        serialized: dict[str, Any],
-        input_str: str,
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        inputs: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> None:
-        tool_run = self._create_tool_run(
-            serialized=serialized,
-            input_str=input_str,
-            run_id=run_id,
-            tags=tags,
-            parent_run_id=parent_run_id,
-            metadata=metadata,
-            inputs=inputs,
-            **kwargs,
-        )
-        tasks = [self._start_trace(tool_run), self._on_tool_start(tool_run)]
-        await asyncio.gather(*tasks)
-
-    async def on_tool_end(
-        self,
-        output: Any,
-        *,
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> None:
-        tool_run = self._complete_tool_run(
-            output=output,
-            run_id=run_id,
-            **kwargs,
-        )
-        tasks = [self._end_trace(tool_run), self._on_tool_end(tool_run)]
-        await asyncio.gather(*tasks)
-
-    async def on_tool_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        tool_run = self._errored_tool_run(
-            error=error,
-            run_id=run_id,
-        )
-        tasks = [self._end_trace(tool_run), self._on_tool_error(tool_run)]
-        await asyncio.gather(*tasks)
-
-    async def on_retriever_start(
-        self,
-        serialized: dict[str, Any],
-        query: str,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> None:
-        retriever_run = self._create_retrieval_run(
-            serialized=serialized,
-            query=query,
-            run_id=run_id,
-            parent_run_id=parent_run_id,
-            tags=tags,
-            metadata=metadata,
-            name=name,
-        )
-        tasks = [
-            self._start_trace(retriever_run),
-            self._on_retriever_start(retriever_run),
-        ]
-        await asyncio.gather(*tasks)
-
-    async def on_retriever_error(
-        self,
-        error: BaseException,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        retrieval_run = self._errored_retrieval_run(
-            error=error,
-            run_id=run_id,
-            **kwargs,
-        )
-        tasks = [
-            self._end_trace(retrieval_run),
-            self._on_retriever_error(retrieval_run),
-        ]
-        await asyncio.gather(*tasks)
-
-    async def on_retriever_end(
-        self,
-        documents: Sequence[Document],
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        retrieval_run = self._complete_retrieval_run(
-            documents=documents,
-            run_id=run_id,
-            **kwargs,
-        )
-        tasks = [self._end_trace(retrieval_run), self._on_retriever_end(retrieval_run)]
-        await asyncio.gather(*tasks)
-
-    async def _on_run_create(self, run: Run) -> None:
-        """Process a run upon creation."""
-
-    async def _on_run_update(self, run: Run) -> None:
-        """Process a run upon update."""
-
-    async def _on_llm_start(self, run: Run) -> None:
-        """Process the LLM Run upon start."""
-
-    async def _on_llm_end(self, run: Run) -> None:
-        """Process the LLM Run."""
-
-    async def _on_llm_error(self, run: Run) -> None:
-        """Process the LLM Run upon error."""
-
-    async def _on_llm_new_token(
-        self,
-        run: Run,
-        token: str,
-        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]],
-    ) -> None:
-        """Process new LLM token."""
-
-    async def _on_chain_start(self, run: Run) -> None:
-        """Process the Chain Run upon start."""
-
-    async def _on_chain_end(self, run: Run) -> None:
-        """Process the Chain Run."""
-
-    async def _on_chain_error(self, run: Run) -> None:
-        """Process the Chain Run upon error."""
-
-    async def _on_tool_start(self, run: Run) -> None:
-        """Process the Tool Run upon start."""
-
-    async def _on_tool_end(self, run: Run) -> None:
-        """Process the Tool Run."""
-
-    async def _on_tool_error(self, run: Run) -> None:
-        """Process the Tool Run upon error."""
-
-    async def _on_chat_model_start(self, run: Run) -> None:
-        """Process the Chat Model Run upon start."""
-
-    async def _on_retriever_start(self, run: Run) -> None:
-        """Process the Retriever Run upon start."""
-
-    async def _on_retriever_end(self, run: Run) -> None:
-        """Process the Retriever Run."""
-
-    async def _on_retriever_error(self, run: Run) -> None:
-        """Process the Retriever Run upon error."""
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/context.py ./custom_langchain_core/tracers/context.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/context.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/context.py	1970-01-01 09:00:00
@@ -1,220 +0,0 @@
-from __future__ import annotations
-
-from collections.abc import Generator
-from contextlib import contextmanager
-from contextvars import ContextVar
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Literal,
-    Optional,
-    Union,
-    cast,
-)
-from uuid import UUID
-
-from langsmith import run_helpers as ls_rh
-from langsmith import utils as ls_utils
-
-from langchain_core.tracers.langchain import LangChainTracer
-from langchain_core.tracers.run_collector import RunCollectorCallbackHandler
-from langchain_core.tracers.schemas import TracerSessionV1
-
-if TYPE_CHECKING:
-    from langsmith import Client as LangSmithClient
-
-    from langchain_core.callbacks.base import BaseCallbackHandler, Callbacks
-    from langchain_core.callbacks.manager import AsyncCallbackManager, CallbackManager
-
-# for backwards partial compatibility if this is imported by users but unused
-tracing_callback_var: Any = None
-tracing_v2_callback_var: ContextVar[Optional[LangChainTracer]] = ContextVar(
-    "tracing_callback_v2", default=None
-)
-run_collector_var: ContextVar[Optional[RunCollectorCallbackHandler]] = ContextVar(
-    "run_collector", default=None
-)
-
-
-@contextmanager
-def tracing_enabled(
-    session_name: str = "default",
-) -> Generator[TracerSessionV1, None, None]:
-    """Throw an error because this has been replaced by tracing_v2_enabled."""
-    msg = (
-        "tracing_enabled is no longer supported. Please use tracing_enabled_v2 instead."
-    )
-    raise RuntimeError(msg)
-
-
-@contextmanager
-def tracing_v2_enabled(
-    project_name: Optional[str] = None,
-    *,
-    example_id: Optional[Union[str, UUID]] = None,
-    tags: Optional[list[str]] = None,
-    client: Optional[LangSmithClient] = None,
-) -> Generator[LangChainTracer, None, None]:
-    """Instruct LangChain to log all runs in context to LangSmith.
-
-    Args:
-        project_name (str, optional): The name of the project.
-            Defaults to "default".
-        example_id (str or UUID, optional): The ID of the example.
-            Defaults to None.
-        tags (List[str], optional): The tags to add to the run.
-            Defaults to None.
-        client (LangSmithClient, optional): The client of the langsmith.
-            Defaults to None.
-
-    Yields:
-        LangChainTracer: The LangChain tracer.
-
-    Example:
-        >>> with tracing_v2_enabled():
-        ...     # LangChain code will automatically be traced
-
-        You can use this to fetch the LangSmith run URL:
-
-        >>> with tracing_v2_enabled() as cb:
-        ...     chain.invoke("foo")
-        ...     run_url = cb.get_run_url()
-    """
-    if isinstance(example_id, str):
-        example_id = UUID(example_id)
-    cb = LangChainTracer(
-        example_id=example_id,
-        project_name=project_name,
-        tags=tags,
-        client=client,
-    )
-    try:
-        tracing_v2_callback_var.set(cb)
-        yield cb
-    finally:
-        tracing_v2_callback_var.set(None)
-
-
-@contextmanager
-def collect_runs() -> Generator[RunCollectorCallbackHandler, None, None]:
-    """Collect all run traces in context.
-
-    Yields:
-        run_collector.RunCollectorCallbackHandler: The run collector callback handler.
-
-    Example:
-        >>> with collect_runs() as runs_cb:
-                chain.invoke("foo")
-                run_id = runs_cb.traced_runs[0].id
-    """
-    cb = RunCollectorCallbackHandler()
-    run_collector_var.set(cb)
-    yield cb
-    run_collector_var.set(None)
-
-
-def _get_trace_callbacks(
-    project_name: Optional[str] = None,
-    example_id: Optional[Union[str, UUID]] = None,
-    callback_manager: Optional[Union[CallbackManager, AsyncCallbackManager]] = None,
-) -> Callbacks:
-    if _tracing_v2_is_enabled():
-        project_name_ = project_name or _get_tracer_project()
-        tracer = tracing_v2_callback_var.get() or LangChainTracer(
-            project_name=project_name_,
-            example_id=example_id,
-        )
-        if callback_manager is None:
-            from langchain_core.callbacks.base import Callbacks
-
-            cb = cast(Callbacks, [tracer])
-        else:
-            if not any(
-                isinstance(handler, LangChainTracer)
-                for handler in callback_manager.handlers
-            ):
-                callback_manager.add_handler(tracer, True)
-                # If it already has a LangChainTracer, we don't need to add another one.
-                # this would likely mess up the trace hierarchy.
-            cb = callback_manager
-    else:
-        cb = None
-    return cb
-
-
-def _tracing_v2_is_enabled() -> Union[bool, Literal["local"]]:
-    if tracing_v2_callback_var.get() is not None:
-        return True
-    return ls_utils.tracing_is_enabled()
-
-
-def _get_tracer_project() -> str:
-    tracing_context = ls_rh.get_tracing_context()
-    run_tree = tracing_context["parent"]
-    if run_tree is None and tracing_context["project_name"] is not None:
-        return tracing_context["project_name"]
-    return getattr(
-        run_tree,
-        "session_name",
-        getattr(
-            # Note, if people are trying to nest @traceable functions and the
-            # tracing_v2_enabled context manager, this will likely mess up the
-            # tree structure.
-            tracing_v2_callback_var.get(),
-            "project",
-            # Have to set this to a string even though it always will return
-            # a string because `get_tracer_project` technically can return
-            # None, but only when a specific argument is supplied.
-            # Therefore, this just tricks the mypy type checker
-            str(ls_utils.get_tracer_project()),
-        ),
-    )
-
-
-_configure_hooks: list[
-    tuple[
-        ContextVar[Optional[BaseCallbackHandler]],
-        bool,
-        Optional[type[BaseCallbackHandler]],
-        Optional[str],
-    ]
-] = []
-
-
-def register_configure_hook(
-    context_var: ContextVar[Optional[Any]],
-    inheritable: bool,
-    handle_class: Optional[type[BaseCallbackHandler]] = None,
-    env_var: Optional[str] = None,
-) -> None:
-    """Register a configure hook.
-
-    Args:
-        context_var (ContextVar[Optional[Any]]): The context variable.
-        inheritable (bool): Whether the context variable is inheritable.
-        handle_class (Optional[Type[BaseCallbackHandler]], optional):
-          The callback handler class. Defaults to None.
-        env_var (Optional[str], optional): The environment variable. Defaults to None.
-
-    Raises:
-        ValueError: If env_var is set, handle_class must also be set
-          to a non-None value.
-    """
-    if env_var is not None and handle_class is None:
-        msg = "If env_var is set, handle_class must also be set to a non-None value."
-        raise ValueError(msg)
-    from langchain_core.callbacks.base import BaseCallbackHandler
-
-    _configure_hooks.append(
-        (
-            # the typings of ContextVar do not have the generic arg set as covariant
-            # so we have to cast it
-            cast(ContextVar[Optional[BaseCallbackHandler]], context_var),
-            inheritable,
-            handle_class,
-            env_var,
-        )
-    )
-
-
-register_configure_hook(run_collector_var, False)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/core.py ./custom_langchain_core/tracers/core.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/core.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/core.py	1970-01-01 09:00:00
@@ -1,586 +0,0 @@
-"""Utilities for the root listener."""
-
-from __future__ import annotations
-
-import logging
-import sys
-import traceback
-from abc import ABC, abstractmethod
-from collections.abc import Coroutine, Sequence
-from datetime import datetime, timezone
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Literal,
-    Optional,
-    Union,
-    cast,
-)
-from uuid import UUID
-
-from tenacity import RetryCallState
-
-from langchain_core.exceptions import TracerException
-from langchain_core.load import dumpd
-from langchain_core.messages import BaseMessage
-from langchain_core.outputs import (
-    ChatGeneration,
-    ChatGenerationChunk,
-    GenerationChunk,
-    LLMResult,
-)
-from langchain_core.tracers.schemas import Run
-
-if TYPE_CHECKING:
-    from langchain_core.documents import Document
-
-logger = logging.getLogger(__name__)
-
-SCHEMA_FORMAT_TYPE = Literal["original", "streaming_events"]
-
-
-class _TracerCore(ABC):
-    """Abstract base class for tracers.
-
-    This class provides common methods, and reusable methods for tracers.
-    """
-
-    log_missing_parent: bool = True
-
-    def __init__(
-        self,
-        *,
-        _schema_format: Literal[
-            "original", "streaming_events", "original+chat"
-        ] = "original",
-        **kwargs: Any,
-    ) -> None:
-        """Initialize the tracer.
-
-        Args:
-            _schema_format: Primarily changes how the inputs and outputs are
-                handled. For internal use only. This API will change.
-
-                - 'original' is the format used by all current tracers.
-                  This format is slightly inconsistent with respect to inputs
-                  and outputs.
-                - 'streaming_events' is used for supporting streaming events,
-                  for internal usage. It will likely change in the future, or
-                  be deprecated entirely in favor of a dedicated async tracer
-                  for streaming events.
-                - 'original+chat' is a format that is the same as 'original'
-                  except it does NOT raise an attribute error on_chat_model_start
-            kwargs: Additional keyword arguments that will be passed to
-                the superclass.
-        """
-        super().__init__(**kwargs)
-        self._schema_format = _schema_format  # For internal use only API will change.
-        self.run_map: dict[str, Run] = {}
-        """Map of run ID to run. Cleared on run end."""
-        self.order_map: dict[UUID, tuple[UUID, str]] = {}
-        """Map of run ID to (trace_id, dotted_order). Cleared when tracer GCed."""
-
-    @abstractmethod
-    def _persist_run(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Persist a run."""
-
-    @staticmethod
-    def _add_child_run(
-        parent_run: Run,
-        child_run: Run,
-    ) -> None:
-        """Add child run to a chain run or tool run."""
-        parent_run.child_runs.append(child_run)
-
-    @staticmethod
-    def _get_stacktrace(error: BaseException) -> str:
-        """Get the stacktrace of the parent error."""
-        msg = repr(error)
-        try:
-            if sys.version_info < (3, 10):
-                tb = traceback.format_exception(
-                    error.__class__, error, error.__traceback__
-                )
-            else:
-                tb = traceback.format_exception(error)
-            return (msg + "\n\n".join(tb)).strip()
-        except:  # noqa: E722
-            return msg
-
-    def _start_trace(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:  # type: ignore[return]
-        current_dotted_order = run.start_time.strftime("%Y%m%dT%H%M%S%fZ") + str(run.id)
-        if run.parent_run_id:
-            if parent := self.order_map.get(run.parent_run_id):
-                run.trace_id, run.dotted_order = parent
-                run.dotted_order += "." + current_dotted_order
-                if parent_run := self.run_map.get(str(run.parent_run_id)):
-                    self._add_child_run(parent_run, run)
-            else:
-                if self.log_missing_parent:
-                    logger.debug(
-                        f"Parent run {run.parent_run_id} not found for run {run.id}."
-                        " Treating as a root run."
-                    )
-                run.parent_run_id = None
-                run.trace_id = run.id
-                run.dotted_order = current_dotted_order
-        else:
-            run.trace_id = run.id
-            run.dotted_order = current_dotted_order
-        self.order_map[run.id] = (run.trace_id, run.dotted_order)
-        self.run_map[str(run.id)] = run
-
-    def _get_run(
-        self, run_id: UUID, run_type: Union[str, set[str], None] = None
-    ) -> Run:
-        try:
-            run = self.run_map[str(run_id)]
-        except KeyError as exc:
-            msg = f"No indexed run ID {run_id}."
-            raise TracerException(msg) from exc
-
-        if isinstance(run_type, str):
-            run_types: Union[set[str], None] = {run_type}
-        else:
-            run_types = run_type
-        if run_types is not None and run.run_type not in run_types:
-            msg = (
-                f"Found {run.run_type} run at ID {run_id}, "
-                f"but expected {run_types} run."
-            )
-            raise TracerException(msg)
-        return run
-
-    def _create_chat_model_run(
-        self,
-        serialized: dict[str, Any],
-        messages: list[list[BaseMessage]],
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Create a chat model run."""
-        if self._schema_format not in ("streaming_events", "original+chat"):
-            # Please keep this un-implemented for backwards compatibility.
-            # When it's unimplemented old tracers that use the "original" format
-            # fallback on the on_llm_start method implementation if they
-            # find that the on_chat_model_start method is not implemented.
-            # This can eventually be cleaned up by writing a "modern" tracer
-            # that has all the updated schema changes corresponding to
-            # the "streaming_events" format.
-            msg = (
-                f"Chat model tracing is not supported in "
-                f"for {self._schema_format} format."
-            )
-            raise NotImplementedError(msg)
-        start_time = datetime.now(timezone.utc)
-        if metadata:
-            kwargs.update({"metadata": metadata})
-        return Run(
-            id=run_id,
-            parent_run_id=parent_run_id,
-            serialized=serialized,
-            inputs={"messages": [[dumpd(msg) for msg in batch] for batch in messages]},
-            extra=kwargs,
-            events=[{"name": "start", "time": start_time}],
-            start_time=start_time,
-            # WARNING: This is valid ONLY for streaming_events.
-            # run_type="llm" is what's used by virtually all tracers.
-            # Changing this to "chat_model" may break triggering on_llm_start
-            run_type="chat_model",
-            tags=tags,
-            name=name,  # type: ignore[arg-type]
-        )
-
-    def _create_llm_run(
-        self,
-        serialized: dict[str, Any],
-        prompts: list[str],
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Create a llm run."""
-        start_time = datetime.now(timezone.utc)
-        if metadata:
-            kwargs.update({"metadata": metadata})
-        return Run(
-            id=run_id,
-            parent_run_id=parent_run_id,
-            serialized=serialized,
-            # TODO: Figure out how to expose kwargs here
-            inputs={"prompts": prompts},
-            extra=kwargs,
-            events=[{"name": "start", "time": start_time}],
-            start_time=start_time,
-            run_type="llm",
-            tags=tags or [],
-            name=name,  # type: ignore[arg-type]
-        )
-
-    def _llm_run_with_token_event(
-        self,
-        token: str,
-        run_id: UUID,
-        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Append token event to LLM run and return the run."""
-        llm_run = self._get_run(run_id, run_type={"llm", "chat_model"})
-        event_kwargs: dict[str, Any] = {"token": token}
-        if chunk:
-            event_kwargs["chunk"] = chunk
-        llm_run.events.append(
-            {
-                "name": "new_token",
-                "time": datetime.now(timezone.utc),
-                "kwargs": event_kwargs,
-            },
-        )
-        return llm_run
-
-    def _llm_run_with_retry_event(
-        self,
-        retry_state: RetryCallState,
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> Run:
-        llm_run = self._get_run(run_id)
-        retry_d: dict[str, Any] = {
-            "slept": retry_state.idle_for,
-            "attempt": retry_state.attempt_number,
-        }
-        if retry_state.outcome is None:
-            retry_d["outcome"] = "N/A"
-        elif retry_state.outcome.failed:
-            retry_d["outcome"] = "failed"
-            exception = retry_state.outcome.exception()
-            retry_d["exception"] = str(exception)
-            retry_d["exception_type"] = exception.__class__.__name__
-        else:
-            retry_d["outcome"] = "success"
-            retry_d["result"] = str(retry_state.outcome.result())
-        llm_run.events.append(
-            {
-                "name": "retry",
-                "time": datetime.now(timezone.utc),
-                "kwargs": retry_d,
-            },
-        )
-        return llm_run
-
-    def _complete_llm_run(self, response: LLMResult, run_id: UUID) -> Run:
-        llm_run = self._get_run(run_id, run_type={"llm", "chat_model"})
-        llm_run.outputs = response.model_dump()
-        for i, generations in enumerate(response.generations):
-            for j, generation in enumerate(generations):
-                output_generation = llm_run.outputs["generations"][i][j]
-                if "message" in output_generation:
-                    output_generation["message"] = dumpd(
-                        cast(ChatGeneration, generation).message
-                    )
-        llm_run.end_time = datetime.now(timezone.utc)
-        llm_run.events.append({"name": "end", "time": llm_run.end_time})
-
-        return llm_run
-
-    def _errored_llm_run(self, error: BaseException, run_id: UUID) -> Run:
-        llm_run = self._get_run(run_id, run_type={"llm", "chat_model"})
-        llm_run.error = self._get_stacktrace(error)
-        llm_run.end_time = datetime.now(timezone.utc)
-        llm_run.events.append({"name": "error", "time": llm_run.end_time})
-
-        return llm_run
-
-    def _create_chain_run(
-        self,
-        serialized: dict[str, Any],
-        inputs: dict[str, Any],
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        run_type: Optional[str] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Create a chain Run."""
-        start_time = datetime.now(timezone.utc)
-        if metadata:
-            kwargs.update({"metadata": metadata})
-        return Run(
-            id=run_id,
-            parent_run_id=parent_run_id,
-            serialized=serialized,
-            inputs=self._get_chain_inputs(inputs),
-            extra=kwargs,
-            events=[{"name": "start", "time": start_time}],
-            start_time=start_time,
-            child_runs=[],
-            run_type=run_type or "chain",
-            name=name,  # type: ignore[arg-type]
-            tags=tags or [],
-        )
-
-    def _get_chain_inputs(self, inputs: Any) -> Any:
-        """Get the inputs for a chain run."""
-        if self._schema_format in ("original", "original+chat"):
-            return inputs if isinstance(inputs, dict) else {"input": inputs}
-        elif self._schema_format == "streaming_events":
-            return {
-                "input": inputs,
-            }
-        else:
-            msg = f"Invalid format: {self._schema_format}"
-            raise ValueError(msg)
-
-    def _get_chain_outputs(self, outputs: Any) -> Any:
-        """Get the outputs for a chain run."""
-        if self._schema_format in ("original", "original+chat"):
-            return outputs if isinstance(outputs, dict) else {"output": outputs}
-        elif self._schema_format == "streaming_events":
-            return {
-                "output": outputs,
-            }
-        else:
-            msg = f"Invalid format: {self._schema_format}"
-            raise ValueError(msg)
-
-    def _complete_chain_run(
-        self,
-        outputs: dict[str, Any],
-        run_id: UUID,
-        inputs: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Update a chain run with outputs and end time."""
-        chain_run = self._get_run(run_id)
-        chain_run.outputs = self._get_chain_outputs(outputs)
-        chain_run.end_time = datetime.now(timezone.utc)
-        chain_run.events.append({"name": "end", "time": chain_run.end_time})
-        if inputs is not None:
-            chain_run.inputs = self._get_chain_inputs(inputs)
-        return chain_run
-
-    def _errored_chain_run(
-        self,
-        error: BaseException,
-        inputs: Optional[dict[str, Any]],
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> Run:
-        chain_run = self._get_run(run_id)
-        chain_run.error = self._get_stacktrace(error)
-        chain_run.end_time = datetime.now(timezone.utc)
-        chain_run.events.append({"name": "error", "time": chain_run.end_time})
-        if inputs is not None:
-            chain_run.inputs = self._get_chain_inputs(inputs)
-        return chain_run
-
-    def _create_tool_run(
-        self,
-        serialized: dict[str, Any],
-        input_str: str,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        inputs: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Create a tool run."""
-        start_time = datetime.now(timezone.utc)
-        if metadata:
-            kwargs.update({"metadata": metadata})
-
-        if self._schema_format in ("original", "original+chat"):
-            inputs = {"input": input_str}
-        elif self._schema_format == "streaming_events":
-            inputs = {"input": inputs}
-        else:
-            msg = f"Invalid format: {self._schema_format}"
-            raise AssertionError(msg)
-
-        return Run(
-            id=run_id,
-            parent_run_id=parent_run_id,
-            serialized=serialized,
-            # Wrapping in dict since Run requires a dict object.
-            inputs=inputs,
-            extra=kwargs,
-            events=[{"name": "start", "time": start_time}],
-            start_time=start_time,
-            child_runs=[],
-            run_type="tool",
-            tags=tags or [],
-            name=name,  # type: ignore[arg-type]
-        )
-
-    def _complete_tool_run(
-        self,
-        output: dict[str, Any],
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> Run:
-        """Update a tool run with outputs and end time."""
-        tool_run = self._get_run(run_id, run_type="tool")
-        tool_run.outputs = {"output": output}
-        tool_run.end_time = datetime.now(timezone.utc)
-        tool_run.events.append({"name": "end", "time": tool_run.end_time})
-        return tool_run
-
-    def _errored_tool_run(
-        self,
-        error: BaseException,
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> Run:
-        """Update a tool run with error and end time."""
-        tool_run = self._get_run(run_id, run_type="tool")
-        tool_run.error = self._get_stacktrace(error)
-        tool_run.end_time = datetime.now(timezone.utc)
-        tool_run.events.append({"name": "error", "time": tool_run.end_time})
-        return tool_run
-
-    def _create_retrieval_run(
-        self,
-        serialized: dict[str, Any],
-        query: str,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Create a retrieval run."""
-        start_time = datetime.now(timezone.utc)
-        if metadata:
-            kwargs.update({"metadata": metadata})
-        return Run(
-            id=run_id,
-            name=name or "Retriever",
-            parent_run_id=parent_run_id,
-            serialized=serialized,
-            inputs={"query": query},
-            extra=kwargs,
-            events=[{"name": "start", "time": start_time}],
-            start_time=start_time,
-            tags=tags,
-            child_runs=[],
-            run_type="retriever",
-        )
-
-    def _complete_retrieval_run(
-        self,
-        documents: Sequence[Document],
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> Run:
-        """Update a retrieval run with outputs and end time."""
-        retrieval_run = self._get_run(run_id, run_type="retriever")
-        retrieval_run.outputs = {"documents": documents}
-        retrieval_run.end_time = datetime.now(timezone.utc)
-        retrieval_run.events.append({"name": "end", "time": retrieval_run.end_time})
-        return retrieval_run
-
-    def _errored_retrieval_run(
-        self,
-        error: BaseException,
-        run_id: UUID,
-        **kwargs: Any,
-    ) -> Run:
-        retrieval_run = self._get_run(run_id, run_type="retriever")
-        retrieval_run.error = self._get_stacktrace(error)
-        retrieval_run.end_time = datetime.now(timezone.utc)
-        retrieval_run.events.append({"name": "error", "time": retrieval_run.end_time})
-        return retrieval_run
-
-    def __deepcopy__(self, memo: dict) -> _TracerCore:
-        """Deepcopy the tracer."""
-        return self
-
-    def __copy__(self) -> _TracerCore:
-        """Copy the tracer."""
-        return self
-
-    def _end_trace(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """End a trace for a run."""
-        return None
-
-    def _on_run_create(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process a run upon creation."""
-        return None
-
-    def _on_run_update(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process a run upon update."""
-        return None
-
-    def _on_llm_start(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process the LLM Run upon start."""
-        return None
-
-    def _on_llm_new_token(
-        self,
-        run: Run,
-        token: str,
-        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]],
-    ) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process new LLM token."""
-        return None
-
-    def _on_llm_end(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process the LLM Run."""
-        return None
-
-    def _on_llm_error(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process the LLM Run upon error."""
-        return None
-
-    def _on_chain_start(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process the Chain Run upon start."""
-        return None
-
-    def _on_chain_end(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process the Chain Run."""
-        return None
-
-    def _on_chain_error(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process the Chain Run upon error."""
-        return None
-
-    def _on_tool_start(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process the Tool Run upon start."""
-        return None
-
-    def _on_tool_end(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process the Tool Run."""
-        return None
-
-    def _on_tool_error(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process the Tool Run upon error."""
-        return None
-
-    def _on_chat_model_start(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process the Chat Model Run upon start."""
-        return None
-
-    def _on_retriever_start(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process the Retriever Run upon start."""
-        return None
-
-    def _on_retriever_end(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process the Retriever Run."""
-        return None
-
-    def _on_retriever_error(self, run: Run) -> Union[None, Coroutine[Any, Any, None]]:
-        """Process the Retriever Run upon error."""
-        return None
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/evaluation.py ./custom_langchain_core/tracers/evaluation.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/evaluation.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/evaluation.py	1970-01-01 09:00:00
@@ -1,218 +0,0 @@
-"""A tracer that runs evaluators over completed runs."""
-
-from __future__ import annotations
-
-import logging
-import threading
-import weakref
-from collections.abc import Sequence
-from concurrent.futures import Future, ThreadPoolExecutor, wait
-from typing import Any, Optional, Union, cast
-from uuid import UUID
-
-import langsmith
-from langsmith.evaluation.evaluator import EvaluationResult, EvaluationResults
-
-from langchain_core.tracers import langchain as langchain_tracer
-from langchain_core.tracers.base import BaseTracer
-from langchain_core.tracers.context import tracing_v2_enabled
-from langchain_core.tracers.langchain import _get_executor
-from langchain_core.tracers.schemas import Run
-
-logger = logging.getLogger(__name__)
-
-_TRACERS: weakref.WeakSet[EvaluatorCallbackHandler] = weakref.WeakSet()
-
-
-def wait_for_all_evaluators() -> None:
-    """Wait for all tracers to finish."""
-    global _TRACERS
-    for tracer in list(_TRACERS):
-        if tracer is not None:
-            tracer.wait_for_futures()
-
-
-class EvaluatorCallbackHandler(BaseTracer):
-    """Tracer that runs a run evaluator whenever a run is persisted.
-
-    Args:
-        evaluators : Sequence[RunEvaluator]
-            The run evaluators to apply to all top level runs.
-        client : LangSmith Client, optional
-            The LangSmith client instance to use for evaluating the runs.
-            If not specified, a new instance will be created.
-        example_id : Union[UUID, str], optional
-            The example ID to be associated with the runs.
-        project_name : str, optional
-            The LangSmith project name to be organize eval chain runs under.
-
-    Attributes:
-        example_id : Union[UUID, None]
-            The example ID associated with the runs.
-        client : Client
-            The LangSmith client instance used for evaluating the runs.
-        evaluators : Sequence[RunEvaluator]
-            The sequence of run evaluators to be executed.
-        executor : ThreadPoolExecutor
-            The thread pool executor used for running the evaluators.
-        futures : Set[Future]
-            The set of futures representing the running evaluators.
-        skip_unfinished : bool
-            Whether to skip runs that are not finished or raised
-            an error.
-        project_name : Optional[str]
-            The LangSmith project name to be organize eval chain runs under.
-    """
-
-    name: str = "evaluator_callback_handler"
-
-    def __init__(
-        self,
-        evaluators: Sequence[langsmith.RunEvaluator],
-        client: Optional[langsmith.Client] = None,
-        example_id: Optional[Union[UUID, str]] = None,
-        skip_unfinished: bool = True,
-        project_name: Optional[str] = "evaluators",
-        max_concurrency: Optional[int] = None,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(**kwargs)
-        self.example_id = (
-            UUID(example_id) if isinstance(example_id, str) else example_id
-        )
-        self.client = client or langchain_tracer.get_client()
-        self.evaluators = evaluators
-        if max_concurrency is None:
-            self.executor: Optional[ThreadPoolExecutor] = _get_executor()
-        elif max_concurrency > 0:
-            self.executor = ThreadPoolExecutor(max_workers=max_concurrency)
-            weakref.finalize(
-                self,
-                lambda: cast(ThreadPoolExecutor, self.executor).shutdown(wait=True),
-            )
-        else:
-            self.executor = None
-        self.futures: weakref.WeakSet[Future] = weakref.WeakSet()
-        self.skip_unfinished = skip_unfinished
-        self.project_name = project_name
-        self.logged_eval_results: dict[tuple[str, str], list[EvaluationResult]] = {}
-        self.lock = threading.Lock()
-        global _TRACERS
-        _TRACERS.add(self)
-
-    def _evaluate_in_project(self, run: Run, evaluator: langsmith.RunEvaluator) -> None:
-        """Evaluate the run in the project.
-
-        Args:
-        ----------
-        run : Run
-            The run to be evaluated.
-        evaluator : RunEvaluator
-            The evaluator to use for evaluating the run.
-
-        """
-        try:
-            if self.project_name is None:
-                eval_result = self.client.evaluate_run(run, evaluator)
-                eval_results = [eval_result]
-            with tracing_v2_enabled(
-                project_name=self.project_name, tags=["eval"], client=self.client
-            ) as cb:
-                reference_example = (
-                    self.client.read_example(run.reference_example_id)
-                    if run.reference_example_id
-                    else None
-                )
-                evaluation_result = evaluator.evaluate_run(
-                    # This is subclass, but getting errors for some reason
-                    run,  # type: ignore
-                    example=reference_example,
-                )
-                eval_results = self._log_evaluation_feedback(
-                    evaluation_result,
-                    run,
-                    source_run_id=cb.latest_run.id if cb.latest_run else None,
-                )
-        except Exception as e:
-            logger.error(
-                f"Error evaluating run {run.id} with "
-                f"{evaluator.__class__.__name__}: {repr(e)}",
-                exc_info=True,
-            )
-            raise
-        example_id = str(run.reference_example_id)
-        with self.lock:
-            for res in eval_results:
-                run_id = str(getattr(res, "target_run_id", run.id))
-                self.logged_eval_results.setdefault((run_id, example_id), []).append(
-                    res
-                )
-
-    def _select_eval_results(
-        self,
-        results: Union[EvaluationResult, EvaluationResults],
-    ) -> list[EvaluationResult]:
-        if isinstance(results, EvaluationResult):
-            results_ = [results]
-        elif isinstance(results, dict) and "results" in results:
-            results_ = cast(list[EvaluationResult], results["results"])
-        else:
-            msg = (
-                f"Invalid evaluation result type {type(results)}."
-                " Expected EvaluationResult or EvaluationResults."
-            )
-            raise TypeError(msg)
-        return results_
-
-    def _log_evaluation_feedback(
-        self,
-        evaluator_response: Union[EvaluationResult, EvaluationResults],
-        run: Run,
-        source_run_id: Optional[UUID] = None,
-    ) -> list[EvaluationResult]:
-        results = self._select_eval_results(evaluator_response)
-        for res in results:
-            source_info_: dict[str, Any] = {}
-            if res.evaluator_info:
-                source_info_ = {**res.evaluator_info, **source_info_}
-            run_id_ = getattr(res, "target_run_id", None)
-            if run_id_ is None:
-                run_id_ = run.id
-            self.client.create_feedback(
-                run_id_,
-                res.key,
-                score=res.score,
-                value=res.value,
-                comment=res.comment,
-                correction=res.correction,
-                source_info=source_info_,
-                source_run_id=res.source_run_id or source_run_id,
-                feedback_source_type=langsmith.schemas.FeedbackSourceType.MODEL,
-            )
-        return results
-
-    def _persist_run(self, run: Run) -> None:
-        """Run the evaluator on the run.
-
-        Args:
-        ----------
-        run : Run
-            The run to be evaluated.
-
-        """
-        if self.skip_unfinished and not run.outputs:
-            logger.debug(f"Skipping unfinished run {run.id}")
-            return
-        run_ = run.copy()
-        run_.reference_example_id = self.example_id
-        for evaluator in self.evaluators:
-            if self.executor is None:
-                self._evaluate_in_project(run_, evaluator)
-            else:
-                self.futures.add(
-                    self.executor.submit(self._evaluate_in_project, run_, evaluator)
-                )
-
-    def wait_for_futures(self) -> None:
-        """Wait for all futures to complete."""
-        wait(self.futures)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py ./custom_langchain_core/tracers/event_stream.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/event_stream.py	1970-01-01 09:00:00
@@ -1,1012 +0,0 @@
-"""Internal tracer to power the event stream API."""
-
-from __future__ import annotations
-
-import asyncio
-import contextlib
-import logging
-from collections.abc import AsyncIterator, Iterator, Sequence
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Optional,
-    TypeVar,
-    Union,
-    cast,
-)
-from uuid import UUID, uuid4
-
-from typing_extensions import NotRequired, TypedDict
-
-from langchain_core.callbacks.base import AsyncCallbackHandler
-from langchain_core.messages import AIMessageChunk, BaseMessage, BaseMessageChunk
-from langchain_core.outputs import (
-    ChatGenerationChunk,
-    GenerationChunk,
-    LLMResult,
-)
-from langchain_core.runnables.schema import (
-    CustomStreamEvent,
-    EventData,
-    StandardStreamEvent,
-    StreamEvent,
-)
-from langchain_core.runnables.utils import (
-    Input,
-    Output,
-    _RootEventFilter,
-)
-from langchain_core.tracers._streaming import _StreamingCallbackHandler
-from langchain_core.tracers.log_stream import LogEntry
-from langchain_core.tracers.memory_stream import _MemoryStream
-from langchain_core.utils.aiter import aclosing, py_anext
-
-if TYPE_CHECKING:
-    from langchain_core.documents import Document
-    from langchain_core.runnables import Runnable, RunnableConfig
-
-logger = logging.getLogger(__name__)
-
-
-class RunInfo(TypedDict):
-    """Information about a run.
-
-    This is used to keep track of the metadata associated with a run.
-
-    Parameters:
-        name: The name of the run.
-        tags: The tags associated with the run.
-        metadata: The metadata associated with the run.
-        run_type: The type of the run.
-        inputs: The inputs to the run.
-        parent_run_id: The ID of the parent run.
-    """
-
-    name: str
-    tags: list[str]
-    metadata: dict[str, Any]
-    run_type: str
-    inputs: NotRequired[Any]
-    parent_run_id: Optional[UUID]
-
-
-def _assign_name(name: Optional[str], serialized: Optional[dict[str, Any]]) -> str:
-    """Assign a name to a run."""
-    if name is not None:
-        return name
-    if serialized is not None:
-        if "name" in serialized:
-            return serialized["name"]
-        elif "id" in serialized:
-            return serialized["id"][-1]
-    return "Unnamed"
-
-
-T = TypeVar("T")
-
-
-class _AstreamEventsCallbackHandler(AsyncCallbackHandler, _StreamingCallbackHandler):
-    """An implementation of an async callback handler for astream events."""
-
-    def __init__(
-        self,
-        *args: Any,
-        include_names: Optional[Sequence[str]] = None,
-        include_types: Optional[Sequence[str]] = None,
-        include_tags: Optional[Sequence[str]] = None,
-        exclude_names: Optional[Sequence[str]] = None,
-        exclude_types: Optional[Sequence[str]] = None,
-        exclude_tags: Optional[Sequence[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Initialize the tracer."""
-        super().__init__(*args, **kwargs)
-        # Map of run ID to run info.
-        # the entry corresponding to a given run id is cleaned
-        # up when each corresponding run ends.
-        self.run_map: dict[UUID, RunInfo] = {}
-        # The callback event that corresponds to the end of a parent run
-        # may be invoked BEFORE the callback event that corresponds to the end
-        # of a child run, which results in clean up of run_map.
-        # So we keep track of the mapping between children and parent run IDs
-        # in a separate container. This container is GCed when the tracer is GCed.
-        self.parent_map: dict[UUID, Optional[UUID]] = {}
-
-        self.is_tapped: dict[UUID, Any] = {}
-
-        # Filter which events will be sent over the queue.
-        self.root_event_filter = _RootEventFilter(
-            include_names=include_names,
-            include_types=include_types,
-            include_tags=include_tags,
-            exclude_names=exclude_names,
-            exclude_types=exclude_types,
-            exclude_tags=exclude_tags,
-        )
-
-        loop = asyncio.get_event_loop()
-        memory_stream = _MemoryStream[StreamEvent](loop)
-        self.send_stream = memory_stream.get_send_stream()
-        self.receive_stream = memory_stream.get_receive_stream()
-
-    def _get_parent_ids(self, run_id: UUID) -> list[str]:
-        """Get the parent IDs of a run (non-recursively) cast to strings."""
-        parent_ids = []
-
-        while parent_id := self.parent_map.get(run_id):
-            str_parent_id = str(parent_id)
-            if str_parent_id in parent_ids:
-                msg = (
-                    f"Parent ID {parent_id} is already in the parent_ids list. "
-                    f"This should never happen."
-                )
-                raise AssertionError(msg)
-            parent_ids.append(str_parent_id)
-            run_id = parent_id
-
-        # Return the parent IDs in reverse order, so that the first
-        # parent ID is the root and the last ID is the immediate parent.
-        return parent_ids[::-1]
-
-    def _send(self, event: StreamEvent, event_type: str) -> None:
-        """Send an event to the stream."""
-        if self.root_event_filter.include_event(event, event_type):
-            self.send_stream.send_nowait(event)
-
-    def __aiter__(self) -> AsyncIterator[Any]:
-        """Iterate over the receive stream."""
-        return self.receive_stream.__aiter__()
-
-    async def tap_output_aiter(
-        self, run_id: UUID, output: AsyncIterator[T]
-    ) -> AsyncIterator[T]:
-        """Tap the output aiter.
-
-        This method is used to tap the output of a Runnable that produces
-        an async iterator. It is used to generate stream events for the
-        output of the Runnable.
-
-        Args:
-            run_id: The ID of the run.
-            output: The output of the Runnable.
-
-        Yields:
-            T: The output of the Runnable.
-        """
-        sentinel = object()
-        # atomic check and set
-        tap = self.is_tapped.setdefault(run_id, sentinel)
-        # wait for first chunk
-        first = await py_anext(output, default=sentinel)
-        if first is sentinel:
-            return
-        # get run info
-        run_info = self.run_map.get(run_id)
-        if run_info is None:
-            # run has finished, don't issue any stream events
-            yield cast(T, first)
-            return
-        if tap is sentinel:
-            # if we are the first to tap, issue stream events
-            event: StandardStreamEvent = {
-                "event": f"on_{run_info['run_type']}_stream",
-                "run_id": str(run_id),
-                "name": run_info["name"],
-                "tags": run_info["tags"],
-                "metadata": run_info["metadata"],
-                "data": {},
-                "parent_ids": self._get_parent_ids(run_id),
-            }
-            self._send({**event, "data": {"chunk": first}}, run_info["run_type"])
-            yield cast(T, first)
-            # consume the rest of the output
-            async for chunk in output:
-                self._send(
-                    {**event, "data": {"chunk": chunk}},
-                    run_info["run_type"],
-                )
-                yield chunk
-        else:
-            # otherwise just pass through
-            yield cast(T, first)
-            # consume the rest of the output
-            async for chunk in output:
-                yield chunk
-
-    def tap_output_iter(self, run_id: UUID, output: Iterator[T]) -> Iterator[T]:
-        """Tap the output aiter.
-
-        Args:
-            run_id: The ID of the run.
-            output: The output of the Runnable.
-
-        Yields:
-            T: The output of the Runnable.
-        """
-        sentinel = object()
-        # atomic check and set
-        tap = self.is_tapped.setdefault(run_id, sentinel)
-        # wait for first chunk
-        first = next(output, sentinel)
-        if first is sentinel:
-            return
-        # get run info
-        run_info = self.run_map.get(run_id)
-        if run_info is None:
-            # run has finished, don't issue any stream events
-            yield cast(T, first)
-            return
-        if tap is sentinel:
-            # if we are the first to tap, issue stream events
-            event: StandardStreamEvent = {
-                "event": f"on_{run_info['run_type']}_stream",
-                "run_id": str(run_id),
-                "name": run_info["name"],
-                "tags": run_info["tags"],
-                "metadata": run_info["metadata"],
-                "data": {},
-                "parent_ids": self._get_parent_ids(run_id),
-            }
-            self._send({**event, "data": {"chunk": first}}, run_info["run_type"])
-            yield cast(T, first)
-            # consume the rest of the output
-            for chunk in output:
-                self._send(
-                    {**event, "data": {"chunk": chunk}},
-                    run_info["run_type"],
-                )
-                yield chunk
-        else:
-            # otherwise just pass through
-            yield cast(T, first)
-            # consume the rest of the output
-            for chunk in output:
-                yield chunk
-
-    def _write_run_start_info(
-        self,
-        run_id: UUID,
-        *,
-        tags: Optional[list[str]],
-        metadata: Optional[dict[str, Any]],
-        parent_run_id: Optional[UUID],
-        name_: str,
-        run_type: str,
-        **kwargs: Any,
-    ) -> None:
-        """Update the run info."""
-        info: RunInfo = {
-            "tags": tags or [],
-            "metadata": metadata or {},
-            "name": name_,
-            "run_type": run_type,
-            "parent_run_id": parent_run_id,
-        }
-
-        if "inputs" in kwargs:
-            # Handle inputs in a special case to allow inputs to be an
-            # optionally provided and distinguish between missing value
-            # vs. None value.
-            info["inputs"] = kwargs["inputs"]
-
-        self.run_map[run_id] = info
-        self.parent_map[run_id] = parent_run_id
-
-    async def on_chat_model_start(
-        self,
-        serialized: dict[str, Any],
-        messages: list[list[BaseMessage]],
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Start a trace for an LLM run."""
-        name_ = _assign_name(name, serialized)
-        run_type = "chat_model"
-
-        self._write_run_start_info(
-            run_id,
-            tags=tags,
-            metadata=metadata,
-            parent_run_id=parent_run_id,
-            name_=name_,
-            run_type=run_type,
-            inputs={"messages": messages},
-        )
-
-        self._send(
-            {
-                "event": "on_chat_model_start",
-                "data": {
-                    "input": {"messages": messages},
-                },
-                "name": name_,
-                "tags": tags or [],
-                "run_id": str(run_id),
-                "metadata": metadata or {},
-                "parent_ids": self._get_parent_ids(run_id),
-            },
-            run_type,
-        )
-
-    async def on_llm_start(
-        self,
-        serialized: dict[str, Any],
-        prompts: list[str],
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Start a trace for an LLM run."""
-        name_ = _assign_name(name, serialized)
-        run_type = "llm"
-
-        self._write_run_start_info(
-            run_id,
-            tags=tags,
-            metadata=metadata,
-            parent_run_id=parent_run_id,
-            name_=name_,
-            run_type=run_type,
-            inputs={"prompts": prompts},
-        )
-
-        self._send(
-            {
-                "event": "on_llm_start",
-                "data": {
-                    "input": {
-                        "prompts": prompts,
-                    }
-                },
-                "name": name_,
-                "tags": tags or [],
-                "run_id": str(run_id),
-                "metadata": metadata or {},
-                "parent_ids": self._get_parent_ids(run_id),
-            },
-            run_type,
-        )
-
-    async def on_custom_event(
-        self,
-        name: str,
-        data: Any,
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Generate a custom astream event."""
-        event = CustomStreamEvent(
-            event="on_custom_event",
-            run_id=str(run_id),
-            name=name,
-            tags=tags or [],
-            metadata=metadata or {},
-            data=data,
-            parent_ids=self._get_parent_ids(run_id),
-        )
-        self._send(event, name)
-
-    async def on_llm_new_token(
-        self,
-        token: str,
-        *,
-        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run on new LLM token. Only available when streaming is enabled."""
-        run_info = self.run_map.get(run_id)
-        chunk_: Union[GenerationChunk, BaseMessageChunk]
-
-        if run_info is None:
-            msg = f"Run ID {run_id} not found in run map."
-            raise AssertionError(msg)
-        if self.is_tapped.get(run_id):
-            return
-        if run_info["run_type"] == "chat_model":
-            event = "on_chat_model_stream"
-
-            if chunk is None:
-                chunk_ = AIMessageChunk(content=token)
-            else:
-                chunk_ = cast(ChatGenerationChunk, chunk).message
-
-        elif run_info["run_type"] == "llm":
-            event = "on_llm_stream"
-            if chunk is None:
-                chunk_ = GenerationChunk(text=token)
-            else:
-                chunk_ = cast(GenerationChunk, chunk)
-        else:
-            msg = f"Unexpected run type: {run_info['run_type']}"
-            raise ValueError(msg)
-
-        self._send(
-            {
-                "event": event,
-                "data": {
-                    "chunk": chunk_,
-                },
-                "run_id": str(run_id),
-                "name": run_info["name"],
-                "tags": run_info["tags"],
-                "metadata": run_info["metadata"],
-                "parent_ids": self._get_parent_ids(run_id),
-            },
-            run_info["run_type"],
-        )
-
-    async def on_llm_end(
-        self, response: LLMResult, *, run_id: UUID, **kwargs: Any
-    ) -> None:
-        """End a trace for an LLM run."""
-        run_info = self.run_map.pop(run_id)
-        inputs_ = run_info["inputs"]
-
-        generations: Union[list[list[GenerationChunk]], list[list[ChatGenerationChunk]]]
-        output: Union[dict, BaseMessage] = {}
-
-        if run_info["run_type"] == "chat_model":
-            generations = cast(list[list[ChatGenerationChunk]], response.generations)
-            for gen in generations:
-                if output != {}:
-                    break
-                for chunk in gen:
-                    output = chunk.message
-                    break
-
-            event = "on_chat_model_end"
-        elif run_info["run_type"] == "llm":
-            generations = cast(list[list[GenerationChunk]], response.generations)
-            output = {
-                "generations": [
-                    [
-                        {
-                            "text": chunk.text,
-                            "generation_info": chunk.generation_info,
-                            "type": chunk.type,
-                        }
-                        for chunk in gen
-                    ]
-                    for gen in generations
-                ],
-                "llm_output": response.llm_output,
-            }
-            event = "on_llm_end"
-        else:
-            msg = f"Unexpected run type: {run_info['run_type']}"
-            raise ValueError(msg)
-
-        self._send(
-            {
-                "event": event,
-                "data": {"output": output, "input": inputs_},
-                "run_id": str(run_id),
-                "name": run_info["name"],
-                "tags": run_info["tags"],
-                "metadata": run_info["metadata"],
-                "parent_ids": self._get_parent_ids(run_id),
-            },
-            run_info["run_type"],
-        )
-
-    async def on_chain_start(
-        self,
-        serialized: dict[str, Any],
-        inputs: dict[str, Any],
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        run_type: Optional[str] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Start a trace for a chain run."""
-        name_ = _assign_name(name, serialized)
-        run_type_ = run_type or "chain"
-
-        data: EventData = {}
-
-        # Work-around Runnable core code not sending input in some
-        # cases.
-        if inputs != {"input": ""}:
-            data["input"] = inputs
-            kwargs["inputs"] = inputs
-
-        self._write_run_start_info(
-            run_id,
-            tags=tags,
-            metadata=metadata,
-            parent_run_id=parent_run_id,
-            name_=name_,
-            run_type=run_type_,
-            **kwargs,
-        )
-
-        self._send(
-            {
-                "event": f"on_{run_type_}_start",
-                "data": data,
-                "name": name_,
-                "tags": tags or [],
-                "run_id": str(run_id),
-                "metadata": metadata or {},
-                "parent_ids": self._get_parent_ids(run_id),
-            },
-            run_type_,
-        )
-
-    async def on_chain_end(
-        self,
-        outputs: dict[str, Any],
-        *,
-        run_id: UUID,
-        inputs: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """End a trace for a chain run."""
-        run_info = self.run_map.pop(run_id)
-        run_type = run_info["run_type"]
-
-        event = f"on_{run_type}_end"
-
-        inputs = inputs or run_info.get("inputs") or {}
-
-        data: EventData = {
-            "output": outputs,
-            "input": inputs,
-        }
-
-        self._send(
-            {
-                "event": event,
-                "data": data,
-                "run_id": str(run_id),
-                "name": run_info["name"],
-                "tags": run_info["tags"],
-                "metadata": run_info["metadata"],
-                "parent_ids": self._get_parent_ids(run_id),
-            },
-            run_type,
-        )
-
-    async def on_tool_start(
-        self,
-        serialized: dict[str, Any],
-        input_str: str,
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        inputs: Optional[dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Start a trace for a tool run."""
-        name_ = _assign_name(name, serialized)
-
-        self._write_run_start_info(
-            run_id,
-            tags=tags,
-            metadata=metadata,
-            parent_run_id=parent_run_id,
-            name_=name_,
-            run_type="tool",
-            inputs=inputs,
-        )
-
-        self._send(
-            {
-                "event": "on_tool_start",
-                "data": {
-                    "input": inputs or {},
-                },
-                "name": name_,
-                "tags": tags or [],
-                "run_id": str(run_id),
-                "metadata": metadata or {},
-                "parent_ids": self._get_parent_ids(run_id),
-            },
-            "tool",
-        )
-
-    async def on_tool_end(self, output: Any, *, run_id: UUID, **kwargs: Any) -> None:
-        """End a trace for a tool run."""
-        run_info = self.run_map.pop(run_id)
-        if "inputs" not in run_info:
-            msg = (
-                f"Run ID {run_id} is a tool call and is expected to have "
-                f"inputs associated with it."
-            )
-            raise AssertionError(msg)
-        inputs = run_info["inputs"]
-
-        self._send(
-            {
-                "event": "on_tool_end",
-                "data": {
-                    "output": output,
-                    "input": inputs,
-                },
-                "run_id": str(run_id),
-                "name": run_info["name"],
-                "tags": run_info["tags"],
-                "metadata": run_info["metadata"],
-                "parent_ids": self._get_parent_ids(run_id),
-            },
-            "tool",
-        )
-
-    async def on_retriever_start(
-        self,
-        serialized: dict[str, Any],
-        query: str,
-        *,
-        run_id: UUID,
-        parent_run_id: Optional[UUID] = None,
-        tags: Optional[list[str]] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Run when Retriever starts running."""
-        name_ = _assign_name(name, serialized)
-        run_type = "retriever"
-
-        self._write_run_start_info(
-            run_id,
-            tags=tags,
-            metadata=metadata,
-            parent_run_id=parent_run_id,
-            name_=name_,
-            run_type=run_type,
-            inputs={"query": query},
-        )
-
-        self._send(
-            {
-                "event": "on_retriever_start",
-                "data": {
-                    "input": {
-                        "query": query,
-                    }
-                },
-                "name": name_,
-                "tags": tags or [],
-                "run_id": str(run_id),
-                "metadata": metadata or {},
-                "parent_ids": self._get_parent_ids(run_id),
-            },
-            run_type,
-        )
-
-    async def on_retriever_end(
-        self, documents: Sequence[Document], *, run_id: UUID, **kwargs: Any
-    ) -> None:
-        """Run when Retriever ends running."""
-        run_info = self.run_map.pop(run_id)
-
-        self._send(
-            {
-                "event": "on_retriever_end",
-                "data": {
-                    "output": documents,
-                    "input": run_info["inputs"],
-                },
-                "run_id": str(run_id),
-                "name": run_info["name"],
-                "tags": run_info["tags"],
-                "metadata": run_info["metadata"],
-                "parent_ids": self._get_parent_ids(run_id),
-            },
-            run_info["run_type"],
-        )
-
-    def __deepcopy__(self, memo: dict) -> _AstreamEventsCallbackHandler:
-        """Deepcopy the tracer."""
-        return self
-
-    def __copy__(self) -> _AstreamEventsCallbackHandler:
-        """Copy the tracer."""
-        return self
-
-
-async def _astream_events_implementation_v1(
-    runnable: Runnable[Input, Output],
-    input: Any,
-    config: Optional[RunnableConfig] = None,
-    *,
-    include_names: Optional[Sequence[str]] = None,
-    include_types: Optional[Sequence[str]] = None,
-    include_tags: Optional[Sequence[str]] = None,
-    exclude_names: Optional[Sequence[str]] = None,
-    exclude_types: Optional[Sequence[str]] = None,
-    exclude_tags: Optional[Sequence[str]] = None,
-    **kwargs: Any,
-) -> AsyncIterator[StandardStreamEvent]:
-    from langchain_core.runnables import ensure_config
-    from langchain_core.runnables.utils import _RootEventFilter
-    from langchain_core.tracers.log_stream import (
-        LogStreamCallbackHandler,
-        RunLog,
-        _astream_log_implementation,
-    )
-
-    stream = LogStreamCallbackHandler(
-        auto_close=False,
-        include_names=include_names,
-        include_types=include_types,
-        include_tags=include_tags,
-        exclude_names=exclude_names,
-        exclude_types=exclude_types,
-        exclude_tags=exclude_tags,
-        _schema_format="streaming_events",
-    )
-
-    run_log = RunLog(state=None)  # type: ignore[arg-type]
-    encountered_start_event = False
-
-    _root_event_filter = _RootEventFilter(
-        include_names=include_names,
-        include_types=include_types,
-        include_tags=include_tags,
-        exclude_names=exclude_names,
-        exclude_types=exclude_types,
-        exclude_tags=exclude_tags,
-    )
-
-    config = ensure_config(config)
-    root_tags = config.get("tags", [])
-    root_metadata = config.get("metadata", {})
-    root_name = config.get("run_name", runnable.get_name())
-
-    # Ignoring mypy complaint about too many different union combinations
-    # This arises because many of the argument types are unions
-    async for log in _astream_log_implementation(  # type: ignore[misc]
-        runnable,
-        input,
-        config=config,
-        stream=stream,
-        diff=True,
-        with_streamed_output_list=True,
-        **kwargs,
-    ):
-        run_log = run_log + log
-
-        if not encountered_start_event:
-            # Yield the start event for the root runnable.
-            encountered_start_event = True
-            state = run_log.state.copy()
-
-            event = StandardStreamEvent(
-                event=f"on_{state['type']}_start",
-                run_id=state["id"],
-                name=root_name,
-                tags=root_tags,
-                metadata=root_metadata,
-                data={
-                    "input": input,
-                },
-                parent_ids=[],  # Not supported in v1
-            )
-
-            if _root_event_filter.include_event(event, state["type"]):
-                yield event
-
-        paths = {
-            op["path"].split("/")[2]
-            for op in log.ops
-            if op["path"].startswith("/logs/")
-        }
-        # Elements in a set should be iterated in the same order
-        # as they were inserted in modern python versions.
-        for path in paths:
-            data: EventData = {}
-            log_entry: LogEntry = run_log.state["logs"][path]
-            if log_entry["end_time"] is None:
-                event_type = "stream" if log_entry["streamed_output"] else "start"
-            else:
-                event_type = "end"
-
-            if event_type == "start":
-                # Include the inputs with the start event if they are available.
-                # Usually they will NOT be available for components that operate
-                # on streams, since those components stream the input and
-                # don't know its final value until the end of the stream.
-                inputs = log_entry["inputs"]
-                if inputs is not None:
-                    data["input"] = inputs
-
-            if event_type == "end":
-                inputs = log_entry["inputs"]
-                if inputs is not None:
-                    data["input"] = inputs
-
-                # None is a VALID output for an end event
-                data["output"] = log_entry["final_output"]
-
-            if event_type == "stream":
-                num_chunks = len(log_entry["streamed_output"])
-                if num_chunks != 1:
-                    msg = (
-                        f"Expected exactly one chunk of streamed output, "
-                        f"got {num_chunks} instead. This is impossible. "
-                        f"Encountered in: {log_entry['name']}"
-                    )
-                    raise AssertionError(msg)
-
-                data = {"chunk": log_entry["streamed_output"][0]}
-                # Clean up the stream, we don't need it anymore.
-                # And this avoids duplicates as well!
-                log_entry["streamed_output"] = []
-
-            yield StandardStreamEvent(
-                event=f"on_{log_entry['type']}_{event_type}",
-                name=log_entry["name"],
-                run_id=log_entry["id"],
-                tags=log_entry["tags"],
-                metadata=log_entry["metadata"],
-                data=data,
-                parent_ids=[],  # Not supported in v1
-            )
-
-        # Finally, we take care of the streaming output from the root chain
-        # if there is any.
-        state = run_log.state
-        if state["streamed_output"]:
-            num_chunks = len(state["streamed_output"])
-            if num_chunks != 1:
-                msg = (
-                    f"Expected exactly one chunk of streamed output, "
-                    f"got {num_chunks} instead. This is impossible. "
-                    f"Encountered in: {state['name']}"
-                )
-                raise AssertionError(msg)
-
-            data = {"chunk": state["streamed_output"][0]}
-            # Clean up the stream, we don't need it anymore.
-            state["streamed_output"] = []
-
-            event = StandardStreamEvent(
-                event=f"on_{state['type']}_stream",
-                run_id=state["id"],
-                tags=root_tags,
-                metadata=root_metadata,
-                name=root_name,
-                data=data,
-                parent_ids=[],  # Not supported in v1
-            )
-            if _root_event_filter.include_event(event, state["type"]):
-                yield event
-
-    state = run_log.state
-
-    # Finally yield the end event for the root runnable.
-    event = StandardStreamEvent(
-        event=f"on_{state['type']}_end",
-        name=root_name,
-        run_id=state["id"],
-        tags=root_tags,
-        metadata=root_metadata,
-        data={
-            "output": state["final_output"],
-        },
-        parent_ids=[],  # Not supported in v1
-    )
-    if _root_event_filter.include_event(event, state["type"]):
-        yield event
-
-
-async def _astream_events_implementation_v2(
-    runnable: Runnable[Input, Output],
-    input: Any,
-    config: Optional[RunnableConfig] = None,
-    *,
-    include_names: Optional[Sequence[str]] = None,
-    include_types: Optional[Sequence[str]] = None,
-    include_tags: Optional[Sequence[str]] = None,
-    exclude_names: Optional[Sequence[str]] = None,
-    exclude_types: Optional[Sequence[str]] = None,
-    exclude_tags: Optional[Sequence[str]] = None,
-    **kwargs: Any,
-) -> AsyncIterator[StandardStreamEvent]:
-    """Implementation of the astream events API for V2 runnables."""
-    from langchain_core.callbacks.base import BaseCallbackManager
-    from langchain_core.runnables import ensure_config
-
-    event_streamer = _AstreamEventsCallbackHandler(
-        include_names=include_names,
-        include_types=include_types,
-        include_tags=include_tags,
-        exclude_names=exclude_names,
-        exclude_types=exclude_types,
-        exclude_tags=exclude_tags,
-    )
-
-    # Assign the stream handler to the config
-    config = ensure_config(config)
-    run_id = cast(UUID, config.setdefault("run_id", uuid4()))
-    callbacks = config.get("callbacks")
-    if callbacks is None:
-        config["callbacks"] = [event_streamer]
-    elif isinstance(callbacks, list):
-        config["callbacks"] = callbacks + [event_streamer]
-    elif isinstance(callbacks, BaseCallbackManager):
-        callbacks = callbacks.copy()
-        callbacks.add_handler(event_streamer, inherit=True)
-        config["callbacks"] = callbacks
-    else:
-        msg = (
-            f"Unexpected type for callbacks: {callbacks}."
-            "Expected None, list or AsyncCallbackManager."
-        )
-        raise ValueError(msg)
-
-    # Call the runnable in streaming mode,
-    # add each chunk to the output stream
-    async def consume_astream() -> None:
-        try:
-            # if astream also calls tap_output_aiter this will be a no-op
-            async with aclosing(runnable.astream(input, config, **kwargs)) as stream:
-                async for _ in event_streamer.tap_output_aiter(run_id, stream):
-                    # All the content will be picked up
-                    pass
-        finally:
-            await event_streamer.send_stream.aclose()
-
-    # Start the runnable in a task, so we can start consuming output
-    task = asyncio.create_task(consume_astream())
-
-    first_event_sent = False
-    first_event_run_id = None
-
-    try:
-        async for event in event_streamer:
-            if not first_event_sent:
-                first_event_sent = True
-                # This is a work-around an issue where the inputs into the
-                # chain are not available until the entire input is consumed.
-                # As a temporary solution, we'll modify the input to be the input
-                # that was passed into the chain.
-                event["data"]["input"] = input
-                first_event_run_id = event["run_id"]
-                yield event
-                continue
-
-            # If it's the end event corresponding to the root runnable
-            # we dont include the input in the event since it's guaranteed
-            # to be included in the first event.
-            if (
-                event["run_id"] == first_event_run_id
-                and event["event"].endswith("_end")
-                and "input" in event["data"]
-            ):
-                del event["data"]["input"]
-
-            yield event
-    except asyncio.CancelledError as exc:
-        # Cancel the task if it's still running
-        task.cancel(exc.args[0] if exc.args else None)
-        raise
-    finally:
-        # Cancel the task if it's still running
-        task.cancel()
-        # Await it anyway, to run any cleanup code, and propagate any exceptions
-        with contextlib.suppress(asyncio.CancelledError):
-            await task
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/langchain.py ./custom_langchain_core/tracers/langchain.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/langchain.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/langchain.py	1970-01-01 09:00:00
@@ -1,323 +0,0 @@
-"""A Tracer implementation that records to LangChain endpoint."""
-
-from __future__ import annotations
-
-import logging
-import warnings
-from concurrent.futures import ThreadPoolExecutor
-from datetime import datetime, timezone
-from typing import TYPE_CHECKING, Any, Optional, Union
-from uuid import UUID
-
-from langsmith import Client
-from langsmith import run_trees as rt
-from langsmith import utils as ls_utils
-from pydantic import PydanticDeprecationWarning
-from tenacity import (
-    Retrying,
-    retry_if_exception_type,
-    stop_after_attempt,
-    wait_exponential_jitter,
-)
-
-from langchain_core.env import get_runtime_environment
-from langchain_core.load import dumpd
-from langchain_core.outputs import ChatGenerationChunk, GenerationChunk
-from langchain_core.tracers.base import BaseTracer
-from langchain_core.tracers.schemas import Run
-
-if TYPE_CHECKING:
-    from langchain_core.messages import BaseMessage
-
-logger = logging.getLogger(__name__)
-_LOGGED = set()
-_EXECUTOR: Optional[ThreadPoolExecutor] = None
-
-
-def log_error_once(method: str, exception: Exception) -> None:
-    """Log an error once.
-
-    Args:
-        method: The method that raised the exception.
-        exception: The exception that was raised.
-    """
-    global _LOGGED
-    if (method, type(exception)) in _LOGGED:
-        return
-    _LOGGED.add((method, type(exception)))
-    logger.error(exception)
-
-
-def wait_for_all_tracers() -> None:
-    """Wait for all tracers to finish."""
-    if rt._CLIENT is not None and rt._CLIENT.tracing_queue is not None:
-        rt._CLIENT.tracing_queue.join()
-
-
-def get_client() -> Client:
-    """Get the client."""
-    return rt.get_cached_client()
-
-
-def _get_executor() -> ThreadPoolExecutor:
-    """Get the executor."""
-    global _EXECUTOR
-    if _EXECUTOR is None:
-        _EXECUTOR = ThreadPoolExecutor()
-    return _EXECUTOR
-
-
-def _run_to_dict(run: Run, exclude_inputs: bool = False) -> dict:
-    # TODO: Update once langsmith moves to Pydantic V2 and we can swap run.dict for
-    # run.model_dump
-    with warnings.catch_warnings():
-        warnings.simplefilter("ignore", category=PydanticDeprecationWarning)
-
-        res = {
-            **run.dict(exclude={"child_runs", "inputs", "outputs"}),
-            "outputs": run.outputs,
-        }
-        if not exclude_inputs:
-            res["inputs"] = run.inputs
-    return res
-
-
-class LangChainTracer(BaseTracer):
-    """Implementation of the SharedTracer that POSTS to the LangChain endpoint."""
-
-    run_inline = True
-
-    def __init__(
-        self,
-        example_id: Optional[Union[UUID, str]] = None,
-        project_name: Optional[str] = None,
-        client: Optional[Client] = None,
-        tags: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Initialize the LangChain tracer.
-
-        Args:
-            example_id: The example ID.
-            project_name: The project name. Defaults to the tracer project.
-            client: The client. Defaults to the global client.
-            tags: The tags. Defaults to an empty list.
-            kwargs: Additional keyword arguments.
-        """
-        super().__init__(**kwargs)
-        self.example_id = (
-            UUID(example_id) if isinstance(example_id, str) else example_id
-        )
-        self.project_name = project_name or ls_utils.get_tracer_project()
-        self.client = client or get_client()
-        self.tags = tags or []
-        self.latest_run: Optional[Run] = None
-
-    def _start_trace(self, run: Run) -> None:
-        if self.project_name:
-            run.session_name = self.project_name
-        if self.tags is not None:
-            if run.tags:
-                run.tags = sorted(set(run.tags + self.tags))
-            else:
-                run.tags = self.tags.copy()
-
-        super()._start_trace(run)
-        if run._client is None:
-            run._client = self.client  # type: ignore
-
-    def on_chat_model_start(
-        self,
-        serialized: dict[str, Any],
-        messages: list[list[BaseMessage]],
-        *,
-        run_id: UUID,
-        tags: Optional[list[str]] = None,
-        parent_run_id: Optional[UUID] = None,
-        metadata: Optional[dict[str, Any]] = None,
-        name: Optional[str] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Start a trace for an LLM run.
-
-        Args:
-            serialized: The serialized model.
-            messages: The messages.
-            run_id: The run ID.
-            tags: The tags. Defaults to None.
-            parent_run_id: The parent run ID. Defaults to None.
-            metadata: The metadata. Defaults to None.
-            name: The name. Defaults to None.
-            kwargs: Additional keyword arguments.
-
-        Returns:
-            Run: The run.
-        """
-        start_time = datetime.now(timezone.utc)
-        if metadata:
-            kwargs.update({"metadata": metadata})
-        chat_model_run = Run(
-            id=run_id,
-            parent_run_id=parent_run_id,
-            serialized=serialized,
-            inputs={"messages": [[dumpd(msg) for msg in batch] for batch in messages]},
-            extra=kwargs,
-            events=[{"name": "start", "time": start_time}],
-            start_time=start_time,
-            run_type="llm",
-            tags=tags,
-            name=name,  # type: ignore[arg-type]
-        )
-        self._start_trace(chat_model_run)
-        self._on_chat_model_start(chat_model_run)
-        return chat_model_run
-
-    def _persist_run(self, run: Run) -> None:
-        self.latest_run = run
-
-    def get_run_url(self) -> str:
-        """Get the LangSmith root run URL.
-
-        Returns:
-            str: The LangSmith root run URL.
-
-        Raises:
-            ValueError: If no traced run is found.
-            ValueError: If the run URL cannot be found.
-        """
-        if not self.latest_run:
-            msg = "No traced run found."
-            raise ValueError(msg)
-        # If this is the first run in a project, the project may not yet be created.
-        # This method is only really useful for debugging flows, so we will assume
-        # there is some tolerace for latency.
-        for attempt in Retrying(
-            stop=stop_after_attempt(5),
-            wait=wait_exponential_jitter(),
-            retry=retry_if_exception_type(ls_utils.LangSmithError),
-        ):
-            with attempt:
-                return self.client.get_run_url(
-                    run=self.latest_run, project_name=self.project_name
-                )
-        msg = "Failed to get run URL."
-        raise ValueError(msg)
-
-    def _get_tags(self, run: Run) -> list[str]:
-        """Get combined tags for a run."""
-        tags = set(run.tags or [])
-        tags.update(self.tags or [])
-        return list(tags)
-
-    def _persist_run_single(self, run: Run) -> None:
-        """Persist a run."""
-        try:
-            run_dict = _run_to_dict(run)
-            run_dict["tags"] = self._get_tags(run)
-            extra = run_dict.get("extra", {})
-            extra["runtime"] = get_runtime_environment()
-            run_dict["extra"] = extra
-            inputs_is_truthy = bool(run_dict.get("inputs"))
-            run.extra["inputs_is_truthy"] = inputs_is_truthy
-            self.client.create_run(**run_dict, project_name=self.project_name)
-        except Exception as e:
-            # Errors are swallowed by the thread executor so we need to log them here
-            log_error_once("post", e)
-            raise
-
-    def _update_run_single(self, run: Run) -> None:
-        """Update a run."""
-        try:
-            exclude_inputs = run.extra.get("inputs_is_truthy", False)
-            run_dict = _run_to_dict(run, exclude_inputs=exclude_inputs)
-            run_dict["tags"] = self._get_tags(run)
-            self.client.update_run(run.id, **run_dict)
-        except Exception as e:
-            # Errors are swallowed by the thread executor so we need to log them here
-            log_error_once("patch", e)
-            raise
-
-    def _on_llm_start(self, run: Run) -> None:
-        """Persist an LLM run."""
-        if run.parent_run_id is None:
-            run.reference_example_id = self.example_id
-        self._persist_run_single(run)
-
-    def _llm_run_with_token_event(
-        self,
-        token: str,
-        run_id: UUID,
-        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None,
-        parent_run_id: Optional[UUID] = None,
-        **kwargs: Any,
-    ) -> Run:
-        """Append token event to LLM run and return the run."""
-        return super()._llm_run_with_token_event(
-            # Drop the chunk; we don't need to save it
-            token,
-            run_id,
-            chunk=None,
-            parent_run_id=parent_run_id,
-            **kwargs,
-        )
-
-    def _on_chat_model_start(self, run: Run) -> None:
-        """Persist an LLM run."""
-        if run.parent_run_id is None:
-            run.reference_example_id = self.example_id
-        self._persist_run_single(run)
-
-    def _on_llm_end(self, run: Run) -> None:
-        """Process the LLM Run."""
-        self._update_run_single(run)
-
-    def _on_llm_error(self, run: Run) -> None:
-        """Process the LLM Run upon error."""
-        self._update_run_single(run)
-
-    def _on_chain_start(self, run: Run) -> None:
-        """Process the Chain Run upon start."""
-        if run.parent_run_id is None:
-            run.reference_example_id = self.example_id
-        self._persist_run_single(run)
-
-    def _on_chain_end(self, run: Run) -> None:
-        """Process the Chain Run."""
-        self._update_run_single(run)
-
-    def _on_chain_error(self, run: Run) -> None:
-        """Process the Chain Run upon error."""
-        self._update_run_single(run)
-
-    def _on_tool_start(self, run: Run) -> None:
-        """Process the Tool Run upon start."""
-        if run.parent_run_id is None:
-            run.reference_example_id = self.example_id
-        self._persist_run_single(run)
-
-    def _on_tool_end(self, run: Run) -> None:
-        """Process the Tool Run."""
-        self._update_run_single(run)
-
-    def _on_tool_error(self, run: Run) -> None:
-        """Process the Tool Run upon error."""
-        self._update_run_single(run)
-
-    def _on_retriever_start(self, run: Run) -> None:
-        """Process the Retriever Run upon start."""
-        if run.parent_run_id is None:
-            run.reference_example_id = self.example_id
-        self._persist_run_single(run)
-
-    def _on_retriever_end(self, run: Run) -> None:
-        """Process the Retriever Run."""
-        self._update_run_single(run)
-
-    def _on_retriever_error(self, run: Run) -> None:
-        """Process the Retriever Run upon error."""
-        self._update_run_single(run)
-
-    def wait_for_futures(self) -> None:
-        """Wait for the given futures to complete."""
-        if self.client is not None and self.client.tracing_queue is not None:
-            self.client.tracing_queue.join()
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/langchain_v1.py ./custom_langchain_core/tracers/langchain_v1.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/langchain_v1.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/langchain_v1.py	1970-01-01 09:00:00
@@ -1,18 +0,0 @@
-from typing import Any
-
-
-def get_headers(*args: Any, **kwargs: Any) -> Any:
-    """Throw an error because this has been replaced by get_headers."""
-    msg = (
-        "get_headers for LangChainTracerV1 is no longer supported. "
-        "Please use LangChainTracer instead."
-    )
-    raise RuntimeError(msg)
-
-
-def LangChainTracerV1(*args: Any, **kwargs: Any) -> Any:  # noqa: N802
-    """Throw an error because this has been replaced by LangChainTracer."""
-    msg = (
-        "LangChainTracerV1 is no longer supported. Please use LangChainTracer instead."
-    )
-    raise RuntimeError(msg)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py ./custom_langchain_core/tracers/log_stream.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/log_stream.py	1970-01-01 09:00:00
@@ -1,675 +0,0 @@
-from __future__ import annotations
-
-import asyncio
-import contextlib
-import copy
-import threading
-from collections import defaultdict
-from collections.abc import AsyncIterator, Iterator, Sequence
-from typing import (
-    Any,
-    Literal,
-    Optional,
-    TypeVar,
-    Union,
-    overload,
-)
-from uuid import UUID
-
-import jsonpatch  # type: ignore[import]
-from typing_extensions import NotRequired, TypedDict
-
-from langchain_core.load import dumps
-from langchain_core.load.load import load
-from langchain_core.outputs import ChatGenerationChunk, GenerationChunk
-from langchain_core.runnables import Runnable, RunnableConfig, ensure_config
-from langchain_core.runnables.utils import Input, Output
-from langchain_core.tracers._streaming import _StreamingCallbackHandler
-from langchain_core.tracers.base import BaseTracer
-from langchain_core.tracers.memory_stream import _MemoryStream
-from langchain_core.tracers.schemas import Run
-
-
-class LogEntry(TypedDict):
-    """A single entry in the run log."""
-
-    id: str
-    """ID of the sub-run."""
-    name: str
-    """Name of the object being run."""
-    type: str
-    """Type of the object being run, eg. prompt, chain, llm, etc."""
-    tags: list[str]
-    """List of tags for the run."""
-    metadata: dict[str, Any]
-    """Key-value pairs of metadata for the run."""
-    start_time: str
-    """ISO-8601 timestamp of when the run started."""
-
-    streamed_output_str: list[str]
-    """List of LLM tokens streamed by this run, if applicable."""
-    streamed_output: list[Any]
-    """List of output chunks streamed by this run, if available."""
-    inputs: NotRequired[Optional[Any]]
-    """Inputs to this run. Not available currently via astream_log."""
-    final_output: Optional[Any]
-    """Final output of this run.
-
-    Only available after the run has finished successfully."""
-    end_time: Optional[str]
-    """ISO-8601 timestamp of when the run ended.
-    Only available after the run has finished."""
-
-
-class RunState(TypedDict):
-    """State of the run."""
-
-    id: str
-    """ID of the run."""
-    streamed_output: list[Any]
-    """List of output chunks streamed by Runnable.stream()"""
-    final_output: Optional[Any]
-    """Final output of the run, usually the result of aggregating (`+`) streamed_output.
-    Updated throughout the run when supported by the Runnable."""
-
-    name: str
-    """Name of the object being run."""
-    type: str
-    """Type of the object being run, eg. prompt, chain, llm, etc."""
-
-    # Do we want tags/metadata on the root run? Client kinda knows it in most situations
-    # tags: List[str]
-
-    logs: dict[str, LogEntry]
-    """Map of run names to sub-runs. If filters were supplied, this list will
-    contain only the runs that matched the filters."""
-
-
-class RunLogPatch:
-    """Patch to the run log."""
-
-    ops: list[dict[str, Any]]
-    """List of jsonpatch operations, which describe how to create the run state
-    from an empty dict. This is the minimal representation of the log, designed to
-    be serialized as JSON and sent over the wire to reconstruct the log on the other
-    side. Reconstruction of the state can be done with any jsonpatch-compliant library,
-    see https://jsonpatch.com for more information."""
-
-    def __init__(self, *ops: dict[str, Any]) -> None:
-        self.ops = list(ops)
-
-    def __add__(self, other: Union[RunLogPatch, Any]) -> RunLog:
-        if type(other) is RunLogPatch:
-            ops = self.ops + other.ops
-            state = jsonpatch.apply_patch(None, copy.deepcopy(ops))
-            return RunLog(*ops, state=state)
-
-        msg = f"unsupported operand type(s) for +: '{type(self)}' and '{type(other)}'"
-        raise TypeError(msg)
-
-    def __repr__(self) -> str:
-        from pprint import pformat
-
-        # 1:-1 to get rid of the [] around the list
-        return f"RunLogPatch({pformat(self.ops)[1:-1]})"
-
-    def __eq__(self, other: object) -> bool:
-        return isinstance(other, RunLogPatch) and self.ops == other.ops
-
-
-class RunLog(RunLogPatch):
-    """Run log."""
-
-    state: RunState
-    """Current state of the log, obtained from applying all ops in sequence."""
-
-    def __init__(self, *ops: dict[str, Any], state: RunState) -> None:
-        super().__init__(*ops)
-        self.state = state
-
-    def __add__(self, other: Union[RunLogPatch, Any]) -> RunLog:
-        if type(other) is RunLogPatch:
-            ops = self.ops + other.ops
-            state = jsonpatch.apply_patch(self.state, other.ops)
-            return RunLog(*ops, state=state)
-
-        msg = f"unsupported operand type(s) for +: '{type(self)}' and '{type(other)}'"
-        raise TypeError(msg)
-
-    def __repr__(self) -> str:
-        from pprint import pformat
-
-        return f"RunLog({pformat(self.state)})"
-
-    def __eq__(self, other: object) -> bool:
-        # First compare that the state is the same
-        if not isinstance(other, RunLog):
-            return False
-        if self.state != other.state:
-            return False
-        # Then compare that the ops are the same
-        return super().__eq__(other)
-
-
-T = TypeVar("T")
-
-
-class LogStreamCallbackHandler(BaseTracer, _StreamingCallbackHandler):
-    """Tracer that streams run logs to a stream."""
-
-    def __init__(
-        self,
-        *,
-        auto_close: bool = True,
-        include_names: Optional[Sequence[str]] = None,
-        include_types: Optional[Sequence[str]] = None,
-        include_tags: Optional[Sequence[str]] = None,
-        exclude_names: Optional[Sequence[str]] = None,
-        exclude_types: Optional[Sequence[str]] = None,
-        exclude_tags: Optional[Sequence[str]] = None,
-        # Schema format is for internal use only.
-        _schema_format: Literal["original", "streaming_events"] = "streaming_events",
-    ) -> None:
-        """A tracer that streams run logs to a stream.
-
-        Args:
-            auto_close: Whether to close the stream when the root run finishes.
-            include_names: Only include runs from Runnables with matching names.
-            include_types: Only include runs from Runnables with matching types.
-            include_tags: Only include runs from Runnables with matching tags.
-            exclude_names: Exclude runs from Runnables with matching names.
-            exclude_types: Exclude runs from Runnables with matching types.
-            exclude_tags: Exclude runs from Runnables with matching tags.
-            _schema_format: Primarily changes how the inputs and outputs are
-                handled.
-                **For internal use only. This API will change.**
-                - 'original' is the format used by all current tracers.
-                  This format is slightly inconsistent with respect to inputs
-                  and outputs.
-                - 'streaming_events' is used for supporting streaming events,
-                  for internal usage. It will likely change in the future, or
-                  be deprecated entirely in favor of a dedicated async tracer
-                  for streaming events.
-
-        Raises:
-            ValueError: If an invalid schema format is provided (internal use only).
-        """
-        if _schema_format not in {"original", "streaming_events"}:
-            msg = (
-                f"Invalid schema format: {_schema_format}. "
-                f"Expected one of 'original', 'streaming_events'."
-            )
-            raise ValueError(msg)
-        super().__init__(_schema_format=_schema_format)
-
-        self.auto_close = auto_close
-        self.include_names = include_names
-        self.include_types = include_types
-        self.include_tags = include_tags
-        self.exclude_names = exclude_names
-        self.exclude_types = exclude_types
-        self.exclude_tags = exclude_tags
-
-        loop = asyncio.get_event_loop()
-        memory_stream = _MemoryStream[RunLogPatch](loop)
-        self.lock = threading.Lock()
-        self.send_stream = memory_stream.get_send_stream()
-        self.receive_stream = memory_stream.get_receive_stream()
-        self._key_map_by_run_id: dict[UUID, str] = {}
-        self._counter_map_by_name: dict[str, int] = defaultdict(int)
-        self.root_id: Optional[UUID] = None
-
-    def __aiter__(self) -> AsyncIterator[RunLogPatch]:
-        return self.receive_stream.__aiter__()
-
-    def send(self, *ops: dict[str, Any]) -> bool:
-        """Send a patch to the stream, return False if the stream is closed.
-
-        Args:
-            *ops: The operations to send to the stream.
-
-        Returns:
-            bool: True if the patch was sent successfully, False if the stream
-                is closed.
-        """
-        # We will likely want to wrap this in try / except at some point
-        # to handle exceptions that might arise at run time.
-        # For now we'll let the exception bubble up, and always return
-        # True on the happy path.
-        self.send_stream.send_nowait(RunLogPatch(*ops))
-        return True
-
-    async def tap_output_aiter(
-        self, run_id: UUID, output: AsyncIterator[T]
-    ) -> AsyncIterator[T]:
-        """Tap an output async iterator to stream its values to the log.
-
-        Args:
-            run_id: The ID of the run.
-            output: The output async iterator.
-
-        Yields:
-            T: The output value.
-        """
-        async for chunk in output:
-            # root run is handled in .astream_log()
-            # if we can't find the run silently ignore
-            # eg. because this run wasn't included in the log
-            if (
-                run_id != self.root_id
-                and (key := self._key_map_by_run_id.get(run_id))
-                and (
-                    not self.send(
-                        {
-                            "op": "add",
-                            "path": f"/logs/{key}/streamed_output/-",
-                            "value": chunk,
-                        }
-                    )
-                )
-            ):
-                break
-
-            yield chunk
-
-    def tap_output_iter(self, run_id: UUID, output: Iterator[T]) -> Iterator[T]:
-        """Tap an output async iterator to stream its values to the log.
-
-        Args:
-            run_id: The ID of the run.
-            output: The output iterator.
-
-        Yields:
-            T: The output value.
-        """
-        for chunk in output:
-            # root run is handled in .astream_log()
-            # if we can't find the run silently ignore
-            # eg. because this run wasn't included in the log
-            if (
-                run_id != self.root_id
-                and (key := self._key_map_by_run_id.get(run_id))
-                and (
-                    not self.send(
-                        {
-                            "op": "add",
-                            "path": f"/logs/{key}/streamed_output/-",
-                            "value": chunk,
-                        }
-                    )
-                )
-            ):
-                break
-
-            yield chunk
-
-    def include_run(self, run: Run) -> bool:
-        """Check if a Run should be included in the log.
-
-        Args:
-            run: The Run to check.
-
-        Returns:
-            bool: True if the run should be included, False otherwise.
-        """
-        if run.id == self.root_id:
-            return False
-
-        run_tags = run.tags or []
-
-        if (
-            self.include_names is None
-            and self.include_types is None
-            and self.include_tags is None
-        ):
-            include = True
-        else:
-            include = False
-
-        if self.include_names is not None:
-            include = include or run.name in self.include_names
-        if self.include_types is not None:
-            include = include or run.run_type in self.include_types
-        if self.include_tags is not None:
-            include = include or any(tag in self.include_tags for tag in run_tags)
-
-        if self.exclude_names is not None:
-            include = include and run.name not in self.exclude_names
-        if self.exclude_types is not None:
-            include = include and run.run_type not in self.exclude_types
-        if self.exclude_tags is not None:
-            include = include and all(tag not in self.exclude_tags for tag in run_tags)
-
-        return include
-
-    def _persist_run(self, run: Run) -> None:
-        # This is a legacy method only called once for an entire run tree
-        # therefore not useful here
-        pass
-
-    def _on_run_create(self, run: Run) -> None:
-        """Start a run."""
-        if self.root_id is None:
-            self.root_id = run.id
-            if not self.send(
-                {
-                    "op": "replace",
-                    "path": "",
-                    "value": RunState(
-                        id=str(run.id),
-                        streamed_output=[],
-                        final_output=None,
-                        logs={},
-                        name=run.name,
-                        type=run.run_type,
-                    ),
-                }
-            ):
-                return
-
-        if not self.include_run(run):
-            return
-
-        # Determine previous index, increment by 1
-        with self.lock:
-            self._counter_map_by_name[run.name] += 1
-            count = self._counter_map_by_name[run.name]
-            self._key_map_by_run_id[run.id] = (
-                run.name if count == 1 else f"{run.name}:{count}"
-            )
-
-        entry = LogEntry(
-            id=str(run.id),
-            name=run.name,
-            type=run.run_type,
-            tags=run.tags or [],
-            metadata=(run.extra or {}).get("metadata", {}),
-            start_time=run.start_time.isoformat(timespec="milliseconds"),
-            streamed_output=[],
-            streamed_output_str=[],
-            final_output=None,
-            end_time=None,
-        )
-
-        if self._schema_format == "streaming_events":
-            # If using streaming events let's add inputs as well
-            entry["inputs"] = _get_standardized_inputs(run, self._schema_format)
-
-        # Add the run to the stream
-        self.send(
-            {
-                "op": "add",
-                "path": f"/logs/{self._key_map_by_run_id[run.id]}",
-                "value": entry,
-            }
-        )
-
-    def _on_run_update(self, run: Run) -> None:
-        """Finish a run."""
-        try:
-            index = self._key_map_by_run_id.get(run.id)
-
-            if index is None:
-                return
-
-            ops = []
-
-            if self._schema_format == "streaming_events":
-                ops.append(
-                    {
-                        "op": "replace",
-                        "path": f"/logs/{index}/inputs",
-                        "value": _get_standardized_inputs(run, self._schema_format),
-                    }
-                )
-
-            ops.extend(
-                [
-                    # Replace 'inputs' with final inputs
-                    # This is needed because in many cases the inputs are not
-                    # known until after the run is finished and the entire
-                    # input stream has been processed by the runnable.
-                    {
-                        "op": "add",
-                        "path": f"/logs/{index}/final_output",
-                        # to undo the dumpd done by some runnables / tracer / etc
-                        "value": _get_standardized_outputs(run, self._schema_format),
-                    },
-                    {
-                        "op": "add",
-                        "path": f"/logs/{index}/end_time",
-                        "value": run.end_time.isoformat(timespec="milliseconds")
-                        if run.end_time is not None
-                        else None,
-                    },
-                ]
-            )
-
-            self.send(*ops)
-        finally:
-            if run.id == self.root_id and self.auto_close:
-                self.send_stream.close()
-
-    def _on_llm_new_token(
-        self,
-        run: Run,
-        token: str,
-        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]],
-    ) -> None:
-        """Process new LLM token."""
-        index = self._key_map_by_run_id.get(run.id)
-
-        if index is None:
-            return
-
-        self.send(
-            {
-                "op": "add",
-                "path": f"/logs/{index}/streamed_output_str/-",
-                "value": token,
-            },
-            {
-                "op": "add",
-                "path": f"/logs/{index}/streamed_output/-",
-                "value": chunk.message
-                if isinstance(chunk, ChatGenerationChunk)
-                else token,
-            },
-        )
-
-
-def _get_standardized_inputs(
-    run: Run, schema_format: Literal["original", "streaming_events"]
-) -> Optional[dict[str, Any]]:
-    """Extract standardized inputs from a run.
-
-    Standardizes the inputs based on the type of the runnable used.
-
-    Args:
-        run: Run object
-        schema_format: The schema format to use.
-
-    Returns:
-        Valid inputs are only dict. By conventions, inputs always represented
-        invocation using named arguments.
-        None means that the input is not yet known!
-    """
-    if schema_format == "original":
-        msg = (
-            "Do not assign inputs with original schema drop the key for now."
-            "When inputs are added to astream_log they should be added with "
-            "standardized schema for streaming events."
-        )
-        raise NotImplementedError(msg)
-
-    inputs = load(run.inputs)
-
-    if run.run_type in {"retriever", "llm", "chat_model"}:
-        return inputs
-
-    # new style chains
-    # These nest an additional 'input' key inside the 'inputs' to make sure
-    # the input is always a dict. We need to unpack and user the inner value.
-    inputs = inputs["input"]
-    # We should try to fix this in Runnables and callbacks/tracers
-    # Runnables should be using a None type here not a placeholder
-    # dict.
-    if inputs == {"input": ""}:  # Workaround for Runnables not using None
-        # The input is not known, so we don't assign data['input']
-        return None
-    return inputs
-
-
-def _get_standardized_outputs(
-    run: Run, schema_format: Literal["original", "streaming_events", "original+chat"]
-) -> Optional[Any]:
-    """Extract standardized output from a run.
-
-    Standardizes the outputs based on the type of the runnable used.
-
-    Args:
-        log: The log entry.
-        schema_format: The schema format to use.
-
-    Returns:
-        An output if returned, otherwise a None
-    """
-    outputs = load(run.outputs)
-    if schema_format == "original":
-        if run.run_type == "prompt" and "output" in outputs:
-            # These were previously dumped before the tracer.
-            # Now we needn't do anything to them.
-            return outputs["output"]
-        # Return the old schema, without standardizing anything
-        return outputs
-
-    if run.run_type in {"retriever", "llm", "chat_model"}:
-        return outputs
-
-    if isinstance(outputs, dict):
-        return outputs.get("output", None)
-
-    return None
-
-
-@overload
-def _astream_log_implementation(
-    runnable: Runnable[Input, Output],
-    input: Any,
-    config: Optional[RunnableConfig] = None,
-    *,
-    stream: LogStreamCallbackHandler,
-    diff: Literal[True] = True,
-    with_streamed_output_list: bool = True,
-    **kwargs: Any,
-) -> AsyncIterator[RunLogPatch]: ...
-
-
-@overload
-def _astream_log_implementation(
-    runnable: Runnable[Input, Output],
-    input: Any,
-    config: Optional[RunnableConfig] = None,
-    *,
-    stream: LogStreamCallbackHandler,
-    diff: Literal[False],
-    with_streamed_output_list: bool = True,
-    **kwargs: Any,
-) -> AsyncIterator[RunLog]: ...
-
-
-async def _astream_log_implementation(
-    runnable: Runnable[Input, Output],
-    input: Any,
-    config: Optional[RunnableConfig] = None,
-    *,
-    stream: LogStreamCallbackHandler,
-    diff: bool = True,
-    with_streamed_output_list: bool = True,
-    **kwargs: Any,
-) -> Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]:
-    """Implementation of astream_log for a given runnable.
-
-    The implementation has been factored out (at least temporarily) as both
-    astream_log and astream_events relies on it.
-    """
-    import jsonpatch  # type: ignore[import]
-
-    from langchain_core.callbacks.base import BaseCallbackManager
-    from langchain_core.tracers.log_stream import (
-        RunLog,
-        RunLogPatch,
-    )
-
-    # Assign the stream handler to the config
-    config = ensure_config(config)
-    callbacks = config.get("callbacks")
-    if callbacks is None:
-        config["callbacks"] = [stream]
-    elif isinstance(callbacks, list):
-        config["callbacks"] = callbacks + [stream]
-    elif isinstance(callbacks, BaseCallbackManager):
-        callbacks = callbacks.copy()
-        callbacks.add_handler(stream, inherit=True)
-        config["callbacks"] = callbacks
-    else:
-        msg = (
-            f"Unexpected type for callbacks: {callbacks}."
-            "Expected None, list or AsyncCallbackManager."
-        )
-        raise ValueError(msg)
-
-    # Call the runnable in streaming mode,
-    # add each chunk to the output stream
-    async def consume_astream() -> None:
-        try:
-            prev_final_output: Optional[Output] = None
-            final_output: Optional[Output] = None
-
-            async for chunk in runnable.astream(input, config, **kwargs):
-                prev_final_output = final_output
-                if final_output is None:
-                    final_output = chunk
-                else:
-                    try:
-                        final_output = final_output + chunk  # type: ignore
-                    except TypeError:
-                        prev_final_output = None
-                        final_output = chunk
-                patches: list[dict[str, Any]] = []
-                if with_streamed_output_list:
-                    patches.append(
-                        {
-                            "op": "add",
-                            "path": "/streamed_output/-",
-                            # chunk cannot be shared between
-                            # streamed_output and final_output
-                            # otherwise jsonpatch.apply will
-                            # modify both
-                            "value": copy.deepcopy(chunk),
-                        }
-                    )
-                for op in jsonpatch.JsonPatch.from_diff(
-                    prev_final_output, final_output, dumps=dumps
-                ):
-                    patches.append({**op, "path": f"/final_output{op['path']}"})
-                await stream.send_stream.send(RunLogPatch(*patches))
-        finally:
-            await stream.send_stream.aclose()
-
-    # Start the runnable in a task, so we can start consuming output
-    task = asyncio.create_task(consume_astream())
-    try:
-        # Yield each chunk from the output stream
-        if diff:
-            async for log in stream:
-                yield log
-        else:
-            state = RunLog(state=None)  # type: ignore[arg-type]
-            async for log in stream:
-                state = state + log
-                yield state
-    finally:
-        # Wait for the runnable to finish, if not cancelled (eg. by break)
-        with contextlib.suppress(asyncio.CancelledError):
-            await task
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/memory_stream.py ./custom_langchain_core/tracers/memory_stream.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/memory_stream.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/memory_stream.py	1970-01-01 09:00:00
@@ -1,145 +0,0 @@
-"""Module implements a memory stream for communication between two co-routines.
-
-This module provides a way to communicate between two co-routines using a memory
-channel. The writer and reader can be in the same event loop or in different event
-loops. When they're in different event loops, they will also be in different
-threads.
-
-This is useful in situations when there's a mix of synchronous and asynchronous
-used in the code.
-"""
-
-import asyncio
-from asyncio import AbstractEventLoop, Queue
-from collections.abc import AsyncIterator
-from typing import Generic, TypeVar
-
-T = TypeVar("T")
-
-
-class _SendStream(Generic[T]):
-    def __init__(
-        self, reader_loop: AbstractEventLoop, queue: Queue, done: object
-    ) -> None:
-        """Create a writer for the queue and done object.
-
-        Args:
-            reader_loop: The event loop to use for the writer. This loop will be used
-                         to schedule the writes to the queue.
-            queue: The queue to write to. This is an asyncio queue.
-            done: Special sentinel object to indicate that the writer is done.
-        """
-        self._reader_loop = reader_loop
-        self._queue = queue
-        self._done = done
-
-    async def send(self, item: T) -> None:
-        """Schedule the item to be written to the queue using the original loop.
-
-        This is a coroutine that can be awaited.
-
-        Args:
-            item: The item to write to the queue.
-        """
-        return self.send_nowait(item)
-
-    def send_nowait(self, item: T) -> None:
-        """Schedule the item to be written to the queue using the original loop.
-
-        This is a non-blocking call.
-
-        Args:
-            item: The item to write to the queue.
-
-        Raises:
-            RuntimeError: If the event loop is already closed when trying to write
-                            to the queue.
-        """
-        try:
-            self._reader_loop.call_soon_threadsafe(self._queue.put_nowait, item)
-        except RuntimeError:
-            if not self._reader_loop.is_closed():
-                raise  # Raise the exception if the loop is not closed
-
-    async def aclose(self) -> None:
-        """Async schedule the done object write the queue using the original loop."""
-        return self.close()
-
-    def close(self) -> None:
-        """Schedule the done object write the queue using the original loop.
-
-        This is a non-blocking call.
-
-        Raises:
-            RuntimeError: If the event loop is already closed when trying to write
-                            to the queue.
-        """
-        try:
-            self._reader_loop.call_soon_threadsafe(self._queue.put_nowait, self._done)
-        except RuntimeError:
-            if not self._reader_loop.is_closed():
-                raise  # Raise the exception if the loop is not closed
-
-
-class _ReceiveStream(Generic[T]):
-    def __init__(self, queue: Queue, done: object) -> None:
-        """Create a reader for the queue and done object.
-
-        This reader should be used in the same loop as the loop that was passed
-        to the channel.
-        """
-        self._queue = queue
-        self._done = done
-        self._is_closed = False
-
-    async def __aiter__(self) -> AsyncIterator[T]:
-        while True:
-            item = await self._queue.get()
-            if item is self._done:
-                self._is_closed = True
-                break
-            yield item
-
-
-class _MemoryStream(Generic[T]):
-    """Stream data from a writer to a reader even if they are in different threads.
-
-    Uses asyncio queues to communicate between two co-routines. This implementation
-    should work even if the writer and reader co-routines belong to two different
-    event loops (e.g. one running from an event loop in the main thread
-    and the other running in an event loop in a background thread).
-
-    This implementation is meant to be used with a single writer and a single reader.
-
-    This is an internal implementation to LangChain. Please do not use it directly.
-    """
-
-    def __init__(self, loop: AbstractEventLoop) -> None:
-        """Create a channel for the given loop.
-
-        Args:
-            loop: The event loop to use for the channel. The reader is assumed
-                  to be running in the same loop as the one passed to this constructor.
-                  This will NOT be validated at run time.
-        """
-        self._loop = loop
-        self._queue: asyncio.Queue = asyncio.Queue(maxsize=0)
-        self._done = object()
-
-    def get_send_stream(self) -> _SendStream[T]:
-        """Get a writer for the channel.
-
-        Returns:
-            _SendStream: The writer for the channel.
-        """
-        return _SendStream[T](
-            reader_loop=self._loop, queue=self._queue, done=self._done
-        )
-
-    def get_receive_stream(self) -> _ReceiveStream[T]:
-        """Get a reader for the channel.
-
-        Returns:
-            _ReceiveStream: The reader for the channel.
-        """
-        return _ReceiveStream[T](queue=self._queue, done=self._done)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/root_listeners.py ./custom_langchain_core/tracers/root_listeners.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/root_listeners.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/root_listeners.py	1970-01-01 09:00:00
@@ -1,146 +0,0 @@
-from collections.abc import Awaitable
-from typing import Callable, Optional, Union
-from uuid import UUID
-
-from langchain_core.runnables.config import (
-    RunnableConfig,
-    acall_func_with_variable_args,
-    call_func_with_variable_args,
-)
-from langchain_core.tracers.base import AsyncBaseTracer, BaseTracer
-from langchain_core.tracers.schemas import Run
-
-Listener = Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]
-AsyncListener = Union[
-    Callable[[Run], Awaitable[None]], Callable[[Run, RunnableConfig], Awaitable[None]]
-]
-
-
-class RootListenersTracer(BaseTracer):
-    """Tracer that calls listeners on run start, end, and error.
-
-    Parameters:
-        log_missing_parent: Whether to log a warning if the parent is missing.
-            Default is False.
-        config: The runnable config.
-        on_start: The listener to call on run start.
-        on_end: The listener to call on run end.
-        on_error: The listener to call on run error.
-    """
-
-    log_missing_parent = False
-
-    def __init__(
-        self,
-        *,
-        config: RunnableConfig,
-        on_start: Optional[Listener],
-        on_end: Optional[Listener],
-        on_error: Optional[Listener],
-    ) -> None:
-        """Initialize the tracer.
-
-        Args:
-            config: The runnable config.
-            on_start: The listener to call on run start.
-            on_end: The listener to call on run end.
-            on_error: The listener to call on run error
-        """
-        super().__init__(_schema_format="original+chat")
-
-        self.config = config
-        self._arg_on_start = on_start
-        self._arg_on_end = on_end
-        self._arg_on_error = on_error
-        self.root_id: Optional[UUID] = None
-
-    def _persist_run(self, run: Run) -> None:
-        # This is a legacy method only called once for an entire run tree
-        # therefore not useful here
-        pass
-
-    def _on_run_create(self, run: Run) -> None:
-        if self.root_id is not None:
-            return
-
-        self.root_id = run.id
-
-        if self._arg_on_start is not None:
-            call_func_with_variable_args(self._arg_on_start, run, self.config)
-
-    def _on_run_update(self, run: Run) -> None:
-        if run.id != self.root_id:
-            return
-
-        if run.error is None:
-            if self._arg_on_end is not None:
-                call_func_with_variable_args(self._arg_on_end, run, self.config)
-        else:
-            if self._arg_on_error is not None:
-                call_func_with_variable_args(self._arg_on_error, run, self.config)
-
-
-class AsyncRootListenersTracer(AsyncBaseTracer):
-    """Async Tracer that calls listeners on run start, end, and error.
-
-    Parameters:
-        log_missing_parent: Whether to log a warning if the parent is missing.
-            Default is False.
-        config: The runnable config.
-        on_start: The listener to call on run start.
-        on_end: The listener to call on run end.
-        on_error: The listener to call on run error.
-    """
-
-    log_missing_parent = False
-
-    def __init__(
-        self,
-        *,
-        config: RunnableConfig,
-        on_start: Optional[AsyncListener],
-        on_end: Optional[AsyncListener],
-        on_error: Optional[AsyncListener],
-    ) -> None:
-        """Initialize the tracer.
-
-        Args:
-            config: The runnable config.
-            on_start: The listener to call on run start.
-            on_end: The listener to call on run end.
-            on_error: The listener to call on run error
-        """
-        super().__init__(_schema_format="original+chat")
-
-        self.config = config
-        self._arg_on_start = on_start
-        self._arg_on_end = on_end
-        self._arg_on_error = on_error
-        self.root_id: Optional[UUID] = None
-
-    async def _persist_run(self, run: Run) -> None:
-        # This is a legacy method only called once for an entire run tree
-        # therefore not useful here
-        pass
-
-    async def _on_run_create(self, run: Run) -> None:
-        if self.root_id is not None:
-            return
-
-        self.root_id = run.id
-
-        if self._arg_on_start is not None:
-            await acall_func_with_variable_args(self._arg_on_start, run, self.config)
-
-    async def _on_run_update(self, run: Run) -> None:
-        if run.id != self.root_id:
-            return
-
-        if run.error is None:
-            if self._arg_on_end is not None:
-                await acall_func_with_variable_args(self._arg_on_end, run, self.config)
-        else:
-            if self._arg_on_error is not None:
-                await acall_func_with_variable_args(
-                    self._arg_on_error, run, self.config
-                )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/run_collector.py ./custom_langchain_core/tracers/run_collector.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/run_collector.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/run_collector.py	1970-01-01 09:00:00
@@ -1,52 +0,0 @@
-"""A tracer that collects all nested runs in a list."""
-
-from typing import Any, Optional, Union
-from uuid import UUID
-
-from langchain_core.tracers.base import BaseTracer
-from langchain_core.tracers.schemas import Run
-
-
-class RunCollectorCallbackHandler(BaseTracer):
-    """Tracer that collects all nested runs in a list.
-
-    This tracer is useful for inspection and evaluation purposes.
-
-    Parameters
-    ----------
-    name : str, default="run-collector_callback_handler"
-    example_id : Optional[Union[UUID, str]], default=None
-        The ID of the example being traced. It can be either a UUID or a string.
-    """
-
-    name: str = "run-collector_callback_handler"
-
-    def __init__(
-        self, example_id: Optional[Union[UUID, str]] = None, **kwargs: Any
-    ) -> None:
-        """Initialize the RunCollectorCallbackHandler.
-
-        Parameters
-        ----------
-        example_id : Optional[Union[UUID, str]], default=None
-            The ID of the example being traced. It can be either a UUID or a string.
-        **kwargs : Any
-            Additional keyword arguments
-        """
-        super().__init__(**kwargs)
-        self.example_id = (
-            UUID(example_id) if isinstance(example_id, str) else example_id
-        )
-        self.traced_runs: list[Run] = []
-
-    def _persist_run(self, run: Run) -> None:
-        """Persist a run by adding it to the traced_runs list.
-
-        Parameters
-        ----------
-        run : Run
-            The run to be persisted.
-        """
-        run_ = run.copy()
-        run_.reference_example_id = self.example_id
-        self.traced_runs.append(run_)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/schemas.py ./custom_langchain_core/tracers/schemas.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/schemas.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/schemas.py	1970-01-01 09:00:00
@@ -1,139 +0,0 @@
-"""Schemas for tracers."""
-
-from __future__ import annotations
-
-import datetime
-import warnings
-from typing import Any, Optional
-from uuid import UUID
-
-from langsmith import RunTree
-from langsmith.schemas import RunTypeEnum as RunTypeEnumDep
-from pydantic import PydanticDeprecationWarning
-from pydantic.v1 import BaseModel as BaseModelV1
-from pydantic.v1 import Field as FieldV1
-
-from langchain_core._api import deprecated
-
-
-@deprecated("0.1.0", alternative="Use string instead.", removal="1.0")
-def RunTypeEnum() -> type[RunTypeEnumDep]:  # noqa: N802
-    """RunTypeEnum."""
-    warnings.warn(
-        "RunTypeEnum is deprecated. Please directly use a string instead"
-        " (e.g. 'llm', 'chain', 'tool').",
-        DeprecationWarning,
-        stacklevel=2,
-    )
-    return RunTypeEnumDep
-
-
-@deprecated("0.1.0", removal="1.0")
-class TracerSessionV1Base(BaseModelV1):
-    """Base class for TracerSessionV1."""
-
-    start_time: datetime.datetime = FieldV1(default_factory=datetime.datetime.utcnow)
-    name: Optional[str] = None
-    extra: Optional[dict[str, Any]] = None
-
-
-@deprecated("0.1.0", removal="1.0")
-class TracerSessionV1Create(TracerSessionV1Base):
-    """Create class for TracerSessionV1."""
-
-
-@deprecated("0.1.0", removal="1.0")
-class TracerSessionV1(TracerSessionV1Base):
-    """TracerSessionV1 schema."""
-
-    id: int
-
-
-@deprecated("0.1.0", removal="1.0")
-class TracerSessionBase(TracerSessionV1Base):
-    """Base class for TracerSession."""
-
-    tenant_id: UUID
-
-
-@deprecated("0.1.0", removal="1.0")
-class TracerSession(TracerSessionBase):
-    """TracerSessionV1 schema for the V2 API."""
-
-    id: UUID
-
-
-@deprecated("0.1.0", alternative="Run", removal="1.0")
-class BaseRun(BaseModelV1):
-    """Base class for Run."""
-
-    uuid: str
-    parent_uuid: Optional[str] = None
-    start_time: datetime.datetime = FieldV1(default_factory=datetime.datetime.utcnow)
-    end_time: datetime.datetime = FieldV1(default_factory=datetime.datetime.utcnow)
-    extra: Optional[dict[str, Any]] = None
-    execution_order: int
-    child_execution_order: int
-    serialized: dict[str, Any]
-    session_id: int
-    error: Optional[str] = None
-
-
-@deprecated("0.1.0", alternative="Run", removal="1.0")
-class LLMRun(BaseRun):
-    """Class for LLMRun."""
-
-    prompts: list[str]
-    # Temporarily, remove but we will completely remove LLMRun
-    # response: Optional[LLMResult] = None
-
-
-@deprecated("0.1.0", alternative="Run", removal="1.0")
-class ChainRun(BaseRun):
-    """Class for ChainRun."""
-
-    inputs: dict[str, Any]
-    outputs: Optional[dict[str, Any]] = None
-    child_llm_runs: list[LLMRun] = FieldV1(default_factory=list)
-    child_chain_runs: list[ChainRun] = FieldV1(default_factory=list)
-    child_tool_runs: list[ToolRun] = FieldV1(default_factory=list)
-
-
-@deprecated("0.1.0", alternative="Run", removal="1.0")
-class ToolRun(BaseRun):
-    """Class for ToolRun."""
-
-    tool_input: str
-    output: Optional[str] = None
-    action: str
-    child_llm_runs: list[LLMRun] = FieldV1(default_factory=list)
-    child_chain_runs: list[ChainRun] = FieldV1(default_factory=list)
-    child_tool_runs: list[ToolRun] = FieldV1(default_factory=list)
-
-
-# Begin V2 API Schemas
-
-
-Run = RunTree  # For backwards compatibility
-
-# TODO: Update once langsmith moves to Pydantic V2 and we can swap Run.model_rebuild
-# for Run.update_forward_refs
-with warnings.catch_warnings():
-    warnings.simplefilter("ignore", category=PydanticDeprecationWarning)
-
-    ChainRun.update_forward_refs()
-    ToolRun.update_forward_refs()
-
-__all__ = [
-    "BaseRun",
-    "ChainRun",
-    "LLMRun",
-    "Run",
-    "RunTypeEnum",
-    "ToolRun",
-    "TracerSession",
-    "TracerSessionBase",
-    "TracerSessionV1",
-    "TracerSessionV1Base",
-    "TracerSessionV1Create",
-]
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/tracers/stdout.py ./custom_langchain_core/tracers/stdout.py
--- .venv/lib/python3.12/site-packages/langchain_core/tracers/stdout.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/tracers/stdout.py	1970-01-01 09:00:00
@@ -1,195 +0,0 @@
-import json
-from typing import Any, Callable
-
-from langchain_core.tracers.base import BaseTracer
-from langchain_core.tracers.schemas import Run
-from langchain_core.utils.input import get_bolded_text, get_colored_text
-
-
-def try_json_stringify(obj: Any, fallback: str) -> str:
-    """Try to stringify an object to JSON.
-
-    Args:
-        obj: Object to stringify.
-        fallback: Fallback string to return if the object cannot be stringified.
-
-    Returns:
-        A JSON string if the object can be stringified, otherwise the fallback string.
-    """
-    try:
-        return json.dumps(obj, indent=2, ensure_ascii=False)
-    except Exception:
-        return fallback
-
-
-def elapsed(run: Any) -> str:
-    """Get the elapsed time of a run.
-
-    Args:
-        run: any object with a start_time and end_time attribute.
-
-    Returns:
-        A string with the elapsed time in seconds or
-            milliseconds if time is less than a second.
-
-    """
-    elapsed_time = run.end_time - run.start_time
-    milliseconds = elapsed_time.total_seconds() * 1000
-    if milliseconds < 1000:
-        return f"{milliseconds:.0f}ms"
-    return f"{(milliseconds / 1000):.2f}s"
-
-
-class FunctionCallbackHandler(BaseTracer):
-    """Tracer that calls a function with a single str parameter."""
-
-    name: str = "function_callback_handler"
-    """The name of the tracer. This is used to identify the tracer in the logs.
-    Default is "function_callback_handler"."""
-
-    def __init__(self, function: Callable[[str], None], **kwargs: Any) -> None:
-        super().__init__(**kwargs)
-        self.function_callback = function
-
-    def _persist_run(self, run: Run) -> None:
-        pass
-
-    def get_parents(self, run: Run) -> list[Run]:
-        """Get the parents of a run.
-
-        Args:
-            run: The run to get the parents of.
-
-        Returns:
-            A list of parent runs.
-        """
-        parents = []
-        current_run = run
-        while current_run.parent_run_id:
-            parent = self.run_map.get(str(current_run.parent_run_id))
-            if parent:
-                parents.append(parent)
-                current_run = parent
-            else:
-                break
-        return parents
-
-    def get_breadcrumbs(self, run: Run) -> str:
-        """Get the breadcrumbs of a run.
-
-        Args:
-            run: The run to get the breadcrumbs of.
-
-        Returns:
-            A string with the breadcrumbs of the run.
-        """
-        parents = self.get_parents(run)[::-1]
-        string = " > ".join(
-            f"{parent.run_type}:{parent.name}"
-            if i != len(parents) - 1
-            else f"{parent.run_type}:{parent.name}"
-            for i, parent in enumerate(parents + [run])
-        )
-        return string
-
-    # logging methods
-    def _on_chain_start(self, run: Run) -> None:
-        crumbs = self.get_breadcrumbs(run)
-        run_type = run.run_type.capitalize()
-        self.function_callback(
-            f"{get_colored_text('[chain/start]', color='green')} "
-            + get_bolded_text(f"[{crumbs}] Entering {run_type} run with input:\n")
-            + f"{try_json_stringify(run.inputs, '[inputs]')}"
-        )
-
-    def _on_chain_end(self, run: Run) -> None:
-        crumbs = self.get_breadcrumbs(run)
-        run_type = run.run_type.capitalize()
-        self.function_callback(
-            f"{get_colored_text('[chain/end]', color='blue')} "
-            + get_bolded_text(
-                f"[{crumbs}] [{elapsed(run)}] Exiting {run_type} run with output:\n"
-            )
-            + f"{try_json_stringify(run.outputs, '[outputs]')}"
-        )
-
-    def _on_chain_error(self, run: Run) -> None:
-        crumbs = self.get_breadcrumbs(run)
-        run_type = run.run_type.capitalize()
-        self.function_callback(
-            f"{get_colored_text('[chain/error]', color='red')} "
-            + get_bolded_text(
-                f"[{crumbs}] [{elapsed(run)}] {run_type} run errored with error:\n"
-            )
-            + f"{try_json_stringify(run.error, '[error]')}"
-        )
-
-    def _on_llm_start(self, run: Run) -> None:
-        crumbs = self.get_breadcrumbs(run)
-        inputs = (
-            {"prompts": [p.strip() for p in run.inputs["prompts"]]}
-            if "prompts" in run.inputs
-            else run.inputs
-        )
-        self.function_callback(
-            f"{get_colored_text('[llm/start]', color='green')} "
-            + get_bolded_text(f"[{crumbs}] Entering LLM run with input:\n")
-            + f"{try_json_stringify(inputs, '[inputs]')}"
-        )
-
-    def _on_llm_end(self, run: Run) -> None:
-        crumbs = self.get_breadcrumbs(run)
-        self.function_callback(
-            f"{get_colored_text('[llm/end]', color='blue')} "
-            + get_bolded_text(
-                f"[{crumbs}] [{elapsed(run)}] Exiting LLM run with output:\n"
-            )
-            + f"{try_json_stringify(run.outputs, '[response]')}"
-        )
-
-    def _on_llm_error(self, run: Run) -> None:
-        crumbs = self.get_breadcrumbs(run)
-        self.function_callback(
-            f"{get_colored_text('[llm/error]', color='red')} "
-            + get_bolded_text(
-                f"[{crumbs}] [{elapsed(run)}] LLM run errored with error:\n"
-            )
-            + f"{try_json_stringify(run.error, '[error]')}"
-        )
-
-    def _on_tool_start(self, run: Run) -> None:
-        crumbs = self.get_breadcrumbs(run)
-        self.function_callback(
-            f"{get_colored_text('[tool/start]', color='green')} "
-            + get_bolded_text(f"[{crumbs}] Entering Tool run with input:\n")
-            + f'"{run.inputs["input"].strip()}"'
-        )
-
-    def _on_tool_end(self, run: Run) -> None:
-        crumbs = self.get_breadcrumbs(run)
-        if run.outputs:
-            self.function_callback(
-                f"{get_colored_text('[tool/end]', color='blue')} "
-                + get_bolded_text(
-                    f"[{crumbs}] [{elapsed(run)}] Exiting Tool run with output:\n"
-                )
-                + f'"{str(run.outputs["output"]).strip()}"'
-            )
-
-    def _on_tool_error(self, run: Run) -> None:
-        crumbs = self.get_breadcrumbs(run)
-        self.function_callback(
-            f"{get_colored_text('[tool/error]', color='red')} "
-            + get_bolded_text(f"[{crumbs}] [{elapsed(run)}] ")
-            + f"Tool run errored with error:\n"
-            f"{run.error}"
-        )
-
-
-class ConsoleCallbackHandler(FunctionCallbackHandler):
-    """Tracer that prints to the console."""
-
-    name: str = "console_callback_handler"
-
-    def __init__(self, **kwargs: Any) -> None:
-        super().__init__(function=print, **kwargs)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/__init__.py ./custom_langchain_core/utils/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/__init__.py	1970-01-01 09:00:00
@@ -1,60 +0,0 @@
-"""**Utility functions** for LangChain.
-
-These functions do not depend on any other LangChain module.
-"""
-
-from langchain_core.utils import image
-from langchain_core.utils.aiter import abatch_iterate
-from langchain_core.utils.env import get_from_dict_or_env, get_from_env
-from langchain_core.utils.formatting import StrictFormatter, formatter
-from langchain_core.utils.input import (
-    get_bolded_text,
-    get_color_mapping,
-    get_colored_text,
-    print_text,
-)
-from langchain_core.utils.iter import batch_iterate
-from langchain_core.utils.loading import try_load_from_hub
-from langchain_core.utils.pydantic import pre_init
-from langchain_core.utils.strings import comma_list, stringify_dict, stringify_value
-from langchain_core.utils.utils import (
-    build_extra_kwargs,
-    check_package_version,
-    convert_to_secret_str,
-    from_env,
-    get_pydantic_field_names,
-    guard_import,
-    mock_now,
-    raise_for_status_with_text,
-    secret_from_env,
-    xor_args,
-)
-
-__all__ = [
-    "build_extra_kwargs",
-    "StrictFormatter",
-    "check_package_version",
-    "convert_to_secret_str",
-    "formatter",
-    "get_bolded_text",
-    "get_color_mapping",
-    "get_colored_text",
-    "get_pydantic_field_names",
-    "guard_import",
-    "mock_now",
-    "print_text",
-    "raise_for_status_with_text",
-    "xor_args",
-    "try_load_from_hub",
-    "image",
-    "get_from_env",
-    "get_from_dict_or_env",
-    "stringify_dict",
-    "comma_list",
-    "stringify_value",
-    "pre_init",
-    "batch_iterate",
-    "abatch_iterate",
-    "from_env",
-    "secret_from_env",
-]
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/_merge.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/_merge.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/aiter.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/aiter.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/env.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/env.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/formatting.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/formatting.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/function_calling.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/function_calling.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/html.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/html.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/image.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/image.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/input.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/input.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/interactive_env.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/interactive_env.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/iter.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/iter.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/json.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/json.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/json_schema.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/json_schema.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/loading.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/loading.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/mustache.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/mustache.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/pydantic.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/pydantic.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/strings.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/strings.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/usage.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/usage.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/utils/__pycache__/utils.cpython-312.pyc and ./custom_langchain_core/utils/__pycache__/utils.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/_merge.py ./custom_langchain_core/utils/_merge.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/_merge.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/_merge.py	1970-01-01 09:00:00
@@ -1,148 +0,0 @@
-from __future__ import annotations
-
-from typing import Any, Optional
-
-
-def merge_dicts(left: dict[str, Any], *others: dict[str, Any]) -> dict[str, Any]:
-    """Merge many dicts, handling specific scenarios where a key exists in both
-    dictionaries but has a value of None in 'left'. In such cases, the method uses the
-    value from 'right' for that key in the merged dictionary.
-
-    Args:
-        left: The first dictionary to merge.
-        others: The other dictionaries to merge.
-
-    Returns:
-        The merged dictionary.
-
-    Raises:
-        TypeError: If the key exists in both dictionaries but has a different type.
-        TypeError: If the value has an unsupported type.
-
-    Example:
-        If left = {"function_call": {"arguments": None}} and
-        right = {"function_call": {"arguments": "{\n"}}
-        then, after merging, for the key "function_call",
-        the value from 'right' is used,
-        resulting in merged = {"function_call": {"arguments": "{\n"}}.
-    """
-    merged = left.copy()
-    for right in others:
-        for right_k, right_v in right.items():
-            if right_k not in merged or right_v is not None and merged[right_k] is None:
-                merged[right_k] = right_v
-            elif right_v is None:
-                continue
-            elif type(merged[right_k]) is not type(right_v):
-                msg = (
-                    f'additional_kwargs["{right_k}"] already exists in this message,'
-                    " but with a different type."
-                )
-                raise TypeError(msg)
-            elif isinstance(merged[right_k], str):
-                # TODO: Add below special handling for 'type' key in 0.3 and remove
-                # merge_lists 'type' logic.
-                #
-                # if right_k == "type":
-                #     if merged[right_k] == right_v:
-                #         continue
-                #     else:
-                #         raise ValueError(
-                #             "Unable to merge. Two different values seen for special "
-                #             f"key 'type': {merged[right_k]} and {right_v}. 'type' "
-                #             "should either occur once or have the same value across "
-                #             "all dicts."
-                #         )
-                merged[right_k] += right_v
-            elif isinstance(merged[right_k], dict):
-                merged[right_k] = merge_dicts(merged[right_k], right_v)
-            elif isinstance(merged[right_k], list):
-                merged[right_k] = merge_lists(merged[right_k], right_v)
-            elif merged[right_k] == right_v:
-                continue
-            else:
-                msg = (
-                    f"Additional kwargs key {right_k} already exists in left dict and "
-                    f"value has unsupported type {type(merged[right_k])}."
-                )
-                raise TypeError(msg)
-    return merged
-
-
-def merge_lists(left: Optional[list], *others: Optional[list]) -> Optional[list]:
-    """Add many lists, handling None.
-
-    Args:
-        left: The first list to merge.
-        others: The other lists to merge.
-
-    Returns:
-        The merged list.
-    """
-    merged = left.copy() if left is not None else None
-    for other in others:
-        if other is None:
-            continue
-        elif merged is None:
-            merged = other.copy()
-        else:
-            for e in other:
-                if isinstance(e, dict) and "index" in e and isinstance(e["index"], int):
-                    to_merge = [
-                        i
-                        for i, e_left in enumerate(merged)
-                        if e_left["index"] == e["index"]
-                    ]
-                    if to_merge:
-                        # TODO: Remove this once merge_dict is updated with special
-                        # handling for 'type'.
-                        if "type" in e:
-                            e = {k: v for k, v in e.items() if k != "type"}
-                        merged[to_merge[0]] = merge_dicts(merged[to_merge[0]], e)
-                    else:
-                        merged.append(e)
-                else:
-                    merged.append(e)
-    return merged
-
-
-def merge_obj(left: Any, right: Any) -> Any:
-    """Merge two objects.
-
-    It handles specific scenarios where a key exists in both
-    dictionaries but has a value of None in 'left'. In such cases, the method uses the
-    value from 'right' for that key in the merged dictionary.
-
-    Args:
-        left: The first object to merge.
-        right: The other object to merge.
-
-    Returns:
-        The merged object.
-
-    Raises:
-        TypeError: If the key exists in both dictionaries but has a different type.
-        ValueError: If the two objects cannot be merged.
-    """
-    if left is None or right is None:
-        return left if left is not None else right
-    elif type(left) is not type(right):
-        msg = (
-            f"left and right are of different types. Left type:  {type(left)}. Right "
-            f"type: {type(right)}."
-        )
-        raise TypeError(msg)
-    elif isinstance(left, str):
-        return left + right
-    elif isinstance(left, dict):
-        return merge_dicts(left, right)
-    elif isinstance(left, list):
-        return merge_lists(left, right)
-    elif left == right:
-        return left
-    else:
-        msg = (
-            f"Unable to merge {left=} and {right=}. Both must be of type str, dict, or "
-            f"list, or else be two equal objects."
-        )
-        raise ValueError(msg)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/aiter.py ./custom_langchain_core/utils/aiter.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/aiter.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/aiter.py	1970-01-01 09:00:00
@@ -1,295 +0,0 @@
-"""Adapted from
-https://github.com/maxfischer2781/asyncstdlib/blob/master/asyncstdlib/itertools.py
-MIT License.
-"""
-
-from collections import deque
-from collections.abc import (
-    AsyncGenerator,
-    AsyncIterable,
-    AsyncIterator,
-    Awaitable,
-    Iterator,
-)
-from contextlib import AbstractAsyncContextManager
-from types import TracebackType
-from typing import (
-    Any,
-    Callable,
-    Generic,
-    Optional,
-    TypeVar,
-    Union,
-    cast,
-    overload,
-)
-
-T = TypeVar("T")
-
-_no_default = object()
-
-
-# https://github.com/python/cpython/blob/main/Lib/test/test_asyncgen.py#L54
-# before 3.10, the builtin anext() was not available
-def py_anext(
-    iterator: AsyncIterator[T], default: Union[T, Any] = _no_default
-) -> Awaitable[Union[T, None, Any]]:
-    """Pure-Python implementation of anext() for testing purposes.
-
-    Closely matches the builtin anext() C implementation.
-    Can be used to compare the built-in implementation of the inner
-    coroutines machinery to C-implementation of __anext__() and send()
-    or throw() on the returned generator.
-
-    Args:
-        iterator: The async iterator to advance.
-        default: The value to return if the iterator is exhausted.
-            If not provided, a StopAsyncIteration exception is raised.
-
-    Returns:
-        The next value from the iterator, or the default value
-            if the iterator is exhausted.
-
-    Raises:
-        TypeError: If the iterator is not an async iterator.
-    """
-    try:
-        __anext__ = cast(
-            Callable[[AsyncIterator[T]], Awaitable[T]], type(iterator).__anext__
-        )
-    except AttributeError as e:
-        msg = f"{iterator!r} is not an async iterator"
-        raise TypeError(msg) from e
-
-    if default is _no_default:
-        return __anext__(iterator)
-
-    async def anext_impl() -> Union[T, Any]:
-        try:
-            # The C code is way more low-level than this, as it implements
-            # all methods of the iterator protocol. In this implementation
-            # we're relying on higher-level coroutine concepts, but that's
-            # exactly what we want -- crosstest pure-Python high-level
-            # implementation and low-level C anext() iterators.
-            return await __anext__(iterator)
-        except StopAsyncIteration:
-            return default
-
-    return anext_impl()
-
-
-class NoLock:
-    """Dummy lock that provides the proper interface but no protection."""
-
-    async def __aenter__(self) -> None:
-        pass
-
-    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> bool:
-        return False
-
-
-async def tee_peer(
-    iterator: AsyncIterator[T],
-    # the buffer specific to this peer
-    buffer: deque[T],
-    # the buffers of all peers, including our own
-    peers: list[deque[T]],
-    lock: AbstractAsyncContextManager[Any],
-) -> AsyncGenerator[T, None]:
-    """An individual iterator of a :py:func:`~.tee`.
-
-    This function is a generator that yields items from the shared iterator
-    ``iterator``. It buffers items until the least advanced iterator has
-    yielded them as well. The buffer is shared with all other peers.
-
-    Args:
-        iterator: The shared iterator.
-        buffer: The buffer for this peer.
-        peers: The buffers of all peers.
-        lock: The lock to synchronise access to the shared buffers.
-
-    Yields:
-        The next item from the shared iterator.
-    """
-    try:
-        while True:
-            if not buffer:
-                async with lock:
-                    # Another peer produced an item while we were waiting for the lock.
-                    # Proceed with the next loop iteration to yield the item.
-                    if buffer:
-                        continue
-                    try:
-                        item = await iterator.__anext__()
-                    except StopAsyncIteration:
-                        break
-                    else:
-                        # Append to all buffers, including our own. We'll fetch our
-                        # item from the buffer again, instead of yielding it directly.
-                        # This ensures the proper item ordering if any of our peers
-                        # are fetching items concurrently. They may have buffered their
-                        # item already.
-                        for peer_buffer in peers:
-                            peer_buffer.append(item)
-            yield buffer.popleft()
-    finally:
-        async with lock:
-            # this peer is done – remove its buffer
-            for idx, peer_buffer in enumerate(peers):  # pragma: no branch
-                if peer_buffer is buffer:
-                    peers.pop(idx)
-                    break
-            # if we are the last peer, try and close the iterator
-            if not peers and hasattr(iterator, "aclose"):
-                await iterator.aclose()
-
-
-class Tee(Generic[T]):
-    """Create ``n`` separate asynchronous iterators over ``iterable``.
-
-    This splits a single ``iterable`` into multiple iterators, each providing
-    the same items in the same order.
-    All child iterators may advance separately but share the same items
-    from ``iterable`` -- when the most advanced iterator retrieves an item,
-    it is buffered until the least advanced iterator has yielded it as well.
-    A ``tee`` works lazily and can handle an infinite ``iterable``, provided
-    that all iterators advance.
-
-    .. code-block:: python3
-
-        async def derivative(sensor_data):
-            previous, current = a.tee(sensor_data, n=2)
-            await a.anext(previous)  # advance one iterator
-            return a.map(operator.sub, previous, current)
-
-    Unlike :py:func:`itertools.tee`, :py:func:`~.tee` returns a custom type instead
-    of a :py:class:`tuple`. Like a tuple, it can be indexed, iterated and unpacked
-    to get the child iterators. In addition, its :py:meth:`~.tee.aclose` method
-    immediately closes all children, and it can be used in an ``async with`` context
-    for the same effect.
-
-    If ``iterable`` is an iterator and read elsewhere, ``tee`` will *not*
-    provide these items. Also, ``tee`` must internally buffer each item until the
-    last iterator has yielded it; if the most and least advanced iterator differ
-    by most data, using a :py:class:`list` is more efficient (but not lazy).
-
-    If the underlying iterable is concurrency safe (``anext`` may be awaited
-    concurrently) the resulting iterators are concurrency safe as well. Otherwise,
-    the iterators are safe if there is only ever one single "most advanced" iterator.
-    To enforce sequential use of ``anext``, provide a ``lock``
-    - e.g. an :py:class:`asyncio.Lock` instance in an :py:mod:`asyncio` application -
-    and access is automatically synchronised.
-    """
-
-    def __init__(
-        self,
-        iterable: AsyncIterator[T],
-        n: int = 2,
-        *,
-        lock: Optional[AbstractAsyncContextManager[Any]] = None,
-    ):
-        self._iterator = iterable.__aiter__()  # before 3.10 aiter() doesn't exist
-        self._buffers: list[deque[T]] = [deque() for _ in range(n)]
-        self._children = tuple(
-            tee_peer(
-                iterator=self._iterator,
-                buffer=buffer,
-                peers=self._buffers,
-                lock=lock if lock is not None else NoLock(),
-            )
-            for buffer in self._buffers
-        )
-
-    def __len__(self) -> int:
-        return len(self._children)
-
-    @overload
-    def __getitem__(self, item: int) -> AsyncIterator[T]: ...
-
-    @overload
-    def __getitem__(self, item: slice) -> tuple[AsyncIterator[T], ...]: ...
-
-    def __getitem__(
-        self, item: Union[int, slice]
-    ) -> Union[AsyncIterator[T], tuple[AsyncIterator[T], ...]]:
-        return self._children[item]
-
-    def __iter__(self) -> Iterator[AsyncIterator[T]]:
-        yield from self._children
-
-    async def __aenter__(self) -> "Tee[T]":
-        return self
-
-    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> bool:
-        await self.aclose()
-        return False
-
-    async def aclose(self) -> None:
-        """Async close all child iterators."""
-        for child in self._children:
-            await child.aclose()
-
-
-atee = Tee
-
-
-class aclosing(AbstractAsyncContextManager):  # noqa: N801
-    """Async context manager for safely finalizing an asynchronously cleaned-up
-    resource such as an async generator, calling its ``aclose()`` method.
-
-    Code like this:
-
-        async with aclosing(<module>.fetch(<arguments>)) as agen:
-            <block>
-
-    is equivalent to this:
-
-        agen = <module>.fetch(<arguments>)
-        try:
-            <block>
-        finally:
-            await agen.aclose()
-
-    """
-
-    def __init__(
-        self, thing: Union[AsyncGenerator[Any, Any], AsyncIterator[Any]]
-    ) -> None:
-        self.thing = thing
-
-    async def __aenter__(self) -> Union[AsyncGenerator[Any, Any], AsyncIterator[Any]]:
-        return self.thing
-
-    async def __aexit__(
-        self,
-        exc_type: Optional[type[BaseException]],
-        exc_value: Optional[BaseException],
-        traceback: Optional[TracebackType],
-    ) -> None:
-        if hasattr(self.thing, "aclose"):
-            await self.thing.aclose()
-
-
-async def abatch_iterate(
-    size: int, iterable: AsyncIterable[T]
-) -> AsyncIterator[list[T]]:
-    """Utility batching function for async iterables.
-
-    Args:
-        size: The size of the batch.
-        iterable: The async iterable to batch.
-
-    Returns:
-        An async iterator over the batches.
-    """
-    batch: list[T] = []
-    async for element in iterable:
-        if len(batch) < size:
-            batch.append(element)
-
-        if len(batch) >= size:
-            yield batch
-            batch = []
-
-    if batch:
-        yield batch
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/env.py ./custom_langchain_core/utils/env.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/env.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/env.py	1970-01-01 09:00:00
@@ -1,81 +0,0 @@
-from __future__ import annotations
-
-import os
-from typing import Any, Optional, Union
-
-
-def env_var_is_set(env_var: str) -> bool:
-    """Check if an environment variable is set.
-
-    Args:
-        env_var (str): The name of the environment variable.
-
-    Returns:
-        bool: True if the environment variable is set, False otherwise.
-    """
-    return env_var in os.environ and os.environ[env_var] not in (
-        "",
-        "0",
-        "false",
-        "False",
-    )
-
-
-def get_from_dict_or_env(
-    data: dict[str, Any],
-    key: Union[str, list[str]],
-    env_key: str,
-    default: Optional[str] = None,
-) -> str:
-    """Get a value from a dictionary or an environment variable.
-
-    Args:
-        data: The dictionary to look up the key in.
-        key: The key to look up in the dictionary. This can be a list of keys to try
-            in order.
-        env_key: The environment variable to look up if the key is not
-            in the dictionary.
-        default: The default value to return if the key is not in the dictionary
-            or the environment. Defaults to None.
-    """
-    if isinstance(key, (list, tuple)):
-        for k in key:
-            if k in data and data[k]:
-                return data[k]
-
-    if isinstance(key, str) and key in data and data[key]:
-        return data[key]
-
-    key_for_err = key[0] if isinstance(key, (list, tuple)) else key
-
-    return get_from_env(key_for_err, env_key, default=default)
-
-
-def get_from_env(key: str, env_key: str, default: Optional[str] = None) -> str:
-    """Get a value from a dictionary or an environment variable.
-
-    Args:
-        key: The key to look up in the dictionary.
-        env_key: The environment variable to look up if the key is not
-            in the dictionary.
-        default: The default value to return if the key is not in the dictionary
-            or the environment. Defaults to None.
-
-    Returns:
-        str: The value of the key.
-
-    Raises:
-        ValueError: If the key is not in the dictionary and no default value is
-            provided or if the environment variable is not set.
-    """
-    if env_key in os.environ and os.environ[env_key]:
-        return os.environ[env_key]
-    elif default is not None:
-        return default
-    else:
-        msg = (
-            f"Did not find {key}, please add an environment variable"
-            f" `{env_key}` which contains it, or pass"
-            f" `{key}` as a named parameter."
-        )
-        raise ValueError(msg)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/formatting.py ./custom_langchain_core/utils/formatting.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/formatting.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/formatting.py	1970-01-01 09:00:00
@@ -1,51 +0,0 @@
-"""Utilities for formatting strings."""
-
-from collections.abc import Mapping, Sequence
-from string import Formatter
-from typing import Any
-
-
-class StrictFormatter(Formatter):
-    """Formatter that checks for extra keys."""
-
-    def vformat(
-        self, format_string: str, args: Sequence, kwargs: Mapping[str, Any]
-    ) -> str:
-        """Check that no arguments are provided.
-
-        Args:
-            format_string: The format string.
-            args: The arguments.
-            kwargs: The keyword arguments.
-
-        Returns:
-            The formatted string.
-
-        Raises:
-            ValueError: If any arguments are provided.
-        """
-        if len(args) > 0:
-            msg = (
-                "No arguments should be provided, "
-                "everything should be passed as keyword arguments."
-            )
-            raise ValueError(msg)
-        return super().vformat(format_string, args, kwargs)
-
-    def validate_input_variables(
-        self, format_string: str, input_variables: list[str]
-    ) -> None:
-        """Check that all input variables are used in the format string.
-
-        Args:
-            format_string: The format string.
-            input_variables: The input variables.
-
-        Raises:
-            ValueError: If any input variables are not used in the format string.
-        """
-        dummy_inputs = dict.fromkeys(input_variables, "foo")
-        super().format(format_string, **dummy_inputs)
-
-
-formatter = StrictFormatter()
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/function_calling.py ./custom_langchain_core/utils/function_calling.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/function_calling.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/function_calling.py	1970-01-01 09:00:00
@@ -1,741 +0,0 @@
-"""Methods for creating function specs in the style of OpenAI Functions."""
-
-from __future__ import annotations
-
-import collections
-import inspect
-import logging
-import types
-import typing
-import uuid
-from typing import (
-    TYPE_CHECKING,
-    Annotated,
-    Any,
-    Callable,
-    Literal,
-    Optional,
-    Union,
-    cast,
-)
-
-from pydantic import BaseModel
-from pydantic.v1 import BaseModel as BaseModelV1
-from typing_extensions import TypedDict, get_args, get_origin, is_typeddict
-
-from langchain_core._api import beta, deprecated
-from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage
-from langchain_core.utils.json_schema import dereference_refs
-from langchain_core.utils.pydantic import is_basemodel_subclass
-
-if TYPE_CHECKING:
-    from langchain_core.tools import BaseTool
-
-logger = logging.getLogger(__name__)
-
-PYTHON_TO_JSON_TYPES = {
-    "str": "string",
-    "int": "integer",
-    "float": "number",
-    "bool": "boolean",
-}
-
-
-class FunctionDescription(TypedDict):
-    """Representation of a callable function to send to an LLM."""
-
-    name: str
-    """The name of the function."""
-    description: str
-    """A description of the function."""
-    parameters: dict
-    """The parameters of the function."""
-
-
-class ToolDescription(TypedDict):
-    """Representation of a callable function to the OpenAI API."""
-
-    type: Literal["function"]
-    """The type of the tool."""
-    function: FunctionDescription
-    """The function description."""
-
-
-def _rm_titles(kv: dict, prev_key: str = "") -> dict:
-    new_kv = {}
-    for k, v in kv.items():
-        if k == "title":
-            if isinstance(v, dict) and prev_key == "properties" and "title" in v:
-                new_kv[k] = _rm_titles(v, k)
-            else:
-                continue
-        elif isinstance(v, dict):
-            new_kv[k] = _rm_titles(v, k)
-        else:
-            new_kv[k] = v
-    return new_kv
-
-
-def _convert_json_schema_to_openai_function(
-    schema: dict,
-    *,
-    name: Optional[str] = None,
-    description: Optional[str] = None,
-    rm_titles: bool = True,
-) -> FunctionDescription:
-    """Converts a Pydantic model to a function description for the OpenAI API.
-
-    Args:
-        schema: The JSON schema to convert.
-        name: The name of the function. If not provided, the title of the schema will be
-            used.
-        description: The description of the function. If not provided, the description
-            of the schema will be used.
-        rm_titles: Whether to remove titles from the schema. Defaults to True.
-
-    Returns:
-        The function description.
-    """
-    schema = dereference_refs(schema)
-    if "definitions" in schema:  # pydantic 1
-        schema.pop("definitions", None)
-    if "$defs" in schema:  # pydantic 2
-        schema.pop("$defs", None)
-    title = schema.pop("title", "")
-    default_description = schema.pop("description", "")
-    return {
-        "name": name or title,
-        "description": description or default_description,
-        "parameters": _rm_titles(schema) if rm_titles else schema,
-    }
-
-
-def _convert_pydantic_to_openai_function(
-    model: type,
-    *,
-    name: Optional[str] = None,
-    description: Optional[str] = None,
-    rm_titles: bool = True,
-) -> FunctionDescription:
-    """Converts a Pydantic model to a function description for the OpenAI API.
-
-    Args:
-        model: The Pydantic model to convert.
-        name: The name of the function. If not provided, the title of the schema will be
-            used.
-        description: The description of the function. If not provided, the description
-            of the schema will be used.
-        rm_titles: Whether to remove titles from the schema. Defaults to True.
-
-    Returns:
-        The function description.
-    """
-    if hasattr(model, "model_json_schema"):
-        schema = model.model_json_schema()  # Pydantic 2
-    elif hasattr(model, "schema"):
-        schema = model.schema()  # Pydantic 1
-    else:
-        msg = "Model must be a Pydantic model."
-        raise TypeError(msg)
-    return _convert_json_schema_to_openai_function(
-        schema, name=name, description=description, rm_titles=rm_titles
-    )
-
-
-convert_pydantic_to_openai_function = deprecated(
-    "0.1.16",
-    alternative="langchain_core.utils.function_calling.convert_to_openai_function()",
-    removal="1.0",
-)(_convert_pydantic_to_openai_function)
-
-
-@deprecated(
-    "0.1.16",
-    alternative="langchain_core.utils.function_calling.convert_to_openai_tool()",
-    removal="1.0",
-)
-def convert_pydantic_to_openai_tool(
-    model: type[BaseModel],
-    *,
-    name: Optional[str] = None,
-    description: Optional[str] = None,
-) -> ToolDescription:
-    """Converts a Pydantic model to a function description for the OpenAI API.
-
-    Args:
-        model: The Pydantic model to convert.
-        name: The name of the function. If not provided, the title of the schema will be
-            used.
-        description: The description of the function. If not provided, the description
-            of the schema will be used.
-
-    Returns:
-        The tool description.
-    """
-    function = _convert_pydantic_to_openai_function(
-        model, name=name, description=description
-    )
-    return {"type": "function", "function": function}
-
-
-def _get_python_function_name(function: Callable) -> str:
-    """Get the name of a Python function."""
-    return function.__name__
-
-
-def _convert_python_function_to_openai_function(
-    function: Callable,
-) -> FunctionDescription:
-    """Convert a Python function to an OpenAI function-calling API compatible dict.
-
-    Assumes the Python function has type hints and a docstring with a description. If
-        the docstring has Google Python style argument descriptions, these will be
-        included as well.
-
-    Args:
-        function: The Python function to convert.
-
-    Returns:
-        The OpenAI function description.
-    """
-    from langchain_core.tools.base import create_schema_from_function
-
-    func_name = _get_python_function_name(function)
-    model = create_schema_from_function(
-        func_name,
-        function,
-        filter_args=(),
-        parse_docstring=True,
-        error_on_invalid_docstring=False,
-        include_injected=False,
-    )
-    return _convert_pydantic_to_openai_function(
-        model,
-        name=func_name,
-        description=model.__doc__,
-    )
-
-
-convert_python_function_to_openai_function = deprecated(
-    "0.1.16",
-    alternative="langchain_core.utils.function_calling.convert_to_openai_function()",
-    removal="1.0",
-)(_convert_python_function_to_openai_function)
-
-
-def _convert_typed_dict_to_openai_function(typed_dict: type) -> FunctionDescription:
-    visited: dict = {}
-    from pydantic.v1 import BaseModel
-
-    model = cast(
-        type[BaseModel],
-        _convert_any_typed_dicts_to_pydantic(typed_dict, visited=visited),
-    )
-    return _convert_pydantic_to_openai_function(model)  # type: ignore
-
-
-_MAX_TYPED_DICT_RECURSION = 25
-
-
-def _convert_any_typed_dicts_to_pydantic(
-    type_: type,
-    *,
-    visited: dict,
-    depth: int = 0,
-) -> type:
-    from pydantic.v1 import Field as Field_v1
-    from pydantic.v1 import create_model as create_model_v1
-
-    if type_ in visited:
-        return visited[type_]
-    elif depth >= _MAX_TYPED_DICT_RECURSION:
-        return type_
-    elif is_typeddict(type_):
-        typed_dict = type_
-        docstring = inspect.getdoc(typed_dict)
-        annotations_ = typed_dict.__annotations__
-        description, arg_descriptions = _parse_google_docstring(
-            docstring, list(annotations_)
-        )
-        fields: dict = {}
-        for arg, arg_type in annotations_.items():
-            if get_origin(arg_type) is Annotated:
-                annotated_args = get_args(arg_type)
-                new_arg_type = _convert_any_typed_dicts_to_pydantic(
-                    annotated_args[0], depth=depth + 1, visited=visited
-                )
-                field_kwargs = dict(zip(("default", "description"), annotated_args[1:]))
-                if (field_desc := field_kwargs.get("description")) and not isinstance(
-                    field_desc, str
-                ):
-                    msg = (
-                        f"Invalid annotation for field {arg}. Third argument to "
-                        f"Annotated must be a string description, received value of "
-                        f"type {type(field_desc)}."
-                    )
-                    raise ValueError(msg)
-                elif arg_desc := arg_descriptions.get(arg):
-                    field_kwargs["description"] = arg_desc
-                else:
-                    pass
-                fields[arg] = (new_arg_type, Field_v1(**field_kwargs))
-            else:
-                new_arg_type = _convert_any_typed_dicts_to_pydantic(
-                    arg_type, depth=depth + 1, visited=visited
-                )
-                field_kwargs = {"default": ...}
-                if arg_desc := arg_descriptions.get(arg):
-                    field_kwargs["description"] = arg_desc
-                fields[arg] = (new_arg_type, Field_v1(**field_kwargs))
-        model = create_model_v1(typed_dict.__name__, **fields)
-        model.__doc__ = description
-        visited[typed_dict] = model
-        return model
-    elif (origin := get_origin(type_)) and (type_args := get_args(type_)):
-        subscriptable_origin = _py_38_safe_origin(origin)
-        type_args = tuple(
-            _convert_any_typed_dicts_to_pydantic(arg, depth=depth + 1, visited=visited)
-            for arg in type_args  # type: ignore[index]
-        )
-        return subscriptable_origin[type_args]  # type: ignore[index]
-    else:
-        return type_
-
-
-def _format_tool_to_openai_function(tool: BaseTool) -> FunctionDescription:
-    """Format tool into the OpenAI function API.
-
-    Args:
-        tool: The tool to format.
-
-    Returns:
-        The function description.
-    """
-    from langchain_core.tools import simple
-
-    is_simple_oai_tool = isinstance(tool, simple.Tool) and not tool.args_schema
-    if tool.tool_call_schema and not is_simple_oai_tool:
-        if isinstance(tool.tool_call_schema, dict):
-            return _convert_json_schema_to_openai_function(
-                tool.tool_call_schema, name=tool.name, description=tool.description
-            )
-        elif issubclass(tool.tool_call_schema, (BaseModel, BaseModelV1)):
-            return _convert_pydantic_to_openai_function(
-                tool.tool_call_schema, name=tool.name, description=tool.description
-            )
-        else:
-            error_msg = (
-                f"Unsupported tool call schema: {tool.tool_call_schema}. "
-                "Tool call schema must be a JSON schema dict or a Pydantic model."
-            )
-            raise ValueError(error_msg)
-    else:
-        return {
-            "name": tool.name,
-            "description": tool.description,
-            "parameters": {
-                # This is a hack to get around the fact that some tools
-                # do not expose an args_schema, and expect an argument
-                # which is a string.
-                # And Open AI does not support an array type for the
-                # parameters.
-                "properties": {
-                    "__arg1": {"title": "__arg1", "type": "string"},
-                },
-                "required": ["__arg1"],
-                "type": "object",
-            },
-        }
-
-
-format_tool_to_openai_function = deprecated(
-    "0.1.16",
-    alternative="langchain_core.utils.function_calling.convert_to_openai_function()",
-    removal="1.0",
-)(_format_tool_to_openai_function)
-
-
-@deprecated(
-    "0.1.16",
-    alternative="langchain_core.utils.function_calling.convert_to_openai_tool()",
-    removal="1.0",
-)
-def format_tool_to_openai_tool(tool: BaseTool) -> ToolDescription:
-    """Format tool into the OpenAI function API.
-
-    Args:
-        tool: The tool to format.
-
-    Returns:
-        The tool description.
-    """
-    function = _format_tool_to_openai_function(tool)
-    return {"type": "function", "function": function}
-
-
-def convert_to_openai_function(
-    function: Union[dict[str, Any], type, Callable, BaseTool],
-    *,
-    strict: Optional[bool] = None,
-) -> dict[str, Any]:
-    """Convert a raw function/class to an OpenAI function.
-
-    Args:
-        function:
-            A dictionary, Pydantic BaseModel class, TypedDict class, a LangChain
-            Tool object, or a Python function. If a dictionary is passed in, it is
-            assumed to already be a valid OpenAI function, a JSON schema with
-            top-level 'title' key specified, an Anthropic format
-            tool, or an Amazon Bedrock Converse format tool.
-        strict:
-            If True, model output is guaranteed to exactly match the JSON Schema
-            provided in the function definition. If None, ``strict`` argument will not
-            be included in function definition.
-
-    Returns:
-        A dict version of the passed in function which is compatible with the OpenAI
-        function-calling API.
-
-    Raises:
-        ValueError: If function is not in a supported format.
-
-    .. versionchanged:: 0.2.29
-
-        ``strict`` arg added.
-
-    .. versionchanged:: 0.3.13
-
-        Support for Anthropic format tools added.
-
-    .. versionchanged:: 0.3.14
-
-        Support for Amazon Bedrock Converse format tools added.
-
-    .. versionchanged:: 0.3.16
-
-        'description' and 'parameters' keys are now optional. Only 'name' is
-        required and guaranteed to be part of the output.
-    """
-    from langchain_core.tools import BaseTool
-
-    # an Anthropic format tool
-    if isinstance(function, dict) and all(
-        k in function for k in ("name", "input_schema")
-    ):
-        oai_function = {
-            "name": function["name"],
-            "parameters": function["input_schema"],
-        }
-        if "description" in function:
-            oai_function["description"] = function["description"]
-    # an Amazon Bedrock Converse format tool
-    elif isinstance(function, dict) and "toolSpec" in function:
-        oai_function = {
-            "name": function["toolSpec"]["name"],
-            "parameters": function["toolSpec"]["inputSchema"]["json"],
-        }
-        if "description" in function["toolSpec"]:
-            oai_function["description"] = function["toolSpec"]["description"]
-    # already in OpenAI function format
-    elif isinstance(function, dict) and "name" in function:
-        oai_function = {
-            k: v
-            for k, v in function.items()
-            if k in ("name", "description", "parameters", "strict")
-        }
-    # a JSON schema with title and description
-    elif isinstance(function, dict) and "title" in function:
-        function_copy = function.copy()
-        oai_function = {"name": function_copy.pop("title")}
-        if "description" in function_copy:
-            oai_function["description"] = function_copy.pop("description")
-        if function_copy and "properties" in function_copy:
-            oai_function["parameters"] = function_copy
-    elif isinstance(function, type) and is_basemodel_subclass(function):
-        oai_function = cast(dict, _convert_pydantic_to_openai_function(function))
-    elif is_typeddict(function):
-        oai_function = cast(
-            dict, _convert_typed_dict_to_openai_function(cast(type, function))
-        )
-    elif isinstance(function, BaseTool):
-        oai_function = cast(dict, _format_tool_to_openai_function(function))
-    elif callable(function):
-        oai_function = cast(dict, _convert_python_function_to_openai_function(function))
-    else:
-        msg = (
-            f"Unsupported function\n\n{function}\n\nFunctions must be passed in"
-            " as Dict, pydantic.BaseModel, or Callable. If they're a dict they must"
-            " either be in OpenAI function format or valid JSON schema with top-level"
-            " 'title' and 'description' keys."
-        )
-        raise ValueError(msg)
-
-    if strict is not None:
-        if "strict" in oai_function and oai_function["strict"] != strict:
-            msg = (
-                f"Tool/function already has a 'strict' key wth value "
-                f"{oai_function['strict']} which is different from the explicit "
-                f"`strict` arg received {strict=}."
-            )
-            raise ValueError(msg)
-        oai_function["strict"] = strict
-        if strict:
-            # As of 08/06/24, OpenAI requires that additionalProperties be supplied and
-            # set to False if strict is True.
-            # All properties layer needs 'additionalProperties=False'
-            oai_function["parameters"] = _recursive_set_additional_properties_false(
-                oai_function["parameters"]
-            )
-    return oai_function
-
-
-def convert_to_openai_tool(
-    tool: Union[dict[str, Any], type[BaseModel], Callable, BaseTool],
-    *,
-    strict: Optional[bool] = None,
-) -> dict[str, Any]:
-    """Convert a tool-like object to an OpenAI tool schema.
-
-    OpenAI tool schema reference:
-    https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools
-
-    Args:
-        tool:
-            Either a dictionary, a pydantic.BaseModel class, Python function, or
-            BaseTool. If a dictionary is passed in, it is
-            assumed to already be a valid OpenAI function, a JSON schema with
-            top-level 'title' key specified, an Anthropic format
-            tool, or an Amazon Bedrock Converse format tool.
-        strict:
-            If True, model output is guaranteed to exactly match the JSON Schema
-            provided in the function definition. If None, ``strict`` argument will not
-            be included in tool definition.
-
-    Returns:
-        A dict version of the passed in tool which is compatible with the
-        OpenAI tool-calling API.
-
-    .. versionchanged:: 0.2.29
-
-        ``strict`` arg added.
-
-    .. versionchanged:: 0.3.13
-
-        Support for Anthropic format tools added.
-
-    .. versionchanged:: 0.3.14
-
-        Support for Amazon Bedrock Converse format tools added.
-
-    .. versionchanged:: 0.3.16
-
-        'description' and 'parameters' keys are now optional. Only 'name' is
-        required and guaranteed to be part of the output.
-    """
-    if isinstance(tool, dict) and tool.get("type") == "function" and "function" in tool:
-        return tool
-    oai_function = convert_to_openai_function(tool, strict=strict)
-    return {"type": "function", "function": oai_function}
-
-
-@beta()
-def tool_example_to_messages(
-    input: str,
-    tool_calls: list[BaseModel],
-    tool_outputs: Optional[list[str]] = None,
-    *,
-    ai_response: Optional[str] = None,
-) -> list[BaseMessage]:
-    """Convert an example into a list of messages that can be fed into an LLM.
-
-    This code is an adapter that converts a single example to a list of messages
-    that can be fed into a chat model.
-
-    The list of messages per example by default corresponds to:
-
-    1) HumanMessage: contains the content from which content should be extracted.
-    2) AIMessage: contains the extracted information from the model
-    3) ToolMessage: contains confirmation to the model that the model requested a tool
-        correctly.
-
-    If `ai_response` is specified, there will be a final AIMessage with that response.
-
-    The ToolMessage is required because some chat models are hyper-optimized for agents
-    rather than for an extraction use case.
-
-    Arguments:
-        input: string, the user input
-        tool_calls: List[BaseModel], a list of tool calls represented as Pydantic
-            BaseModels
-        tool_outputs: Optional[List[str]], a list of tool call outputs.
-            Does not need to be provided. If not provided, a placeholder value
-            will be inserted. Defaults to None.
-        ai_response: Optional[str], if provided, content for a final AIMessage.
-
-    Returns:
-        A list of messages
-
-    Examples:
-
-        .. code-block:: python
-
-            from typing import List, Optional
-            from pydantic import BaseModel, Field
-            from langchain_openai import ChatOpenAI
-
-            class Person(BaseModel):
-                '''Information about a person.'''
-                name: Optional[str] = Field(..., description="The name of the person")
-                hair_color: Optional[str] = Field(
-                    ..., description="The color of the person's hair if known"
-                )
-                height_in_meters: Optional[str] = Field(
-                    ..., description="Height in METERs"
-                )
-
-            examples = [
-                (
-                    "The ocean is vast and blue. It's more than 20,000 feet deep.",
-                    Person(name=None, height_in_meters=None, hair_color=None),
-                ),
-                (
-                    "Fiona traveled far from France to Spain.",
-                    Person(name="Fiona", height_in_meters=None, hair_color=None),
-                ),
-            ]
-
-
-            messages = []
-
-            for txt, tool_call in examples:
-                messages.extend(
-                    tool_example_to_messages(txt, [tool_call])
-                )
-    """
-    messages: list[BaseMessage] = [HumanMessage(content=input)]
-    openai_tool_calls = []
-    for tool_call in tool_calls:
-        openai_tool_calls.append(
-            {
-                "id": str(uuid.uuid4()),
-                "type": "function",
-                "function": {
-                    # The name of the function right now corresponds to the name
-                    # of the pydantic model. This is implicit in the API right now,
-                    # and will be improved over time.
-                    "name": tool_call.__class__.__name__,
-                    "arguments": tool_call.model_dump_json(),
-                },
-            }
-        )
-    messages.append(
-        AIMessage(content="", additional_kwargs={"tool_calls": openai_tool_calls})
-    )
-    tool_outputs = tool_outputs or ["You have correctly called this tool."] * len(
-        openai_tool_calls
-    )
-    for output, tool_call_dict in zip(tool_outputs, openai_tool_calls):
-        messages.append(ToolMessage(content=output, tool_call_id=tool_call_dict["id"]))  # type: ignore
-
-    if ai_response:
-        messages.append(AIMessage(content=ai_response))
-    return messages
-
-
-def _parse_google_docstring(
-    docstring: Optional[str],
-    args: list[str],
-    *,
-    error_on_invalid_docstring: bool = False,
-) -> tuple[str, dict]:
-    """Parse the function and argument descriptions from the docstring of a function.
-
-    Assumes the function docstring follows Google Python style guide.
-    """
-    if docstring:
-        docstring_blocks = docstring.split("\n\n")
-        if error_on_invalid_docstring:
-            filtered_annotations = {
-                arg for arg in args if arg not in ("run_manager", "callbacks", "return")
-            }
-            if filtered_annotations and (
-                len(docstring_blocks) < 2
-                or not any(block.startswith("Args:") for block in docstring_blocks[1:])
-            ):
-                msg = "Found invalid Google-Style docstring."
-                raise ValueError(msg)
-        descriptors = []
-        args_block = None
-        past_descriptors = False
-        for block in docstring_blocks:
-            if block.startswith("Args:"):
-                args_block = block
-                break
-            elif block.startswith(("Returns:", "Example:")):
-                # Don't break in case Args come after
-                past_descriptors = True
-            elif not past_descriptors:
-                descriptors.append(block)
-            else:
-                continue
-        description = " ".join(descriptors)
-    else:
-        if error_on_invalid_docstring:
-            msg = "Found invalid Google-Style docstring."
-            raise ValueError(msg)
-        description = ""
-        args_block = None
-    arg_descriptions = {}
-    if args_block:
-        arg = None
-        for line in args_block.split("\n")[1:]:
-            if ":" in line:
-                arg, desc = line.split(":", maxsplit=1)
-                arg = arg.strip()
-                arg_name, _, _annotations = arg.partition(" ")
-                if _annotations.startswith("(") and _annotations.endswith(")"):
-                    arg = arg_name
-                arg_descriptions[arg] = desc.strip()
-            elif arg:
-                arg_descriptions[arg] += " " + line.strip()
-    return description, arg_descriptions
-
-
-def _py_38_safe_origin(origin: type) -> type:
-    origin_union_type_map: dict[type, Any] = (
-        {types.UnionType: Union} if hasattr(types, "UnionType") else {}
-    )
-
-    origin_map: dict[type, Any] = {
-        dict: dict,
-        list: list,
-        tuple: tuple,
-        set: set,
-        collections.abc.Iterable: typing.Iterable,
-        collections.abc.Mapping: typing.Mapping,
-        collections.abc.Sequence: typing.Sequence,
-        collections.abc.MutableMapping: typing.MutableMapping,
-        **origin_union_type_map,
-    }
-    return cast(type, origin_map.get(origin, origin))
-
-
-def _recursive_set_additional_properties_false(
-    schema: dict[str, Any],
-) -> dict[str, Any]:
-    if isinstance(schema, dict):
-        # Check if 'required' is a key at the current level or if the schema is empty,
-        # in which case additionalProperties still needs to be specified.
-        if "required" in schema or (
-            "properties" in schema and not schema["properties"]
-        ):
-            schema["additionalProperties"] = False
-
-        # Recursively check 'properties' and 'items' if they exist
-        if "properties" in schema:
-            for value in schema["properties"].values():
-                _recursive_set_additional_properties_false(value)
-        if "items" in schema:
-            _recursive_set_additional_properties_false(schema["items"])
-
-    return schema
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/html.py ./custom_langchain_core/utils/html.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/html.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/html.py	1970-01-01 09:00:00
@@ -1,119 +0,0 @@
-import logging
-import re
-from collections.abc import Sequence
-from typing import Optional, Union
-from urllib.parse import urljoin, urlparse
-
-logger = logging.getLogger(__name__)
-
-PREFIXES_TO_IGNORE = ("javascript:", "mailto:", "#")
-SUFFIXES_TO_IGNORE = (
-    ".css",
-    ".js",
-    ".ico",
-    ".png",
-    ".jpg",
-    ".jpeg",
-    ".gif",
-    ".svg",
-    ".csv",
-    ".bz2",
-    ".zip",
-    ".epub",
-)
-SUFFIXES_TO_IGNORE_REGEX = (
-    "(?!" + "|".join([re.escape(s) + r"[\#'\"]" for s in SUFFIXES_TO_IGNORE]) + ")"
-)
-PREFIXES_TO_IGNORE_REGEX = (
-    "(?!" + "|".join([re.escape(s) for s in PREFIXES_TO_IGNORE]) + ")"
-)
-DEFAULT_LINK_REGEX = (
-    rf"href=[\"']{PREFIXES_TO_IGNORE_REGEX}((?:{SUFFIXES_TO_IGNORE_REGEX}.)*?)[\#'\"]"
-)
-
-
-def find_all_links(
-    raw_html: str, *, pattern: Union[str, re.Pattern, None] = None
-) -> list[str]:
-    """Extract all links from a raw HTML string.
-
-    Args:
-        raw_html: original HTML.
-        pattern: Regex to use for extracting links from raw HTML.
-
-    Returns:
-        List[str]: all links
-    """
-    pattern = pattern or DEFAULT_LINK_REGEX
-    return list(set(re.findall(pattern, raw_html)))
-
-
-def extract_sub_links(
-    raw_html: str,
-    url: str,
-    *,
-    base_url: Optional[str] = None,
-    pattern: Union[str, re.Pattern, None] = None,
-    prevent_outside: bool = True,
-    exclude_prefixes: Sequence[str] = (),
-    continue_on_failure: bool = False,
-) -> list[str]:
-    """Extract all links from a raw HTML string and convert into absolute paths.
-
-    Args:
-        raw_html: original HTML.
-        url: the url of the HTML.
-        base_url: the base URL to check for outside links against.
-        pattern: Regex to use for extracting links from raw HTML.
-        prevent_outside: If True, ignore external links which are not children
-            of the base URL.
-        exclude_prefixes: Exclude any URLs that start with one of these prefixes.
-        continue_on_failure: If True, continue if parsing a specific link raises an
-            exception. Otherwise, raise the exception.
-
-    Returns:
-        List[str]: sub links.
-    """
-    base_url_to_use = base_url if base_url is not None else url
-    parsed_base_url = urlparse(base_url_to_use)
-    parsed_url = urlparse(url)
-    all_links = find_all_links(raw_html, pattern=pattern)
-    absolute_paths = set()
-    for link in all_links:
-        try:
-            parsed_link = urlparse(link)
-            # Some may be absolute links like https://to/path
-            if parsed_link.scheme == "http" or parsed_link.scheme == "https":
-                absolute_path = link
-            # Some may have omitted the protocol like //to/path
-            elif link.startswith("//"):
-                absolute_path = f"{parsed_url.scheme}:{link}"
-            else:
-                absolute_path = urljoin(url, parsed_link.path)
-                if parsed_link.query:
-                    absolute_path += f"?{parsed_link.query}"
-            absolute_paths.add(absolute_path)
-        except Exception as e:
-            if continue_on_failure:
-                logger.warning(f"Unable to load link {link}. Raised exception:\n\n{e}")
-                continue
-            raise
-
-    results = []
-    for path in absolute_paths:
-        if any(path.startswith(exclude_prefix) for exclude_prefix in exclude_prefixes):
-            continue
-
-        if prevent_outside:
-            parsed_path = urlparse(path)
-
-            if parsed_base_url.netloc != parsed_path.netloc:
-                continue
-
-            # Will take care of verifying rest of path after netloc
-            # if it's more specific
-            if not path.startswith(base_url_to_use):
-                continue
-
-        results.append(path)
-    return results
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/image.py ./custom_langchain_core/utils/image.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/image.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/image.py	1970-01-01 09:00:00
@@ -1,13 +0,0 @@
-from typing import Any
-
-
-def __getattr__(name: str) -> Any:
-    if name in ("encode_image", "image_to_data_url"):
-        msg = (
-            f"'{name}' has been removed for security reasons.\n\n"
-            f"Usage of this utility in environments with user-input paths is a "
-            f"security vulnerability. Out of an abundance of caution, the utility "
-            f"has been removed to prevent possible misuse."
-        )
-        raise ValueError(msg)
-    raise AttributeError(name)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/input.py ./custom_langchain_core/utils/input.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/input.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/input.py	1970-01-01 09:00:00
@@ -1,76 +0,0 @@
-"""Handle chained inputs."""
-
-from typing import Optional, TextIO
-
-_TEXT_COLOR_MAPPING = {
-    "blue": "36;1",
-    "yellow": "33;1",
-    "pink": "38;5;200",
-    "green": "32;1",
-    "red": "31;1",
-}
-
-
-def get_color_mapping(
-    items: list[str], excluded_colors: Optional[list] = None
-) -> dict[str, str]:
-    """Get mapping for items to a support color.
-
-    Args:
-        items: The items to map to colors.
-        excluded_colors: The colors to exclude.
-
-    Returns:
-        The mapping of items to colors.
-    """
-    colors = list(_TEXT_COLOR_MAPPING.keys())
-    if excluded_colors is not None:
-        colors = [c for c in colors if c not in excluded_colors]
-    color_mapping = {item: colors[i % len(colors)] for i, item in enumerate(items)}
-    return color_mapping
-
-
-def get_colored_text(text: str, color: str) -> str:
-    """Get colored text.
-
-    Args:
-        text: The text to color.
-        color: The color to use.
-
-    Returns:
-        The colored text.
-    """
-    color_str = _TEXT_COLOR_MAPPING[color]
-    return f"\u001b[{color_str}m\033[1;3m{text}\u001b[0m"
-
-
-def get_bolded_text(text: str) -> str:
-    """Get bolded text.
-
-    Args:
-        text: The text to bold.
-
-    Returns:
-        The bolded text.
-    """
-    return f"\033[1m{text}\033[0m"
-
-
-def print_text(
-    text: str, color: Optional[str] = None, end: str = "", file: Optional[TextIO] = None
-) -> None:
-    """Print text with highlighting and no end characters.
-
-    If a color is provided, the text will be printed in that color.
-    If a file is provided, the text will be written to that file.
-
-    Args:
-        text: The text to print.
-        color: The color to use. Defaults to None.
-        end: The end character to use. Defaults to "".
-        file: The file to write to. Defaults to None.
-    """
-    text_to_print = get_colored_text(text, color) if color else text
-    print(text_to_print, end=end, file=file)
-    if file:
-        file.flush()  # ensure all printed content are written to file
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/interactive_env.py ./custom_langchain_core/utils/interactive_env.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/interactive_env.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/interactive_env.py	1970-01-01 09:00:00
@@ -1,5 +0,0 @@
-def is_interactive_env() -> bool:
-    """Determine if running within IPython or Jupyter."""
-    import sys
-
-    return hasattr(sys, "ps2")
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/iter.py ./custom_langchain_core/utils/iter.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/iter.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/iter.py	1970-01-01 09:00:00
@@ -1,197 +0,0 @@
-from collections import deque
-from collections.abc import Generator, Iterable, Iterator
-from contextlib import AbstractContextManager
-from itertools import islice
-from typing import (
-    Any,
-    Generic,
-    Optional,
-    TypeVar,
-    Union,
-    overload,
-)
-
-from typing_extensions import Literal
-
-T = TypeVar("T")
-
-
-class NoLock:
-    """Dummy lock that provides the proper interface but no protection."""
-
-    def __enter__(self) -> None:
-        pass
-
-    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> Literal[False]:
-        return False
-
-
-def tee_peer(
-    iterator: Iterator[T],
-    # the buffer specific to this peer
-    buffer: deque[T],
-    # the buffers of all peers, including our own
-    peers: list[deque[T]],
-    lock: AbstractContextManager[Any],
-) -> Generator[T, None, None]:
-    """An individual iterator of a :py:func:`~.tee`.
-
-    This function is a generator that yields items from the shared iterator
-    ``iterator``. It buffers items until the least advanced iterator has
-    yielded them as well. The buffer is shared with all other peers.
-
-    Args:
-        iterator: The shared iterator.
-        buffer: The buffer for this peer.
-        peers: The buffers of all peers.
-        lock: The lock to synchronise access to the shared buffers.
-
-    Yields:
-        The next item from the shared iterator.
-    """
-    try:
-        while True:
-            if not buffer:
-                with lock:
-                    # Another peer produced an item while we were waiting for the lock.
-                    # Proceed with the next loop iteration to yield the item.
-                    if buffer:
-                        continue
-                    try:
-                        item = next(iterator)
-                    except StopIteration:
-                        break
-                    else:
-                        # Append to all buffers, including our own. We'll fetch our
-                        # item from the buffer again, instead of yielding it directly.
-                        # This ensures the proper item ordering if any of our peers
-                        # are fetching items concurrently. They may have buffered their
-                        # item already.
-                        for peer_buffer in peers:
-                            peer_buffer.append(item)
-            yield buffer.popleft()
-    finally:
-        with lock:
-            # this peer is done – remove its buffer
-            for idx, peer_buffer in enumerate(peers):  # pragma: no branch
-                if peer_buffer is buffer:
-                    peers.pop(idx)
-                    break
-            # if we are the last peer, try and close the iterator
-            if not peers and hasattr(iterator, "close"):
-                iterator.close()
-
-
-class Tee(Generic[T]):
-    """Create ``n`` separate asynchronous iterators over ``iterable``.
-
-    This splits a single ``iterable`` into multiple iterators, each providing
-    the same items in the same order.
-    All child iterators may advance separately but share the same items
-    from ``iterable`` -- when the most advanced iterator retrieves an item,
-    it is buffered until the least advanced iterator has yielded it as well.
-    A ``tee`` works lazily and can handle an infinite ``iterable``, provided
-    that all iterators advance.
-
-    .. code-block:: python3
-
-        async def derivative(sensor_data):
-            previous, current = a.tee(sensor_data, n=2)
-            await a.anext(previous)  # advance one iterator
-            return a.map(operator.sub, previous, current)
-
-    Unlike :py:func:`itertools.tee`, :py:func:`~.tee` returns a custom type instead
-    of a :py:class:`tuple`. Like a tuple, it can be indexed, iterated and unpacked
-    to get the child iterators. In addition, its :py:meth:`~.tee.aclose` method
-    immediately closes all children, and it can be used in an ``async with`` context
-    for the same effect.
-
-    If ``iterable`` is an iterator and read elsewhere, ``tee`` will *not*
-    provide these items. Also, ``tee`` must internally buffer each item until the
-    last iterator has yielded it; if the most and least advanced iterator differ
-    by most data, using a :py:class:`list` is more efficient (but not lazy).
-
-    If the underlying iterable is concurrency safe (``anext`` may be awaited
-    concurrently) the resulting iterators are concurrency safe as well. Otherwise,
-    the iterators are safe if there is only ever one single "most advanced" iterator.
-    To enforce sequential use of ``anext``, provide a ``lock``
-    - e.g. an :py:class:`asyncio.Lock` instance in an :py:mod:`asyncio` application -
-    and access is automatically synchronised.
-    """
-
-    def __init__(
-        self,
-        iterable: Iterator[T],
-        n: int = 2,
-        *,
-        lock: Optional[AbstractContextManager[Any]] = None,
-    ):
-        """Create a new ``tee``.
-
-        Args:
-            iterable: The iterable to split.
-            n: The number of iterators to create. Defaults to 2.
-            lock: The lock to synchronise access to the shared buffers.
-                Defaults to None.
-        """
-        self._iterator = iter(iterable)
-        self._buffers: list[deque[T]] = [deque() for _ in range(n)]
-        self._children = tuple(
-            tee_peer(
-                iterator=self._iterator,
-                buffer=buffer,
-                peers=self._buffers,
-                lock=lock if lock is not None else NoLock(),
-            )
-            for buffer in self._buffers
-        )
-
-    def __len__(self) -> int:
-        return len(self._children)
-
-    @overload
-    def __getitem__(self, item: int) -> Iterator[T]: ...
-
-    @overload
-    def __getitem__(self, item: slice) -> tuple[Iterator[T], ...]: ...
-
-    def __getitem__(
-        self, item: Union[int, slice]
-    ) -> Union[Iterator[T], tuple[Iterator[T], ...]]:
-        return self._children[item]
-
-    def __iter__(self) -> Iterator[Iterator[T]]:
-        yield from self._children
-
-    def __enter__(self) -> "Tee[T]":
-        return self
-
-    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> Literal[False]:
-        self.close()
-        return False
-
-    def close(self) -> None:
-        for child in self._children:
-            child.close()
-
-
-# Why this is needed https://stackoverflow.com/a/44638570
-safetee = Tee
-
-
-def batch_iterate(size: Optional[int], iterable: Iterable[T]) -> Iterator[list[T]]:
-    """Utility batching function.
-
-    Args:
-        size: The size of the batch. If None, returns a single batch.
-        iterable: The iterable to batch.
-
-    Yields:
-        The batches of the iterable.
-    """
-    it = iter(iterable)
-    while True:
-        chunk = list(islice(it, size))
-        if not chunk:
-            return
-        yield chunk
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/json.py ./custom_langchain_core/utils/json.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/json.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/json.py	1970-01-01 09:00:00
@@ -1,191 +0,0 @@
-from __future__ import annotations
-
-import json
-import re
-from typing import Any, Callable
-
-from langchain_core.exceptions import OutputParserException
-
-
-def _replace_new_line(match: re.Match[str]) -> str:
-    value = match.group(2)
-    value = re.sub(r"\n", r"\\n", value)
-    value = re.sub(r"\r", r"\\r", value)
-    value = re.sub(r"\t", r"\\t", value)
-    value = re.sub(r'(?<!\\)"', r"\"", value)
-
-    return match.group(1) + value + match.group(3)
-
-
-def _custom_parser(multiline_string: str) -> str:
-    """The LLM response for `action_input` may be a multiline
-    string containing unescaped newlines, tabs or quotes. This function
-    replaces those characters with their escaped counterparts.
-    (newlines in JSON must be double-escaped: `\\n`).
-    """
-    if isinstance(multiline_string, (bytes, bytearray)):
-        multiline_string = multiline_string.decode()
-
-    multiline_string = re.sub(
-        r'("action_input"\:\s*")(.*?)(")',
-        _replace_new_line,
-        multiline_string,
-        flags=re.DOTALL,
-    )
-
-    return multiline_string
-
-
-# Adapted from https://github.com/KillianLucas/open-interpreter/blob/5b6080fae1f8c68938a1e4fa8667e3744084ee21/interpreter/utils/parse_partial_json.py
-# MIT License
-
-
-def parse_partial_json(s: str, *, strict: bool = False) -> Any:
-    """Parse a JSON string that may be missing closing braces.
-
-    Args:
-        s: The JSON string to parse.
-        strict: Whether to use strict parsing. Defaults to False.
-
-    Returns:
-        The parsed JSON object as a Python dictionary.
-    """
-    # Attempt to parse the string as-is.
-    try:
-        return json.loads(s, strict=strict)
-    except json.JSONDecodeError:
-        pass
-
-    # Initialize variables.
-    new_chars = []
-    stack = []
-    is_inside_string = False
-    escaped = False
-
-    # Process each character in the string one at a time.
-    for char in s:
-        if is_inside_string:
-            if char == '"' and not escaped:
-                is_inside_string = False
-            elif char == "\n" and not escaped:
-                char = "\\n"  # Replace the newline character with the escape sequence.
-            elif char == "\\":
-                escaped = not escaped
-            else:
-                escaped = False
-        else:
-            if char == '"':
-                is_inside_string = True
-                escaped = False
-            elif char == "{":
-                stack.append("}")
-            elif char == "[":
-                stack.append("]")
-            elif char == "}" or char == "]":
-                if stack and stack[-1] == char:
-                    stack.pop()
-                else:
-                    # Mismatched closing character; the input is malformed.
-                    return None
-
-        # Append the processed character to the new string.
-        new_chars.append(char)
-
-    # If we're still inside a string at the end of processing,
-    # we need to close the string.
-    if is_inside_string:
-        if escaped:  # Remoe unterminated escape character
-            new_chars.pop()
-        new_chars.append('"')
-
-    # Reverse the stack to get the closing characters.
-    stack.reverse()
-
-    # Try to parse mods of string until we succeed or run out of characters.
-    while new_chars:
-        # Close any remaining open structures in the reverse
-        # order that they were opened.
-        # Attempt to parse the modified string as JSON.
-        try:
-            return json.loads("".join(new_chars + stack), strict=strict)
-        except json.JSONDecodeError:
-            # If we still can't parse the string as JSON,
-            # try removing the last character
-            new_chars.pop()
-
-    # If we got here, we ran out of characters to remove
-    # and still couldn't parse the string as JSON, so return the parse error
-    # for the original string.
-    return json.loads(s, strict=strict)
-
-
-_json_markdown_re = re.compile(r"```(json)?(.*)", re.DOTALL)
-
-
-def parse_json_markdown(
-    json_string: str, *, parser: Callable[[str], Any] = parse_partial_json
-) -> dict:
-    """Parse a JSON string from a Markdown string.
-
-    Args:
-        json_string: The Markdown string.
-
-    Returns:
-        The parsed JSON object as a Python dictionary.
-    """
-    try:
-        return _parse_json(json_string, parser=parser)
-    except json.JSONDecodeError:
-        # Try to find JSON string within triple backticks
-        match = _json_markdown_re.search(json_string)
-
-        # If no match found, assume the entire string is a JSON string
-        # Else, use the content within the backticks
-        json_str = json_string if match is None else match.group(2)
-    return _parse_json(json_str, parser=parser)
-
-
-_json_strip_chars = " \n\r\t`"
-
-
-def _parse_json(
-    json_str: str, *, parser: Callable[[str], Any] = parse_partial_json
-) -> dict:
-    # Strip whitespace,newlines,backtick from the start and end
-    json_str = json_str.strip(_json_strip_chars)
-
-    # handle newlines and other special characters inside the returned value
-    json_str = _custom_parser(json_str)
-
-    # Parse the JSON string into a Python dictionary
-    return parser(json_str)
-
-
-def parse_and_check_json_markdown(text: str, expected_keys: list[str]) -> dict:
-    """Parse a JSON string from a Markdown string and check that it
-    contains the expected keys.
-
-    Args:
-        text: The Markdown string.
-        expected_keys: The expected keys in the JSON string.
-
-    Returns:
-        The parsed JSON object as a Python dictionary.
-
-    Raises:
-        OutputParserException: If the JSON string is invalid or does not contain
-            the expected keys.
-    """
-    try:
-        json_obj = parse_json_markdown(text)
-    except json.JSONDecodeError as e:
-        msg = f"Got invalid JSON object. Error: {e}"
-        raise OutputParserException(msg) from e
-    for key in expected_keys:
-        if key not in json_obj:
-            msg = (
-                f"Got invalid return object. Expected key `{key}` "
-                f"to be present, but got {json_obj}"
-            )
-            raise OutputParserException(msg)
-    return json_obj
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/json_schema.py ./custom_langchain_core/utils/json_schema.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/json_schema.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/json_schema.py	1970-01-01 09:00:00
@@ -1,114 +0,0 @@
-from __future__ import annotations
-
-from collections.abc import Sequence
-from copy import deepcopy
-from typing import Any, Optional
-
-
-def _retrieve_ref(path: str, schema: dict) -> dict:
-    components = path.split("/")
-    if components[0] != "#":
-        msg = (
-            "ref paths are expected to be URI fragments, meaning they should start "
-            "with #."
-        )
-        raise ValueError(msg)
-    out = schema
-    for component in components[1:]:
-        if component in out:
-            out = out[component]
-        elif component.isdigit() and int(component) in out:
-            out = out[int(component)]
-        else:
-            msg = f"Reference '{path}' not found."
-            raise KeyError(msg)
-    return deepcopy(out)
-
-
-def _dereference_refs_helper(
-    obj: Any,
-    full_schema: dict[str, Any],
-    skip_keys: Sequence[str],
-    processed_refs: Optional[set[str]] = None,
-) -> Any:
-    if processed_refs is None:
-        processed_refs = set()
-
-    if isinstance(obj, dict):
-        obj_out = {}
-        for k, v in obj.items():
-            if k in skip_keys:
-                obj_out[k] = v
-            elif k == "$ref":
-                if v in processed_refs:
-                    continue
-                processed_refs.add(v)
-                ref = _retrieve_ref(v, full_schema)
-                full_ref = _dereference_refs_helper(
-                    ref, full_schema, skip_keys, processed_refs
-                )
-                processed_refs.remove(v)
-                return full_ref
-            elif isinstance(v, (list, dict)):
-                obj_out[k] = _dereference_refs_helper(
-                    v, full_schema, skip_keys, processed_refs
-                )
-            else:
-                obj_out[k] = v
-        return obj_out
-    elif isinstance(obj, list):
-        return [
-            _dereference_refs_helper(el, full_schema, skip_keys, processed_refs)
-            for el in obj
-        ]
-    else:
-        return obj
-
-
-def _infer_skip_keys(
-    obj: Any, full_schema: dict, processed_refs: Optional[set[str]] = None
-) -> list[str]:
-    if processed_refs is None:
-        processed_refs = set()
-
-    keys = []
-    if isinstance(obj, dict):
-        for k, v in obj.items():
-            if k == "$ref":
-                if v in processed_refs:
-                    continue
-                processed_refs.add(v)
-                ref = _retrieve_ref(v, full_schema)
-                keys.append(v.split("/")[1])
-                keys += _infer_skip_keys(ref, full_schema, processed_refs)
-            elif isinstance(v, (list, dict)):
-                keys += _infer_skip_keys(v, full_schema, processed_refs)
-    elif isinstance(obj, list):
-        for el in obj:
-            keys += _infer_skip_keys(el, full_schema, processed_refs)
-    return keys
-
-
-def dereference_refs(
-    schema_obj: dict,
-    *,
-    full_schema: Optional[dict] = None,
-    skip_keys: Optional[Sequence[str]] = None,
-) -> dict:
-    """Try to substitute $refs in JSON Schema.
-
-    Args:
-        schema_obj: The schema object to dereference.
-        full_schema: The full schema object. Defaults to None.
-        skip_keys: The keys to skip. Defaults to None.
-
-    Returns:
-        The dereferenced schema object.
-    """
-    full_schema = full_schema or schema_obj
-    skip_keys = (
-        skip_keys
-        if skip_keys is not None
-        else _infer_skip_keys(schema_obj, full_schema)
-    )
-    return _dereference_refs_helper(schema_obj, full_schema, skip_keys)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/loading.py ./custom_langchain_core/utils/loading.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/loading.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/loading.py	1970-01-01 09:00:00
@@ -1,30 +0,0 @@
-"""Utilities for loading configurations from langchain_core-hub."""
-
-import warnings
-from typing import Any
-
-from langchain_core._api.deprecation import deprecated
-
-
-@deprecated(
-    since="0.1.30",
-    removal="1.0",
-    message=(
-        "Using the hwchase17/langchain-hub "
-        "repo for prompts is deprecated. Please use "
-        "<https://smith.langchain.com/hub> instead."
-    ),
-)
-def try_load_from_hub(
-    *args: Any,
-    **kwargs: Any,
-) -> Any:
-    warnings.warn(
-        "Loading from the deprecated github-based Hub is no longer supported. "
-        "Please use the new LangChain Hub at https://smith.langchain.com/hub instead.",
-        DeprecationWarning,
-        stacklevel=2,
-    )
-    # return None, which indicates that we shouldn't load from old hub
-    # and might just be a filepath for e.g. load_chain
-    return None
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/mustache.py ./custom_langchain_core/utils/mustache.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/mustache.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/mustache.py	1970-01-01 09:00:00
@@ -1,653 +0,0 @@
-"""Adapted from https://github.com/noahmorrison/chevron
-MIT License.
-"""
-
-from __future__ import annotations
-
-import logging
-from collections.abc import Iterator, Mapping, Sequence
-from types import MappingProxyType
-from typing import (
-    Any,
-    Literal,
-    Optional,
-    Union,
-    cast,
-)
-
-from typing_extensions import TypeAlias
-
-logger = logging.getLogger(__name__)
-
-
-Scopes: TypeAlias = list[Union[Literal[False, 0], Mapping[str, Any]]]
-
-
-# Globals
-_CURRENT_LINE = 1
-_LAST_TAG_LINE = None
-
-
-class ChevronError(SyntaxError):
-    """Custom exception for Chevron errors."""
-
-
-#
-# Helper functions
-#
-
-
-def grab_literal(template: str, l_del: str) -> tuple[str, str]:
-    """Parse a literal from the template.
-
-    Args:
-        template: The template to parse.
-        l_del: The left delimiter.
-
-    Returns:
-        Tuple[str, str]: The literal and the template.
-    """
-    global _CURRENT_LINE
-
-    try:
-        # Look for the next tag and move the template to it
-        literal, template = template.split(l_del, 1)
-        _CURRENT_LINE += literal.count("\n")
-
-    # There are no more tags in the template?
-    except ValueError:
-        # Then the rest of the template is a literal
-        return (template, "")
-
-    return (literal, template)
-
-
-def l_sa_check(template: str, literal: str, is_standalone: bool) -> bool:
-    """Do a preliminary check to see if a tag could be a standalone.
-
-    Args:
-        template: The template. (Not used.)
-        literal: The literal.
-        is_standalone: Whether the tag is standalone.
-
-    Returns:
-        bool: Whether the tag could be a standalone.
-    """
-    # If there is a newline, or the previous tag was a standalone
-    if literal.find("\n") != -1 or is_standalone:
-        padding = literal.split("\n")[-1]
-
-        # If all the characters since the last newline are spaces
-        # Then the next tag could be a standalone
-        # Otherwise it can't be
-        return padding.isspace() or padding == ""
-    else:
-        return False
-
-
-def r_sa_check(template: str, tag_type: str, is_standalone: bool) -> bool:
-    """Do a final check to see if a tag could be a standalone.
-
-    Args:
-        template: The template.
-        tag_type: The type of the tag.
-        is_standalone: Whether the tag is standalone.
-
-    Returns:
-        bool: Whether the tag could be a standalone.
-    """
-    # Check right side if we might be a standalone
-    if is_standalone and tag_type not in ["variable", "no escape"]:
-        on_newline = template.split("\n", 1)
-
-        # If the stuff to the right of us are spaces we're a standalone
-        return on_newline[0].isspace() or not on_newline[0]
-
-    # If we're a tag can't be a standalone
-    else:
-        return False
-
-
-def parse_tag(template: str, l_del: str, r_del: str) -> tuple[tuple[str, str], str]:
-    """Parse a tag from a template.
-
-    Args:
-        template: The template.
-        l_del: The left delimiter.
-        r_del: The right delimiter.
-
-    Returns:
-        Tuple[Tuple[str, str], str]: The tag and the template.
-
-    Raises:
-        ChevronError: If the tag is unclosed.
-        ChevronError: If the set delimiter tag is unclosed.
-    """
-    global _CURRENT_LINE, _LAST_TAG_LINE
-
-    tag_types = {
-        "!": "comment",
-        "#": "section",
-        "^": "inverted section",
-        "/": "end",
-        ">": "partial",
-        "=": "set delimiter?",
-        "{": "no escape?",
-        "&": "no escape",
-    }
-
-    # Get the tag
-    try:
-        tag, template = template.split(r_del, 1)
-    except ValueError as e:
-        msg = f"unclosed tag at line {_CURRENT_LINE}"
-        raise ChevronError(msg) from e
-
-    # Find the type meaning of the first character
-    tag_type = tag_types.get(tag[0], "variable")
-
-    # If the type is not a variable
-    if tag_type != "variable":
-        # Then that first character is not needed
-        tag = tag[1:]
-
-    # If we might be a set delimiter tag
-    if tag_type == "set delimiter?":
-        # Double check to make sure we are
-        if tag.endswith("="):
-            tag_type = "set delimiter"
-            # Remove the equal sign
-            tag = tag[:-1]
-
-        # Otherwise we should complain
-        else:
-            msg = f"unclosed set delimiter tag\nat line {_CURRENT_LINE}"
-            raise ChevronError(msg)
-
-    elif (
-        # If we might be a no html escape tag
-        tag_type == "no escape?"
-        # And we have a third curly brace
-        # (And are using curly braces as delimiters)
-        and l_del == "{{"
-        and r_del == "}}"
-        and template.startswith("}")
-    ):
-        # Then we are a no html escape tag
-        template = template[1:]
-        tag_type = "no escape"
-
-    # Strip the whitespace off the key and return
-    return ((tag_type, tag.strip()), template)
-
-
-#
-# The main tokenizing function
-#
-
-
-def tokenize(
-    template: str, def_ldel: str = "{{", def_rdel: str = "}}"
-) -> Iterator[tuple[str, str]]:
-    """Tokenize a mustache template.
-
-    Tokenizes a mustache template in a generator fashion,
-    using file-like objects. It also accepts a string containing
-    the template.
-
-    Args:
-        template: a file-like object, or a string of a mustache template
-        def_ldel: The default left delimiter
-            ("{{" by default, as in spec compliant mustache)
-        def_rdel: The default right delimiter
-            ("}}" by default, as in spec compliant mustache)
-
-    Returns:
-        A generator of mustache tags in the form of a tuple (tag_type, tag_key)
-            Where tag_type is one of:
-             * literal
-             * section
-             * inverted section
-             * end
-             * partial
-             * no escape
-            And tag_key is either the key or in the case of a literal tag,
-            the literal itself.
-    """
-    global _CURRENT_LINE, _LAST_TAG_LINE
-    _CURRENT_LINE = 1
-    _LAST_TAG_LINE = None
-
-    is_standalone = True
-    open_sections = []
-    l_del = def_ldel
-    r_del = def_rdel
-
-    while template:
-        literal, template = grab_literal(template, l_del)
-
-        # If the template is completed
-        if not template:
-            # Then yield the literal and leave
-            yield ("literal", literal)
-            break
-
-        # Do the first check to see if we could be a standalone
-        is_standalone = l_sa_check(template, literal, is_standalone)
-
-        # Parse the tag
-        tag, template = parse_tag(template, l_del, r_del)
-        tag_type, tag_key = tag
-
-        # Special tag logic
-
-        # If we are a set delimiter tag
-        if tag_type == "set delimiter":
-            # Then get and set the delimiters
-            dels = tag_key.strip().split(" ")
-            l_del, r_del = dels[0], dels[-1]
-
-        # If we are a section tag
-        elif tag_type in ["section", "inverted section"]:
-            # Then open a new section
-            open_sections.append(tag_key)
-            _LAST_TAG_LINE = _CURRENT_LINE
-
-        # If we are an end tag
-        elif tag_type == "end":
-            # Then check to see if the last opened section
-            # is the same as us
-            try:
-                last_section = open_sections.pop()
-            except IndexError as e:
-                msg = (
-                    f'Trying to close tag "{tag_key}"\n'
-                    "Looks like it was not opened.\n"
-                    f"line {_CURRENT_LINE + 1}"
-                )
-                raise ChevronError(msg) from e
-            if tag_key != last_section:
-                # Otherwise we need to complain
-                msg = (
-                    f'Trying to close tag "{tag_key}"\n'
-                    f'last open tag is "{last_section}"\n'
-                    f"line {_CURRENT_LINE + 1}"
-                )
-                raise ChevronError(msg)
-
-        # Do the second check to see if we're a standalone
-        is_standalone = r_sa_check(template, tag_type, is_standalone)
-
-        # Which if we are
-        if is_standalone:
-            # Remove the stuff before the newline
-            template = template.split("\n", 1)[-1]
-
-            # Partials need to keep the spaces on their left
-            if tag_type != "partial":
-                # But other tags don't
-                literal = literal.rstrip(" ")
-
-        # Start yielding
-        # Ignore literals that are empty
-        if literal != "":
-            yield ("literal", literal)
-
-        # Ignore comments and set delimiters
-        if tag_type not in ["comment", "set delimiter?"]:
-            yield (tag_type, tag_key)
-
-    # If there are any open sections when we're done
-    if open_sections:
-        # Then we need to complain
-        msg = (
-            "Unexpected EOF\n"
-            f'the tag "{open_sections[-1]}" was never closed\n'
-            f"was opened at line {_LAST_TAG_LINE}"
-        )
-        raise ChevronError(msg)
-
-
-#
-# Helper functions
-#
-
-
-def _html_escape(string: str) -> str:
-    """HTML escape all of these " & < >."""
-    html_codes = {
-        '"': "&quot;",
-        "<": "&lt;",
-        ">": "&gt;",
-    }
-
-    # & must be handled first
-    string = string.replace("&", "&amp;")
-    for char in html_codes:
-        string = string.replace(char, html_codes[char])
-    return string
-
-
-def _get_key(
-    key: str,
-    scopes: Scopes,
-    warn: bool,
-    keep: bool,
-    def_ldel: str,
-    def_rdel: str,
-) -> Any:
-    """Get a key from the current scope."""
-    # If the key is a dot
-    if key == ".":
-        # Then just return the current scope
-        return scopes[0]
-
-    # Loop through the scopes
-    for scope in scopes:
-        try:
-            # Return an empty string if falsy, with two exceptions
-            # 0 should return 0, and False should return False
-            if scope in (0, False):
-                return scope
-
-            # For every dot separated key
-            for child in key.split("."):
-                # Return an empty string if falsy, with two exceptions
-                # 0 should return 0, and False should return False
-                if scope in (0, False):
-                    return scope
-                # Move into the scope
-                try:
-                    # Try subscripting (Normal dictionaries)
-                    scope = cast(dict[str, Any], scope)[child]
-                except (TypeError, AttributeError):
-                    try:
-                        scope = getattr(scope, child)
-                    except (TypeError, AttributeError):
-                        # Try as a list
-                        scope = scope[int(child)]  # type: ignore
-
-            try:
-                # This allows for custom falsy data types
-                # https://github.com/noahmorrison/chevron/issues/35
-                if scope._CHEVRON_return_scope_when_falsy:  # type: ignore
-                    return scope
-            except AttributeError:
-                if scope in (0, False):
-                    return scope
-                return scope or ""
-        except (AttributeError, KeyError, IndexError, ValueError):
-            # We couldn't find the key in the current scope
-            # We'll try again on the next pass
-            pass
-
-    # We couldn't find the key in any of the scopes
-
-    if warn:
-        logger.warn(f"Could not find key '{key}'")
-
-    if keep:
-        return f"{def_ldel} {key} {def_rdel}"
-
-    return ""
-
-
-def _get_partial(name: str, partials_dict: Mapping[str, str]) -> str:
-    """Load a partial."""
-    try:
-        # Maybe the partial is in the dictionary
-        return partials_dict[name]
-    except KeyError:
-        return ""
-
-
-#
-# The main rendering function
-#
-g_token_cache: dict[str, list[tuple[str, str]]] = {}
-
-EMPTY_DICT: MappingProxyType[str, str] = MappingProxyType({})
-
-
-def render(
-    template: Union[str, list[tuple[str, str]]] = "",
-    data: Mapping[str, Any] = EMPTY_DICT,
-    partials_dict: Mapping[str, str] = EMPTY_DICT,
-    padding: str = "",
-    def_ldel: str = "{{",
-    def_rdel: str = "}}",
-    scopes: Optional[Scopes] = None,
-    warn: bool = False,
-    keep: bool = False,
-) -> str:
-    """Render a mustache template.
-
-    Renders a mustache template with a data scope and inline partial capability.
-
-    Args:
-        template: A file-like object or a string containing the template.
-        data: A python dictionary with your data scope.
-        partials_path: The path to where your partials are stored.
-             If set to None, then partials won't be loaded from the file system
-             (defaults to '.').
-        partials_ext: The extension that you want the parser to look for
-            (defaults to 'mustache').
-        partials_dict: A python dictionary which will be search for partials
-             before the filesystem is. {'include': 'foo'} is the same
-             as a file called include.mustache
-             (defaults to {}).
-        padding: This is for padding partials, and shouldn't be used
-            (but can be if you really want to).
-        def_ldel: The default left delimiter
-             ("{{" by default, as in spec compliant mustache).
-        def_rdel: The default right delimiter
-             ("}}" by default, as in spec compliant mustache).
-        scopes: The list of scopes that get_key will look through.
-        warn: Log a warning when a template substitution isn't found in the data
-        keep: Keep unreplaced tags when a substitution isn't found in the data.
-
-    Returns:
-        A string containing the rendered template.
-    """
-    # If the template is a sequence but not derived from a string
-    if isinstance(template, Sequence) and not isinstance(template, str):
-        # Then we don't need to tokenize it
-        # But it does need to be a generator
-        tokens: Iterator[tuple[str, str]] = (token for token in template)
-    else:
-        if template in g_token_cache:
-            tokens = (token for token in g_token_cache[template])
-        else:
-            # Otherwise make a generator
-            tokens = tokenize(template, def_ldel, def_rdel)
-
-    output = ""
-
-    if scopes is None:
-        scopes = [data]
-
-    # Run through the tokens
-    for tag, key in tokens:
-        # Set the current scope
-        current_scope = scopes[0]
-
-        # If we're an end tag
-        if tag == "end":
-            # Pop out of the latest scope
-            del scopes[0]
-
-        # If the current scope is falsy and not the only scope
-        elif not current_scope and len(scopes) != 1:
-            if tag in ["section", "inverted section"]:
-                # Set the most recent scope to a falsy value
-                scopes.insert(0, False)
-
-        # If we're a literal tag
-        elif tag == "literal":
-            # Add padding to the key and add it to the output
-            output += key.replace("\n", "\n" + padding)
-
-        # If we're a variable tag
-        elif tag == "variable":
-            # Add the html escaped key to the output
-            thing = _get_key(
-                key, scopes, warn=warn, keep=keep, def_ldel=def_ldel, def_rdel=def_rdel
-            )
-            if thing is True and key == ".":
-                # if we've coerced into a boolean by accident
-                # (inverted tags do this)
-                # then get the un-coerced object (next in the stack)
-                thing = scopes[1]
-            if not isinstance(thing, str):
-                thing = str(thing)
-            output += _html_escape(thing)
-
-        # If we're a no html escape tag
-        elif tag == "no escape":
-            # Just lookup the key and add it
-            thing = _get_key(
-                key, scopes, warn=warn, keep=keep, def_ldel=def_ldel, def_rdel=def_rdel
-            )
-            if not isinstance(thing, str):
-                thing = str(thing)
-            output += thing
-
-        # If we're a section tag
-        elif tag == "section":
-            # Get the sections scope
-            scope = _get_key(
-                key, scopes, warn=warn, keep=keep, def_ldel=def_ldel, def_rdel=def_rdel
-            )
-
-            # If the scope is a callable (as described in
-            # https://mustache.github.io/mustache.5.html)
-            if callable(scope):
-                # Generate template text from tags
-                text = ""
-                tags: list[tuple[str, str]] = []
-                for token in tokens:
-                    if token == ("end", key):
-                        break
-
-                    tags.append(token)
-                    tag_type, tag_key = token
-                    if tag_type == "literal":
-                        text += tag_key
-                    elif tag_type == "no escape":
-                        text += f"{def_ldel}& {tag_key} {def_rdel}"
-                    else:
-                        text += "{}{} {}{}".format(
-                            def_ldel,
-                            {
-                                "comment": "!",
-                                "section": "#",
-                                "inverted section": "^",
-                                "end": "/",
-                                "partial": ">",
-                                "set delimiter": "=",
-                                "no escape": "&",
-                                "variable": "",
-                            }[tag_type],
-                            tag_key,
-                            def_rdel,
-                        )
-
-                g_token_cache[text] = tags
-
-                rend = scope(
-                    text,
-                    lambda template, data=None: render(
-                        template,
-                        data={},
-                        partials_dict=partials_dict,
-                        padding=padding,
-                        def_ldel=def_ldel,
-                        def_rdel=def_rdel,
-                        scopes=data and [data] + scopes or scopes,
-                        warn=warn,
-                        keep=keep,
-                    ),
-                )
-
-                output += rend
-
-            # If the scope is a sequence, an iterator or generator but not
-            # derived from a string
-            elif isinstance(scope, (Sequence, Iterator)) and not isinstance(scope, str):
-                # Then we need to do some looping
-
-                # Gather up all the tags inside the section
-                # (And don't be tricked by nested end tags with the same key)
-                # TODO: This feels like it still has edge cases, no?
-                tags = []
-                tags_with_same_key = 0
-                for token in tokens:
-                    if token == ("section", key):
-                        tags_with_same_key += 1
-                    if token == ("end", key):
-                        tags_with_same_key -= 1
-                        if tags_with_same_key < 0:
-                            break
-                    tags.append(token)
-
-                # For every item in the scope
-                for thing in scope:
-                    # Append it as the most recent scope and render
-                    new_scope = [thing] + scopes
-                    rend = render(
-                        template=tags,
-                        scopes=new_scope,
-                        padding=padding,
-                        partials_dict=partials_dict,
-                        def_ldel=def_ldel,
-                        def_rdel=def_rdel,
-                        warn=warn,
-                        keep=keep,
-                    )
-
-                    output += rend
-
-            else:
-                # Otherwise we're just a scope section
-                scopes.insert(0, scope)
-
-        # If we're an inverted section
-        elif tag == "inverted section":
-            # Add the flipped scope to the scopes
-            scope = _get_key(
-                key, scopes, warn=warn, keep=keep, def_ldel=def_ldel, def_rdel=def_rdel
-            )
-            scopes.insert(0, cast(Literal[False], not scope))
-
-        # If we're a partial
-        elif tag == "partial":
-            # Load the partial
-            partial = _get_partial(key, partials_dict)
-
-            # Find what to pad the partial with
-            left = output.rpartition("\n")[2]
-            part_padding = padding
-            if left.isspace():
-                part_padding += left
-
-            # Render the partial
-            part_out = render(
-                template=partial,
-                partials_dict=partials_dict,
-                def_ldel=def_ldel,
-                def_rdel=def_rdel,
-                padding=part_padding,
-                scopes=scopes,
-                warn=warn,
-                keep=keep,
-            )
-
-            # If the partial was indented
-            if left.isspace():
-                # then remove the spaces from the end
-                part_out = part_out.rstrip(" \t")
-
-            # Add the partials output to the output
-            output += part_out
-
-    return output
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/pydantic.py ./custom_langchain_core/utils/pydantic.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/pydantic.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/pydantic.py	1970-01-01 09:00:00
@@ -1,647 +0,0 @@
-"""Utilities for pydantic."""
-
-from __future__ import annotations
-
-import inspect
-import textwrap
-import warnings
-from contextlib import nullcontext
-from functools import lru_cache, wraps
-from types import GenericAlias
-from typing import (
-    Any,
-    Callable,
-    Optional,
-    TypeVar,
-    Union,
-    cast,
-    overload,
-)
-
-import pydantic
-from pydantic import (
-    BaseModel,
-    ConfigDict,
-    PydanticDeprecationWarning,
-    RootModel,
-    root_validator,
-)
-from pydantic import (
-    create_model as _create_model_base,
-)
-from pydantic.json_schema import (
-    DEFAULT_REF_TEMPLATE,
-    GenerateJsonSchema,
-    JsonSchemaMode,
-    JsonSchemaValue,
-)
-from pydantic_core import core_schema
-
-
-def get_pydantic_major_version() -> int:
-    """Get the major version of Pydantic."""
-    try:
-        import pydantic
-
-        return int(pydantic.__version__.split(".")[0])
-    except ImportError:
-        return 0
-
-
-def _get_pydantic_minor_version() -> int:
-    """Get the minor version of Pydantic."""
-    try:
-        import pydantic
-
-        return int(pydantic.__version__.split(".")[1])
-    except ImportError:
-        return 0
-
-
-PYDANTIC_MAJOR_VERSION = get_pydantic_major_version()
-PYDANTIC_MINOR_VERSION = _get_pydantic_minor_version()
-
-
-if PYDANTIC_MAJOR_VERSION == 1:
-    from pydantic.fields import FieldInfo as FieldInfoV1
-
-    PydanticBaseModel = pydantic.BaseModel
-    TypeBaseModel = type[BaseModel]
-elif PYDANTIC_MAJOR_VERSION == 2:
-    from pydantic.v1.fields import FieldInfo as FieldInfoV1  # type: ignore[assignment]
-
-    # Union type needs to be last assignment to PydanticBaseModel to make mypy happy.
-    PydanticBaseModel = Union[BaseModel, pydantic.BaseModel]  # type: ignore
-    TypeBaseModel = Union[type[BaseModel], type[pydantic.BaseModel]]  # type: ignore
-else:
-    msg = f"Unsupported Pydantic version: {PYDANTIC_MAJOR_VERSION}"
-    raise ValueError(msg)
-
-
-TBaseModel = TypeVar("TBaseModel", bound=PydanticBaseModel)
-
-
-def is_pydantic_v1_subclass(cls: type) -> bool:
-    """Check if the installed Pydantic version is 1.x-like."""
-    if PYDANTIC_MAJOR_VERSION == 1:
-        return True
-    elif PYDANTIC_MAJOR_VERSION == 2:
-        from pydantic.v1 import BaseModel as BaseModelV1
-
-        if issubclass(cls, BaseModelV1):
-            return True
-    return False
-
-
-def is_pydantic_v2_subclass(cls: type) -> bool:
-    """Check if the installed Pydantic version is 1.x-like."""
-    from pydantic import BaseModel
-
-    return PYDANTIC_MAJOR_VERSION == 2 and issubclass(cls, BaseModel)
-
-
-def is_basemodel_subclass(cls: type) -> bool:
-    """Check if the given class is a subclass of Pydantic BaseModel.
-
-    Check if the given class is a subclass of any of the following:
-
-    * pydantic.BaseModel in Pydantic 1.x
-    * pydantic.BaseModel in Pydantic 2.x
-    * pydantic.v1.BaseModel in Pydantic 2.x
-    """
-    # Before we can use issubclass on the cls we need to check if it is a class
-    if not inspect.isclass(cls) or isinstance(cls, GenericAlias):
-        return False
-
-    if PYDANTIC_MAJOR_VERSION == 1:
-        from pydantic import BaseModel as BaseModelV1Proper
-
-        if issubclass(cls, BaseModelV1Proper):
-            return True
-    elif PYDANTIC_MAJOR_VERSION == 2:
-        from pydantic import BaseModel as BaseModelV2
-        from pydantic.v1 import BaseModel as BaseModelV1
-
-        if issubclass(cls, BaseModelV2):
-            return True
-
-        if issubclass(cls, BaseModelV1):
-            return True
-    else:
-        msg = f"Unsupported Pydantic version: {PYDANTIC_MAJOR_VERSION}"
-        raise ValueError(msg)
-    return False
-
-
-def is_basemodel_instance(obj: Any) -> bool:
-    """Check if the given class is an instance of Pydantic BaseModel.
-
-    Check if the given class is an instance of any of the following:
-
-    * pydantic.BaseModel in Pydantic 1.x
-    * pydantic.BaseModel in Pydantic 2.x
-    * pydantic.v1.BaseModel in Pydantic 2.x
-    """
-    if PYDANTIC_MAJOR_VERSION == 1:
-        from pydantic import BaseModel as BaseModelV1Proper
-
-        if isinstance(obj, BaseModelV1Proper):
-            return True
-    elif PYDANTIC_MAJOR_VERSION == 2:
-        from pydantic import BaseModel as BaseModelV2
-        from pydantic.v1 import BaseModel as BaseModelV1
-
-        if isinstance(obj, BaseModelV2):
-            return True
-
-        if isinstance(obj, BaseModelV1):
-            return True
-    else:
-        msg = f"Unsupported Pydantic version: {PYDANTIC_MAJOR_VERSION}"
-        raise ValueError(msg)
-    return False
-
-
-# How to type hint this?
-def pre_init(func: Callable) -> Any:
-    """Decorator to run a function before model initialization.
-
-    Args:
-        func (Callable): The function to run before model initialization.
-
-    Returns:
-        Any: The decorated function.
-    """
-    with warnings.catch_warnings():
-        warnings.filterwarnings(action="ignore", category=PydanticDeprecationWarning)
-
-        @root_validator(pre=True)
-        @wraps(func)
-        def wrapper(cls: type[BaseModel], values: dict[str, Any]) -> dict[str, Any]:
-            """Decorator to run a function before model initialization.
-
-            Args:
-                cls (Type[BaseModel]): The model class.
-                values (Dict[str, Any]): The values to initialize the model with.
-
-            Returns:
-                Dict[str, Any]: The values to initialize the model with.
-            """
-            # Insert default values
-            fields = cls.model_fields
-            for name, field_info in fields.items():
-                # Check if allow_population_by_field_name is enabled
-                # If yes, then set the field name to the alias
-                if (
-                    hasattr(cls, "Config")
-                    and hasattr(cls.Config, "allow_population_by_field_name")
-                    and cls.Config.allow_population_by_field_name
-                    and field_info.alias in values
-                ):
-                    values[name] = values.pop(field_info.alias)
-                if (
-                    hasattr(cls, "model_config")
-                    and cls.model_config.get("populate_by_name")
-                    and field_info.alias in values
-                ):
-                    values[name] = values.pop(field_info.alias)
-
-                if (
-                    name not in values or values[name] is None
-                ) and not field_info.is_required():
-                    if field_info.default_factory is not None:
-                        values[name] = field_info.default_factory()  # type: ignore
-                    else:
-                        values[name] = field_info.default
-
-            # Call the decorated function
-            return func(cls, values)
-
-    return wrapper
-
-
-class _IgnoreUnserializable(GenerateJsonSchema):
-    """A JSON schema generator that ignores unknown types.
-
-    https://docs.pydantic.dev/latest/concepts/json_schema/#customizing-the-json-schema-generation-process
-    """
-
-    def handle_invalid_for_json_schema(
-        self, schema: core_schema.CoreSchema, error_info: str
-    ) -> JsonSchemaValue:
-        return {}
-
-
-def _create_subset_model_v1(
-    name: str,
-    model: type[BaseModel],
-    field_names: list,
-    *,
-    descriptions: Optional[dict] = None,
-    fn_description: Optional[str] = None,
-) -> type[BaseModel]:
-    """Create a pydantic model with only a subset of model's fields."""
-    if PYDANTIC_MAJOR_VERSION == 1:
-        from pydantic import create_model
-    elif PYDANTIC_MAJOR_VERSION == 2:
-        from pydantic.v1 import create_model  # type: ignore
-    else:
-        msg = f"Unsupported pydantic version: {PYDANTIC_MAJOR_VERSION}"
-        raise NotImplementedError(msg)
-
-    fields = {}
-
-    for field_name in field_names:
-        # Using pydantic v1 so can access __fields__ as a dict.
-        field = model.__fields__[field_name]  # type: ignore
-        t = (
-            # this isn't perfect but should work for most functions
-            field.outer_type_
-            if field.required and not field.allow_none
-            else Optional[field.outer_type_]
-        )
-        if descriptions and field_name in descriptions:
-            field.field_info.description = descriptions[field_name]
-        fields[field_name] = (t, field.field_info)
-
-    rtn = create_model(name, **fields)  # type: ignore
-    rtn.__doc__ = textwrap.dedent(fn_description or model.__doc__ or "")
-    return rtn
-
-
-def _create_subset_model_v2(
-    name: str,
-    model: type[pydantic.BaseModel],
-    field_names: list[str],
-    *,
-    descriptions: Optional[dict] = None,
-    fn_description: Optional[str] = None,
-) -> type[pydantic.BaseModel]:
-    """Create a pydantic model with a subset of the model fields."""
-    from pydantic import create_model
-    from pydantic.fields import FieldInfo
-
-    descriptions_ = descriptions or {}
-    fields = {}
-    for field_name in field_names:
-        field = model.model_fields[field_name]  # type: ignore
-        description = descriptions_.get(field_name, field.description)
-        field_info = FieldInfo(description=description, default=field.default)
-        if field.metadata:
-            field_info.metadata = field.metadata
-        fields[field_name] = (field.annotation, field_info)
-
-    rtn = create_model(  # type: ignore
-        name, **fields, __config__=ConfigDict(arbitrary_types_allowed=True)
-    )
-
-    # TODO(0.3): Determine if there is a more "pydantic" way to preserve annotations.
-    # This is done to preserve __annotations__ when working with pydantic 2.x
-    # and using the Annotated type with TypedDict.
-    # Comment out the following line, to trigger the relevant test case.
-    selected_annotations = [
-        (name, annotation)
-        for name, annotation in model.__annotations__.items()
-        if name in field_names
-    ]
-
-    rtn.__annotations__ = dict(selected_annotations)
-    rtn.__doc__ = textwrap.dedent(fn_description or model.__doc__ or "")
-    return rtn
-
-
-# Private functionality to create a subset model that's compatible across
-# different versions of pydantic.
-# Handles pydantic versions 1.x and 2.x. including v1 of pydantic in 2.x.
-# However, can't find a way to type hint this.
-def _create_subset_model(
-    name: str,
-    model: TypeBaseModel,
-    field_names: list[str],
-    *,
-    descriptions: Optional[dict] = None,
-    fn_description: Optional[str] = None,
-) -> type[BaseModel]:
-    """Create subset model using the same pydantic version as the input model."""
-    if PYDANTIC_MAJOR_VERSION == 1:
-        return _create_subset_model_v1(
-            name,
-            model,
-            field_names,
-            descriptions=descriptions,
-            fn_description=fn_description,
-        )
-    elif PYDANTIC_MAJOR_VERSION == 2:
-        from pydantic.v1 import BaseModel as BaseModelV1
-
-        if issubclass(model, BaseModelV1):
-            return _create_subset_model_v1(
-                name,
-                model,
-                field_names,
-                descriptions=descriptions,
-                fn_description=fn_description,
-            )
-        else:
-            return _create_subset_model_v2(
-                name,
-                model,
-                field_names,
-                descriptions=descriptions,
-                fn_description=fn_description,
-            )
-    else:
-        msg = f"Unsupported pydantic version: {PYDANTIC_MAJOR_VERSION}"
-        raise NotImplementedError(msg)
-
-
-if PYDANTIC_MAJOR_VERSION == 2:
-    from pydantic import BaseModel as BaseModelV2
-    from pydantic.fields import FieldInfo as FieldInfoV2
-    from pydantic.v1 import BaseModel as BaseModelV1
-
-    @overload
-    def get_fields(model: type[BaseModelV2]) -> dict[str, FieldInfoV2]: ...
-
-    @overload
-    def get_fields(model: BaseModelV2) -> dict[str, FieldInfoV2]: ...
-
-    @overload
-    def get_fields(model: type[BaseModelV1]) -> dict[str, FieldInfoV1]: ...
-
-    @overload
-    def get_fields(model: BaseModelV1) -> dict[str, FieldInfoV1]: ...
-
-    def get_fields(
-        model: Union[
-            BaseModelV2,
-            BaseModelV1,
-            type[BaseModelV2],
-            type[BaseModelV1],
-        ],
-    ) -> Union[dict[str, FieldInfoV2], dict[str, FieldInfoV1]]:
-        """Get the field names of a Pydantic model."""
-        if hasattr(model, "model_fields"):
-            return model.model_fields  # type: ignore
-
-        elif hasattr(model, "__fields__"):
-            return model.__fields__  # type: ignore
-        else:
-            msg = f"Expected a Pydantic model. Got {type(model)}"
-            raise TypeError(msg)
-
-elif PYDANTIC_MAJOR_VERSION == 1:
-    from pydantic import BaseModel as BaseModelV1_
-
-    def get_fields(  # type: ignore[no-redef]
-        model: Union[type[BaseModelV1_], BaseModelV1_],
-    ) -> dict[str, FieldInfoV1]:
-        """Get the field names of a Pydantic model."""
-        return model.__fields__  # type: ignore
-
-else:
-    msg = f"Unsupported Pydantic version: {PYDANTIC_MAJOR_VERSION}"
-    raise ValueError(msg)
-
-_SchemaConfig = ConfigDict(
-    arbitrary_types_allowed=True, frozen=True, protected_namespaces=()
-)
-
-NO_DEFAULT = object()
-
-
-def _create_root_model(
-    name: str,
-    type_: Any,
-    module_name: Optional[str] = None,
-    default_: object = NO_DEFAULT,
-) -> type[BaseModel]:
-    """Create a base class."""
-
-    def schema(
-        cls: type[BaseModel],
-        by_alias: bool = True,
-        ref_template: str = DEFAULT_REF_TEMPLATE,
-    ) -> dict[str, Any]:
-        # Complains about schema not being defined in superclass
-        schema_ = super(cls, cls).schema(  # type: ignore[misc]
-            by_alias=by_alias, ref_template=ref_template
-        )
-        schema_["title"] = name
-        return schema_
-
-    def model_json_schema(
-        cls: type[BaseModel],
-        by_alias: bool = True,
-        ref_template: str = DEFAULT_REF_TEMPLATE,
-        schema_generator: type[GenerateJsonSchema] = GenerateJsonSchema,
-        mode: JsonSchemaMode = "validation",
-    ) -> dict[str, Any]:
-        # Complains about model_json_schema not being defined in superclass
-        schema_ = super(cls, cls).model_json_schema(  # type: ignore[misc]
-            by_alias=by_alias,
-            ref_template=ref_template,
-            schema_generator=schema_generator,
-            mode=mode,
-        )
-        schema_["title"] = name
-        return schema_
-
-    base_class_attributes = {
-        "__annotations__": {"root": type_},
-        "model_config": ConfigDict(arbitrary_types_allowed=True),
-        "schema": classmethod(schema),
-        "model_json_schema": classmethod(model_json_schema),
-        "__module__": module_name or "langchain_core.runnables.utils",
-    }
-
-    if default_ is not NO_DEFAULT:
-        base_class_attributes["root"] = default_
-    with warnings.catch_warnings():
-        try:
-            if (
-                isinstance(type_, type)
-                and not isinstance(type_, GenericAlias)
-                and issubclass(type_, BaseModelV1)
-            ):
-                warnings.filterwarnings(
-                    action="ignore", category=PydanticDeprecationWarning
-                )
-        except TypeError:
-            pass
-        custom_root_type = type(name, (RootModel,), base_class_attributes)
-    return cast(type[BaseModel], custom_root_type)
-
-
-@lru_cache(maxsize=256)
-def _create_root_model_cached(
-    model_name: str,
-    type_: Any,
-    *,
-    module_name: Optional[str] = None,
-    default_: object = NO_DEFAULT,
-) -> type[BaseModel]:
-    return _create_root_model(
-        model_name, type_, default_=default_, module_name=module_name
-    )
-
-
-@lru_cache(maxsize=256)
-def _create_model_cached(
-    __model_name: str,
-    **field_definitions: Any,
-) -> type[BaseModel]:
-    return _create_model_base(
-        __model_name,
-        __config__=_SchemaConfig,
-        **_remap_field_definitions(field_definitions),
-    )
-
-
-def create_model(
-    __model_name: str,
-    __module_name: Optional[str] = None,
-    **field_definitions: Any,
-) -> type[BaseModel]:
-    """Create a pydantic model with the given field definitions.
-
-    Please use create_model_v2 instead of this function.
-
-    Args:
-        __model_name: The name of the model.
-        __module_name: The name of the module where the model is defined.
-            This is used by Pydantic to resolve any forward references.
-        **field_definitions: The field definitions for the model.
-
-    Returns:
-        Type[BaseModel]: The created model.
-    """
-    kwargs = {}
-    if "__root__" in field_definitions:
-        kwargs["root"] = field_definitions.pop("__root__")
-
-    return create_model_v2(
-        __model_name,
-        module_name=__module_name,
-        field_definitions=field_definitions,
-        **kwargs,
-    )
-
-
-# Reserved names should capture all the `public` names / methods that are
-# used by BaseModel internally. This will keep the reserved names up-to-date.
-# For reference, the reserved names are:
-# "construct", "copy", "dict", "from_orm", "json", "parse_file", "parse_obj",
-# "parse_raw", "schema", "schema_json", "update_forward_refs", "validate",
-# "model_computed_fields", "model_config", "model_construct", "model_copy",
-# "model_dump", "model_dump_json", "model_extra", "model_fields",
-# "model_fields_set", "model_json_schema", "model_parametrized_name",
-# "model_post_init", "model_rebuild", "model_validate", "model_validate_json",
-# "model_validate_strings"
-_RESERVED_NAMES = {key for key in dir(BaseModel) if not key.startswith("_")}
-
-
-def _remap_field_definitions(field_definitions: dict[str, Any]) -> dict[str, Any]:
-    """This remaps fields to avoid colliding with internal pydantic fields."""
-    from pydantic import Field
-    from pydantic.fields import FieldInfo
-
-    remapped = {}
-    for key, value in field_definitions.items():
-        if key.startswith("_") or key in _RESERVED_NAMES:
-            # Let's add a prefix to avoid colliding with internal pydantic fields
-            if isinstance(value, FieldInfo):
-                msg = (
-                    f"Remapping for fields starting with '_' or fields with a name "
-                    f"matching a reserved name {_RESERVED_NAMES} is not supported if "
-                    f" the field is a pydantic Field instance. Got {key}."
-                )
-                raise NotImplementedError(msg)
-            type_, default_ = value
-            remapped[f"private_{key}"] = (
-                type_,
-                Field(
-                    default=default_,
-                    alias=key,
-                    serialization_alias=key,
-                    title=key.lstrip("_").replace("_", " ").title(),
-                ),
-            )
-        else:
-            remapped[key] = value
-    return remapped
-
-
-def create_model_v2(
-    model_name: str,
-    *,
-    module_name: Optional[str] = None,
-    field_definitions: Optional[dict[str, Any]] = None,
-    root: Optional[Any] = None,
-) -> type[BaseModel]:
-    """Create a pydantic model with the given field definitions.
-
-    Attention:
-        Please do not use outside of langchain packages. This API
-        is subject to change at any time.
-
-    Args:
-        model_name: The name of the model.
-        module_name: The name of the module where the model is defined.
-            This is used by Pydantic to resolve any forward references.
-        field_definitions: The field definitions for the model.
-        root: Type for a root model (RootModel)
-
-    Returns:
-        Type[BaseModel]: The created model.
-    """
-    field_definitions = cast(dict[str, Any], field_definitions or {})  # type: ignore[no-redef]
-
-    if root:
-        if field_definitions:
-            msg = (
-                "When specifying __root__ no other "
-                f"fields should be provided. Got {field_definitions}"
-            )
-            raise NotImplementedError(msg)
-
-        if isinstance(root, tuple):
-            kwargs = {"type_": root[0], "default_": root[1]}
-        else:
-            kwargs = {"type_": root}
-
-        try:
-            named_root_model = _create_root_model_cached(
-                model_name, module_name=module_name, **kwargs
-            )
-        except TypeError:
-            # something in the arguments into _create_root_model_cached is not hashable
-            named_root_model = _create_root_model(
-                model_name,
-                module_name=module_name,
-                **kwargs,
-            )
-        return named_root_model
-
-    # No root, just field definitions
-    names = set(field_definitions.keys())
-
-    capture_warnings = False
-
-    for name in names:
-        # Also if any non-reserved name is used (e.g., model_id or model_name)
-        if name.startswith("model"):
-            capture_warnings = True
-
-    with warnings.catch_warnings() if capture_warnings else nullcontext():  # type: ignore[attr-defined]
-        if capture_warnings:
-            warnings.filterwarnings(action="ignore")
-        try:
-            return _create_model_cached(model_name, **field_definitions)
-        except TypeError:
-            # something in field definitions is not hashable
-            return _create_model_base(
-                model_name,
-                __config__=_SchemaConfig,
-                **_remap_field_definitions(field_definitions),
-            )
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/strings.py ./custom_langchain_core/utils/strings.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/strings.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/strings.py	1970-01-01 09:00:00
@@ -1,47 +0,0 @@
-from typing import Any
-
-
-def stringify_value(val: Any) -> str:
-    """Stringify a value.
-
-    Args:
-        val: The value to stringify.
-
-    Returns:
-        str: The stringified value.
-    """
-    if isinstance(val, str):
-        return val
-    elif isinstance(val, dict):
-        return "\n" + stringify_dict(val)
-    elif isinstance(val, list):
-        return "\n".join(stringify_value(v) for v in val)
-    else:
-        return str(val)
-
-
-def stringify_dict(data: dict) -> str:
-    """Stringify a dictionary.
-
-    Args:
-        data: The dictionary to stringify.
-
-    Returns:
-        str: The stringified dictionary.
-    """
-    text = ""
-    for key, value in data.items():
-        text += key + ": " + stringify_value(value) + "\n"
-    return text
-
-
-def comma_list(items: list[Any]) -> str:
-    """Convert a list to a comma-separated string.
-
-    Args:
-        items: The list to convert.
-
-    Returns:
-        str: The comma-separated string.
-    """
-    return ", ".join(str(item) for item in items)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/usage.py ./custom_langchain_core/utils/usage.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/usage.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/usage.py	1970-01-01 09:00:00
@@ -1,37 +0,0 @@
-from typing import Callable
-
-
-def _dict_int_op(
-    left: dict,
-    right: dict,
-    op: Callable[[int, int], int],
-    *,
-    default: int = 0,
-    depth: int = 0,
-    max_depth: int = 100,
-) -> dict:
-    if depth >= max_depth:
-        msg = f"{max_depth=} exceeded, unable to combine dicts."
-        raise ValueError(msg)
-    combined: dict = {}
-    for k in set(left).union(right):
-        if isinstance(left.get(k, default), int) and isinstance(
-            right.get(k, default), int
-        ):
-            combined[k] = op(left.get(k, default), right.get(k, default))
-        elif isinstance(left.get(k, {}), dict) and isinstance(right.get(k, {}), dict):
-            combined[k] = _dict_int_op(
-                left.get(k, {}),
-                right.get(k, {}),
-                op,
-                default=default,
-                depth=depth + 1,
-                max_depth=max_depth,
-            )
-        else:
-            types = [type(d[k]) for d in (left, right) if k in d]
-            msg = (
-                f"Unknown value types: {types}. Only dict and int values are supported."
-            )
-            raise ValueError(msg)  # noqa: TRY004
-    return combined
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/utils/utils.py ./custom_langchain_core/utils/utils.py
--- .venv/lib/python3.12/site-packages/langchain_core/utils/utils.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/utils/utils.py	1970-01-01 09:00:00
@@ -1,469 +0,0 @@
-"""Generic utility functions."""
-
-import contextlib
-import datetime
-import functools
-import importlib
-import os
-import warnings
-from collections.abc import Sequence
-from importlib.metadata import version
-from typing import Any, Callable, Optional, Union, overload
-
-from packaging.version import parse
-from pydantic import SecretStr
-from requests import HTTPError, Response
-
-from langchain_core.utils.pydantic import (
-    is_pydantic_v1_subclass,
-)
-
-
-def xor_args(*arg_groups: tuple[str, ...]) -> Callable:
-    """Validate specified keyword args are mutually exclusive.".
-
-    Args:
-        *arg_groups (Tuple[str, ...]): Groups of mutually exclusive keyword args.
-
-    Returns:
-        Callable: Decorator that validates the specified keyword args
-            are mutually exclusive
-
-    Raises:
-        ValueError: If more than one arg in a group is defined.
-    """
-
-    def decorator(func: Callable) -> Callable:
-        @functools.wraps(func)
-        def wrapper(*args: Any, **kwargs: Any) -> Any:
-            """Validate exactly one arg in each group is not None."""
-            counts = [
-                sum(1 for arg in arg_group if kwargs.get(arg) is not None)
-                for arg_group in arg_groups
-            ]
-            invalid_groups = [i for i, count in enumerate(counts) if count != 1]
-            if invalid_groups:
-                invalid_group_names = [", ".join(arg_groups[i]) for i in invalid_groups]
-                msg = (
-                    "Exactly one argument in each of the following"
-                    " groups must be defined:"
-                    f" {', '.join(invalid_group_names)}"
-                )
-                raise ValueError(msg)
-            return func(*args, **kwargs)
-
-        return wrapper
-
-    return decorator
-
-
-def raise_for_status_with_text(response: Response) -> None:
-    """Raise an error with the response text.
-
-    Args:
-        response (Response): The response to check for errors.
-
-    Raises:
-        ValueError: If the response has an error status code.
-    """
-    try:
-        response.raise_for_status()
-    except HTTPError as e:
-        raise ValueError(response.text) from e
-
-
-@contextlib.contextmanager
-def mock_now(dt_value):  # type: ignore
-    """Context manager for mocking out datetime.now() in unit tests.
-
-    Args:
-        dt_value: The datetime value to use for datetime.now().
-
-    Yields:
-        datetime.datetime: The mocked datetime class.
-
-    Example:
-    with mock_now(datetime.datetime(2011, 2, 3, 10, 11)):
-        assert datetime.datetime.now() == datetime.datetime(2011, 2, 3, 10, 11)
-    """
-
-    class MockDateTime(datetime.datetime):
-        """Mock datetime.datetime.now() with a fixed datetime."""
-
-        @classmethod
-        def now(cls):  # type: ignore
-            # Create a copy of dt_value.
-            return datetime.datetime(
-                dt_value.year,
-                dt_value.month,
-                dt_value.day,
-                dt_value.hour,
-                dt_value.minute,
-                dt_value.second,
-                dt_value.microsecond,
-                dt_value.tzinfo,
-            )
-
-    real_datetime = datetime.datetime
-    datetime.datetime = MockDateTime
-    try:
-        yield datetime.datetime
-    finally:
-        datetime.datetime = real_datetime
-
-
-def guard_import(
-    module_name: str, *, pip_name: Optional[str] = None, package: Optional[str] = None
-) -> Any:
-    """Dynamically import a module and raise an exception if the module is not
-    installed.
-
-    Args:
-        module_name (str): The name of the module to import.
-        pip_name (str, optional): The name of the module to install with pip.
-            Defaults to None.
-        package (str, optional): The package to import the module from.
-            Defaults to None.
-
-    Returns:
-        Any: The imported module.
-
-    Raises:
-        ImportError: If the module is not installed.
-    """
-    try:
-        module = importlib.import_module(module_name, package)
-    except (ImportError, ModuleNotFoundError) as e:
-        pip_name = pip_name or module_name.split(".")[0].replace("_", "-")
-        msg = (
-            f"Could not import {module_name} python package. "
-            f"Please install it with `pip install {pip_name}`."
-        )
-        raise ImportError(msg) from e
-    return module
-
-
-def check_package_version(
-    package: str,
-    lt_version: Optional[str] = None,
-    lte_version: Optional[str] = None,
-    gt_version: Optional[str] = None,
-    gte_version: Optional[str] = None,
-) -> None:
-    """Check the version of a package.
-
-    Args:
-        package (str): The name of the package.
-        lt_version (str, optional): The version must be less than this.
-            Defaults to None.
-        lte_version (str, optional): The version must be less than or equal to this.
-            Defaults to None.
-        gt_version (str, optional): The version must be greater than this.
-            Defaults to None.
-        gte_version (str, optional): The version must be greater than or equal to this.
-            Defaults to None.
-
-    Raises:
-        ValueError: If the package version does not meet the requirements.
-    """
-    imported_version = parse(version(package))
-    if lt_version is not None and imported_version >= parse(lt_version):
-        msg = (
-            f"Expected {package} version to be < {lt_version}. Received "
-            f"{imported_version}."
-        )
-        raise ValueError(msg)
-    if lte_version is not None and imported_version > parse(lte_version):
-        msg = (
-            f"Expected {package} version to be <= {lte_version}. Received "
-            f"{imported_version}."
-        )
-        raise ValueError(msg)
-    if gt_version is not None and imported_version <= parse(gt_version):
-        msg = (
-            f"Expected {package} version to be > {gt_version}. Received "
-            f"{imported_version}."
-        )
-        raise ValueError(msg)
-    if gte_version is not None and imported_version < parse(gte_version):
-        msg = (
-            f"Expected {package} version to be >= {gte_version}. Received "
-            f"{imported_version}."
-        )
-        raise ValueError(msg)
-
-
-def get_pydantic_field_names(pydantic_cls: Any) -> set[str]:
-    """Get field names, including aliases, for a pydantic class.
-
-    Args:
-        pydantic_cls: Pydantic class.
-
-    Returns:
-        Set[str]: Field names.
-    """
-    all_required_field_names = set()
-    if is_pydantic_v1_subclass(pydantic_cls):
-        for field in pydantic_cls.__fields__.values():
-            all_required_field_names.add(field.name)
-            if field.has_alias:
-                all_required_field_names.add(field.alias)
-    else:  # Assuming pydantic 2 for now
-        for name, field in pydantic_cls.model_fields.items():
-            all_required_field_names.add(name)
-            if field.alias:
-                all_required_field_names.add(field.alias)
-    return all_required_field_names
-
-
-def _build_model_kwargs(
-    values: dict[str, Any],
-    all_required_field_names: set[str],
-) -> dict[str, Any]:
-    """Build "model_kwargs" param from Pydanitc constructor values.
-
-    Args:
-        values: All init args passed in by user.
-        all_required_field_names: All required field names for the pydantic class.
-
-    Returns:
-        Dict[str, Any]: Extra kwargs.
-
-    Raises:
-        ValueError: If a field is specified in both values and extra_kwargs.
-        ValueError: If a field is specified in model_kwargs.
-    """
-    extra_kwargs = values.get("model_kwargs", {})
-    for field_name in list(values):
-        if field_name in extra_kwargs:
-            msg = f"Found {field_name} supplied twice."
-            raise ValueError(msg)
-        if field_name not in all_required_field_names:
-            warnings.warn(
-                f"""WARNING! {field_name} is not default parameter.
-                {field_name} was transferred to model_kwargs.
-                Please confirm that {field_name} is what you intended.""",
-                stacklevel=7,
-            )
-            extra_kwargs[field_name] = values.pop(field_name)
-
-    invalid_model_kwargs = all_required_field_names.intersection(extra_kwargs.keys())
-    if invalid_model_kwargs:
-        warnings.warn(
-            f"Parameters {invalid_model_kwargs} should be specified explicitly. "
-            f"Instead they were passed in as part of `model_kwargs` parameter.",
-            stacklevel=7,
-        )
-        for k in invalid_model_kwargs:
-            values[k] = extra_kwargs.pop(k)
-
-    values["model_kwargs"] = extra_kwargs
-    return values
-
-
-# DON'T USE! Kept for backwards-compatibility but should never have been public.
-def build_extra_kwargs(
-    extra_kwargs: dict[str, Any],
-    values: dict[str, Any],
-    all_required_field_names: set[str],
-) -> dict[str, Any]:
-    """Build extra kwargs from values and extra_kwargs.
-
-    Args:
-        extra_kwargs: Extra kwargs passed in by user.
-        values: Values passed in by user.
-        all_required_field_names: All required field names for the pydantic class.
-
-    Returns:
-        Dict[str, Any]: Extra kwargs.
-
-    Raises:
-        ValueError: If a field is specified in both values and extra_kwargs.
-        ValueError: If a field is specified in model_kwargs.
-    """
-    for field_name in list(values):
-        if field_name in extra_kwargs:
-            msg = f"Found {field_name} supplied twice."
-            raise ValueError(msg)
-        if field_name not in all_required_field_names:
-            warnings.warn(
-                f"""WARNING! {field_name} is not default parameter.
-                {field_name} was transferred to model_kwargs.
-                Please confirm that {field_name} is what you intended.""",
-                stacklevel=7,
-            )
-            extra_kwargs[field_name] = values.pop(field_name)
-
-    invalid_model_kwargs = all_required_field_names.intersection(extra_kwargs.keys())
-    if invalid_model_kwargs:
-        msg = (
-            f"Parameters {invalid_model_kwargs} should be specified explicitly. "
-            f"Instead they were passed in as part of `model_kwargs` parameter."
-        )
-        raise ValueError(msg)
-
-    return extra_kwargs
-
-
-def convert_to_secret_str(value: Union[SecretStr, str]) -> SecretStr:
-    """Convert a string to a SecretStr if needed.
-
-    Args:
-        value (Union[SecretStr, str]): The value to convert.
-
-    Returns:
-        SecretStr: The SecretStr value.
-    """
-    if isinstance(value, SecretStr):
-        return value
-    return SecretStr(value)
-
-
-class _NoDefaultType:
-    """Type to indicate no default value is provided."""
-
-
-_NoDefault = _NoDefaultType()
-
-
-@overload
-def from_env(key: str, /) -> Callable[[], str]: ...
-
-
-@overload
-def from_env(key: str, /, *, default: str) -> Callable[[], str]: ...
-
-
-@overload
-def from_env(key: Sequence[str], /, *, default: str) -> Callable[[], str]: ...
-
-
-@overload
-def from_env(key: str, /, *, error_message: str) -> Callable[[], str]: ...
-
-
-@overload
-def from_env(
-    key: Union[str, Sequence[str]], /, *, default: str, error_message: Optional[str]
-) -> Callable[[], str]: ...
-
-
-@overload
-def from_env(
-    key: str, /, *, default: None, error_message: Optional[str]
-) -> Callable[[], Optional[str]]: ...
-
-
-@overload
-def from_env(
-    key: Union[str, Sequence[str]], /, *, default: None
-) -> Callable[[], Optional[str]]: ...
-
-
-def from_env(
-    key: Union[str, Sequence[str]],
-    /,
-    *,
-    default: Union[str, _NoDefaultType, None] = _NoDefault,
-    error_message: Optional[str] = None,
-) -> Union[Callable[[], str], Callable[[], Optional[str]]]:
-    """Create a factory method that gets a value from an environment variable.
-
-    Args:
-        key: The environment variable to look up. If a list of keys is provided,
-            the first key found in the environment will be used.
-            If no key is found, the default value will be used if set,
-            otherwise an error will be raised.
-        default: The default value to return if the environment variable is not set.
-        error_message: the error message which will be raised if the key is not found
-            and no default value is provided.
-            This will be raised as a ValueError.
-    """
-
-    def get_from_env_fn() -> Optional[str]:
-        """Get a value from an environment variable."""
-        if isinstance(key, (list, tuple)):
-            for k in key:
-                if k in os.environ:
-                    return os.environ[k]
-        if isinstance(key, str) and key in os.environ:
-            return os.environ[key]
-
-        if isinstance(default, (str, type(None))):
-            return default
-        else:
-            if error_message:
-                raise ValueError(error_message)
-            else:
-                msg = (
-                    f"Did not find {key}, please add an environment variable"
-                    f" `{key}` which contains it, or pass"
-                    f" `{key}` as a named parameter."
-                )
-                raise ValueError(msg)
-
-    return get_from_env_fn
-
-
-@overload
-def secret_from_env(key: Union[str, Sequence[str]], /) -> Callable[[], SecretStr]: ...
-
-
-@overload
-def secret_from_env(key: str, /, *, default: str) -> Callable[[], SecretStr]: ...
-
-
-@overload
-def secret_from_env(
-    key: Union[str, Sequence[str]], /, *, default: None
-) -> Callable[[], Optional[SecretStr]]: ...
-
-
-@overload
-def secret_from_env(key: str, /, *, error_message: str) -> Callable[[], SecretStr]: ...
-
-
-def secret_from_env(
-    key: Union[str, Sequence[str]],
-    /,
-    *,
-    default: Union[str, _NoDefaultType, None] = _NoDefault,
-    error_message: Optional[str] = None,
-) -> Union[Callable[[], Optional[SecretStr]], Callable[[], SecretStr]]:
-    """Secret from env.
-
-    Args:
-        key: The environment variable to look up.
-        default: The default value to return if the environment variable is not set.
-        error_message: the error message which will be raised if the key is not found
-            and no default value is provided.
-            This will be raised as a ValueError.
-
-    Returns:
-        factory method that will look up the secret from the environment.
-    """
-
-    def get_secret_from_env() -> Optional[SecretStr]:
-        """Get a value from an environment variable."""
-        if isinstance(key, (list, tuple)):
-            for k in key:
-                if k in os.environ:
-                    return SecretStr(os.environ[k])
-        if isinstance(key, str) and key in os.environ:
-            return SecretStr(os.environ[key])
-        if isinstance(default, str):
-            return SecretStr(default)
-        elif default is None:
-            return None
-        else:
-            if error_message:
-                raise ValueError(error_message)
-            else:
-                msg = (
-                    f"Did not find {key}, please add an environment variable"
-                    f" `{key}` which contains it, or pass"
-                    f" `{key}` as a named parameter."
-                )
-                raise ValueError(msg)
-
-    return get_secret_from_env
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/vectorstores/__init__.py ./custom_langchain_core/vectorstores/__init__.py
--- .venv/lib/python3.12/site-packages/langchain_core/vectorstores/__init__.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/vectorstores/__init__.py	1970-01-01 09:00:00
@@ -1,9 +0,0 @@
-from langchain_core.vectorstores.base import VST, VectorStore, VectorStoreRetriever
-from langchain_core.vectorstores.in_memory import InMemoryVectorStore
-
-__all__ = [
-    "VectorStore",
-    "VST",
-    "VectorStoreRetriever",
-    "InMemoryVectorStore",
-]
Binary files .venv/lib/python3.12/site-packages/langchain_core/vectorstores/__pycache__/__init__.cpython-312.pyc and ./custom_langchain_core/vectorstores/__pycache__/__init__.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/vectorstores/__pycache__/base.cpython-312.pyc and ./custom_langchain_core/vectorstores/__pycache__/base.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/vectorstores/__pycache__/in_memory.cpython-312.pyc and ./custom_langchain_core/vectorstores/__pycache__/in_memory.cpython-312.pyc differ
Binary files .venv/lib/python3.12/site-packages/langchain_core/vectorstores/__pycache__/utils.cpython-312.pyc and ./custom_langchain_core/vectorstores/__pycache__/utils.cpython-312.pyc differ
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py ./custom_langchain_core/vectorstores/base.py
--- .venv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py	2025-02-22 15:01:34
+++ ./custom_langchain_core/vectorstores/base.py	1970-01-01 09:00:00
@@ -1,1146 +0,0 @@
-"""**Vector store** stores embedded data and performs vector search.
-
-One of the most common ways to store and search over unstructured data is to
-embed it and store the resulting embedding vectors, and then query the store
-and retrieve the data that are 'most similar' to the embedded query.
-
-**Class hierarchy:**
-
-.. code-block::
-
-    VectorStore --> <name>  # Examples: Annoy, FAISS, Milvus
-
-    BaseRetriever --> VectorStoreRetriever --> <name>Retriever  # Example: VespaRetriever
-
-**Main helpers:**
-
-.. code-block::
-
-    Embeddings, Document
-"""  # noqa: E501
-
-from __future__ import annotations
-
-import logging
-import math
-import warnings
-from abc import ABC, abstractmethod
-from collections.abc import Collection, Iterable, Iterator, Sequence
-from itertools import cycle
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    ClassVar,
-    Optional,
-    TypeVar,
-)
-
-from pydantic import ConfigDict, Field, model_validator
-
-from langchain_core.embeddings import Embeddings
-from langchain_core.retrievers import BaseRetriever, LangSmithRetrieverParams
-from langchain_core.runnables.config import run_in_executor
-
-if TYPE_CHECKING:
-    from langchain_core.callbacks.manager import (
-        AsyncCallbackManagerForRetrieverRun,
-        CallbackManagerForRetrieverRun,
-    )
-    from langchain_core.documents import Document
-
-logger = logging.getLogger(__name__)
-
-VST = TypeVar("VST", bound="VectorStore")
-
-
-class VectorStore(ABC):
-    """Interface for vector store."""
-
-    def add_texts(
-        self,
-        texts: Iterable[str],
-        metadatas: Optional[list[dict]] = None,
-        *,
-        ids: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> list[str]:
-        """Run more texts through the embeddings and add to the vectorstore.
-
-        Args:
-            texts: Iterable of strings to add to the vectorstore.
-            metadatas: Optional list of metadatas associated with the texts.
-            ids: Optional list of IDs associated with the texts.
-            **kwargs: vectorstore specific parameters.
-                One of the kwargs should be `ids` which is a list of ids
-                associated with the texts.
-
-        Returns:
-            List of ids from adding the texts into the vectorstore.
-
-        Raises:
-            ValueError: If the number of metadatas does not match the number of texts.
-            ValueError: If the number of ids does not match the number of texts.
-        """
-        if type(self).add_documents != VectorStore.add_documents:
-            # Import document in local scope to avoid circular imports
-            from langchain_core.documents import Document
-
-            # This condition is triggered if the subclass has provided
-            # an implementation of the upsert method.
-            # The existing add_texts
-            texts_: Sequence[str] = (
-                texts if isinstance(texts, (list, tuple)) else list(texts)
-            )
-            if metadatas and len(metadatas) != len(texts_):
-                msg = (
-                    "The number of metadatas must match the number of texts."
-                    f"Got {len(metadatas)} metadatas and {len(texts_)} texts."
-                )
-                raise ValueError(msg)
-            metadatas_ = iter(metadatas) if metadatas else cycle([{}])
-            ids_: Iterator[Optional[str]] = iter(ids) if ids else cycle([None])
-            docs = [
-                Document(id=id_, page_content=text, metadata=metadata_)
-                for text, metadata_, id_ in zip(texts, metadatas_, ids_)
-            ]
-            if ids is not None:
-                # For backward compatibility
-                kwargs["ids"] = ids
-
-            return self.add_documents(docs, **kwargs)
-        msg = f"`add_texts` has not been implemented for {self.__class__.__name__} "
-        raise NotImplementedError(msg)
-
-    @property
-    def embeddings(self) -> Optional[Embeddings]:
-        """Access the query embedding object if available."""
-        logger.debug(
-            f"The embeddings property has not been "
-            f"implemented for {self.__class__.__name__}"
-        )
-        return None
-
-    def delete(self, ids: Optional[list[str]] = None, **kwargs: Any) -> Optional[bool]:
-        """Delete by vector ID or other criteria.
-
-        Args:
-            ids: List of ids to delete. If None, delete all. Default is None.
-            **kwargs: Other keyword arguments that subclasses might use.
-
-        Returns:
-            Optional[bool]: True if deletion is successful,
-            False otherwise, None if not implemented.
-        """
-        msg = "delete method must be implemented by subclass."
-        raise NotImplementedError(msg)
-
-    def get_by_ids(self, ids: Sequence[str], /) -> list[Document]:
-        """Get documents by their IDs.
-
-        The returned documents are expected to have the ID field set to the ID of the
-        document in the vector store.
-
-        Fewer documents may be returned than requested if some IDs are not found or
-        if there are duplicated IDs.
-
-        Users should not assume that the order of the returned documents matches
-        the order of the input IDs. Instead, users should rely on the ID field of the
-        returned documents.
-
-        This method should **NOT** raise exceptions if no documents are found for
-        some IDs.
-
-        Args:
-            ids: List of ids to retrieve.
-
-        Returns:
-            List of Documents.
-
-        .. versionadded:: 0.2.11
-        """
-        msg = f"{self.__class__.__name__} does not yet support get_by_ids."
-        raise NotImplementedError(msg)
-
-    # Implementations should override this method to provide an async native version.
-    async def aget_by_ids(self, ids: Sequence[str], /) -> list[Document]:
-        """Async get documents by their IDs.
-
-        The returned documents are expected to have the ID field set to the ID of the
-        document in the vector store.
-
-        Fewer documents may be returned than requested if some IDs are not found or
-        if there are duplicated IDs.
-
-        Users should not assume that the order of the returned documents matches
-        the order of the input IDs. Instead, users should rely on the ID field of the
-        returned documents.
-
-        This method should **NOT** raise exceptions if no documents are found for
-        some IDs.
-
-        Args:
-            ids: List of ids to retrieve.
-
-        Returns:
-            List of Documents.
-
-        .. versionadded:: 0.2.11
-        """
-        return await run_in_executor(None, self.get_by_ids, ids)
-
-    async def adelete(
-        self, ids: Optional[list[str]] = None, **kwargs: Any
-    ) -> Optional[bool]:
-        """Async delete by vector ID or other criteria.
-
-        Args:
-            ids: List of ids to delete. If None, delete all. Default is None.
-            **kwargs: Other keyword arguments that subclasses might use.
-
-        Returns:
-            Optional[bool]: True if deletion is successful,
-            False otherwise, None if not implemented.
-        """
-        return await run_in_executor(None, self.delete, ids, **kwargs)
-
-    async def aadd_texts(
-        self,
-        texts: Iterable[str],
-        metadatas: Optional[list[dict]] = None,
-        *,
-        ids: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> list[str]:
-        """Async run more texts through the embeddings and add to the vectorstore.
-
-        Args:
-            texts: Iterable of strings to add to the vectorstore.
-            metadatas: Optional list of metadatas associated with the texts.
-                Default is None.
-            ids: Optional list
-            **kwargs: vectorstore specific parameters.
-
-        Returns:
-            List of ids from adding the texts into the vectorstore.
-
-        Raises:
-            ValueError: If the number of metadatas does not match the number of texts.
-            ValueError: If the number of ids does not match the number of texts.
-        """
-        if ids is not None:
-            # For backward compatibility
-            kwargs["ids"] = ids
-        if type(self).aadd_documents != VectorStore.aadd_documents:
-            # Import document in local scope to avoid circular imports
-            from langchain_core.documents import Document
-
-            # This condition is triggered if the subclass has provided
-            # an implementation of the upsert method.
-            # The existing add_texts
-            texts_: Sequence[str] = (
-                texts if isinstance(texts, (list, tuple)) else list(texts)
-            )
-            if metadatas and len(metadatas) != len(texts_):
-                msg = (
-                    "The number of metadatas must match the number of texts."
-                    f"Got {len(metadatas)} metadatas and {len(texts_)} texts."
-                )
-                raise ValueError(msg)
-            metadatas_ = iter(metadatas) if metadatas else cycle([{}])
-            ids_: Iterator[Optional[str]] = iter(ids) if ids else cycle([None])
-
-            docs = [
-                Document(id=id_, page_content=text, metadata=metadata_)
-                for text, metadata_, id_ in zip(texts, metadatas_, ids_)
-            ]
-            return await self.aadd_documents(docs, **kwargs)
-        return await run_in_executor(None, self.add_texts, texts, metadatas, **kwargs)
-
-    def add_documents(self, documents: list[Document], **kwargs: Any) -> list[str]:
-        """Add or update documents in the vectorstore.
-
-        Args:
-            documents: Documents to add to the vectorstore.
-            kwargs: Additional keyword arguments.
-                if kwargs contains ids and documents contain ids,
-                the ids in the kwargs will receive precedence.
-
-        Returns:
-            List of IDs of the added texts.
-
-        Raises:
-            ValueError: If the number of ids does not match the number of documents.
-        """
-        if type(self).add_texts != VectorStore.add_texts:
-            if "ids" not in kwargs:
-                ids = [doc.id for doc in documents]
-
-                # If there's at least one valid ID, we'll assume that IDs
-                # should be used.
-                if any(ids):
-                    kwargs["ids"] = ids
-
-            texts = [doc.page_content for doc in documents]
-            metadatas = [doc.metadata for doc in documents]
-            return self.add_texts(texts, metadatas, **kwargs)
-        msg = (
-            f"`add_documents` and `add_texts` has not been implemented "
-            f"for {self.__class__.__name__} "
-        )
-        raise NotImplementedError(msg)
-
-    async def aadd_documents(
-        self, documents: list[Document], **kwargs: Any
-    ) -> list[str]:
-        """Async run more documents through the embeddings and add to
-        the vectorstore.
-
-        Args:
-            documents: Documents to add to the vectorstore.
-            kwargs: Additional keyword arguments.
-
-        Returns:
-            List of IDs of the added texts.
-
-        Raises:
-            ValueError: If the number of IDs does not match the number of documents.
-        """
-        # If the async method has been overridden, we'll use that.
-        if type(self).aadd_texts != VectorStore.aadd_texts:
-            if "ids" not in kwargs:
-                ids = [doc.id for doc in documents]
-
-                # If there's at least one valid ID, we'll assume that IDs
-                # should be used.
-                if any(ids):
-                    kwargs["ids"] = ids
-
-            texts = [doc.page_content for doc in documents]
-            metadatas = [doc.metadata for doc in documents]
-            return await self.aadd_texts(texts, metadatas, **kwargs)
-
-        return await run_in_executor(None, self.add_documents, documents, **kwargs)
-
-    def search(self, query: str, search_type: str, **kwargs: Any) -> list[Document]:
-        """Return docs most similar to query using a specified search type.
-
-        Args:
-            query: Input text
-            search_type: Type of search to perform. Can be "similarity",
-                "mmr", or "similarity_score_threshold".
-            **kwargs: Arguments to pass to the search method.
-
-        Returns:
-            List of Documents most similar to the query.
-
-        Raises:
-            ValueError: If search_type is not one of "similarity",
-                "mmr", or "similarity_score_threshold".
-        """
-        if search_type == "similarity":
-            return self.similarity_search(query, **kwargs)
-        elif search_type == "similarity_score_threshold":
-            docs_and_similarities = self.similarity_search_with_relevance_scores(
-                query, **kwargs
-            )
-            return [doc for doc, _ in docs_and_similarities]
-        elif search_type == "mmr":
-            return self.max_marginal_relevance_search(query, **kwargs)
-        else:
-            msg = (
-                f"search_type of {search_type} not allowed. Expected "
-                "search_type to be 'similarity', 'similarity_score_threshold'"
-                " or 'mmr'."
-            )
-            raise ValueError(msg)
-
-    async def asearch(
-        self, query: str, search_type: str, **kwargs: Any
-    ) -> list[Document]:
-        """Async return docs most similar to query using a specified search type.
-
-        Args:
-            query: Input text.
-            search_type: Type of search to perform. Can be "similarity",
-                "mmr", or "similarity_score_threshold".
-            **kwargs: Arguments to pass to the search method.
-
-        Returns:
-            List of Documents most similar to the query.
-
-        Raises:
-            ValueError: If search_type is not one of "similarity",
-                "mmr", or "similarity_score_threshold".
-        """
-        if search_type == "similarity":
-            return await self.asimilarity_search(query, **kwargs)
-        elif search_type == "similarity_score_threshold":
-            docs_and_similarities = await self.asimilarity_search_with_relevance_scores(
-                query, **kwargs
-            )
-            return [doc for doc, _ in docs_and_similarities]
-        elif search_type == "mmr":
-            return await self.amax_marginal_relevance_search(query, **kwargs)
-        else:
-            msg = (
-                f"search_type of {search_type} not allowed. Expected "
-                "search_type to be 'similarity', 'similarity_score_threshold' or 'mmr'."
-            )
-            raise ValueError(msg)
-
-    @abstractmethod
-    def similarity_search(
-        self, query: str, k: int = 4, **kwargs: Any
-    ) -> list[Document]:
-        """Return docs most similar to query.
-
-        Args:
-            query: Input text.
-            k: Number of Documents to return. Defaults to 4.
-            **kwargs: Arguments to pass to the search method.
-
-        Returns:
-            List of Documents most similar to the query.
-        """
-
-    @staticmethod
-    def _euclidean_relevance_score_fn(distance: float) -> float:
-        """Return a similarity score on a scale [0, 1]."""
-        # The 'correct' relevance function
-        # may differ depending on a few things, including:
-        # - the distance / similarity metric used by the VectorStore
-        # - the scale of your embeddings (OpenAI's are unit normed. Many
-        #  others are not!)
-        # - embedding dimensionality
-        # - etc.
-        # This function converts the Euclidean norm of normalized embeddings
-        # (0 is most similar, sqrt(2) most dissimilar)
-        # to a similarity function (0 to 1)
-        return 1.0 - distance / math.sqrt(2)
-
-    @staticmethod
-    def _cosine_relevance_score_fn(distance: float) -> float:
-        """Normalize the distance to a score on a scale [0, 1]."""
-        return 1.0 - distance
-
-    @staticmethod
-    def _max_inner_product_relevance_score_fn(distance: float) -> float:
-        """Normalize the distance to a score on a scale [0, 1]."""
-        if distance > 0:
-            return 1.0 - distance
-
-        return -1.0 * distance
-
-    def _select_relevance_score_fn(self) -> Callable[[float], float]:
-        """The 'correct' relevance function
-        may differ depending on a few things, including:
-        - the distance / similarity metric used by the VectorStore
-        - the scale of your embeddings (OpenAI's are unit normed. Many others are not!)
-        - embedding dimensionality
-        - etc.
-
-        Vectorstores should define their own selection-based method of relevance.
-        """
-        raise NotImplementedError
-
-    def similarity_search_with_score(
-        self, *args: Any, **kwargs: Any
-    ) -> list[tuple[Document, float]]:
-        """Run similarity search with distance.
-
-        Args:
-            *args: Arguments to pass to the search method.
-            **kwargs: Arguments to pass to the search method.
-
-        Returns:
-            List of Tuples of (doc, similarity_score).
-        """
-        raise NotImplementedError
-
-    async def asimilarity_search_with_score(
-        self, *args: Any, **kwargs: Any
-    ) -> list[tuple[Document, float]]:
-        """Async run similarity search with distance.
-
-        Args:
-            *args: Arguments to pass to the search method.
-            **kwargs: Arguments to pass to the search method.
-
-        Returns:
-            List of Tuples of (doc, similarity_score).
-        """
-        # This is a temporary workaround to make the similarity search
-        # asynchronous. The proper solution is to make the similarity search
-        # asynchronous in the vector store implementations.
-        return await run_in_executor(
-            None, self.similarity_search_with_score, *args, **kwargs
-        )
-
-    def _similarity_search_with_relevance_scores(
-        self,
-        query: str,
-        k: int = 4,
-        **kwargs: Any,
-    ) -> list[tuple[Document, float]]:
-        """Default similarity search with relevance scores. Modify if necessary
-        in subclass.
-        Return docs and relevance scores in the range [0, 1].
-
-        0 is dissimilar, 1 is most similar.
-
-        Args:
-            query: Input text.
-            k: Number of Documents to return. Defaults to 4.
-            **kwargs: kwargs to be passed to similarity search. Should include:
-                score_threshold: Optional, a floating point value between 0 to 1 to
-                    filter the resulting set of retrieved docs
-
-        Returns:
-            List of Tuples of (doc, similarity_score)
-        """
-        relevance_score_fn = self._select_relevance_score_fn()
-        docs_and_scores = self.similarity_search_with_score(query, k, **kwargs)
-        print("docs_and_scores")
-        print(docs_and_scores)
-        for doc, score in docs_and_scores:
-            doc.metadata["similarity_score"] = score
-            doc.metadata["relevance_score"] = relevance_score_fn(
-                score)
-        return [(doc, relevance_score_fn(score)) for doc, score in docs_and_scores]
-
-    async def _asimilarity_search_with_relevance_scores(
-        self,
-        query: str,
-        k: int = 4,
-        **kwargs: Any,
-    ) -> list[tuple[Document, float]]:
-        """Default similarity search with relevance scores. Modify if necessary
-        in subclass.
-        Return docs and relevance scores in the range [0, 1].
-
-        0 is dissimilar, 1 is most similar.
-
-        Args:
-            query: Input text.
-            k: Number of Documents to return. Defaults to 4.
-            **kwargs: kwargs to be passed to similarity search. Should include:
-                score_threshold: Optional, a floating point value between 0 to 1 to
-                    filter the resulting set of retrieved docs
-
-        Returns:
-            List of Tuples of (doc, similarity_score)
-        """
-        relevance_score_fn = self._select_relevance_score_fn()
-        docs_and_scores = await self.asimilarity_search_with_score(query, k, **kwargs)
-        return [(doc, relevance_score_fn(score)) for doc, score in docs_and_scores]
-
-    def similarity_search_with_relevance_scores(
-        self,
-        query: str,
-        k: int = 4,
-        **kwargs: Any,
-    ) -> list[tuple[Document, float]]:
-        """Return docs and relevance scores in the range [0, 1].
-
-        0 is dissimilar, 1 is most similar.
-
-        Args:
-            query: Input text.
-            k: Number of Documents to return. Defaults to 4.
-            **kwargs: kwargs to be passed to similarity search. Should include:
-                score_threshold: Optional, a floating point value between 0 to 1 to
-                    filter the resulting set of retrieved docs.
-
-        Returns:
-            List of Tuples of (doc, similarity_score).
-        """
-        score_threshold = kwargs.pop("score_threshold", None)
-
-        docs_and_similarities = self._similarity_search_with_relevance_scores(
-            query, k=k, **kwargs
-        )
-        if any(
-            similarity < 0.0 or similarity > 1.0
-            for _, similarity in docs_and_similarities
-        ):
-            warnings.warn(
-                "Relevance scores must be between"
-                f" 0 and 1, got {docs_and_similarities}",
-                stacklevel=2,
-            )
-
-        if score_threshold is not None:
-            docs_and_similarities = [
-                (doc, similarity)
-                for doc, similarity in docs_and_similarities
-                if similarity >= score_threshold
-            ]
-            if len(docs_and_similarities) == 0:
-                logger.warning(
-                    "No relevant docs were retrieved using the relevance score"
-                    f" threshold {score_threshold}"
-                )
-        return docs_and_similarities
-
-    async def asimilarity_search_with_relevance_scores(
-        self,
-        query: str,
-        k: int = 4,
-        **kwargs: Any,
-    ) -> list[tuple[Document, float]]:
-        """Async return docs and relevance scores in the range [0, 1].
-
-        0 is dissimilar, 1 is most similar.
-
-        Args:
-            query: Input text.
-            k: Number of Documents to return. Defaults to 4.
-            **kwargs: kwargs to be passed to similarity search. Should include:
-                score_threshold: Optional, a floating point value between 0 to 1 to
-                    filter the resulting set of retrieved docs
-
-        Returns:
-            List of Tuples of (doc, similarity_score)
-        """
-        score_threshold = kwargs.pop("score_threshold", None)
-
-        docs_and_similarities = await self._asimilarity_search_with_relevance_scores(
-            query, k=k, **kwargs
-        )
-        if any(
-            similarity < 0.0 or similarity > 1.0
-            for _, similarity in docs_and_similarities
-        ):
-            warnings.warn(
-                "Relevance scores must be between"
-                f" 0 and 1, got {docs_and_similarities}",
-                stacklevel=2,
-            )
-
-        if score_threshold is not None:
-            docs_and_similarities = [
-                (doc, similarity)
-                for doc, similarity in docs_and_similarities
-                if similarity >= score_threshold
-            ]
-            if len(docs_and_similarities) == 0:
-                logger.warning(
-                    "No relevant docs were retrieved using the relevance score"
-                    f" threshold {score_threshold}"
-                )
-        return docs_and_similarities
-
-    async def asimilarity_search(
-        self, query: str, k: int = 4, **kwargs: Any
-    ) -> list[Document]:
-        """Async return docs most similar to query.
-
-        Args:
-            query: Input text.
-            k: Number of Documents to return. Defaults to 4.
-            **kwargs: Arguments to pass to the search method.
-
-        Returns:
-            List of Documents most similar to the query.
-        """
-        # This is a temporary workaround to make the similarity search
-        # asynchronous. The proper solution is to make the similarity search
-        # asynchronous in the vector store implementations.
-        return await run_in_executor(None, self.similarity_search, query, k=k, **kwargs)
-
-    def similarity_search_by_vector(
-        self, embedding: list[float], k: int = 4, **kwargs: Any
-    ) -> list[Document]:
-        """Return docs most similar to embedding vector.
-
-        Args:
-            embedding: Embedding to look up documents similar to.
-            k: Number of Documents to return. Defaults to 4.
-            **kwargs: Arguments to pass to the search method.
-
-        Returns:
-            List of Documents most similar to the query vector.
-        """
-        raise NotImplementedError
-
-    async def asimilarity_search_by_vector(
-        self, embedding: list[float], k: int = 4, **kwargs: Any
-    ) -> list[Document]:
-        """Async return docs most similar to embedding vector.
-
-        Args:
-            embedding: Embedding to look up documents similar to.
-            k: Number of Documents to return. Defaults to 4.
-            **kwargs: Arguments to pass to the search method.
-
-        Returns:
-            List of Documents most similar to the query vector.
-        """
-        # This is a temporary workaround to make the similarity search
-        # asynchronous. The proper solution is to make the similarity search
-        # asynchronous in the vector store implementations.
-        return await run_in_executor(
-            None, self.similarity_search_by_vector, embedding, k=k, **kwargs
-        )
-
-    def max_marginal_relevance_search(
-        self,
-        query: str,
-        k: int = 4,
-        fetch_k: int = 20,
-        lambda_mult: float = 0.5,
-        **kwargs: Any,
-    ) -> list[Document]:
-        """Return docs selected using the maximal marginal relevance.
-
-        Maximal marginal relevance optimizes for similarity to query AND diversity
-        among selected documents.
-
-        Args:
-            query: Text to look up documents similar to.
-            k: Number of Documents to return. Defaults to 4.
-            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
-                Default is 20.
-            lambda_mult: Number between 0 and 1 that determines the degree
-                of diversity among the results with 0 corresponding
-                to maximum diversity and 1 to minimum diversity.
-                Defaults to 0.5.
-            **kwargs: Arguments to pass to the search method.
-
-        Returns:
-            List of Documents selected by maximal marginal relevance.
-        """
-        raise NotImplementedError
-
-    async def amax_marginal_relevance_search(
-        self,
-        query: str,
-        k: int = 4,
-        fetch_k: int = 20,
-        lambda_mult: float = 0.5,
-        **kwargs: Any,
-    ) -> list[Document]:
-        """Async return docs selected using the maximal marginal relevance.
-
-        Maximal marginal relevance optimizes for similarity to query AND diversity
-        among selected documents.
-
-        Args:
-            query: Text to look up documents similar to.
-            k: Number of Documents to return. Defaults to 4.
-            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
-                Default is 20.
-            lambda_mult: Number between 0 and 1 that determines the degree
-                of diversity among the results with 0 corresponding
-                to maximum diversity and 1 to minimum diversity.
-                Defaults to 0.5.
-
-        Returns:
-            List of Documents selected by maximal marginal relevance.
-        """
-        # This is a temporary workaround to make the similarity search
-        # asynchronous. The proper solution is to make the similarity search
-        # asynchronous in the vector store implementations.
-        return await run_in_executor(
-            None,
-            self.max_marginal_relevance_search,
-            query,
-            k=k,
-            fetch_k=fetch_k,
-            lambda_mult=lambda_mult,
-            **kwargs,
-        )
-
-    def max_marginal_relevance_search_by_vector(
-        self,
-        embedding: list[float],
-        k: int = 4,
-        fetch_k: int = 20,
-        lambda_mult: float = 0.5,
-        **kwargs: Any,
-    ) -> list[Document]:
-        """Return docs selected using the maximal marginal relevance.
-
-        Maximal marginal relevance optimizes for similarity to query AND diversity
-        among selected documents.
-
-        Args:
-            embedding: Embedding to look up documents similar to.
-            k: Number of Documents to return. Defaults to 4.
-            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
-                Default is 20.
-            lambda_mult: Number between 0 and 1 that determines the degree
-                of diversity among the results with 0 corresponding
-                to maximum diversity and 1 to minimum diversity.
-                Defaults to 0.5.
-            **kwargs: Arguments to pass to the search method.
-
-        Returns:
-            List of Documents selected by maximal marginal relevance.
-        """
-        raise NotImplementedError
-
-    async def amax_marginal_relevance_search_by_vector(
-        self,
-        embedding: list[float],
-        k: int = 4,
-        fetch_k: int = 20,
-        lambda_mult: float = 0.5,
-        **kwargs: Any,
-    ) -> list[Document]:
-        """Async return docs selected using the maximal marginal relevance.
-
-        Maximal marginal relevance optimizes for similarity to query AND diversity
-        among selected documents.
-
-        Args:
-            embedding: Embedding to look up documents similar to.
-            k: Number of Documents to return. Defaults to 4.
-            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
-                Default is 20.
-            lambda_mult: Number between 0 and 1 that determines the degree
-                of diversity among the results with 0 corresponding
-                to maximum diversity and 1 to minimum diversity.
-                Defaults to 0.5.
-            **kwargs: Arguments to pass to the search method.
-
-        Returns:
-            List of Documents selected by maximal marginal relevance.
-        """
-        return await run_in_executor(
-            None,
-            self.max_marginal_relevance_search_by_vector,
-            embedding,
-            k=k,
-            fetch_k=fetch_k,
-            lambda_mult=lambda_mult,
-            **kwargs,
-        )
-
-    @classmethod
-    def from_documents(
-        cls: type[VST],
-        documents: list[Document],
-        embedding: Embeddings,
-        **kwargs: Any,
-    ) -> VST:
-        """Return VectorStore initialized from documents and embeddings.
-
-        Args:
-            documents: List of Documents to add to the vectorstore.
-            embedding: Embedding function to use.
-            kwargs: Additional keyword arguments.
-
-        Returns:
-            VectorStore: VectorStore initialized from documents and embeddings.
-        """
-        texts = [d.page_content for d in documents]
-        metadatas = [d.metadata for d in documents]
-
-        if "ids" not in kwargs:
-            ids = [doc.id for doc in documents]
-
-            # If there's at least one valid ID, we'll assume that IDs
-            # should be used.
-            if any(ids):
-                kwargs["ids"] = ids
-
-        return cls.from_texts(texts, embedding, metadatas=metadatas, **kwargs)
-
-    @classmethod
-    async def afrom_documents(
-        cls: type[VST],
-        documents: list[Document],
-        embedding: Embeddings,
-        **kwargs: Any,
-    ) -> VST:
-        """Async return VectorStore initialized from documents and embeddings.
-
-        Args:
-            documents: List of Documents to add to the vectorstore.
-            embedding: Embedding function to use.
-            kwargs: Additional keyword arguments.
-
-        Returns:
-            VectorStore: VectorStore initialized from documents and embeddings.
-        """
-        texts = [d.page_content for d in documents]
-        metadatas = [d.metadata for d in documents]
-
-        if "ids" not in kwargs:
-            ids = [doc.id for doc in documents]
-
-            # If there's at least one valid ID, we'll assume that IDs
-            # should be used.
-            if any(ids):
-                kwargs["ids"] = ids
-
-        return await cls.afrom_texts(texts, embedding, metadatas=metadatas, **kwargs)
-
-    @classmethod
-    @abstractmethod
-    def from_texts(
-        cls: type[VST],
-        texts: list[str],
-        embedding: Embeddings,
-        metadatas: Optional[list[dict]] = None,
-        *,
-        ids: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> VST:
-        """Return VectorStore initialized from texts and embeddings.
-
-        Args:
-            texts: Texts to add to the vectorstore.
-            embedding: Embedding function to use.
-            metadatas: Optional list of metadatas associated with the texts.
-                Default is None.
-            ids: Optional list of IDs associated with the texts.
-            kwargs: Additional keyword arguments.
-
-        Returns:
-            VectorStore: VectorStore initialized from texts and embeddings.
-        """
-
-    @classmethod
-    async def afrom_texts(
-        cls: type[VST],
-        texts: list[str],
-        embedding: Embeddings,
-        metadatas: Optional[list[dict]] = None,
-        *,
-        ids: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> VST:
-        """Async return VectorStore initialized from texts and embeddings.
-
-        Args:
-            texts: Texts to add to the vectorstore.
-            embedding: Embedding function to use.
-            metadatas: Optional list of metadatas associated with the texts.
-                Default is None.
-            ids: Optional list of IDs associated with the texts.
-            kwargs: Additional keyword arguments.
-
-        Returns:
-            VectorStore: VectorStore initialized from texts and embeddings.
-        """
-        if ids is not None:
-            kwargs["ids"] = ids
-        return await run_in_executor(
-            None, cls.from_texts, texts, embedding, metadatas, **kwargs
-        )
-
-    def _get_retriever_tags(self) -> list[str]:
-        """Get tags for retriever."""
-        tags = [self.__class__.__name__]
-        if self.embeddings:
-            tags.append(self.embeddings.__class__.__name__)
-        return tags
-
-    def as_retriever(self, **kwargs: Any) -> VectorStoreRetriever:
-        """Return VectorStoreRetriever initialized from this VectorStore.
-
-        Args:
-            **kwargs: Keyword arguments to pass to the search function.
-                Can include:
-                search_type (Optional[str]): Defines the type of search that
-                    the Retriever should perform.
-                    Can be "similarity" (default), "mmr", or
-                    "similarity_score_threshold".
-                search_kwargs (Optional[Dict]): Keyword arguments to pass to the
-                    search function. Can include things like:
-                        k: Amount of documents to return (Default: 4)
-                        score_threshold: Minimum relevance threshold
-                            for similarity_score_threshold
-                        fetch_k: Amount of documents to pass to MMR algorithm
-                            (Default: 20)
-                        lambda_mult: Diversity of results returned by MMR;
-                            1 for minimum diversity and 0 for maximum. (Default: 0.5)
-                        filter: Filter by document metadata
-
-        Returns:
-            VectorStoreRetriever: Retriever class for VectorStore.
-
-        Examples:
-
-        .. code-block:: python
-
-            # Retrieve more documents with higher diversity
-            # Useful if your dataset has many similar documents
-            docsearch.as_retriever(
-                search_type="mmr",
-                search_kwargs={'k': 6, 'lambda_mult': 0.25}
-            )
-
-            # Fetch more documents for the MMR algorithm to consider
-            # But only return the top 5
-            docsearch.as_retriever(
-                search_type="mmr",
-                search_kwargs={'k': 5, 'fetch_k': 50}
-            )
-
-            # Only retrieve documents that have a relevance score
-            # Above a certain threshold
-            docsearch.as_retriever(
-                search_type="similarity_score_threshold",
-                search_kwargs={'score_threshold': 0.8}
-            )
-
-            # Only get the single most similar document from the dataset
-            docsearch.as_retriever(search_kwargs={'k': 1})
-
-            # Use a filter to only retrieve documents from a specific paper
-            docsearch.as_retriever(
-                search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}
-            )
-        """
-        tags = kwargs.pop("tags", None) or [] + self._get_retriever_tags()
-        return VectorStoreRetriever(vectorstore=self, tags=tags, **kwargs)
-
-
-class VectorStoreRetriever(BaseRetriever):
-    """Base Retriever class for VectorStore."""
-
-    vectorstore: VectorStore
-    """VectorStore to use for retrieval."""
-    search_type: str = "similarity"
-    """Type of search to perform. Defaults to "similarity"."""
-    search_kwargs: dict = Field(default_factory=dict)
-    """Keyword arguments to pass to the search function."""
-    allowed_search_types: ClassVar[Collection[str]] = (
-        "similarity",
-        "similarity_score_threshold",
-        "mmr",
-    )
-
-    model_config = ConfigDict(
-        arbitrary_types_allowed=True,
-    )
-
-    @model_validator(mode="before")
-    @classmethod
-    def validate_search_type(cls, values: dict) -> Any:
-        """Validate search type.
-
-        Args:
-            values: Values to validate.
-
-        Returns:
-            Values: Validated values.
-
-        Raises:
-            ValueError: If search_type is not one of the allowed search types.
-            ValueError: If score_threshold is not specified with a float value(0~1)
-        """
-        search_type = values.get("search_type", "similarity")
-        if search_type not in cls.allowed_search_types:
-            msg = (
-                f"search_type of {search_type} not allowed. Valid values are: "
-                f"{cls.allowed_search_types}"
-            )
-            raise ValueError(msg)
-        if search_type == "similarity_score_threshold":
-            score_threshold = values.get(
-                "search_kwargs", {}).get("score_threshold")
-            if (score_threshold is None) or (not isinstance(score_threshold, float)):
-                msg = (
-                    "`score_threshold` is not specified with a float value(0~1) "
-                    "in `search_kwargs`."
-                )
-                raise ValueError(msg)
-        return values
-
-    def _get_ls_params(self, **kwargs: Any) -> LangSmithRetrieverParams:
-        """Get standard params for tracing."""
-        _kwargs = self.search_kwargs | kwargs
-
-        ls_params = super()._get_ls_params(**_kwargs)
-        ls_params["ls_vector_store_provider"] = self.vectorstore.__class__.__name__
-
-        if self.vectorstore.embeddings:
-            ls_params["ls_embedding_provider"] = (
-                self.vectorstore.embeddings.__class__.__name__
-            )
-        elif hasattr(self.vectorstore, "embedding") and isinstance(
-            self.vectorstore.embedding, Embeddings
-        ):
-            ls_params["ls_embedding_provider"] = (
-                self.vectorstore.embedding.__class__.__name__
-            )
-
-        return ls_params
-
-    def _get_relevant_documents(
-        self, query: str, *, run_manager: CallbackManagerForRetrieverRun, **kwargs: Any
-    ) -> list[Document]:
-        _kwargs = self.search_kwargs | kwargs
-        if self.search_type == "similarity":
-            docs = self.vectorstore.similarity_search(query, **_kwargs)
-        elif self.search_type == "similarity_score_threshold":
-            docs_and_similarities = (
-                self.vectorstore.similarity_search_with_relevance_scores(
-                    query, **_kwargs
-                )
-            )
-            docs = [doc for doc, _ in docs_and_similarities]
-        elif self.search_type == "mmr":
-            docs = self.vectorstore.max_marginal_relevance_search(
-                query, **_kwargs)
-        else:
-            msg = f"search_type of {self.search_type} not allowed."
-            raise ValueError(msg)
-        return docs
-
-    async def _aget_relevant_documents(
-        self,
-        query: str,
-        *,
-        run_manager: AsyncCallbackManagerForRetrieverRun,
-        **kwargs: Any,
-    ) -> list[Document]:
-        _kwargs = self.search_kwargs | kwargs
-        if self.search_type == "similarity":
-            docs = await self.vectorstore.asimilarity_search(query, **_kwargs)
-        elif self.search_type == "similarity_score_threshold":
-            docs_and_similarities = (
-                await self.vectorstore.asimilarity_search_with_relevance_scores(
-                    query, **_kwargs
-                )
-            )
-            docs = [doc for doc, _ in docs_and_similarities]
-        elif self.search_type == "mmr":
-            docs = await self.vectorstore.amax_marginal_relevance_search(
-                query, **_kwargs
-            )
-        else:
-            msg = f"search_type of {self.search_type} not allowed."
-            raise ValueError(msg)
-        return docs
-
-    def add_documents(self, documents: list[Document], **kwargs: Any) -> list[str]:
-        """Add documents to the vectorstore.
-
-        Args:
-            documents: Documents to add to the vectorstore.
-            **kwargs: Other keyword arguments that subclasses might use.
-
-        Returns:
-            List of IDs of the added texts.
-        """
-        return self.vectorstore.add_documents(documents, **kwargs)
-
-    async def aadd_documents(
-        self, documents: list[Document], **kwargs: Any
-    ) -> list[str]:
-        """Async add documents to the vectorstore.
-
-        Args:
-            documents: Documents to add to the vectorstore.
-            **kwargs: Other keyword arguments that subclasses might use.
-
-        Returns:
-            List of IDs of the added texts.
-        """
-        return await self.vectorstore.aadd_documents(documents, **kwargs)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/vectorstores/in_memory.py ./custom_langchain_core/vectorstores/in_memory.py
--- .venv/lib/python3.12/site-packages/langchain_core/vectorstores/in_memory.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/vectorstores/in_memory.py	1970-01-01 09:00:00
@@ -1,567 +0,0 @@
-from __future__ import annotations
-
-import json
-import uuid
-from collections.abc import Iterator, Sequence
-from pathlib import Path
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    Optional,
-)
-
-from langchain_core._api import deprecated
-from langchain_core.documents import Document
-from langchain_core.embeddings import Embeddings
-from langchain_core.load import dumpd, load
-from langchain_core.vectorstores import VectorStore
-from langchain_core.vectorstores.utils import _cosine_similarity as cosine_similarity
-from langchain_core.vectorstores.utils import maximal_marginal_relevance
-
-if TYPE_CHECKING:
-    from langchain_core.indexing import UpsertResponse
-
-
-class InMemoryVectorStore(VectorStore):
-    """In-memory vector store implementation.
-
-    Uses a dictionary, and computes cosine similarity for search using numpy.
-
-    Setup:
-        Install ``langchain-core``.
-
-        .. code-block:: bash
-
-            pip install -U langchain-core
-
-    Key init args — indexing params:
-        embedding_function: Embeddings
-            Embedding function to use.
-
-    Instantiate:
-        .. code-block:: python
-
-            from langchain_core.vectorstores import InMemoryVectorStore
-            from langchain_openai import OpenAIEmbeddings
-
-            vector_store = InMemoryVectorStore(OpenAIEmbeddings())
-
-    Add Documents:
-        .. code-block:: python
-
-            from langchain_core.documents import Document
-
-            document_1 = Document(id="1", page_content="foo", metadata={"baz": "bar"})
-            document_2 = Document(id="2", page_content="thud", metadata={"bar": "baz"})
-            document_3 = Document(id="3", page_content="i will be deleted :(")
-
-            documents = [document_1, document_2, document_3]
-            vector_store.add_documents(documents=documents)
-
-    Inspect documents:
-        .. code-block:: python
-
-            top_n = 10
-            for index, (id, doc) in enumerate(vector_store.store.items()):
-                if index < top_n:
-                    # docs have keys 'id', 'vector', 'text', 'metadata'
-                    print(f"{id}: {doc['text']}")
-                else:
-                    break
-
-    Delete Documents:
-        .. code-block:: python
-
-            vector_store.delete(ids=["3"])
-
-    Search:
-        .. code-block:: python
-
-            results = vector_store.similarity_search(query="thud",k=1)
-            for doc in results:
-                print(f"* {doc.page_content} [{doc.metadata}]")
-
-        .. code-block:: none
-
-            * thud [{'bar': 'baz'}]
-
-    Search with filter:
-        .. code-block:: python
-
-            def _filter_function(doc: Document) -> bool:
-                return doc.metadata.get("bar") == "baz"
-
-            results = vector_store.similarity_search(
-                query="thud", k=1, filter=_filter_function
-            )
-            for doc in results:
-                print(f"* {doc.page_content} [{doc.metadata}]")
-
-        .. code-block:: none
-
-            * thud [{'bar': 'baz'}]
-
-
-    Search with score:
-        .. code-block:: python
-
-            results = vector_store.similarity_search_with_score(
-                query="qux", k=1
-            )
-            for doc, score in results:
-                print(f"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]")
-
-        .. code-block:: none
-
-            * [SIM=0.832268] foo [{'baz': 'bar'}]
-
-    Async:
-        .. code-block:: python
-
-            # add documents
-            # await vector_store.aadd_documents(documents=documents)
-
-            # delete documents
-            # await vector_store.adelete(ids=["3"])
-
-            # search
-            # results = vector_store.asimilarity_search(query="thud", k=1)
-
-            # search with score
-            results = await vector_store.asimilarity_search_with_score(query="qux", k=1)
-            for doc,score in results:
-                print(f"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]")
-
-        .. code-block:: none
-
-            * [SIM=0.832268] foo [{'baz': 'bar'}]
-
-    Use as Retriever:
-        .. code-block:: python
-
-            retriever = vector_store.as_retriever(
-                search_type="mmr",
-                search_kwargs={"k": 1, "fetch_k": 2, "lambda_mult": 0.5},
-            )
-            retriever.invoke("thud")
-
-        .. code-block:: none
-
-            [Document(id='2', metadata={'bar': 'baz'}, page_content='thud')]
-
-    """  # noqa: E501
-
-    def __init__(self, embedding: Embeddings) -> None:
-        """Initialize with the given embedding function.
-
-        Args:
-            embedding: embedding function to use.
-        """
-        # TODO: would be nice to change to
-        # Dict[str, Document] at some point (will be a breaking change)
-        self.store: dict[str, dict[str, Any]] = {}
-        self.embedding = embedding
-
-    @property
-    def embeddings(self) -> Embeddings:
-        return self.embedding
-
-    def delete(self, ids: Optional[Sequence[str]] = None, **kwargs: Any) -> None:
-        if ids:
-            for _id in ids:
-                self.store.pop(_id, None)
-
-    async def adelete(self, ids: Optional[Sequence[str]] = None, **kwargs: Any) -> None:
-        self.delete(ids)
-
-    def add_documents(
-        self,
-        documents: list[Document],
-        ids: Optional[list[str]] = None,
-        **kwargs: Any,
-    ) -> list[str]:
-        """Add documents to the store."""
-        texts = [doc.page_content for doc in documents]
-        vectors = self.embedding.embed_documents(texts)
-
-        if ids and len(ids) != len(texts):
-            msg = (
-                f"ids must be the same length as texts. "
-                f"Got {len(ids)} ids and {len(texts)} texts."
-            )
-            raise ValueError(msg)
-
-        id_iterator: Iterator[Optional[str]] = (
-            iter(ids) if ids else iter(doc.id for doc in documents)
-        )
-
-        ids_ = []
-
-        for doc, vector in zip(documents, vectors):
-            doc_id = next(id_iterator)
-            doc_id_ = doc_id or str(uuid.uuid4())
-            ids_.append(doc_id_)
-            self.store[doc_id_] = {
-                "id": doc_id_,
-                "vector": vector,
-                "text": doc.page_content,
-                "metadata": doc.metadata,
-            }
-
-        return ids_
-
-    async def aadd_documents(
-        self, documents: list[Document], ids: Optional[list[str]] = None, **kwargs: Any
-    ) -> list[str]:
-        """Add documents to the store."""
-        texts = [doc.page_content for doc in documents]
-        vectors = await self.embedding.aembed_documents(texts)
-
-        if ids and len(ids) != len(texts):
-            msg = (
-                f"ids must be the same length as texts. "
-                f"Got {len(ids)} ids and {len(texts)} texts."
-            )
-            raise ValueError(msg)
-
-        id_iterator: Iterator[Optional[str]] = (
-            iter(ids) if ids else iter(doc.id for doc in documents)
-        )
-        ids_: list[str] = []
-
-        for doc, vector in zip(documents, vectors):
-            doc_id = next(id_iterator)
-            doc_id_ = doc_id or str(uuid.uuid4())
-            ids_.append(doc_id_)
-            self.store[doc_id_] = {
-                "id": doc_id_,
-                "vector": vector,
-                "text": doc.page_content,
-                "metadata": doc.metadata,
-            }
-
-        return ids_
-
-    def get_by_ids(self, ids: Sequence[str], /) -> list[Document]:
-        """Get documents by their ids.
-
-        Args:
-            ids: The ids of the documents to get.
-
-        Returns:
-            A list of Document objects.
-        """
-        documents = []
-
-        for doc_id in ids:
-            doc = self.store.get(doc_id)
-            if doc:
-                documents.append(
-                    Document(
-                        id=doc["id"],
-                        page_content=doc["text"],
-                        metadata=doc["metadata"],
-                    )
-                )
-        return documents
-
-    @deprecated(
-        alternative="VectorStore.add_documents",
-        message=(
-            "This was a beta API that was added in 0.2.11. It'll be removed in 0.3.0."
-        ),
-        since="0.2.29",
-        removal="1.0",
-    )
-    def upsert(self, items: Sequence[Document], /, **kwargs: Any) -> UpsertResponse:
-        vectors = self.embedding.embed_documents([item.page_content for item in items])
-        ids = []
-        for item, vector in zip(items, vectors):
-            doc_id = item.id or str(uuid.uuid4())
-            ids.append(doc_id)
-            self.store[doc_id] = {
-                "id": doc_id,
-                "vector": vector,
-                "text": item.page_content,
-                "metadata": item.metadata,
-            }
-        return {
-            "succeeded": ids,
-            "failed": [],
-        }
-
-    @deprecated(
-        alternative="VectorStore.aadd_documents",
-        message=(
-            "This was a beta API that was added in 0.2.11. It'll be removed in 0.3.0."
-        ),
-        since="0.2.29",
-        removal="1.0",
-    )
-    async def aupsert(
-        self, items: Sequence[Document], /, **kwargs: Any
-    ) -> UpsertResponse:
-        vectors = await self.embedding.aembed_documents(
-            [item.page_content for item in items]
-        )
-        ids = []
-        for item, vector in zip(items, vectors):
-            doc_id = item.id or str(uuid.uuid4())
-            ids.append(doc_id)
-            self.store[doc_id] = {
-                "id": doc_id,
-                "vector": vector,
-                "text": item.page_content,
-                "metadata": item.metadata,
-            }
-        return {
-            "succeeded": ids,
-            "failed": [],
-        }
-
-    async def aget_by_ids(self, ids: Sequence[str], /) -> list[Document]:
-        """Async get documents by their ids.
-
-        Args:
-            ids: The ids of the documents to get.
-
-        Returns:
-            A list of Document objects.
-        """
-        return self.get_by_ids(ids)
-
-    def _similarity_search_with_score_by_vector(
-        self,
-        embedding: list[float],
-        k: int = 4,
-        filter: Optional[Callable[[Document], bool]] = None,
-        **kwargs: Any,
-    ) -> list[tuple[Document, float, list[float]]]:
-        # get all docs with fixed order in list
-        docs = list(self.store.values())
-
-        if filter is not None:
-            docs = [
-                doc
-                for doc in docs
-                if filter(Document(page_content=doc["text"], metadata=doc["metadata"]))
-            ]
-
-        if not docs:
-            return []
-
-        similarity = cosine_similarity([embedding], [doc["vector"] for doc in docs])[0]
-
-        # get the indices ordered by similarity score
-        top_k_idx = similarity.argsort()[::-1][:k]
-
-        return [
-            (
-                Document(
-                    id=doc_dict["id"],
-                    page_content=doc_dict["text"],
-                    metadata=doc_dict["metadata"],
-                ),
-                float(similarity[idx].item()),
-                doc_dict["vector"],
-            )
-            for idx in top_k_idx
-            # Assign using walrus operator to avoid multiple lookups
-            if (doc_dict := docs[idx])
-        ]
-
-    def similarity_search_with_score_by_vector(
-        self,
-        embedding: list[float],
-        k: int = 4,
-        filter: Optional[Callable[[Document], bool]] = None,
-        **kwargs: Any,
-    ) -> list[tuple[Document, float]]:
-        return [
-            (doc, similarity)
-            for doc, similarity, _ in self._similarity_search_with_score_by_vector(
-                embedding=embedding, k=k, filter=filter, **kwargs
-            )
-        ]
-
-    def similarity_search_with_score(
-        self,
-        query: str,
-        k: int = 4,
-        **kwargs: Any,
-    ) -> list[tuple[Document, float]]:
-        embedding = self.embedding.embed_query(query)
-        docs = self.similarity_search_with_score_by_vector(
-            embedding,
-            k,
-            **kwargs,
-        )
-        return docs
-
-    async def asimilarity_search_with_score(
-        self, query: str, k: int = 4, **kwargs: Any
-    ) -> list[tuple[Document, float]]:
-        embedding = await self.embedding.aembed_query(query)
-        docs = self.similarity_search_with_score_by_vector(
-            embedding,
-            k,
-            **kwargs,
-        )
-        return docs
-
-    def similarity_search_by_vector(
-        self,
-        embedding: list[float],
-        k: int = 4,
-        **kwargs: Any,
-    ) -> list[Document]:
-        docs_and_scores = self.similarity_search_with_score_by_vector(
-            embedding,
-            k,
-            **kwargs,
-        )
-        return [doc for doc, _ in docs_and_scores]
-
-    async def asimilarity_search_by_vector(
-        self, embedding: list[float], k: int = 4, **kwargs: Any
-    ) -> list[Document]:
-        return self.similarity_search_by_vector(embedding, k, **kwargs)
-
-    def similarity_search(
-        self, query: str, k: int = 4, **kwargs: Any
-    ) -> list[Document]:
-        return [doc for doc, _ in self.similarity_search_with_score(query, k, **kwargs)]
-
-    async def asimilarity_search(
-        self, query: str, k: int = 4, **kwargs: Any
-    ) -> list[Document]:
-        return [
-            doc
-            for doc, _ in await self.asimilarity_search_with_score(query, k, **kwargs)
-        ]
-
-    def max_marginal_relevance_search_by_vector(
-        self,
-        embedding: list[float],
-        k: int = 4,
-        fetch_k: int = 20,
-        lambda_mult: float = 0.5,
-        **kwargs: Any,
-    ) -> list[Document]:
-        prefetch_hits = self._similarity_search_with_score_by_vector(
-            embedding=embedding,
-            k=fetch_k,
-            **kwargs,
-        )
-
-        try:
-            import numpy as np
-        except ImportError as e:
-            msg = (
-                "numpy must be installed to use max_marginal_relevance_search "
-                "pip install numpy"
-            )
-            raise ImportError(msg) from e
-
-        mmr_chosen_indices = maximal_marginal_relevance(
-            np.array(embedding, dtype=np.float32),
-            [vector for _, _, vector in prefetch_hits],
-            k=k,
-            lambda_mult=lambda_mult,
-        )
-        return [prefetch_hits[idx][0] for idx in mmr_chosen_indices]
-
-    def max_marginal_relevance_search(
-        self,
-        query: str,
-        k: int = 4,
-        fetch_k: int = 20,
-        lambda_mult: float = 0.5,
-        **kwargs: Any,
-    ) -> list[Document]:
-        embedding_vector = self.embedding.embed_query(query)
-        return self.max_marginal_relevance_search_by_vector(
-            embedding_vector,
-            k,
-            fetch_k,
-            lambda_mult=lambda_mult,
-            **kwargs,
-        )
-
-    async def amax_marginal_relevance_search(
-        self,
-        query: str,
-        k: int = 4,
-        fetch_k: int = 20,
-        lambda_mult: float = 0.5,
-        **kwargs: Any,
-    ) -> list[Document]:
-        embedding_vector = await self.embedding.aembed_query(query)
-        return self.max_marginal_relevance_search_by_vector(
-            embedding_vector,
-            k,
-            fetch_k,
-            lambda_mult=lambda_mult,
-            **kwargs,
-        )
-
-    @classmethod
-    def from_texts(
-        cls,
-        texts: list[str],
-        embedding: Embeddings,
-        metadatas: Optional[list[dict]] = None,
-        **kwargs: Any,
-    ) -> InMemoryVectorStore:
-        store = cls(
-            embedding=embedding,
-        )
-        store.add_texts(texts=texts, metadatas=metadatas, **kwargs)
-        return store
-
-    @classmethod
-    async def afrom_texts(
-        cls,
-        texts: list[str],
-        embedding: Embeddings,
-        metadatas: Optional[list[dict]] = None,
-        **kwargs: Any,
-    ) -> InMemoryVectorStore:
-        store = cls(
-            embedding=embedding,
-        )
-        await store.aadd_texts(texts=texts, metadatas=metadatas, **kwargs)
-        return store
-
-    @classmethod
-    def load(
-        cls, path: str, embedding: Embeddings, **kwargs: Any
-    ) -> InMemoryVectorStore:
-        """Load a vector store from a file.
-
-        Args:
-            path: The path to load the vector store from.
-            embedding: The embedding to use.
-            kwargs: Additional arguments to pass to the constructor.
-
-        Returns:
-            A VectorStore object.
-        """
-        _path: Path = Path(path)
-        with _path.open("r") as f:
-            store = load(json.load(f))
-        vectorstore = cls(embedding=embedding, **kwargs)
-        vectorstore.store = store
-        return vectorstore
-
-    def dump(self, path: str) -> None:
-        """Dump the vector store to a file.
-
-        Args:
-            path: The path to dump the vector store to.
-        """
-        _path: Path = Path(path)
-        _path.parent.mkdir(exist_ok=True, parents=True)
-        with _path.open("w") as f:
-            json.dump(dumpd(self.store), f, indent=2)
diff -ruN .venv/lib/python3.12/site-packages/langchain_core/vectorstores/utils.py ./custom_langchain_core/vectorstores/utils.py
--- .venv/lib/python3.12/site-packages/langchain_core/vectorstores/utils.py	2025-02-22 14:10:28
+++ ./custom_langchain_core/vectorstores/utils.py	1970-01-01 09:00:00
@@ -1,128 +0,0 @@
-"""Internal utilities for the in memory implementation of VectorStore.
-
-These are part of a private API, and users should not use them directly
-as they can change without notice.
-"""
-
-from __future__ import annotations
-
-import logging
-from typing import TYPE_CHECKING, Union
-
-if TYPE_CHECKING:
-    import numpy as np
-
-    Matrix = Union[list[list[float]], list[np.ndarray], np.ndarray]
-
-logger = logging.getLogger(__name__)
-
-
-def _cosine_similarity(x: Matrix, y: Matrix) -> np.ndarray:
-    """Row-wise cosine similarity between two equal-width matrices.
-
-    Args:
-        x: A matrix of shape (n, m).
-        y: A matrix of shape (k, m).
-
-    Returns:
-        A matrix of shape (n, k) where each element (i, j) is the cosine similarity
-        between the ith row of X and the jth row of Y.
-
-    Raises:
-        ValueError: If the number of columns in X and Y are not the same.
-        ImportError: If numpy is not installed.
-    """
-    try:
-        import numpy as np
-    except ImportError as e:
-        msg = (
-            "cosine_similarity requires numpy to be installed. "
-            "Please install numpy with `pip install numpy`."
-        )
-        raise ImportError(msg) from e
-
-    if len(x) == 0 or len(y) == 0:
-        return np.array([])
-
-    x = np.array(x)
-    y = np.array(y)
-    if x.shape[1] != y.shape[1]:
-        msg = (
-            f"Number of columns in X and Y must be the same. X has shape {x.shape} "
-            f"and Y has shape {y.shape}."
-        )
-        raise ValueError(msg)
-    try:
-        import simsimd as simd  # type: ignore
-    except ImportError:
-        logger.debug(
-            "Unable to import simsimd, defaulting to NumPy implementation. If you want "
-            "to use simsimd please install with `pip install simsimd`."
-        )
-        x_norm = np.linalg.norm(x, axis=1)
-        y_norm = np.linalg.norm(y, axis=1)
-        # Ignore divide by zero errors run time warnings as those are handled below.
-        with np.errstate(divide="ignore", invalid="ignore"):
-            similarity = np.dot(x, y.T) / np.outer(x_norm, y_norm)
-        similarity[np.isnan(similarity) | np.isinf(similarity)] = 0.0
-        return similarity
-
-    x = np.array(x, dtype=np.float32)
-    y = np.array(y, dtype=np.float32)
-    return 1 - np.array(simd.cdist(x, y, metric="cosine"))
-
-
-def maximal_marginal_relevance(
-    query_embedding: np.ndarray,
-    embedding_list: list,
-    lambda_mult: float = 0.5,
-    k: int = 4,
-) -> list[int]:
-    """Calculate maximal marginal relevance.
-
-    Args:
-        query_embedding: The query embedding.
-        embedding_list: A list of embeddings.
-        lambda_mult: The lambda parameter for MMR. Default is 0.5.
-        k: The number of embeddings to return. Default is 4.
-
-    Returns:
-        A list of indices of the embeddings to return.
-
-    Raises:
-        ImportError: If numpy is not installed.
-    """
-    try:
-        import numpy as np
-    except ImportError as e:
-        msg = (
-            "maximal_marginal_relevance requires numpy to be installed. "
-            "Please install numpy with `pip install numpy`."
-        )
-        raise ImportError(msg) from e
-
-    if min(k, len(embedding_list)) <= 0:
-        return []
-    if query_embedding.ndim == 1:
-        query_embedding = np.expand_dims(query_embedding, axis=0)
-    similarity_to_query = _cosine_similarity(query_embedding, embedding_list)[0]
-    most_similar = int(np.argmax(similarity_to_query))
-    idxs = [most_similar]
-    selected = np.array([embedding_list[most_similar]])
-    while len(idxs) < min(k, len(embedding_list)):
-        best_score = -np.inf
-        idx_to_add = -1
-        similarity_to_selected = _cosine_similarity(embedding_list, selected)
-        for i, query_score in enumerate(similarity_to_query):
-            if i in idxs:
-                continue
-            redundant_score = max(similarity_to_selected[i])
-            equation_score = (
-                lambda_mult * query_score - (1 - lambda_mult) * redundant_score
-            )
-            if equation_score > best_score:
-                best_score = equation_score
-                idx_to_add = i
-        idxs.append(idx_to_add)
-        selected = np.append(selected, [embedding_list[idx_to_add]], axis=0)
-    return idxs
